[
    {
        "age": null,
        "album": "",
        "author": "/u/Shape_Weird",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T21:37:01.693403+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T21:34:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been trying to get n8n&#39;s BrowserUse node to help with this tedious data entry job I have, basically taking info from PDFs and putting it into various web forms.<br/> The obvious approach was to have AI extract the data (which works great), then use something like Selenium to fill the forms (which is... not great).</p> <p>Selenium kept breaking because these forms have weird validation, dynamic fields, etc. I spent way too much time debugging selectors that change every week. Then I tried Puppeteer, thinking it would be better... nope, still the same headaches.</p> <p>found this thing called <a href=\"https://filliny.io\">filliny</a>&#39;s chrome extension, that uses the given context, to fill forms using AI, still not quite the fully automated process I wanted to build though. limited as hell compared to BrowserUse of course, but much cheaper and faster instead.</p> <p>Has anyone else tried stuff like this? It&#39;s def not capable of ha",
        "id": 3071798,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lq81e7/anyone_else_struggling_with_browseruse_filling",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone else struggling with BrowserUse filling out forms on websites?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Beyond_Birthday_13",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T15:07:03.652206+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T14:52:44+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1lpxwhg/which_one_goes_more_in_depth/\"> <img src=\"https://b.thumbs.redditmedia.com/RGNrWQpFGhTAAT9vg10AfkKcZ9tXfLjzfqGcszcFxnE.jpg\" alt=\"which one goes more in depth?\" title=\"which one goes more in depth?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Beyond_Birthday_13\"> /u/Beyond_Birthday_13 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1lpxwhg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpxwhg/which_one_goes_more_in_depth/\">[comments]</a></span> </td></tr></table>",
        "id": 3068517,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lpxwhg/which_one_goes_more_in_depth",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/RGNrWQpFGhTAAT9vg10AfkKcZ9tXfLjzfqGcszcFxnE.jpg",
        "title": "which one goes more in depth?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/enki0817",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T15:07:03.886984+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T14:47:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been building and running my own app for 3 years now. It relies on a functional hcap solver to work. We have used a variety of services over the year. </p> <p>However none seem to work or be stable now. </p> <p>Anyone have a solution to this or find a work around?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/enki0817\"> /u/enki0817 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpxs6k/are_hcap_solvers_dead/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpxs6k/are_hcap_solvers_dead/\">[comments]</a></span>",
        "id": 3068518,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lpxs6k/are_hcap_solvers_dead",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are Hcap solvers dead?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Due-Mortgage450",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T16:12:00.877035+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T12:22:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello!</p> <p>Maybe someone can help me, because I&#39;m not strong in this matter. There is an online store where I want to buy a product. When I click on the &quot;buy&quot; button, the Cloudflare anti-bot appears, but it takes a VERY long time for it to appear, spin, etc. The product has already been sold out. How can this be bypassed??? Maybe there is some way?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Due-Mortgage450\"> /u/Due-Mortgage450 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpufxk/help_with_cloudflare/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpufxk/help_with_cloudflare/\">[comments]</a></span>",
        "id": 3069088,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lpufxk/help_with_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with Cloudflare!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JV_Singh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T11:52:04.014872+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T06:46:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, </p> <p>I&#39;m building a tool to track digital marketing job posts in Singapore (just a solo learner project). I&#39;m currently using already build out Actors from Apify for scraping and n8n for automation. But scraping Jobs Portals, I have some issues seems job portals have bot protection.</p> <p>Anyone here successfully scraped it or handled bot protection? Would love to learn how others approached this.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JV_Singh\"> /u/JV_Singh </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpp01w/scraping_digital_marketing_jobs_for_sgbased/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpp01w/scraping_digital_marketing_jobs_for_sgbased/\">[comments]</a></span>",
        "id": 3066448,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lpp01w/scraping_digital_marketing_jobs_for_sgbased",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Digital Marketing jobs for SG-based project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HalfGuardPrince",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T02:57:53.244338+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T02:16:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there,</p> <p>I&#39;ve been scraping basically every bookmaker website in Australia (around 100 of them) for regular odds updates for all their odds. Got it nice and smooth with pretty much every site, using a variety of proxies, 5g modems with rotating IPs, and many more things.</p> <p>But one of the bookmaker software providers (Bet Cloud you can check out their website, it&#39;s been under construction since 2021) is proving to be unpassable like Gandalf stopping the Balrog.</p> <p>Basically, no matter the IP I use, or whatever the process I use, it&#39;s instant perma ban across all sites. They&#39;ve got 15 bookmakers (for example, one of them is <a href=\"https://gigabet.com.au/\">https://gigabet.com.au/</a>) and if iI am trying to scrape horse racing odds, there&#39;s upwards of 650 races in a single day, with constants odds updates (I&#39;m basically scraping every bookmaker site in Australia every 30 seconds right now). </p> <p>As soon as I",
        "id": 3064453,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lpk927/bet_cloud_websites_are_the_bane_of_my_existence",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bet Cloud Websites are the bane of my existence",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lunoxus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T01:52:53.957864+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T01:23:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It seems like some websites (e.g. Hyatt) have been introducing some sort of anti-scraping measure where it would 429 you if it thinks you&#39;re a bot. </p> <p>I&#39;m having trouble trying to get around it, even with <code>patchright</code>.</p> <p>I&#39;ve tried implementing these suggestions for flags: <a href=\"https://www.reddit.com/r/node/comments/p75zal/specific_website_just_wont_load_at_all_with/hc4i6bq/\">https://www.reddit.com/r/node/comments/p75zal/specific_website_just_wont_load_at_all_with/hc4i6bq/</a></p> <p>but even then, it seems like while my personal Mac&#39;s chrome gets around it, using the chrome from a docker image e.g. linuxserver&#39;s gives me the 429 as well.</p> <p>Anyone have pointers into what technology they&#39;re using?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lunoxus\"> /u/Lunoxus </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lpj7e2/getting_429d_on_t",
        "id": 3064234,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lpj7e2/getting_429d_on_the_first_request",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting 429'd on the first request",
        "vote": 0
    }
]