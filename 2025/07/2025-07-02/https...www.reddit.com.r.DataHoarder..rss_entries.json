[
    {
        "age": null,
        "album": "",
        "author": "/u/Big-Tune9944",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T23:01:30.780074+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T22:33:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a lot of photos of me and my family from different places around the world, and I want to organize them. I was thinking of using Pinterest, but my wife suggested not to. Are there any good websites or computer apps specifically made for organizing photos?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Big-Tune9944\"> /u/Big-Tune9944 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq9f5y/best_image_saving_websitecomputer_application/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq9f5y/best_image_saving_websitecomputer_application/\">[comments]</a></span>",
        "id": 3072384,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq9f5y/best_image_saving_websitecomputer_application",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best image saving website/computer application - Kinda like pintrest",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abudab1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T23:01:30.954792+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T22:26:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Download ffmpeg by typing in Powershell:<br/> <code>choco install ffmpeg-full</code></p> <p>then create .bat file which contains:</p> <pre><code>@echo off setlocal enabledelayedexpansion REM Input and output folders set &quot;input=E:\\Videos to encode&quot; set &quot;output=C:\\Output videos&quot; REM Create output root if it doesn&#39;t exist if not exist &quot;%output%&quot; mkdir &quot;%output%&quot; REM Loop through all .mp4, .mkv, .avi files recursively for /r &quot;%input%&quot; %%f in (*.mp4 *.mkv *.avi) do ( REM Get relative path set &quot;relpath=%%~pf&quot; set &quot;relpath=!relpath:%input%=!&quot; REM Create output directory set &quot;outdir=%output%!relpath!&quot; if not exist &quot;!outdir!&quot; mkdir &quot;!outdir!&quot; REM Output file path set &quot;outfile=!outdir!%%~nf.mp4&quot; REM Run ffmpeg encode echo Encoding: &quot;%%f&quot; to &quot;!outfile!&quot; ffmpeg -i &quot;%%f&quot; ^ -c:v av1_nvenc ^ -preset p7 -tune hq ^ -cq 40 ^ -t",
        "id": 3072385,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq99x5/regarding_video_data_savingconvert_to_av1_or_hevc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Regarding video data saving(Convert to AV1 or HEVC using ffmpeg)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RegularVast1045",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T21:55:19.813200+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T21:36:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried Yt dlp and maybe Jdownloader 4kdownloader only aloud 10 videos for a trial. I\u2019m not sure I can code from GitHub using python to download YouTube playlist. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RegularVast1045\"> /u/RegularVast1045 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq82py/best_youtube_playlist_downloadereasy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq82py/best_youtube_playlist_downloadereasy/\">[comments]</a></span>",
        "id": 3071997,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq82py/best_youtube_playlist_downloadereasy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best YouTube playlist downloader(easy)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Emergency-Ice-754",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T20:51:55.894140+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T20:16:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking to replace my current &quot;main&quot; external media drive, which is a WD MyBook. I&#39;ve had a couple Seagate external drives fail on me but this was probably over a decade ago. I&#39;ve also had WD fail on me but overall, I seem to have a better impression of them. I want to get the largest external drive within a reasonable price. Largest MyBook is a 26TB. Largest Seagate is a 28TB and it&#39;s significantly cheaper than the WD.</p> <ol> <li><p>How in the Seagate so much cheaper per TB than the WD? Are news on WD releasing larger MyBooks and making their price more competitive?</p></li> <li><p>How reliable are Seagate drives these days?</p></li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Emergency-Ice-754\"> /u/Emergency-Ice-754 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq651i/thoughts_on_28tb_seagate/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com",
        "id": 3071518,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq651i/thoughts_on_28tb_seagate",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Thoughts on 28TB Seagate?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PussyMangler421",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T19:45:20.685477+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T19:00:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My porn collection was just absurdly large, 220TB. </p> <p>I decided to finally setup Tdarr and throw in a bunch of cheap intel arc GPU&#39;s to encode to AV1. I was nervous because i&#39;d heard it&#39;s a pain to setup but for my scenario it was ridiculously easy, no complex flows, just encode and replace. It is very good at handling failed encodes.</p> <p>It took a good year of 24/7 encoding but I went from 220TB to only 88TB. I was literally able to build a backup array with the space I saved and then have drives left over. I&#39;ll never notice the quality difference, I wasn&#39;t even able to in the few test files I compared at the beginning.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PussyMangler421\"> /u/PussyMangler421 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq49o4/if_you_hoard_video_tdarr_saved_me_132tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/",
        "id": 3071066,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq49o4/if_you_hoard_video_tdarr_saved_me_132tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "If you hoard video, Tdarr saved me 132TB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ffhhkk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T18:41:33.288717+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T18:25:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys. Not gonna bore u with my sob story, but need some help sourcing hard disks for cheaper than usual due to employment issues causing budgetary constraints.</p> <p>Id appreciate any tips, sources for 14TB or greater Satas that I can purchase. I have some friends in the States that will be travelling back so I&#39;ll save on international shipping.</p> <p>Thank u!</p> <p>Also, if someone can help with the definitions of recertified, refurbished and re-anything terms, it wud be helpful.</p> <p>My current setup. DS1819 + EXPANSION. DS 2419+ expansion( the expansion machine for the 2419 doesn&#39;t have any drives so I&#39;m looking to populate it now since I&#39;m running out of space.)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ffhhkk\"> /u/ffhhkk </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq3d65/help_with_hdds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/",
        "id": 3070550,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq3d65/help_with_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with HDDs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/alin_im",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T18:41:33.464098+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T18:24:47+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/alin_im\"> /u/alin_im </a> <br/> <span><a href=\"/r/homelab/comments/1lq3cfg/which_seagate_hdd_for_new_nas_raidz1_or_raidz2/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq3cye/which_seagate_hdd_for_new_nas_raidz1_or_raidz2/\">[comments]</a></span>",
        "id": 3070551,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq3cye/which_seagate_hdd_for_new_nas_raidz1_or_raidz2",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which Seagate HDD for new NAS? Raidz1 or Raidz2 for 5 bays?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zwelly23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T18:41:33.655609+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T17:39:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I can get either 4x500gb or 2x2tb drives to put into a home server for nas purposes, which option would be better? Mainly to store photos and videos and docs</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zwelly23\"> /u/zwelly23 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq272a/2x2tb_vs_4x500gb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq272a/2x2tb_vs_4x500gb/\">[comments]</a></span>",
        "id": 3070552,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq272a/2x2tb_vs_4x500gb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "2x2tb vs 4x500gb",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pmttyji",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T17:35:50.779893+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T16:46:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Time to time, I&#39;m checking here &amp; other subreddits for this one for sometime. I don&#39;t see any recent threads. Looks like so far none have this. So today posting this thread for this.</p> <p>Even if there&#39;s any complete archive, I would like to have some tropes of archive(Writing, Literature, Poetry, Film, Art, Comics, ) for my research. Here few questions.</p> <ol> <li>What <strong>software</strong> is best &amp; possible to download tvtropes site(even partial contents[mentioned tropes above] is fine for now)? In past, I used to download tiny sites using HTtrack, but it&#39;s not working good on sites like tvtropes(both download &amp; archive wise). So please share resources &amp; best practices on this.</li> <li>Plan B : What&#39;re <strong>alternative sites</strong> to tvtropes? So I would search <strong>offline archives</strong> of those sites or will try to download those sites if no offline archives.</li> </ol> <p>I&#39;m not look",
        "id": 3069968,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq0u0s/anyone_got_tv_tropes_offline_need_some_tropes_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone got TV Tropes offline? Need some Tropes for Research",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NekoTrix",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T17:35:50.421201+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T16:38:24+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lq0me1/video_encoding_benchmarks_deep_dive_into_svtav1s/\"> <img src=\"https://preview.redd.it/gljdk8oclhaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97586ba61271428460d82e206f9d79e724b59c02\" alt=\"Video Encoding Benchmarks: &quot;Deep Dive into SVT-AV1's Evolution&quot;\" title=\"Video Encoding Benchmarks: &quot;Deep Dive into SVT-AV1's Evolution&quot;\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello datahoarders!</p> <p>My name is Trix, I&#39;m a video encoder and huge AV1 enthusiast since 2021.</p> <p>I have been doing big benchmarking sessions for two years and started posting my work on the codec wiki a year back.</p> <p>If the topic of video encoding or archiving is of any interest to you, maybe if you want to learn more about AV1 which is gaining traction these past few months and years, you&#39;ll find an enormous amount of data here, be it in the form of graphs, tables and visual comparison",
        "id": 3069967,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lq0me1/video_encoding_benchmarks_deep_dive_into_svtav1s",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/gljdk8oclhaf1.jpeg?width=640&crop=smart&auto=webp&s=97586ba61271428460d82e206f9d79e724b59c02",
        "title": "Video Encoding Benchmarks: \"Deep Dive into SVT-AV1's Evolution\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VviFMCgY",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T16:30:52.036096+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T16:02:02+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpzoy8/automatic_transfer_switch_pdu_in_the_homelab_does/\"> <img src=\"https://external-preview.redd.it/z0lJqXvnPye7XKEGxuSaNcOw5bAyrTunsON1Tp5MiR0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6c0808fef3de9bf427c59d793380ca75a275f8a\" alt=\"Automatic Transfer Switch PDU in The Homelab - Does it make sense?\" title=\"Automatic Transfer Switch PDU in The Homelab - Does it make sense?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VviFMCgY\"> /u/VviFMCgY </a> <br/> <span><a href=\"https://blog.networkprofile.org/automatic-transfer-switch-pdu-in-the-homelab-does-it-make-sense/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpzoy8/automatic_transfer_switch_pdu_in_the_homelab_does/\">[comments]</a></span> </td></tr></table>",
        "id": 3069375,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpzoy8/automatic_transfer_switch_pdu_in_the_homelab_does",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/z0lJqXvnPye7XKEGxuSaNcOw5bAyrTunsON1Tp5MiR0.jpeg?width=640&crop=smart&auto=webp&s=b6c0808fef3de9bf427c59d793380ca75a275f8a",
        "title": "Automatic Transfer Switch PDU in The Homelab - Does it make sense?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/d2racing911",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T15:24:55.879805+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T15:19:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I&#39;m currently creating a 321 backup workflow for my NAS and I&#39;m using this workflow :</p> <ol> <li><p>I run a couple of jobs with SyncBack Free to sync 2-3 shared folders to my external drive(drive #1).</p></li> <li><p>That external drive is then sync to an another external drive (drive #2).</p></li> <li><p>I use Backblaze to backup that second drive (drive #2) to have my offsite Backup.</p></li> </ol> <p>I saw that there&#39;s a Goodsync package for my DS923+.</p> <p>Is there any avantages to install Goodsync directly to my Synology ?</p> <p>Anyone is using their GoodSync connect feature too ? </p> <p>I&#39;m trying to determine if I should buy GoodSync or Syncback Pro to sync my stuff or if I should stick with the Free version of Syncback ?</p> <p>Thanks for your comments :P</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/d2racing911\"> /u/d2racing911 </a> <br/> <span><a href=\"https://www.re",
        "id": 3068771,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpyksj/goodsync_or_syncback_freepro_with_my_synology",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Goodsync or SyncBack Free/Pro with my Synology Workflow ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/redditmail9999",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T15:24:56.053363+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T14:25:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>SPD has 14TB exos new listed for $189.99. it&#39;s labelled as &quot;HP/Seagate Exos X18&quot; with 3 yrs seller warranty.<br/> i&#39;m assuming these are OEM of the exos -- ergo not the usual 5 yrs seagate warranties?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/redditmail9999\"> /u/redditmail9999 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpx8hi/hpseagate_exos_x18_14tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpx8hi/hpseagate_exos_x18_14tb/\">[comments]</a></span>",
        "id": 3068772,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpx8hi/hpseagate_exos_x18_14tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HP/Seagate Exos X18 14TB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mtlynch",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T14:19:27.322878+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T14:13:12+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mtlynch\"> /u/mtlynch </a> <br/> <span><a href=\"https://mtlynch.io/goharddrive-leak/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpwxb9/goharddrive_leaked_personal_data_for_thousands_of/\">[comments]</a></span>",
        "id": 3068099,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpwxb9/goharddrive_leaked_personal_data_for_thousands_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "goHardDrive Leaked Personal Data for Thousands of Customers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pleiad_m45",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T14:19:27.532216+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T13:19:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi fellow DataHoarders. </p> <p>maybe I&#39;m just in a bad mood but seeing some videos about geopolitics and the increasing possibility of SOME kind of conflict, .. I began thinking about a solution to backup &amp; store all my data at a different geolocation (I&#39;m in Europe) so in case I need to move to another country, flee of a &#39;smaller&#39; war .. whatever.. I have all my precious family photos and videos and music etc. at a location where it&#39;s (most probably in that very moment and afterwards for a while) SAFE. </p> <p>A lot of considerations popped up in my mind, some of them I tried to answer by myself.<br/> Feel free to add some options I might forgot but are useful to consider. </p> <ol> <li><p>shall it be instantly accessible or can I wait couple of minutes / hours for the files to be ready and downloadable ?<br/> - Waiting is ok but I really need &#39;pool as files&#39; to be stored in the cloud then, so zfs send into huuuge fil",
        "id": 3068100,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpvoaa/realization_of_a_zfsbased_backup_of_pools_at_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Realization of a zfs-based backup of pool(s) at a remote location (~38TB).",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ElGatoBavaria",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T13:16:55.946677+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T12:47:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am surprised why the Windows File Explorer does not have the option of creating favorites, as is possible in various browsers, for example. In addition to Windows File Explorer, I use Total-Commander, in which you can create a tree structure of favorites. Next to programs like &quot;Everything Search&quot; i still like to have some kind of structure - at least for the favourite paths.</p> <p>I would be interested to know how you handle this? I am explicitly interested in file/folder paths and not in website links etc.</p> <p>-Text file</p> <p>-OneNote / Obsidian</p> <p>....</p> <p>Translated with <a href=\"http://DeepL.com\">DeepL.com</a> (free version)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ElGatoBavaria\"> /u/ElGatoBavaria </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpuy32/how_do_you_save_your_favourite_file_paths_local/\">[link]</a></span> &#32; <span><a href=\"https://www.re",
        "id": 3067529,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpuy32/how_do_you_save_your_favourite_file_paths_local",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you save your favourite file paths ? (local | server)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tahayparker",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T15:24:56.298759+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T11:47:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hey there!</p> <p>i&#39;m looking to get a 4TB SSD to dump all my data onto. Currently, it&#39;s spread across multiple 250GB and 1TB HDDs, some on my laptop, phone and USBs as well.</p> <p>I was looking at the Crucial X10 Pro but browsing reddit recently I see that there are some issues with the drive on Mac (I&#39;m a Windows user tho). I was also looking into the Samsung T7 Shield as people have said that&#39;s a good drive as well, but I don&#39;t like the speeds and i&#39;ve seen some videos showing that they too did start acting up after a couple years. so, what ssd should i buy? the data is mostly pics and vids, and i will not be using the ssd much. it&#39;s mainly as a backup, hdd isnt an option as i will travel with it and i need something that wont die in my backpack. also i need the speeds to be fast.</p> <p>thank youu for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tahayparker\"> /u/taha",
        "id": 3068773,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lptqrj/need_help_choosing_an_ssd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help choosing an SSD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DotDotExe",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T12:09:30.525341+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T09:26:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lprccl/dvd_storage_booklet_vs_iso_backups/\"> <img src=\"https://b.thumbs.redditmedia.com/_gfVBjLYyUXJby4E3ZqLE0-ag7vBnN9JrHjTs2tO_gM.jpg\" alt=\"DVD Storage Booklet vs. ISO Backups\" title=\"DVD Storage Booklet vs. ISO Backups\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Recently looking to get rid of pretty much all our DVDs as they haven&#39;t been touched in years, but for some reason I&#39;m getting hit with the &quot;what if&quot; mindset. The main issue is space, so initially I was thinking of getting a big DVD storage booklet like this one and just getting rid of cases (and likely the heavily scratched discs), but have also considered ripping them as ISOs.</p> <p>I&#39;ve done up an excel spreadsheet to create an &quot;archive&quot; at least in terms of what I have. I&#39;m also considering just keeping this and torrenting when I feel like watching one.</p> <p>Just looking for some advice at what might ",
        "id": 3067017,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lprccl/dvd_storage_booklet_vs_iso_backups",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/_gfVBjLYyUXJby4E3ZqLE0-ag7vBnN9JrHjTs2tO_gM.jpg",
        "title": "DVD Storage Booklet vs. ISO Backups",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SuperbToe2689",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T08:00:32.515601+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T07:52:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Seagate is exponentially cheaper here, for some reason. Should i be worried about the quality? Is WD that much superior , thus the much higher price range for its higher capacity external drives? i don&#39;t have much knowledge on this stuff. Just that i need a reliable storage for all my needs. Will be buying 2 , so the other can act as a cold storage for weekly backups..</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SuperbToe2689\"> /u/SuperbToe2689 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lppzog/between_seagate_expansion_20tb_vs_wd_elements/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lppzog/between_seagate_expansion_20tb_vs_wd_elements/\">[comments]</a></span>",
        "id": 3065605,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lppzog/between_seagate_expansion_20tb_vs_wd_elements",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Between Seagate Expansion 20tb vs WD Elements 20tb, which do i buy?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zandadoum",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T08:00:32.232698+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T06:48:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Story time </p> <p>It started as a simple backup NAS. A simple 2 bay Synology. This was in 2019 or so. </p> <p>Soon a 5 bay extension unit was added, so I could rip my dvd and make a media server. </p> <p>The NAS cpu wasn\u2019t good enough as my library grew. So a minipc was added. </p> <p>By this time I also started to datahoard stuff. Some of it didn\u2019t need much space, like backing up all of gamefaqs or that one subtitle site that closed. Other did occupy a lot, like photo archives, old Linux ISOs, etc. </p> <p>The minipc wasn\u2019t enough anymore either if I wanted to host a few more docker stuff. So 2 more were added for a proxmox cluster. </p> <p>And now I notice how every year I consume 7-10TB, requiring to spend 400\u20ac on yet another HDD. Which I can barely afford, I live in Spain and I am not swimming in money right now. </p> <p>Talking about Spain: it get effing hot here. Dusty too. Right now it\u2019s 30\u00baC outdoor. 28\u00ba inside with a/c off. 32\u00ba in my office",
        "id": 3065604,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpp11r/im_tired_bro_did_i_make_a_mistake_how_to_stop",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Im tired, bro. Did I make a mistake? How to stop?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Haunting_Meal_3157",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T15:24:56.732574+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T06:22:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have around 12k files downloaded with yt-dlp that need renaming because I missed out on adding the upload date in the filename. I have the .json file together with the downloaded video file. Here&#39;s an example of what I want to accomplish</p> <p>Filename Example Old: &quot;Funniest 5 Second Video Ever! [YKsQJVzr3a8].mkv&quot; Desired New Filename: &quot;2010-01-16 Funniest 5 Second Video Ever! [YKsQJVzr3a8].mkv&quot;</p> <p>Additional Files available: &quot;Funniest 5 Second Video Ever! [YKsQJVzr3a8].info.json&quot; containing all necessary metadata like display_id, upload_date, fulltitle.</p> <p>I&#39;ve read that this can be accomplished with scripts, but please consider that I have no knowledge in coding or how to use stuff like bash, jq which I read about, so I can&#39;t write it myself. What do I need to do to accomplish this renaming process.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Haunting_Mea",
        "id": 3068774,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpomta/please_need_help_mass_renaming_files_based_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Please need help mass renaming files based on data in json file (adding upload date to filename)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Rustpit",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T05:32:29.503417+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T05:01:28+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Rustpit\"> /u/Rustpit </a> <br/> <span><a href=\"/r/VHS/comments/1lpn8uk/looking_for_composite_capture_card_that_leaves/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpnbc5/looking_for_composite_capture_card_that_leaves/\">[comments]</a></span>",
        "id": 3065062,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpnbc5/looking_for_composite_capture_card_that_leaves",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for composite capture card that leaves 480i untouched (no deinterlacing, no upscaling)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Corleone_Vito",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T05:32:29.180380+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T04:45:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My Budget is around $500 without drives, I choose Board because it supports upto 6 drives and my Storage wish is 100tb, here&#39;s the list:Does Amd supports plex? </p> <p>[ 1 ] Case: Fractal design node 304 - 120</p> <p>[ 2 ] Board: ASRock B650M Pro RS - 130</p> <p>[ 3 ] RAM - 16GB (2x8GB) DDR5-5200 ECC UDIMM \u2013 ~$70</p> <p>[ 4 ] CPU: AMD Ryzen 3 8300G (APU, 4C/8T, Radeon 740M iGPU) \u2013 ~$120</p> <p>[ 5 ] Power supply: Corsair CX450 (450W, 80+ Bronze) \u2013 ~$60</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Corleone_Vito\"> /u/Corleone_Vito </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpn1gc/is_this_good_diy_build_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpn1gc/is_this_good_diy_build_nas/\">[comments]</a></span>",
        "id": 3065061,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpn1gc/is_this_good_diy_build_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is this good DIY build NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Valiran9",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T04:27:30.048697+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T04:02:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.mediaminer.org/\">Mediaminer</a> is an <strong>old</strong> fanfiction site and not very well supported by the owners, but it was still distressing to see it inaccessible when I checked it out this evening. There were several fanfics I liked on there that weren\u2019t posted anywhere else, so I\u2019m hoping one of the hoarders on this sub has a backup they\u2019d be willing to share.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Valiran9\"> /u/Valiran9 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpm9x8/mediaminer_is_down_did_anyone_here_have_a_backup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpm9x8/mediaminer_is_down_did_anyone_here_have_a_backup/\">[comments]</a></span>",
        "id": 3064734,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpm9x8/mediaminer_is_down_did_anyone_here_have_a_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mediaminer is down, did anyone here have a backup of the fics there?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JuanManuelFangio32",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T04:27:30.221674+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T03:40:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>as titled... want something not too expensive and easy to upload/download my stuff...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JuanManuelFangio32\"> /u/JuanManuelFangio32 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lplvaw/nas_on_the_verge_of_failing_want_to_backup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lplvaw/nas_on_the_verge_of_failing_want_to_backup/\">[comments]</a></span>",
        "id": 3064735,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lplvaw/nas_on_the_verge_of_failing_want_to_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NAS on the verge of failing want to backup everything online before trying to fix it. need 4TB, will need to read back once (or none), what's good option?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Necessary_Isopod3503",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T03:22:32.596543+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T03:03:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a fairly big optical collection that I maintain and I still burn optical from time to time.</p> <p>I do understand the risks and the issues regarding optical media, with the size limitations and possible low longevity. However I do enjoy using optical for SOME things still despite having plenty of HDs and SSDs...</p> <p>However something that&#39;s has been bugging me for some time, especially after Sony announced ceasing it&#39;s production of BDRs recently, for how long will optical media production last?</p> <p>When will it become unprofitable to the point of no longer being produced, to the detriment of those who still use it? </p> <p>I know most optical media if not all, including CDRs, DVDrs and BDRs are almost all exclusively produced in Asia, but with the ongoing trend of streaming and more and more places getting internet access, and also the dying state of official DVDs and Blurays, i wonder if in the next 10-15 years or more, even le",
        "id": 3064562,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpl6a7/do_you_believe_optical_media_will_stop_being",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you believe optical media will stop being produced entirely in the next 10-15 years?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/miscawelo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T03:22:32.795114+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T02:37:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpknux/improving_nfs_performance/\"> <img src=\"https://b.thumbs.redditmedia.com/m-IxB59syAJj-nLexL1Sngo6cRDw8pCrazjmLUhvXho.jpg\" alt=\"Improving NFS Performance\" title=\"Improving NFS Performance\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/miscawelo\"> /u/miscawelo </a> <br/> <span><a href=\"/r/truenas/comments/1lpkniv/improving_nfs_performance/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpknux/improving_nfs_performance/\">[comments]</a></span> </td></tr></table>",
        "id": 3064563,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpknux/improving_nfs_performance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/m-IxB59syAJj-nLexL1Sngo6cRDw8pCrazjmLUhvXho.jpg",
        "title": "Improving NFS Performance",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Theinternetiscrack",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T03:22:32.197074+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T02:17:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpk9xf/the_big_beautiful_bill_is_going_to_make_your_wifi/\"> <img src=\"https://external-preview.redd.it/Y0E3xrew-rDIXwAVzrqfJimvxFo161R5DuVRteEpvAs.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=671c228c9aba5c248ac5d8589cd17c0968b16dde\" alt=\"The big beautiful bill is going to make your Wi-Fi slow, slow slower\" title=\"The big beautiful bill is going to make your Wi-Fi slow, slow slower\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>When you privatize, we all lose.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Theinternetiscrack\"> /u/Theinternetiscrack </a> <br/> <span><a href=\"https://youtube.com/shorts/43KihJzkHxM?si=b1qRDzM71q8PLn0c\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpk9xf/the_big_beautiful_bill_is_going_to_make_your_wifi/\">[comments]</a></span> </td></tr></table>",
        "id": 3064561,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpk9xf/the_big_beautiful_bill_is_going_to_make_your_wifi",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Y0E3xrew-rDIXwAVzrqfJimvxFo161R5DuVRteEpvAs.jpeg?width=320&crop=smart&auto=webp&s=671c228c9aba5c248ac5d8589cd17c0968b16dde",
        "title": "The big beautiful bill is going to make your Wi-Fi slow, slow slower",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CaptainLarryLobster",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T01:09:48.404650+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T01:02:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, I&#39;ve got a homelab running raid 5. I&#39;m out of storage and want to upgrade. I&#39;ve decided that the redundancy isn&#39;t necessary in my application.I found two cheap SAS drives. I want to transfer my data to these drives and then reformat the raid and run it my drives with mergeFS. My case only has room for 6 drives so I&#39;m planning on using the two extra smaller drives in another system.</p> <p><em>My issue</em> is that i have the drives running on an HBA through the only PCIE slot on the board. What would be the best way to connect the SAS drives for the data transfer prior to putting them in the system.</p> <p>I&#39;ve tried researching this but I can&#39;t find anything to help with this.</p> <p>Thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CaptainLarryLobster\"> /u/CaptainLarryLobster </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpirk7/noob_question",
        "id": 3064083,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpirk7/noob_question_about_connecting_sas_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "noob question about connecting SAS drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheLostWanderer47",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T01:09:48.574022+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T00:41:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been exploring different ways to feed live web data into AI pipelines efficiently (using proxy rotation, adaptive crawling, etc.). Curious to know how you handle the balance between speed, scale, and reliability in your setups? </p> <p>Any tools that saved you headaches?</p> <p>PS: If you&#39;ve used tools like Bright Data, Oxylabs, Octoparse, etc., I&#39;d love to know your experience.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheLostWanderer47\"> /u/TheLostWanderer47 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpicek/whats_your_goto_method_for_collecting_realtime/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lpicek/whats_your_goto_method_for_collecting_realtime/\">[comments]</a></span>",
        "id": 3064084,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpicek/whats_your_goto_method_for_collecting_realtime",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s your go-to method for collecting real-time web data without overloading your infra?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Titan_91",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T01:09:48.009143+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T00:39:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://archive.org/details/the-amanda-show-dvd-isos\">https://archive.org/details/the-amanda-show-dvd-isos</a></p> <p>If someone wants to upload ISOs of any discs they have to the Internet Archive that would be great. Season 2 is what I have so far. This is preservation, not piracy. These are from 2012 and have not been available for sale in many years.</p> <p>Nobody has shared the full DVD box set ISO images and seasons 1 and 3 are very expensive and hard to find. If you have any discs from Seasons 1 and 3, please consider uploading and sharing them. You can upload your own item to <a href=\"http://Archive.org\">Archive.org</a> or share a link and I will add it to my item and give you credit. Of the 3 popular Nickelodeon sitcom series, (All That, Kenan &amp; Kel, and The Amanda Show) only the latter has ever had a home media release.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Titan_91\"> /u/Titan_91 <",
        "id": 3064082,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lpian2/im_archiving_the_amanda_show",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm Archiving The Amanda Show",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HTWingNut",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-02T01:09:47.835916+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-02T00:11:35+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lphqgc/jonsbo_snap_on_hard_drive_sled_3d_print/\"> <img src=\"https://b.thumbs.redditmedia.com/9haEjp-y16dhBA_ExTH_F8X-2UFHg5tVIAi43eFdTvc.jpg\" alt=\"Jonsbo Snap On Hard Drive Sled 3D Print\" title=\"Jonsbo Snap On Hard Drive Sled 3D Print\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I made a 3D Printable Jonsbo hard drive sled that you can just snap on and use.</p> <p><a href=\"https://www.thingiverse.com/thing:7080915\">https://www.thingiverse.com/thing:7080915</a></p> <p>I have validated it with the N2 and N3 cases, but should work with any Jonsbo that uses the same type of rubber grommet mount design. Note that this does not offer any vibration dampening. I made it primarily so I could insert a drive into an empty bay to backup / transfer data as needed without having to scrounge for screws an grommets, etc.. Not sure it would hold up well over long periods.</p> <p>It works by using nibs that snap into the s",
        "id": 3064081,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lphqgc/jonsbo_snap_on_hard_drive_sled_3d_print",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/9haEjp-y16dhBA_ExTH_F8X-2UFHg5tVIAi43eFdTvc.jpg",
        "title": "Jonsbo Snap On Hard Drive Sled 3D Print",
        "vote": 0
    }
]