[
    {
        "age": null,
        "album": "",
        "author": "/u/expiredUserAddress",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-13T22:59:57.912513+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T22:31:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to scrape a folder from a repo. The issue is that the repo is large and i only want to get data from one folder, so I can&#39;t clone the whole repo to extract the folder or save it in memory for processing. Using API, it has limit constraints. How do I jhst get data for a single folder along with all files amd subfolders for that repo?? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/expiredUserAddress\"> /u/expiredUserAddress </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lz5vbg/scraping_github/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lz5vbg/scraping_github/\">[comments]</a></span>",
        "id": 3157063,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lz5vbg/scraping_github",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping github",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DVKprofil",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-13T22:59:58.133254+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T22:01:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As the title says \u2014 I\u2019ve ported the <a href=\"https://github.com/Xetera/ghost-cursor\">Ghost Cursor</a> library to Playwright!</p> <p>- It passes the same test suite (with minor adjustments)</p> <p>- Preserves the original API</p> <p>Here is a link<br/> <a href=\"https://github.com/DKprofile/ghost-cursor-playwright\">https://github.com/DKprofile/ghost-cursor-playwright</a></p> <p>You can add it into your project by running</p> <pre><code>pnpm add ghost-cursor-playwright-port </code></pre> <p>Works great with stealth version of chrome</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DVKprofil\"> /u/DVKprofil </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lz56m0/ported_ghost_cursor_to_playwright/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lz56m0/ported_ghost_cursor_to_playwright/\">[comments]</a></span>",
        "id": 3157064,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lz56m0/ported_ghost_cursor_to_playwright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ported Ghost Cursor to Playwright",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mythica44",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-13T20:48:33.789832+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T20:17:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, I&#39;m a backend dev trying to build a personal project to scrape product listings for a specific high-end brand from ~100-200 different retail and second-hand sites. The goal is to extract structured data for each product (name, price, sizes, etc).</p> <p>Fetching a product page&#39;s raw HTML from a small retailer with playwright and processing it with BeautifulSoup seems easy enough. My issue is with the data extraction, I&#39;m trying to build a pipeline that can handle any new retailer site without having to make a custom parser for each one. I&#39;ve tried soup methods and feeding the processed HTML to a local ollama model but results haven&#39;t been great and very unreliable across different sites.</p> <p>What&#39;s the best strategy / tools for this? Are there AI libraries better suited for this than ollama? What am I not considering?</p> <p>I&#39;m trying to do this locally with free tools. Any advice on architecture, strategy, or",
        "id": 3156522,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lz2otx/advice_on_autonomous_retail_extraction_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advice on autonomous retail extraction from unknown HTML structures?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/OkPublic7616",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-14T00:03:36.034372+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T18:03:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! This is my first post, I have commented a few times but I have never published and especially because I have never faced a challenge like this. My goal: Scratch live results from the Aviator game. I&#39;ve been searching on github, rapidapi, youtube and forums, but the solutions are old. Casinos spend money to avoid getting scraped, but I&#39;m pretty sure there must be some solution. There are no functional APIs for it to return live results. Webscraping is old. Casinos block scratch requests, not to mention that you cannot enter the game without being logged in. I was thinking about using cookies from a valid session to avoid crashes. But first I wanted to ask here. Have they tried it? How have you solved this problem? Although there are APIs to scrape live sports results, I want to scrape but from casino games. I listen carefully and appreciate possible solutions. Thank you! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https:/",
        "id": 3157300,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lyzeh7/how_to_scratch_casino_games",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scratch casino games?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mahdix18",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-14T00:03:36.521064+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T17:30:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I\u2019m currently working on a scraper for Hotels.com, but I\u2019m running into heavy anti-bot mechanisms, but with limited success.</p> <p>I need to extract pricing for more than 10,000 hotels over a period of 180 days.</p> <p>Wld really appreciate any insight or even general direction. Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mahdix18\"> /u/mahdix18 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyykm0/has_anyone_managed_to_bypass_hotelscom_antibot/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyykm0/has_anyone_managed_to_bypass_hotelscom_antibot/\">[comments]</a></span>",
        "id": 3157301,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lyykm0/has_anyone_managed_to_bypass_hotelscom_antibot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone managed to bypass Hotels.com anti-bot protection recently?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Relative_Rope4234",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-13T17:34:41.137962+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T16:54:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Guys I want scrape few hundred java script heavy websites. Since scraping with playwright is very slow, is there a way to scrape multiple websites at once for free. Can I use playwright with python threadpool executor? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Relative_Rope4234\"> /u/Relative_Rope4234 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyxo6r/how_to_scrape_multiple_urls_at_once_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyxo6r/how_to_scrape_multiple_urls_at_once_with/\">[comments]</a></span>",
        "id": 3155553,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lyxo6r/how_to_scrape_multiple_urls_at_once_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape multiple urls at once with playwright?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lerpikon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-13T16:28:36.846309+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T16:27:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to make a scraper that searches through a given txt document that contains a list of 250m urls. I want the scraper to search through these urls source code for specific words. How do I make this fast and efficient?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lerpikon\"> /u/Lerpikon </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyx0rq/url_list_source_code_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyx0rq/url_list_source_code_scraper/\">[comments]</a></span>",
        "id": 3155162,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lyx0rq/url_list_source_code_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Url list Source Code Scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Independent_Fan_232",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-14T00:03:36.981510+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T15:33:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is any free/paid tool (github, software ,...) that allow user to search google dorks , scrape each of the raw response code and search for specific words ? Need suggestion.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Independent_Fan_232\"> /u/Independent_Fan_232 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyvpf4/scarpe_googledork_websites/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lyvpf4/scarpe_googledork_websites/\">[comments]</a></span>",
        "id": 3157302,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lyvpf4/scarpe_googledork_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scarpe google-dork websites",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Emergency_Issue_992",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-13T06:44:36.674460+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-13T01:13:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an excel sheet with about 10k lines of product data to import to my online store, but I don&#39;t want my product description to be exactly like what I have scraped. is there a tool that can rephrase that?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Emergency_Issue_992\"> /u/Emergency_Issue_992 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lygbnx/a_tool_to_rephrase_cells_in_a_column/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lygbnx/a_tool_to_rephrase_cells_in_a_column/\">[comments]</a></span>",
        "id": 3152573,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1lygbnx/a_tool_to_rephrase_cells_in_a_column",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "a tool to rephrase cells in a column?",
        "vote": 0
    }
]