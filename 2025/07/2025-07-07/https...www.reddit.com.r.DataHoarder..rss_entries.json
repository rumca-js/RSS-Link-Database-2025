[
    {
        "age": null,
        "album": "",
        "author": "/u/SempiternalParadox",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T22:03:06.639503+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T21:47:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently purchased a ssk external ssd with WiFi and usb connectivity. However if you plug in a usb the wifi shuts off so you can\u2019t have the ssd plugged into device 1 and wirelessly transfer device 2 via WiFi at the same time. So are there any wifi enabled storage devices that will allow me to plug it into device 1 via and read/ modify files from device 2 without having to use one at a time ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SempiternalParadox\"> /u/SempiternalParadox </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu72t2/are_there_any_wireless_ssd_hdds_that_can_be/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu72t2/are_there_any_wireless_ssd_hdds_that_can_be/\">[comments]</a></span>",
        "id": 3107712,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu72t2/are_there_any_wireless_ssd_hdds_that_can_be",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are there any wireless SSD/ hdds that can be accessed over wifi and usb at the same time ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/iwilltalkaboutguns",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T22:03:06.812732+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T21:39:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Greetings,</p> <p>So I been collecting data for a very long time. Started with mame roms, then expanded in other areas. I have about 500TBs fully redundant storage. Mostly entertainment at 4K Atmos (or whatever the highest quality available).</p> <p>Now I&#39;m making a collection of &quot;end of the world&quot; knowledge. I&#39;ve archived quite a bunch of YouTube channels and ebooks... </p> <p>But I have to imagine there must be a massive torrent out there that contains everything I would want on this category... Wikipedia, encyclopedias, technical manuals, blueprints, etc etc.</p> <p>Anyone can point me in the right direction here? </p> <p>As a bonus question, anyone care to share their long term backup/storage plans (other than just hoard spare drivers, which I&#39;m already doing). Do you keep your HDs installed but powered off on your NAS, uninstalled, protected by a Faraday cage and buried? Just wondering what other people have done.</p> </div>",
        "id": 3107713,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu6vzp/prepper_data_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\"prepper data collection\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/enigmacarpc",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T22:03:07.020344+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T21:30:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, I purchase 14TB and 20Tb drives from <strong>goHardDrive</strong> on Ebay and have always had a great experience. Recently for an Arcade system I I bought a 20TB Exos drive. It is constantly in a power saving state and has to spin up, I dont think I get more that a few minutes before it has to wake up. What I mean by this is I click the drive and it spins the file open after 10-20 seconds then I do something else for under 30 seconds and the drive has to spin up again when accessing the same folder. So either its a power saving feature or an access/speed issue. I have 4 20TB drives and 2 14TB drives and the others arent a problem. All drives are used in a DAS Terramaster D4-320 over USB C 3.2. What tool can I use to change the spin down timeouts?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/enigmacarpc\"> /u/enigmacarpc </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu6",
        "id": 3107714,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu6o1i/seagate_exos_x20_20tb_spin_down",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate Exos X20 20TB spin down",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jaythegamer0302",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T20:58:12.773377+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T20:57:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu5tk8/this_is_my_friends_storage_setup_he_refuses_to/\"> <img src=\"https://preview.redd.it/wsf2yowvmibf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff24ea51d90a904bd6fd51c06779adf5b83c9cfc\" alt=\"This is my friend's storage setup. He refuses to clean his thousands of hours of recorded footage\" title=\"This is my friend's storage setup. He refuses to clean his thousands of hours of recorded footage\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Broke ahh</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jaythegamer0302\"> /u/Jaythegamer0302 </a> <br/> <span><a href=\"https://i.redd.it/wsf2yowvmibf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu5tk8/this_is_my_friends_storage_setup_he_refuses_to/\">[comments]</a></span> </td></tr></table>",
        "id": 3107272,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu5tk8/this_is_my_friends_storage_setup_he_refuses_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/wsf2yowvmibf1.png?width=320&crop=smart&auto=webp&s=ff24ea51d90a904bd6fd51c06779adf5b83c9cfc",
        "title": "This is my friend's storage setup. He refuses to clean his thousands of hours of recorded footage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/arch017",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T20:58:12.569637+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T20:01:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have tons and tons of photos. Adobe sucks and is expensive. Like a good gallery app or file app for when viewing photos from hundreds of folders, something with a &quot;favorite&quot; options. Like when you have to find that embarrassing photo to embarasse your kids and family. Something that&#39;s also built with privacy in mind.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/arch017\"> /u/arch017 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu4djc/what_do_you_guys_use_to_manage_photos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu4djc/what_do_you_guys_use_to_manage_photos/\">[comments]</a></span>",
        "id": 3107271,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu4djc/what_do_you_guys_use_to_manage_photos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What do you guys use to manage photos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bAN0NYM0US",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T19:53:08.713581+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T19:31:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought some Seagate Exos X16 drives from the <a href=\"https://www.amazon.ca/stores/page/D63DD2D2-11AF-4928-A4AF-B01D49B706DC?is_byline_deeplink=true&amp;redirect_store_id=1DBE4F37-543D-4F85-9664-78868F7426CE&amp;ref_=ast_bln&amp;lp_asin=B07SPFPKF4&amp;brandname=Seagate\">official Seagate store on Amazon.ca</a> back in 2022. One of the drives failed and my warranty is still valid until 2027 but when trying to submit a claim for a replacement drive. It&#39;s telling me to contact support.</p> <p>I contacted support and they&#39;re telling me that this &quot;new&quot; drive was actually registered in Asia on June 11th and my purchase was on June 27th. So these were open box drives being sold as new, and being sold with the warranty issued in Asia and they can&#39;t give me a replacement drive because I&#39;m in Canada.</p> <p>I just got totally scammed by Seagate and they won&#39;t give me a replacement drive even though I still have almost 2 years of w",
        "id": 3106868,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu3lav/warning_official_seagate_store_on_amazonca",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Warning: Official Seagate Store on Amazon.ca",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No_Sandwich5766",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T19:53:09.092467+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T19:24:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Apologies this is sort of a tangential question but I figure some in this community will have some experience with this.</p> <p>I have a reasonably large collection of movies and old shows. They&#39;re in a variety of formats, mostly .mkv, .avi and .mp4 and stored on a synology NAS system (220j).</p> <p>I find the most straightforward way to use these has been downloading the collection to a hard drive and connecting directly to the TV this way it&#39;s ready to go and I can scroll using the TV remote. </p> <p>However my TV&#39;s have different file reading abilities and some don&#39;t work at all so I&#39;m looking for a solution to make enjoying my collection easy. Our main TV is a Samsung frame.</p> <p>Some options I&#39;m considering:</p> <ul> <li><p>some kind of secondary device like a purpose based laptop I could leave plugged in to the TV that can read all file types (would be annoying if I have to operate separately)</p></li> <li><p>converting",
        "id": 3106870,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu3es0/having_issues_playing_files_off_tv_whats_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Having issues playing files off TV, what\u2019s the easiest solution?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BookShelfRandom",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T19:53:08.885979+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T19:19:44+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu39zn/this_reddit_community_has_been_archived_29428_as/\"> <img src=\"https://preview.redd.it/p8hqou0n4ibf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af0b14225fd15da338e9ce5f3c6f202461dcbbb4\" alt=\"This reddit community has been archived 29,428 as of making this post.\" title=\"This reddit community has been archived 29,428 as of making this post.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>You can view this at: <a href=\"https://www.reddit.com/r/DataHoarder/\">https://www.reddit.com/r/DataHoarder/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BookShelfRandom\"> /u/BookShelfRandom </a> <br/> <span><a href=\"https://i.redd.it/p8hqou0n4ibf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu39zn/this_reddit_community_has_been_archived_29428_as/\">[comments]</a></span> </td></tr></table>",
        "id": 3106869,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu39zn/this_reddit_community_has_been_archived_29428_as",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/p8hqou0n4ibf1.png?width=640&crop=smart&auto=webp&s=af0b14225fd15da338e9ce5f3c6f202461dcbbb4",
        "title": "This reddit community has been archived 29,428 as of making this post.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/caedisdux",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T19:53:09.300011+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T19:09:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have close to a million image files in a NAS directory, and because they are a big part of my work for many years, I wanted to store them to the cloud -- but space is expensive and those files are huge. So I resized the whole damn collection with a freeware I found (took 10 days on a very strong PC), threw both root folders on my NAS, and made sure only the smaller version gets backups so if both my PC and my NAS fail, at least I have safety copies of all the photos even if they&#39;re not the way I took them anymore.</p> <p>First off: I&#39;m not a coder. I tried to hire someone off Fiverr to create this software for me, but their prices were simply too high for me (not saying they weren&#39;t fair -- I just couldn&#39;t afford it). A dev buddy of mine introduced me to Cursor IDE with Claude AI and long story short, I&#39;ve been sitting down a lot of hours to create this software with the help of AI, learning some Python along the way.</p> <p>I&#3",
        "id": 3106871,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu30hx/anyone_interested_in_a_robust_autoresizer_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone interested in a robust auto-resizer for images?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Rotisseriejedi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T18:48:50.134185+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T18:42:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Used to use DVD Fab and it worked fine. Files appeared to be just like the DVD\u2019s I have but I no longer have it and looking for another program </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Rotisseriejedi\"> /u/Rotisseriejedi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu2asz/whats_the_best_free_program_for_making_best/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu2asz/whats_the_best_free_program_for_making_best/\">[comments]</a></span>",
        "id": 3106354,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu2asz/whats_the_best_free_program_for_making_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s the best free program for making best quality ISO files out of DVD sets?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Pirated-Hentai",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T18:48:50.420340+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T18:06:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu1cmn/thoughts/\"> <img src=\"https://preview.redd.it/9kiopupishbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f5dd242833785ff19399fb7ba233e9e72291cf3\" alt=\"Thoughts?\" title=\"Thoughts?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Wanting some extra storage, would \u00a310/TB be a good deal? Owner said they&#39;re open to offers.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pirated-Hentai\"> /u/Pirated-Hentai </a> <br/> <span><a href=\"https://i.redd.it/9kiopupishbf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu1cmn/thoughts/\">[comments]</a></span> </td></tr></table>",
        "id": 3106355,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu1cmn/thoughts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/9kiopupishbf1.png?width=640&crop=smart&auto=webp&s=0f5dd242833785ff19399fb7ba233e9e72291cf3",
        "title": "Thoughts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Alternative-Land5916",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T17:43:48.413206+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T17:39:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lu0mmc/best_option_for_archiving_28gb_of_contents/\"> <img src=\"https://b.thumbs.redditmedia.com/Wd9iqoeJ2NvdZnp7H71F2d0FNjcialnGVThpk01LJsU.jpg\" alt=\"Best option for archiving 28GB of contents\" title=\"Best option for archiving 28GB of contents\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I did a full backup of my entire life and work the other day onto a rare and expensive Japanese Sony 128GB BD-R only to immediately receive verification errors from ImgBurn:</p> <p><a href=\"https://preview.redd.it/vu4rxac4nhbf1.png?width=492&amp;format=png&amp;auto=webp&amp;s=6473725600cfa7773d7e9627b5f316b6586c89e3\">FML</a></p> <p>Diagnosing what&#39;s at fault will require new discs and (potentially) a new drive, and I&#39;m wondering if the wisest option is on disc at all, given how much you hear about disc rot.</p> <p>The folder in particular which I want to archive is 28GB exactly but as it is a project folder I can ",
        "id": 3105740,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lu0mmc/best_option_for_archiving_28gb_of_contents",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/Wd9iqoeJ2NvdZnp7H71F2d0FNjcialnGVThpk01LJsU.jpg",
        "title": "Best option for archiving 28GB of contents",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HolidayFuck",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T17:43:48.585130+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T16:39:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been manually saving the content of a creator I follow who posts content unique to Snapchat fairly randomly. I love their content and don&#39;t want to miss anything, but don&#39;t enjoy checking for new things that may expire all the time. I&#39;d like to be able to automatically download any photos or videos they post as stories or highlights on a regular basis. </p> <p>I came across this tool as a solution but couldn&#39;t get it to work. Any other options or advice on making this work?</p> <p><a href=\"https://github.com/skyme5/snapchat-dl\">https://github.com/skyme5/snapchat-dl</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HolidayFuck\"> /u/HolidayFuck </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltz098/automated_snapchat_archiving/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltz098/automated_snapchat_archiving/\">[comments]</",
        "id": 3105741,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltz098/automated_snapchat_archiving",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automated Snapchat Archiving?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Deadly_35",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T16:38:10.907857+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T15:35:22+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltxbvh/i_built_a_desktop_app_that_automatically_sorts/\"> <img src=\"https://preview.redd.it/2y6zlmck1hbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=060e59589a20c90f354dbd48b6f18e70bbfc71f7\" alt=\"I built a desktop app that automatically sorts your files based on rules you define. Looking for feedback!\" title=\"I built a desktop app that automatically sorts your files based on rules you define. Looking for feedback!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone! I\u2019ve been working on a Python-based desktop app called Sortly \u2014 it&#39;s a file sorting tool that lets you create custom rules (like sorting .pdf files into a \u201cPDFs\u201d folder by year, or .mp3s into genre-based folders).</p> <p>It\u2019s fully local (no cloud or tracking), and you can:</p> <ul> <li>Customize file type rules (extension \u2192 folder \u2192 optional subfolder)</li> <li>Save multiple configurations (for different workflows or users)</li",
        "id": 3105196,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltxbvh/i_built_a_desktop_app_that_automatically_sorts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/2y6zlmck1hbf1.png?width=640&crop=smart&auto=webp&s=060e59589a20c90f354dbd48b6f18e70bbfc71f7",
        "title": "I built a desktop app that automatically sorts your files based on rules you define. Looking for feedback!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/surelyunsure_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T15:32:25.756536+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T14:38:27+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltvv5l/what_would_be_the_best_way_to_travel_with_these/\"> <img src=\"https://preview.redd.it/w0h86c65rgbf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bad06ee616fc36d23da4a3f213d207623ee2720b\" alt=\"What would be the best way to travel with these hard drives internationally?\" title=\"What would be the best way to travel with these hard drives internationally?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Any particular way these should be handled while traveling abroad? Anti-static bags, bubble wrap, a hard case, carry on or checked luggage? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/surelyunsure_\"> /u/surelyunsure_ </a> <br/> <span><a href=\"https://i.redd.it/w0h86c65rgbf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltvv5l/what_would_be_the_best_way_to_travel_with_these/\">[comments]</a></span> </td></tr></table>",
        "id": 3104574,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltvv5l/what_would_be_the_best_way_to_travel_with_these",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/w0h86c65rgbf1.jpeg?width=640&crop=smart&auto=webp&s=bad06ee616fc36d23da4a3f213d207623ee2720b",
        "title": "What would be the best way to travel with these hard drives internationally?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nikgtasa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T15:32:26.189432+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T14:32:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Since the site got acquired i fear a purge and locking of the content will eventually follow to appease shareholders/ad providers. This made me try and backup as many risque mods as i could find in my tracking center but here&#39;s the problem.<br/> Vortex can&#39;t download tracked mods, it can&#39;t download mods in bulk either. It can download collections but they have to be made manually first from downloaded mods. Jdownloader correctly parses nexus links and gives a comprehensive list of archives but it can&#39;t download them. Account manager within the program says it can only download nxm links but i couldn&#39;t get even them to work not to mention that would mean JD would need a mod download link, not a mod page link.<br/> I&#39;ve looked through greasyfork scripts and some github solutions but most of them aim to automate collection downloading, not bulk downloading with parsing mod pages.<br/> I don&#39;t suppose anyone knows how to solve ",
        "id": 3104575,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltvq4b/nexus_mods_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Nexus Mods Backup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lucius1213",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T14:27:47.574023+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T13:21:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m searching for a reliable single-bay 3.5&quot; external drive enclosure, but most of the ones I see don&#39;t automatically power back on after a power loss, they require pressing the power button (electronic switch) to turn them back on.</p> <p>Does anyone know of any solid enclosures that either have a mechanical power switch or don&#39;t have a power switch at all?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lucius1213\"> /u/Lucius1213 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltu0k3/looking_for_a_singlebay_35_drive_enclosure_that/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltu0k3/looking_for_a_singlebay_35_drive_enclosure_that/\">[comments]</a></span>",
        "id": 3104017,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltu0k3/looking_for_a_singlebay_35_drive_enclosure_that",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a single-bay 3.5\" drive enclosure that auto-powers on after power loss",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TekAzurik",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T14:27:47.784400+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T13:16:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I do video production and have been shooting for this client for 10 years. We have about 32TB of data right now, which I have on a Backblaze B2 account and on a RAID 0 drive in my office that I edit off of. The RAID is almost full and the client is getting annoyed at shelling out ~$150 a month for Backblaze. Of course if something ever did happen to our local storage I know getting the data back from Backblaze is going to be an expensive and time consuming process. So I would love some advice from folks who really know this space well on a better solution. Most of the industry folks I work with don&#39;t have such long-term storage needs so I don&#39;t have anyone in my network to turn to. Price is very much a concern for this client so a $5k solution is not going to fly. I was looking at an affordable NAS setup from B&amp;H perhaps. Something in the range of what Backblaze is costing them for a year I might be able to sell. I don&#39;t love the idea ",
        "id": 3104018,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lttx3f/desperately_need_advice_on_longterm_data_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Desperately need advice on long-term data storage for a client (~32TB with room to grow)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/anyusernaem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T13:09:12.330240+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T12:52:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m not sure if we&#39;ll see something better this year on BF and certainly not from WD. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/anyusernaem\"> /u/anyusernaem </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lttdwq/is_the_26tb_seagate_external_stkp26000400_for_250/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lttdwq/is_the_26tb_seagate_external_stkp26000400_for_250/\">[comments]</a></span>",
        "id": 3103393,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lttdwq/is_the_26tb_seagate_external_stkp26000400_for_250",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is the 26TB Seagate External (STKP26000400) for $250 the best deal so far this year?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/manav-1200",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T08:17:21.557660+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T07:49:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was thinking like what will happen to all the data once you are not there? Is it like you have thought to pass it to someone.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/manav-1200\"> /u/manav-1200 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lto8h1/what_will_happen_to_the_data_after_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lto8h1/what_will_happen_to_the_data_after_you/\">[comments]</a></span>",
        "id": 3101780,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lto8h1/what_will_happen_to_the_data_after_you",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What will happen to the data after you?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/xyz941823",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T05:02:28.369658+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T04:10:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When collecting large amounts of data, I found that managing proxies can get expensive and complicated. Recently, I tried <a href=\"http://evomi.com\">evomi.com</a>, which offers rotating residential proxies with no subscriptions and a straightforward pay as you go pricing of $0.49 per GB.</p> <p>The proxies rotate automatically and come from real devices, which helped me avoid blocks during scraping. The dashboard is pretty minimal but gets the job done. It might be a good option if you\u2019re looking for a flexible and cost-effective way to manage IPs for big data projects.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/xyz941823\"> /u/xyz941823 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltkpiq/using_proxies_for_data_collection_a_simple_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltkpiq/using_proxies_for_data_collection_a_simple_and/\">[comm",
        "id": 3101029,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltkpiq/using_proxies_for_data_collection_a_simple_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using proxies for data collection, a simple and affordable option",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Astranauts",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T05:02:28.124374+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T04:03:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I love MyRetroTVs (<a href=\"https://80s.myretrotvs.com/\">https://80s.myretrotvs.com/</a>). I specially put these videos before I go to sleep.</p> <p>Someone should download and store all of them, there&#39;s very valuable content there too.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Astranauts\"> /u/Astranauts </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltkkoe/suggestion_store_all_myretrotvs_videos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltkkoe/suggestion_store_all_myretrotvs_videos/\">[comments]</a></span>",
        "id": 3101028,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltkkoe/suggestion_store_all_myretrotvs_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Suggestion: Store all MyRetroTVs' videos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rangoMangoTangoNamo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T03:57:20.817274+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T03:43:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Seems like plex keeps adding more and more feees. I was trying to share my plex movie library with my friends but apparently plex is trying to make them pay a subscription to access my content.</p> <p>Does anyone have any recommendations how to get around this? Or do you recommend something like jellyfin?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rangoMangoTangoNamo\"> /u/rangoMangoTangoNamo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltk808/plex_too_many_fees_for_sharing_content/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltk808/plex_too_many_fees_for_sharing_content/\">[comments]</a></span>",
        "id": 3100781,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltk808/plex_too_many_fees_for_sharing_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Plex too many fees for sharing content",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/matthew_levi12",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T03:57:20.186739+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T03:36:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Disclaimer: I&#39;m a newbie to the subject. Trying to learn from the experts.</p> <p>Let&#39;s say I have a server running ecommerce with millions of customer&#39;s sensitive data, hosted somewhere else far away from me. It&#39;s fully disk-encrypted with LUKS. So, nobody can see the files decrypted if they stole the disk.</p> <p>But, I have heard that once the server is unlocked with LUKS passphrase, the key resides in RAM. Somebody with physical access to the server could just dump RAM and extract LUKS keys.</p> <p>How could I protect my server from having LUKS keys stolen from RAM as well? Like a cold boot attack, for example?</p> <p>Thank you so much for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/matthew_levi12\"> /u/matthew_levi12 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltk3fx/luks_protection_against_ram_dump_in_a_physical/\">[link]</a></span> &#32; <span><a hr",
        "id": 3100779,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltk3fx/luks_protection_against_ram_dump_in_a_physical",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LUKS: protection against RAM dump in a physical attack?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kyxstrez",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T03:57:20.467498+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T03:28:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to replace <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l08jem/time_to_replace_hard_drives/\">these 2 drives</a> and I&#39;m currently considering a few options:</p> <ul> <li>WD Elements 22TB (400\u20ac)</li> <li>Seagate Exos 24TB (400\u20ac)</li> <li>Seagate Expansion 22TB (350\u20ac)</li> <li>Toshiba MG10AFA22TE 22TB (350\u20ac)</li> </ul> <p>The drive will be placed on my main desktop rig next to my desk, so it needs to be quiet.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kyxstrez\"> /u/Kyxstrez </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltjyaf/what_drive_to_buy_to_replace_812tb_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltjyaf/what_drive_to_buy_to_replace_812tb_drives/\">[comments]</a></span>",
        "id": 3100780,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltjyaf/what_drive_to_buy_to_replace_812tb_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What drive to buy to replace 8+12TB drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/King-of-Plebss",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T02:52:49.132146+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T02:33:26+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltiwy8/new_to_the_space_stumbled_on_a_pallet_of_these_at/\"> <img src=\"https://preview.redd.it/3s8env056dbf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b3ba9a9264aa4a21fbb665d96110fc01642e41ff\" alt=\"New to the space \u2014 stumbled on a pallet of these at a furniture warehouse with my wife. Picked one up for $80\" title=\"New to the space \u2014 stumbled on a pallet of these at a furniture warehouse with my wife. Picked one up for $80\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Always been a fan of data hoarding, but never had the means/opportunity. The furniture warehouse just took over the space from another company, and they had a back room with ton of random pallets of shit. Came across a stack of these and they let me buy it for $80 cash. Bought a 10tb drive a few minute later and I\u2019m so excited to set up a NAS finally! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/use",
        "id": 3100583,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltiwy8/new_to_the_space_stumbled_on_a_pallet_of_these_at",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/3s8env056dbf1.jpeg?width=640&crop=smart&auto=webp&s=b3ba9a9264aa4a21fbb665d96110fc01642e41ff",
        "title": "New to the space \u2014 stumbled on a pallet of these at a furniture warehouse with my wife. Picked one up for $80",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/theonepugparty",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T16:38:11.645113+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T02:14:52+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/theonepugparty\"> /u/theonepugparty </a> <br/> <span><a href=\"/r/OSINT/comments/1ltii0h/idea_browser_extension_to_archive_webpages_via/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltik89/idea_browser_extension_to_archive_webpages_via/\">[comments]</a></span>",
        "id": 3105197,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltik89/idea_browser_extension_to_archive_webpages_via",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[IDEA] Browser Extension to Archive Webpages via Wayback Machine (with Privacy + Control Features)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ZanyDroid",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T01:47:20.244220+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T01:45:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for a DAS array that exposes LUNs over Thunderbolt. This is exploratory, looking for budget &lt;$1000.</p> <p>LUN would abstract a device that has mirrored SSD write cache over some &quot;parity&quot; (IE not mirrored) coded HDD devices underneath. </p> <p>The reason for this is that I want to move from Storage Spaces to something better, but still retain it as a local device from the POV of Backblaze Personal.</p> <p>I also theorycrafted whether iSCSI would work, but have seen mixed signals about whether this works and how wise it is. But Thunderbolt is officially on the Backblaze supported list.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ZanyDroid\"> /u/ZanyDroid </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lti0fz/das_arrays_that_expose_luns_over_thunderbolt/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lti0fz/das_arrays_that_expo",
        "id": 3100404,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lti0fz/das_arrays_that_expose_luns_over_thunderbolt",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DAS arrays that expose LUNs over Thunderbolt",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DieingFetus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T01:47:20.033866+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T01:18:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I enjoy tinkering with projects using Debian. I get a lot of my stuff with git clone commands. I had my first instance of something not being available anymore and now I want to save everything locally now.</p> <p>What would be a good way to add my files in my debian projects with terminal that would work simular to git clone?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DieingFetus\"> /u/DieingFetus </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lthhdu/im_working_on_saving_some_github_projects_ive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lthhdu/im_working_on_saving_some_github_projects_ive/\">[comments]</a></span>",
        "id": 3100403,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lthhdu/im_working_on_saving_some_github_projects_ive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Im working on saving some github projects I've been using over the years. What would be the best format to save them and what would the best way to use them locally in headless Debian?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DaWizardOfThem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-07T01:47:20.449669+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-07T01:09:00+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DaWizardOfThem\"> /u/DaWizardOfThem </a> <br/> <span><a href=\"/r/lostmedia/comments/1ltffps/partially_lost_apples_recently_taken_down_how_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lthaks/partially_lost_apples_recently_taken_down_how_to/\">[comments]</a></span>",
        "id": 3100405,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lthaks/partially_lost_apples_recently_taken_down_how_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[partially lost] Apple\u2019s recently taken down \u201cHow to convince your parents to get you a Mac\u201d",
        "vote": 0
    }
]