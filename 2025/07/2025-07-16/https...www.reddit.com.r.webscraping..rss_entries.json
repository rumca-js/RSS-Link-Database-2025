[
    {
        "age": null,
        "album": "",
        "author": "/u/warkun5400",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T22:57:18.512018+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T22:31:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It seems like most people use pptr/playwright/nodriver etc for their scraping needs. I suppose it makes sense since these libraries are open source and widely used. </p> <p>But it often seems like most people get stuck on the antibot or antiscraping mechanisms with these libraries. Every browser library has leaks and if they don&#39;t, there is often a reliance on the author to supporting the library for the forseeable future. Even managed services like BrowserBase have leaks and struggle to bypass protected sites. </p> <p>So why not use mobile phones to automate? The hardware is real, no leaks, consumer grade sim allows you to get unlimited IPs more or less. Is it because there is generally a lack of support for automation with real hardware/devices? Is it a cost issue? A scale issue? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/warkun5400\"> /u/warkun5400 </a> <br/> <span><a href=\"https://www.reddit.com/r/we",
        "id": 3183892,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m1qoer/mobile_scrapingautomation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mobile Scraping/Automation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PresentDisastrous759",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T20:48:37.919354+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T18:47:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a list of around 5000 substances in a spreadsheet that I need to enter one by one into <a href=\"https://chem.echa.europa.eu/\">https://chem.echa.europa.eu/</a>, check if the substance is present, and return the link to the first result. I am not sure how to go about it or even start a script (if one would work) and have honestly considered doing manually which would take so long. I have been using ChatGPT to help but it isn&#39;t much use - every script or option it gives runs into so many errors. </p> <p>What would be my best course of action? Any advice or help would be appreciated</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PresentDisastrous759\"> /u/PresentDisastrous759 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1m1kyev/search_and_scrape_first_result_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1m1kyev/search_and_scrape_first",
        "id": 3183063,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m1kyev/search_and_scrape_first_result_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Search and Scrape first result help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lafftar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T17:33:49.511947+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T17:20:45+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1m1in12/i_scraped_all_the_bars_in_nyc_34k_from_google/\"> <img src=\"https://external-preview.redd.it/7xAQq9z675hiw-2BcYKMUEYBTLoXKH-EsF9t1xGTDgY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16a251a79fe4cd308039428dcc0fcdbe7cf84df0\" alt=\"I scraped all the bars in nyc (3.4k) from Google Maps, here's how\" title=\"I scraped all the bars in nyc (3.4k) from Google Maps, here's how\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>In this video I go over what I scraped (all the bars in NYC and some cities in San Fran), and one challenge i faced (trying to make the code future proof)</p> <p>I scraped about 100k pictures from these bars And about 200k reviews as well. Could have gone more indepth but that wasnt what the client wanted.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lafftar\"> /u/Lafftar </a> <br/> <span><a href=\"https://youtube.com/shorts/SJOuKut0d2A?feature=shar",
        "id": 3181480,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m1in12/i_scraped_all_the_bars_in_nyc_34k_from_google",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/7xAQq9z675hiw-2BcYKMUEYBTLoXKH-EsF9t1xGTDgY.jpeg?width=320&crop=smart&auto=webp&s=16a251a79fe4cd308039428dcc0fcdbe7cf84df0",
        "title": "I scraped all the bars in nyc (3.4k) from Google Maps, here's how",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jay_nine9",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T20:48:38.088871+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T15:45:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a csv with a lot of Soundcloud profile links so what I am doing is going through then and searching for bio to then apply a filter and see if I can find management email, but apparently my function doesn&#39;t find the bio at all on the web, im quite new to this but I don&#39;t see that I put any tags wrong ... here is a random Soundcloud profile with bio <a href=\"https://m.soundcloud.com/abelbalder\">https://m.soundcloud.com/abelbalder</a> , and here is the function (thanks in advance): </p> <pre><code>def extract_mgmt_email_from_infoStats( html ): soup = BeautifulSoup( html , &quot;html.parser&quot;) # Look specifically for the article with class &#39;infoStats&#39; info_section = soup.find(&quot;article&quot;, class_ =&quot;infoStats&quot;) if not info_section: return None paragraphs = info_section.find_all(&quot;p&quot;) for p in paragraphs: text = p.get_text( separator =&quot;\\n&quot;).lower() if any(keyword in text for keyword in [&quot;mg",
        "id": 3183064,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m1g32z/any_idea_why_this_doesnt_work",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any idea why this doesn't work ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dogchasingatruck",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T11:52:48.332260+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T10:31:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone here having experience scraping Spotify? Specifically, I&#39;m trying to create a tool for Artists to measure if they are following best practices. I just need to grab basic information off the profile, such as their bio, links to social media, featured playlists etc. Not scraping audio or anything like that.</p> <p>I&#39;ve identified the elements and know I can grab them using an automated browser (sign in not required to view artist pages). I&#39;m mainly concerned about how aggressive Spotify is with IP addresses. I know I have a few options: Using a free VPN, using a proxy with cheap Datacentre IP addresses, or using residential IP addresses.</p> <p>I don&#39;t want to be too overkill if possible hence trying to find someone with (recent) experience scraping Spotify. My intuition is that Spotify will be hot on this kind of thing so I don&#39;t want to waste loads of time messing around only to find out it&#39;s more trouble than it&#3",
        "id": 3178332,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m193le/spotify_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Spotify Scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Tetendry",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T09:21:30.386808+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T08:04:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I\u2019m a second-year university student working on my final year project. For this project, I\u2019m required to collect data by web scraping and save it as a CSV file.</p> <p>I chose TheGradCafe as my data source because I want to analyze graduate school admissions. I found some code generated by DeepSeek (an AI assistant) to do the scraping, but I don\u2019t really understand web scraping yet and I\u2019m not able to retrieve any data.</p> <p>I ran the script using libraries like requests and BeautifulSoup (without Selenium). The script runs without errors but the resulting CSV file is empty \u2014 no data is saved. I suspect the site might use JavaScript to load content dynamically, which makes scraping harder.</p> <p>I\u2019m stuck and really need help to move forward, as I don\u2019t want to fail my project because of this. If anyone has successfully scraped TheGradCafe or knows how to get similar data, I\u2019d really appreciate any advice or example code you cou",
        "id": 3177379,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m16u8i/beginner_in_data_science_i_need_help_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Beginner in data science I need help scraping TheGradCafe",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AdPublic8820",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T07:10:30.277564+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T06:15:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi All, </p> <p>I had worked on a web scraping utility using playwright that scrape dynamic html content, captures network log and takes full page screenshot in headless mode. It works great, the only issue is that modern websites have strong anti bot detection and using existing python libraries did not suffice so I built my own stealth injections to bypass.</p> <p>Prior to this, I have tried, requests-html, pydoll, puppeteer, undetected-playwright, stealth-playwright, nodriver and then crawl4ai. </p> <p>I want to build this utility like firecrawl but its not an approved tool to use, so there&#39;s no way it I can get it. And I&#39;m the only developer who knows the project in and out, and have been working on this utility to learn each of their strengths etc. And me alone can&#39;t build an &quot;enterprise&quot; level scrapper that can scrape thousands of urls of the same domain.</p> <p>Crawl4ai actually works great but has an issue with full page ",
        "id": 3176717,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1m1559a/looking_for_guidance_on_a_web_scraping_utility",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for guidance on a web scraping utility. Please help!!!!",
        "vote": 0
    }
]