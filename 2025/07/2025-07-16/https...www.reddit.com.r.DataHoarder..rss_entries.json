[
    {
        "age": null,
        "album": "",
        "author": "/u/ZOODUDE100",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T23:27:05.767174+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T23:00:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://online.fliphtml5.com/jptvh/qipe/\">https://online.fliphtml5.com/jptvh/qipe/</a> </p> <p>I am trying to download this file for offline viewing, but none of the online converters are working. Would anyone happen to have any suggestions or even the ability to convert it yourself?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ZOODUDE100\"> /u/ZOODUDE100 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1rd6z/fliphtml5_to_pdf_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1rd6z/fliphtml5_to_pdf_help/\">[comments]</a></span>",
        "id": 3184036,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1rd6z/fliphtml5_to_pdf_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "FlipHTML5 to PDF Help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/International-Table1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T23:27:05.941270+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T22:31:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to build my first NAS. I was able to get a QNAP TS-233 for at least 100 CAD. Right now I&#39;m trying to find a good deal for HDD. I checked diskprices and serverpartdeals kills me with the shipping fee. I was able to find a 6TB Seagate iron wolf for 140CAD. Is it a good deal?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/International-Table1\"> /u/International-Table1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1qohk/nas_drive_deals/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1qohk/nas_drive_deals/\">[comments]</a></span>",
        "id": 3184037,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1qohk/nas_drive_deals",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NAS Drive deals",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/geo-metro",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T22:22:08.954704+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T22:20:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve got a couple stupid questions and was looking for some input. </p> <p>I&#39;ve got maybe 10 or so 1TB drives and I&#39;d like to move all of the files onto one larger drive. What software is going to be the best for doing this? Some of the drives may have duplicate data so if I could somehow spot that before I copy/move it twice that would be great. I don&#39;t really need to organize all the data I just want to compile it all onto one large disk</p> <p>Another stupid question I&#39;ve got is that once I&#39;ve got all my data in one spot, what&#39;s the best way to make a 1:1 copy of that disk? I read over the wiki and software recommendations briefly but is this more of a job for cloning the disk or using the backup software like borg or rsync? </p> <p>Last question, probably not that important but I bet people here have more experience than I do. Say you&#39;ve got like 40+ SD cards from cameras and flash drives with pictures, how would yo",
        "id": 3183713,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1qfij/no_stupid_questions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "No stupid questions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vaterlandfront",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T22:22:09.124933+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T22:07:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am thinking about Digitizing my dvd(and a little bit of Blu-ray) collection but I\u2019m new in this field and I need your help.</p> <p>I found already small success on an old windows 7 tower pc with an built in dvd player/make mkv (successfull copies johnnyEnglish,Hercules,5 others etc) But no success with makemkv Star WarsDVD for example So maybe I will have to ubgrade to a good external displayer. </p> <p>And I am thinking of buying a nas because I don\u2019t have enough storage. (LAN connected)</p> <p>QNAP TR-004 4 Bay Desktop NAS Expansion - Optional Use as a Direct-Attached Storage Device (230\u20ac) Or QNAP TR-002 USB 3.1-RAID (150\u20ac) Or Asustor Drivestor 2 Lite AS1102TL, 2 Bay NAS Enclosure Network Storage, 1.7GHz Quad Core, 1GbE Port, 1GB RAM DDR4, Network Attached Storage for Cloud Storage (171\u20ac)</p> <p>And a future project/goal would be to make my dvd copy\u2019s streamable with Plex or jellyfin to my phone for example.</p> <p>So any thoughts on my little roa",
        "id": 3183714,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1q32i/hello_fellow_pirates_and_and_data_horders",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hello fellow Pirates and and data Horders",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ankpar80",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T22:22:09.297023+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T22:06:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hello i have looked thru the posts and i am not a linux expert by any means. i have a plex server running on ubuntu but either via that or my mac i want to be able to easily download yt content any good options out there that wont require me to be a linux guru ? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ankpar80\"> /u/ankpar80 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1q2ox/youtube_downloads/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1q2ox/youtube_downloads/\">[comments]</a></span>",
        "id": 3183715,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1q2ox/youtube_downloads",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "youtube downloads",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VimmDotNet",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T22:22:08.744154+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T21:31:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For years the Samsung 870 QVO 8TB has been the workhorse of my file servers. They&#39;re the perfect blend of size and speed (I couldn&#39;t care less about extended write performance) and during the pandemic they retailed for as low as $350. With a large SAS controller I can easily plug 32 into a server.</p> <p>I recently went shopping for spares and discovered Samsung has discontinued the QVO line. I poked around and it seems nobody else makes an 8TB SATA SSD either, just NVMe drives. That&#39;s fine for a typical PC but if a RAID drive fails I need to swap in a replacement and I can&#39;t exactly plug an NVMe drive into a SATA port. This leaves me feeling a bit vulnerable.</p> <p>Is there anyone out there who makes an 8TB SATA SSD?<br/> - or -<br/> Is there any way to connect an NVMe drive to a SATA port?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VimmDotNet\"> /u/VimmDotNet </a> <br/> <span><a href=\"https",
        "id": 3183712,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1p74h/does_anyone_make_an_8tb_sata_ssd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone make an 8TB SATA SSD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/darkkef",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T21:17:47.344746+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T20:59:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, what&#39;s up? I&#39;m currently running my ZimaBlade with ZimaOS, primarily as a NAS setup. I\u2019ve got it working nicely, Jellyfin, the Arr apps, and Ownfoil (for Switch) I&#39;m running 2\u00d7 4TB 3.5&quot; HDDs + a 512GB NVMe SSD via PCIe Everything is running smoothly using the official 12V USB-C power brick But I&#39;ve got carried away lol, and i\u2019m already at about 50% capacity, and I want to expand by adding 2 more 3.5&quot; HDDs.</p> <p>I know the official power brick won\u2019t support that extra load, I\u2019ve been exploring options to externally power the new drives. I&#39;m considering:</p> <p>A PicoPSU</p> <p>A Flex ATX PSU</p> <p>Or a simple barrel-to-SATA power adapter I plan to connect the additional drives via a PCIe SATA expansion card and maybe 3D-print a 4-bay enclosure to tidy it all up.</p> <p>This is my first homelab, and while I\u2019ve solved most things with AI and tutorials, I\u2019m not super advanced, I&#39;ve done soldering (ive soldere",
        "id": 3183235,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1oeis/zimablade_adding_2_extra_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Zimablade adding 2 extra HDD's",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/throndir",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T20:11:47.236749+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T19:19:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys,</p> <p>I&#39;m fairly new to data hoarding (as in where I started to do backups and not just putting a bunch of data on drives). I&#39;ve built a couple of servers machines already, then eventually consolidated to one big one that contains almost 200 TB. I&#39;ve only thus far played around with DrivePool and Windows, and have no experience with Linux. The winning thing for me about DrivePool is the fact I can throw in different-sized drives in it, and it can just handle it fine.</p> <p>Sad news, I&#39;m already about 95% full. All my slots are filled. True I can squeeze out a bit more by buying slightly larger drives, or I could even go the route of buying some NAS devices, or a larger chassis and migrate everything. But I&#39;m realizing this hobby of mine will probably be lifelong, and I&#39;ve seriously been considering buying a server rack, then getting started. I&#39;m willing to drop around $10k-15k for the project (but will space it o",
        "id": 3182737,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1ltqs/taking_the_plunge_on_a_server_rack",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Taking the plunge on a server rack",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Methhead1234",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T19:07:27.113215+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T18:42:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m not looking for any gimmicky systems because there&#39;s a lot out there that feel basically like procrastination porn where you feel like you&#39;re doing something when you&#39;re not. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Methhead1234\"> /u/Methhead1234 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1ktqn/most_efficient_file_organization_system_for_large/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1ktqn/most_efficient_file_organization_system_for_large/\">[comments]</a></span>",
        "id": 3182277,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1ktqn/most_efficient_file_organization_system_for_large",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Most efficient file organization system for large projects and random stuff?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/evildad53",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T18:00:27.445662+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T17:52:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How much should I believe the stats from HD Sentinel Pro when it says &quot;Estimated remaining lifetime: more than 1000 days,&quot; or &quot;Estimated remaining lifetime: 88 days&quot;? In the latter case, it says the health and performance are excellent, but it&#39;s an external that I&#39;ve had for a few years now: &quot;power on time: 1907 days&quot;, which <strong>is</strong> over 5 years.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/evildad53\"> /u/evildad53 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1jh4q/estimated_remaining_lifetime_in_hard_disk_sentinel/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1jh4q/estimated_remaining_lifetime_in_hard_disk_sentinel/\">[comments]</a></span>",
        "id": 3181731,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1jh4q/estimated_remaining_lifetime_in_hard_disk_sentinel",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\"Estimated remaining lifetime\" in Hard Disk Sentinel",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Aggravating_Shame427",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T18:00:27.615927+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T17:48:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been using ImgBurn for many years, since shortly after I bought a DVD-writer for a mid-tower PC.</p> <p>Now I&#39;m using it on a Windows 11 laptop and it&#39;s been great up until maybe the last year. I&#39;ve been having trouble with Verbatim writeable BD-Rs.</p> <p>I&#39;m not sure if it&#39;s the drive or the media, I&#39;ve been assuming the media, but even as I type this I&#39;m thinking it&#39;s more likely the drive:</p> <p>I have six coasters from the past two days, all Verbatim M-Disc BD-R DLs. No successes. Most have failed on writing, one on verify.</p> <p>ScsiStatus 0x02</p> <p>Interpretation: Check Condition</p> <p>CDB: AA 00 00 BA 78 20 00 00 00 20 40 00</p> <p>Interpretation: Write (12) - Sectors: 12220448 - 12220479</p> <p>Sense Area: 71 00 03 00 00 00 0E 00 00 00 00 73 03 00 00 00 00</p> <p>SK Interpretation: Medium Error</p> <p>ASC/ASCQ Interpretation: Power Calibration Area Error.</p> <p>Pioneer Model BDR-XS06, manufacture",
        "id": 3181732,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1jdur/imgburn_2580_vs_verbatim_mdisc_bdr_dl_vs_pioneer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ImgBurn 2.5.8.0 vs. Verbatim M-Disc BD-R DL vs. Pioneer BDR-XS06",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/giunyu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T16:53:37.751084+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T16:31:19+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1havz/power_supply_finally_tapped_out/\"> <img src=\"https://a.thumbs.redditmedia.com/je74Sn9LXh5I-jJGjYIkDED_0JT0hrhAs2t66-Gsxc0.jpg\" alt=\"Power supply finally tapped out\" title=\"Power supply finally tapped out\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Power supply on this 16yr old PC finally gave up, decided to have these two smile for the camera </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/giunyu\"> /u/giunyu </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1m1havz\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1havz/power_supply_finally_tapped_out/\">[comments]</a></span> </td></tr></table>",
        "id": 3181066,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1havz/power_supply_finally_tapped_out",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/je74Sn9LXh5I-jJGjYIkDED_0JT0hrhAs2t66-Gsxc0.jpg",
        "title": "Power supply finally tapped out",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GameDevAtDawn",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T13:34:09.915051+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T13:03:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, I know there are sites that offer to remove the MEGA download limit, and they work.</p> <p>But I&#39;m looking for something that also allows me to bulk download files from mega and also automatically upload files to MEGA, ideally using just a link. Any solutions out there that can handle both uploading and downloading? Bonus if it can be scheduled or used in scripts.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GameDevAtDawn\"> /u/GameDevAtDawn </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1c1fg/anyone_knows_how_to_bypass_mega_download_limit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1c1fg/anyone_knows_how_to_bypass_mega_download_limit/\">[comments]</a></span>",
        "id": 3179196,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1c1fg/anyone_knows_how_to_bypass_mega_download_limit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone knows how to bypass MEGA download limit and bulk upload files automatically?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Wild_Chef6597",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T12:28:35.757697+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T12:02:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1ar4m/feels_like_this/\"> <img src=\"https://preview.redd.it/v7be4l1w78df1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b577c7bb62aa649c63ac5144b00b22562601bdbb\" alt=\"Feels like this\" title=\"Feels like this\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Wild_Chef6597\"> /u/Wild_Chef6597 </a> <br/> <span><a href=\"https://i.redd.it/v7be4l1w78df1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m1ar4m/feels_like_this/\">[comments]</a></span> </td></tr></table>",
        "id": 3178594,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1ar4m/feels_like_this",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/v7be4l1w78df1.jpeg?width=320&crop=smart&auto=webp&s=b577c7bb62aa649c63ac5144b00b22562601bdbb",
        "title": "Feels like this",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lineker14",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T12:28:36.200246+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T11:34:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,<br/> I have a situation where I need to automatically export a database from phpMyAdmin to a MySQL server. Is there any way to do this? It&#39;s important to mention that this database is a mirror of the one provided by my system provider, and I don&#39;t have direct access to their SQL server.</p> <p>My main go is to do a full load on my local mysql server, them an schedule update to get new information on my local mysql server.</p> <p>The pourpose of this is that i need to make a dashboard on powerbi with data from this database</p> <p>Some details that might help:<br/> <strong>Database server:</strong></p> <ul> <li>Server: Localhost via UNIX socket</li> <li>Server type: MySQL</li> <li>SSL: Not being used</li> <li>Server version: 5.7.42-0ubuntu0.18.04.1 (Ubuntu)</li> <li>Protocol version: 10</li> <li>User: [hidden for privacy]</li> <li>Server charset: cp1252 West European (latin1)</li> </ul> <p><strong>Web server:</strong></p> <ul> <",
        "id": 3178595,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m1a7ba/is_there_any_way_to_automatically_export_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there any way to automatically export my database from phpMyAdmin to my own MySQL server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zR0B3ry2VAiH",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T11:23:34.640578+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T11:04:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1m19nxu/update_sandisk_denial_to_refund_unopened_4tb/\"> <img src=\"https://external-preview.redd.it/Eowv_R8iazQQmix-bxyZyVwpTkLNIIgQBfXdjyRY2eI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84179d98c6f84b371a82904ad9d5563233959df4\" alt=\"Update SanDisk denial to refund unopened 4TB SN850X drive\" title=\"Update SanDisk denial to refund unopened 4TB SN850X drive\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zR0B3ry2VAiH\"> /u/zR0B3ry2VAiH </a> <br/> <span><a href=\"/r/pcmasterrace/comments/1m0xg9e/update_sandisk_denial_to_refund_unopened_4tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m19nxu/update_sandisk_denial_to_refund_unopened_4tb/\">[comments]</a></span> </td></tr></table>",
        "id": 3177975,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m19nxu/update_sandisk_denial_to_refund_unopened_4tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Eowv_R8iazQQmix-bxyZyVwpTkLNIIgQBfXdjyRY2eI.jpeg?width=640&crop=smart&auto=webp&s=84179d98c6f84b371a82904ad9d5563233959df4",
        "title": "Update SanDisk denial to refund unopened 4TB SN850X drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NGAF2-lectricBugalou",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T11:23:34.865429+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T10:45:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>SO... i have about 30/32tb across a number of drives and 2x NAS</p> <p>A Synology 214DS 2x10TB<br/> A QNAP TS-412 4x6TB<br/> Home built Unraid Box 1TB 2x512GB Cache 4TB (2X2TB) Local storage Currently (Built for Testing before the move)<br/> Backup PC - 7TB (3+2+2)</p> <p>I want to consolidate and Backup this Data But i need to figure out how to temp store the 30TB currently in use across 6-8 Mixed Drives until i can strip and redo the hardware to put the largest safest Drives into the UNRAID server and end up with a Singular pool of about 32TB on the unraid (Sata ports and case Limits of 4x3.5&#39;&#39; bays) and 16TB in the QNAP Nas</p> <p>I&#39;m having trouble figuring out the process I would use to consolidate without Buying a 40TB/2X20TB drives (I&#39;d do it if anyone&#39;s got a good line in the EU) and just Dumping everything to it so i thought how Expensive could Cloud Storage be for a short term solution a lot don&#39;t have the storage opt",
        "id": 3177976,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m19bqk/data_consolidate_and_rebuild_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data consolidate and Rebuild 2025",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Buggs_y",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T09:51:23.729077+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T08:48:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m genuinely worried about the future of research and datasets given the redefining of anecdotes as acceptable data and the unlogged mass editing of databases. I&#39;m hoping you guys can ease my mind that someone somewhere is protecting research by hoarding it. I kinda see data hoarders as every day super heroes and really need to believe that there are people who care enough about data to save it from the savagery I&#39;m seeing reported. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Buggs_y\"> /u/Buggs_y </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m17hu6/dhers_set_to_become_humanitys_saving_grace/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m17hu6/dhers_set_to_become_humanitys_saving_grace/\">[comments]</a></span>",
        "id": 3177528,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m17hu6/dhers_set_to_become_humanitys_saving_grace",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DHers set to become humanity's saving grace?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Upstairs-Plankton-96",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T08:46:23.315572+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T08:42:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I just lost a bunch of my personal data (photos, video) probably 700gb coz my hard drive got corrupted. Never had a back up. So Im asking whats the consensus most safe and reliable way to back up data? If it\u2019s cloud, whats the most recommended? I\u2019ve read in here \u201cBackblaze\u201d? Can anyone shed a light to how it works? Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Upstairs-Plankton-96\"> /u/Upstairs-Plankton-96 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m17eok/help_a_newbie/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m17eok/help_a_newbie/\">[comments]</a></span>",
        "id": 3177155,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m17eok/help_a_newbie",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help a newbie",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hspindel",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T06:36:54.079783+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T05:49:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://serverpartdeals.com/products/western-digital-wd200edgz-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-recertified-hard-drive\">https://serverpartdeals.com/products/western-digital-wd200edgz-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-recertified-hard-drive</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hspindel\"> /u/hspindel </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m14pxt/good_price_on_20tb_recertified/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m14pxt/good_price_on_20tb_recertified/\">[comments]</a></span>",
        "id": 3176558,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m14pxt/good_price_on_20tb_recertified",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Good price on 20TB recertified",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FiveDragonDstruction",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T03:17:55.899933+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T02:55:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I found a good price for HDD&#39;s, most of them are 2.5 inch and the rest are 3.5 inch. I&#39;m not going to use it in NAS, but, is it good to use it to store my legally owned ROMS? I want to store some PS2 and Switch games there</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FiveDragonDstruction\"> /u/FiveDragonDstruction </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m11ld8/410_500gb_to_1tb_hdd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m11ld8/410_500gb_to_1tb_hdd/\">[comments]</a></span>",
        "id": 3175793,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m11ld8/410_500gb_to_1tb_hdd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "$4-10 500GB to 1TB HDD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Spektre99",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T03:17:56.188475+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T02:23:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a Windows Server 2019 box with 2 Exos X22 20TB drives in mirrored array, 1 20TB SATA drive (as backup), and a 512GB Inland SSD as the boot drive. </p> <p> </p> <p>I wanted to test the speed of the mirrored array and got some strange results. </p> <p>(speed of individual drives per spec)</p> <p>Exos drives: 285MB/s</p> <p>20TB backup: 220MB/s</p> <p>Inland SSD: 450MB/s</p> <p> </p> <p><strong>My test file is a 62GB video file.</strong></p> <p> </p> <p>Writing from the array to the backup drive achieves 245MB/s for about 65% of the transfer and then drops to 135-170MB/s.</p> <p> </p> <p>Writing from the backup drive to the array achieves 215MB/s and is consistent throughout the 62GB, save the last 5-10% where it drops slightly to 200MB/s.</p> <p> </p> <p> </p> <p><strong>So I decided to try transferring the file from my mirrored array to the boot SSD. It transferred for 240MB/s for about 35% of the transfer before dropping to an abysmal 60-80MB/s",
        "id": 3175794,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m10xtx/copying_large_files_slows_down_midway_more_than",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Copying large files slows down midway (more than attributable to buffering).",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ducksncandy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T02:12:54.933437+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T01:50:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone who purchased one recently (within the last couple of weeks) able to chime in? I bought one 2 months ago and it had an Exos inside. Looking for confirmation before I couple a couple more.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ducksncandy\"> /u/ducksncandy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m109dw/do_the_18tb_seagate_expansion_external_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m109dw/do_the_18tb_seagate_expansion_external_drives/\">[comments]</a></span>",
        "id": 3175542,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m109dw/do_the_18tb_seagate_expansion_external_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do the 18TB Seagate Expansion external drives still have Exos in them?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FindKetamine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-16T09:51:24.265063+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-16T01:35:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Without having google drive synced locally, what is the fastest method you\u2019ve found for downloading My Drive and Other Computers while PRESERVING file tree w directory structure </p> <p>(google takeout doesn\u2019t preserve structure)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FindKetamine\"> /u/FindKetamine </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m0zy3y/fastest_way_to_make_local_copy_of_google_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1m0zy3y/fastest_way_to_make_local_copy_of_google_drive/\">[comments]</a></span>",
        "id": 3177530,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1m0zy3y/fastest_way_to_make_local_copy_of_google_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fastest way to make local copy of google drive without Takeout",
        "vote": 0
    }
]