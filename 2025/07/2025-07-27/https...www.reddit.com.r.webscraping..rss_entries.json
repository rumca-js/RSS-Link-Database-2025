[
    {
        "age": null,
        "album": "",
        "author": "/u/makelotsofcash",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T23:59:22.296981+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T20:40:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Web scraping the Amazon website for products being in stock (checking for the Add to Cart and/or Buy Now buttons) using \u201crequests\u201d + Python seems to be out of sync with the actual in stock inventory. </p> <p>Even when scraping every two seconds, and immediately clicking Add to Cart or Buy Now seems to be too late as the item is already out of stock, at least for high demand items. It then takes a few minutes for the buttons to disappear so there\u2019s clearly delays between the UI and actual inventory.</p> <p>How are other people buying these items on Amazon so quickly? Is there an inventory API or something else folks are using? And even if so, how are they then buying it before the buttons are available on the website? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/makelotsofcash\"> /u/makelotsofcash </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mawyw6/amazon_scraping_ui_out_of_sync_with",
        "id": 3213480,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mawyw6/amazon_scraping_ui_out_of_sync_with_actual",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Amazon - scraping UI out of sync with actual inventory?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NathanFallet",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T23:59:22.466727+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T20:26:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been working on this library for 2 months already, and I\u2019ve got something pretty stable. I\u2019m glad to share this library, it\u2019s my contribution to the scraping and browser automation world \ud83d\ude0e <a href=\"https://github.com/cdpdriver/kdriver\">https://github.com/cdpdriver/kdriver</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NathanFallet\"> /u/NathanFallet </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mawmo3/built_an_undetectable_chrome_devtools_protocol/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mawmo3/built_an_undetectable_chrome_devtools_protocol/\">[comments]</a></span>",
        "id": 3213481,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mawmo3/built_an_undetectable_chrome_devtools_protocol",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Built an undetectable Chrome DevTools Protocol wrapper in Kotlin",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Confident_Fly_6187",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T23:59:22.667997+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T18:41:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hi, all! I\u2019m working on a project where I\u2019m essentially trying to build a kind of of aggregator that pulls structured info from thousands of websites across the country. I\u2019m trying to extract the same ~20 fields from all of them and build a normalized database. the tool allows you to look for available meeting spaces to reserve. this will pull information from a huge variety of entities: libraries, local communtiy centers, large corporations.</p> <p>stack: Playwright + BeautifulSoup for web crawling and URL discovery, custom scoring algorithms to identify space reservation-related pages, and OpenAI API to extract needed fields from the identified webpages</p> <p>before it can begin to extract the info I need, my script needs to essentially take the input (the homepage URL of the organization/company) and navigate the website until it identifies the subpages that contain the information. currently, this process looks like:</p> <p>1) fetches homepage, t",
        "id": 3213482,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mau0s2/trying_to_scrape_from_thousands_of_unique",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "trying to scrape from thousands of unique websites... please help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Beneficial-Sound2235",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T16:26:42.569369+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T14:49:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello</p> <p>Im looking to grab data from an extensive (thousands) set of subdomains. Ive found these through a simple &quot;site:maindomain.com&quot; google search yeilding many &quot;subdomain.masterdomain.com&quot; results.</p> <p>I could go each individually but there are so many I thought there has to be a better way.</p> <p>Id like to compile the data into a sheet with the typical datafields for domain, name, company name, phone number, email etc.</p> <p>Is there free/low-cost software or maybe a chrome extension that could do this without bogging down too much as there are potentially 10s of thousands?</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Beneficial-Sound2235\"> /u/Beneficial-Sound2235 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mao66g/grabbing_data_from_subdomains/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/",
        "id": 3208731,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mao66g/grabbing_data_from_subdomains",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Grabbing data from subdomains",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Alk601",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T16:26:42.907812+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T09:47:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m scraping ebay sold products and I handle bot detection well but sometimes they do something weird which is sending me incomplete data. Like instead of receiving 100 products on my request, I have 4.</p> <p>First I gave Claude the 2 files to find a way to identify a pattern but Claude is struggling since it&#39;s huge html file. </p> <p>So I thought of 2 solutions :</p> <p>1) I could just check the last successfull request and If the result has less than 50% of the last successfull request (e.g at least 50 products) then I would request again. (cost 1 request to the database, read)</p> <p>2) I could also check that with the file size, error file is always smaller. (cost cpu)</p> <p>But both those approach might be sensitive to edge cases (e.g., if there&#39;s truly only 3 sold products that match the query).</p> <p>What would you do? I&#39;m using regular proxies because they&#39;re cheaper than residential ones. Most requests go through.</p> <",
        "id": 3208733,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1maichx/scraping_ebay_but_sometimes_it_returns_incomplete",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping eBay, but sometimes it returns incomplete data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/External_Skirt9918",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T16:26:42.399326+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T06:25:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I see lot of people get blocked instantly while doing scraping in large scale. Many residential proxy provider is using this opportunity and heavily increased like 1GB/1$ which is insane cost to scrape the data that we want.</p> <p>I found a cheapest way to do that with the help of One Rooted android mobile(atleast 3GB RAM) + Termux + macrodroid + unlimited mobile data package.</p> <p>Step 1: download macrodroid and configure a http method trigger to turn off and turn on the aeroplane plane.</p> <p>Step 2: install termux and install the python on it</p> <p>Step 3: in your existing python code write a condition whenever you are getting blocked trigger that http request and go to sleep for 20-30 sec. Aeroplane mode will turn on and off. So that will give you new ip. Then again retry mechanism will start Scrapping make a loop of 24/7. Since we have hell lot of IP&#39;s in your hand.</p> <p>Incase any doubt feel free to ask \ud83e\udd73\ud83c\udf89</p> </div><!-- SC_ON --> &#3",
        "id": 3208730,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1maf8nj/alternative_to_residential_proxies_cheap",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Alternative to Residential Proxies - Cheap",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PossibleTomorrow4852",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T16:26:43.076978+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T02:06:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have this weird issue with a particular web app that I&#39;m trying to scrape. It&#39;s a dashboard that holds information about some devices of our company and that info can be exported in csv. They don&#39;t offer an API to get this done programmatically so I&#39;m trying to automate the process using playwright.</p> <p>Thing is all the routes load well (auth, main page, etc) but the one that has the info I need just should the nav bar (the layout of the page). There&#39;s an iframe that should display the info I need and a button to download the csv but the never render.</p> <p>I&#39;ve tried Chrome, Edge, Chromium and it&#39;s the same issue. I&#39;m suspecting that some of the features that playwright disable o. The browser are causing the issue.</p> <p>I&#39;ve tried modifying the CMD args when launching pw but that is actually worst (the library launches the browser process but never gets to connect to it and control the browser).</p> <p>Inve",
        "id": 3208734,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1maan2a/issue_with_the_rendering_of_a_route_in_playwright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Issue with the rendering of a route in playwright",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Alchemist-D",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T16:26:42.738337+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T00:06:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How are SERP api services built that can offer Google searches at a tenth of the official Google charges? Are they massively abusing the free 100 free searches accross thousands of gmails? Coz am sure by their speed they aren&#39;t using browser. Am open to ideas.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Alchemist-D\"> /u/Alchemist-D </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ma8a1x/massive_scraping_scale/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ma8a1x/massive_scraping_scale/\">[comments]</a></span>",
        "id": 3208732,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ma8a1x/massive_scraping_scale",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Massive Scraping Scale",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AdSevere704",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-27T16:26:43.245682+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-27T00:03:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to track specific Best Buy search queries looking to load around 30-50k js pages per month (hitting the same pages around twice a minute for 10 hours a day for the month). I&#39;m debating on whether it is better to just use a AIO web scraping API or attempt to manually do it with proxies.</p> <p>I&#39;m trying to catch certain products as they come out (nothing that is too high demand) and tracking the prices of some specific queries. So I am just trying to get the offer or price change at most a minute after they are available.</p> <p>Most AIO web scraper APIs seems to cover this case pretty simply for $49 but I am wondering if it is worth the effort to do the testing myself. Does anyone have some experience dealing with scraping Best Buy to know whether this is necessary or whether Best Buy doesn&#39;t really have the extensive anti-scrape countermeasures to warrant the use of these APIs.</p> </div><!-- SC_ON --> &#32; submitted by &",
        "id": 3208735,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ma88f5/looking_to_scrape_best_buy_trying_to_figure_out",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to scrape Best Buy- trying to figure out the best solution",
        "vote": 0
    }
]