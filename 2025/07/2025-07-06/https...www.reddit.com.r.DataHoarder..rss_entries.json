[
    {
        "age": null,
        "album": "",
        "author": "/u/maxstenta1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T23:37:27.750872+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T22:50:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to automatically extract data (video/scene list) from a site that loads content dynamically via JavaScript. After saving the HTML page rendered with Selenium, I look in the code or API calls for the JSON that contains the real data, because often they are not directly in the HTML but are loaded by separate API requests. The aim is to identify and replicate these API calls in order to download complete data programmatically.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/maxstenta1\"> /u/maxstenta1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lteg9c/looking_for_help_to_extract_data_from_a_html_page/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lteg9c/looking_for_help_to_extract_data_from_a_html_page/\">[comments]</a></span>",
        "id": 3099973,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lteg9c/looking_for_help_to_extract_data_from_a_html_page",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for help to extract data from a HTML page that loads content dynamically via JavaScript",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Vegetable_One8614",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:55.671978+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T20:14:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi sorry to bother you. I&#39;m desperately trying to find a way to download content from <a href=\"https://www.divicast.com/\">Divicast</a> (i already searched on Reddit with no results) and I&#39;m going insane. Do you guys know any method?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vegetable_One8614\"> /u/Vegetable_One8614 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltautc/do_you_any_of_you_guys_know_a_way_to_download/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltautc/do_you_any_of_you_guys_know_a_way_to_download/\">[comments]</a></span>",
        "id": 3099175,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltautc/do_you_any_of_you_guys_know_a_way_to_download",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you any of you guys know a way to download from Divicast?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/One-End1795",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:55.861862+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T19:52:58+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltac50/best_amazon_prime_day_hard_drive_deals_2025_can/\"> <img src=\"https://external-preview.redd.it/Y1ycRNg6mXLh5s_FYFhh_yBFR7mfqv8k9eRpdS-QUEg.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f91edf5f55775264297b2a343820203eed82edda\" alt=\"Best Amazon Prime Day Hard Drive deals 2025 : Can you guys find any better deals than these?\" title=\"Best Amazon Prime Day Hard Drive deals 2025 : Can you guys find any better deals than these?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I am looking for NAS drives, and I am open to shucking. This seems to be a decent list, but are there better prime day deals yet?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/One-End1795\"> /u/One-End1795 </a> <br/> <span><a href=\"https://www.tomshardware.com/pc-components/ssds/best-hard-drive-deals\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ltac50/b",
        "id": 3099176,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ltac50/best_amazon_prime_day_hard_drive_deals_2025_can",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Y1ycRNg6mXLh5s_FYFhh_yBFR7mfqv8k9eRpdS-QUEg.jpeg?width=640&crop=smart&auto=webp&s=f91edf5f55775264297b2a343820203eed82edda",
        "title": "Best Amazon Prime Day Hard Drive deals 2025 : Can you guys find any better deals than these?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chumley_1953",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:56.042410+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T19:39:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am assisting a nonprofit that is undergoing a hostile takeover by new board members. They have had an active Facebook page with thousands of followers. I am trying to help the founder, who still has administrator access, archive the pages and followers. Any ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Chumley_1953\"> /u/Chumley_1953 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lta08u/archiving_a_facebook_page_for_a_nonprofit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lta08u/archiving_a_facebook_page_for_a_nonprofit/\">[comments]</a></span>",
        "id": 3099177,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lta08u/archiving_a_facebook_page_for_a_nonprofit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Archiving a Facebook Page for a nonprofit",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jamiecruickshank",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:56.210533+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T19:28:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lt9quz/hp_lto7_drive_suddenly_throwing_ec5_error_any/\"> <img src=\"https://b.thumbs.redditmedia.com/aSMiyQsU5KFIQJMxH4g5H6zJse8BK3xri_2xFSVU9No.jpg\" alt=\"HP LTO-7 Drive Suddenly Throwing EC5 Error \u2013 Any Ideas?\" title=\"HP LTO-7 Drive Suddenly Throwing EC5 Error \u2013 Any Ideas?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jamiecruickshank\"> /u/jamiecruickshank </a> <br/> <span><a href=\"/r/homelab/comments/1lt6d5w/hp_lto7_drive_suddenly_throwing_ec5_error_any/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lt9quz/hp_lto7_drive_suddenly_throwing_ec5_error_any/\">[comments]</a></span> </td></tr></table>",
        "id": 3099178,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt9quz/hp_lto7_drive_suddenly_throwing_ec5_error_any",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/aSMiyQsU5KFIQJMxH4g5H6zJse8BK3xri_2xFSVU9No.jpg",
        "title": "HP LTO-7 Drive Suddenly Throwing EC5 Error \u2013 Any Ideas?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheCuriousBread",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T19:17:21.894702+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T18:18:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve got a spare laptop or two that&#39;s just sitting there doing nothing right now so I decided to spin them up and do some seeding for Anna&#39;s Archive. </p> <p>Well. I&#39;ve downloaded the magnet links and they are on my computer now, 0 uploads for the week. </p> <p>What....what even is the point of this exercise? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheCuriousBread\"> /u/TheCuriousBread </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lt82dy/does_anyone_seed_for_annas_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lt82dy/does_anyone_seed_for_annas_archive/\">[comments]</a></span>",
        "id": 3098904,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt82dy/does_anyone_seed_for_annas_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone seed for Anna's Archive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/daxliniere",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:55.505302+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T17:44:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I would like to ditch Google and move all of my media in Google Files to my own storage servers.<br/> I have used Takeout to to generate a list of 78 .ZIP files each 4Gb in size, but I can&#39;t work out how to 1) translate this into a table of direct links and 2) how to download at commandline, considering there is no ability to load a website for Google account authentication.</p> <p>Anyone got any cool solutions here? Or another way to get all the media? I tried rclone, but no matter what I did (including setting up OAuth test user), I couldn&#39;t get it to download a single thing.</p> <p>Thanks for reading this far. :)</p> <p>All the best,<br/> Dax.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/daxliniere\"> /u/daxliniere </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lt78da/downloading_google_takeout_via_linux_commandline/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit",
        "id": 3099174,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt78da/downloading_google_takeout_via_linux_commandline",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading Google Takeout via Linux commandline (>300Gb)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SSMMDS40",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:56.377469+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T17:35:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Our video production company has a QNAP TVS-h1288X NAS which has about 60TB of usable space. I\u2019d like to find a backup solution that can hold around 300TB of data so that we can retain snapshots and I will be able to look back several months or even years to find files if needed. I\u2019d also like to have a separate folder where I can just store old video footage that we may need at some point but I don\u2019t want to keep on our 1288x since it\u2019s likely we won\u2019t ever need the data (it\u2019s just nice to have it in case we need to access it for some reason).</p> <p>I\u2019d like to stick with QNAP, so what are some relatively cheap (under $1K) options for a JBOD enclosure that can hold enough 24TB drives to give me 300TB of usable space for backing up our data) Also, what are the cheapest 24TB drives that are still reliable that I can use? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SSMMDS40\"> /u/SSMMDS40 </a> <br/> <span><a h",
        "id": 3099179,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt70ia/300tb_of_jbod_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "300TB of JBOD storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/redditunderground1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T18:12:22.343309+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T17:26:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I wrote about this topic a few weeks ago elsewhere. I checked in with the Data Hoarders yesterday and see a member here is doing this exact thing. So, good work for them! When you are out and about, you are already there, so why not go the distance and archive it? Don&#39;t make a big goddamn deal of it, just do what you can. Then upload to the I.A. Even recording audio can be worthwhile. It just depends on the subject.</p> <p>Full story...</p> <p>Link nsfw..ish</p> <p><a href=\"https://cinematography.com/index.php?/forums/topic/103559-when-you-go-to-art-photo-shows-special-screenings-lectures-etcdocument-it-nsfw/\">When you go to art &amp; photo shows, special screenings, lectures, etc...document it! (NSFW) - Off Topic - Cinematography.com</a></p> <p>I&#39;m old, health is poor. I don&#39;t get out much anymore. Maybe some food shopping once a week. But I did my share when I was more mobile. Now I just work with other peoples&#39; photos, video, movies",
        "id": 3098544,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt6scm/you_boys_girls_and_zirs_want_a_purpose_in_life",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "You boys, girls and zirs want a purpose in life? Start archiving shows, lectures and exhibitions!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rrredditor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T18:12:22.660590+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T17:18:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need a PCIe card that will handle 4-8 SATA drives for use in a full tower case. I&#39;m not seeing very many choices anymore. I have a cheapy 4 port card that only seems to work on 2 of the ports and doesn&#39;t secure the SATA cables very well.</p> <p>I&#39;d like something better. I suppose SAS cards are the next step, especially if I want 8 ports. I have the PCIe slot available (8X).</p> <p>This is for local storage that is backed up on a NAS. I currently have 5 hard drives and two DVD drives and I&#39;d like room to grow.</p> <p>Any recommendations? I&#39;d like to stay below $150 if possible. I know that makes it more difficult. Used?</p> <p>I haven&#39;t had to buy anything like this for over 10 years so I&#39;m a bit in the dark these days. Any suggestions would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rrredditor\"> /u/rrredditor </a> <br/> <span><a href=\"https://www.reddit.com/r/Da",
        "id": 3098545,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt6m6t/looking_for_a_good_sata_drive_controller_card",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a good SATA drive controller card",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/impracticaldogg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T14:57:26.352224+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T14:26:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>TLDR: I want to avoid data corruption on my small server by occasionally writing archived data from one disk across to another. From lurking on this forum this seems to be a simple way to avoid the quiet corruption of data that can happen if you simply leave it there and don&#39;t access it for years. </p> <p>I&#39;m running Ubuntu Server and just writing a cron script to activate rsync and copy data across every three months seems like an adequate way to do this. I&#39;m thinking of keeping three copies of everything, and overwriting the oldest copy when I run out of space. </p> <p>Does this sound reasonable? I&#39;m not terribly technical and just don&#39;t get round to making multiple backups every month. </p> <p>Detail: I have an old Microserver with a range of hard drives (512GB to 1TB) that ended up being surplus over time. About 12GB of drive space altogether, with 8GB being two 4GB external USB drives. This is about twice as much capacity as I",
        "id": 3097471,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt2ihy/trying_to_avoid_data_corruption_by_automated",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to avoid data corruption by automated (re)writing of archived data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/richiethestick",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T14:57:26.106197+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T14:24:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just counted\u2014I&#39;ve got around 131 movies stashed away, most clocking in at about 10 GB each. That\u2019s well over a terabyte of cinematic intentions that somehow never make it off the drive and onto the screen. It\u2019s not like I don\u2019t want to watch them. I just\u2026 don\u2019t.</p> <p>Even with everything neatly sorted in Plex, I\u2019ll spend more time browsing than actually watching anything. Sometimes I try to spice it up with a random picker, but that usually ends with me questioning my own taste in downloads.</p> <p>To make things worse, I keep defaulting to streaming on Netflix instead. Something about knowing the downloaded stuff is \u201calways there\u201d makes it feel less urgent. Meanwhile, Netflix keeps throwing autoplay at me and suddenly I\u2019m three episodes deep into something I didn\u2019t even plan to watch. The hoard just keeps growing.</p> <p>Honestly, I think I\u2019ve started collecting more for the thrill of the hunt than for the viewing itself. It\u2019s weirdly satisfyin",
        "id": 3097470,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt2gxi/anyone_else_drowning_in_their_movie_backlog",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone else drowning in their movie backlog?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rautakattila",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:56.581267+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T13:46:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>as per title, I\u2019m planning a home server / NAS for personal and family use. This thread is about the safe and holistic handling of data and not the hardware side of things (possibly I\u2019ll buy an off the shelf NAS and install my own OS using Ugreen, Aoostar or similar product. I <em>might</em> build my own system, but I think complete products are a good bet. But again, this is not important here.)</p> <p>The use case is this: </p> <p>- <strong>Data storage of, and LAN access to</strong> critical personal files which means photos, files and so on.<br/> - <strong>Media storage and server</strong> for convenient viewing of films and TV series (Plex or similar).<br/> - <strong>Personal/family cloud service</strong> for easy file syncing/uploading across the user\u2019s own devices, and occasional external sharing of files. The cloud is a bonus feature if it is feasible and secure to do. I\u2019m still unsure how to implement it, Nextcloud is often sug",
        "id": 3099180,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt1n46/babbys_first_homeserver_plans_safe_and_holistic",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Babby\u2019s first homeserver plans: safe and holistic long-term data storage plan, feedback please",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/theoldgaming",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T12:47:23.930494+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T12:36:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Alright so let me start this by saying that where-ever i look i see that MicroSDs are not reliable at all or less reliable than any other storage media, which im pretty sure is true. </p> <p>I&#39;ve done a lot of research on the topic and do know about the NAND technologies being different (SLC/MLC back in ~2012 to TLC and QLC in 2024) and the differences of reliability of those, differences in Error Correction (BCH, LPDC), controllers, channels etc.<br/> But all i managed to get is theoretical or manufacturer stated data or TBW&#39;s which tell me only the <strong>theoretical</strong> reliability not the practical one, i also don&#39;t have the time to test those MicroSDs (Cause doing genuine testing for long term reliability would logically take years) </p> <p>On the flip side i had <strong>older</strong> MicroSDs survive over a decade with only minor corruption and hence my questions:<br/> How reliable long-term are modern, <strong>High end</stron",
        "id": 3096805,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lt06j9/question_about_reliability_of_modern_highend",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about reliability of modern High-end MicroSDs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Echo8620",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T12:47:23.721203+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T12:25:59+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lszzhr/foil_in_seagate_expansion_drive/\"> <img src=\"https://preview.redd.it/08vutv1ky8bf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a55207495c13575338f1368f7af1fe288e3c36fc\" alt=\"Foil in Seagate Expansion drive??\" title=\"Foil in Seagate Expansion drive??\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>My new Seagate Expansion drive has what looks like a wad of foil at the back. Hard to see, this is the best pic I could get. Anyone else have the same with this drive? Looks like it would block airflow.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Echo8620\"> /u/Echo8620 </a> <br/> <span><a href=\"https://i.redd.it/08vutv1ky8bf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lszzhr/foil_in_seagate_expansion_drive/\">[comments]</a></span> </td></tr></table>",
        "id": 3096804,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lszzhr/foil_in_seagate_expansion_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/08vutv1ky8bf1.jpeg?width=640&crop=smart&auto=webp&s=a55207495c13575338f1368f7af1fe288e3c36fc",
        "title": "Foil in Seagate Expansion drive??",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HonestiSwear",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T10:37:22.554746+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T10:00:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there</p> <p>A previous partner of mine has received a message from an Instagram burner account, attempting a smear campaign against me.</p> <p>I\u2019m interested in finding out some more information about this account. Currently I only have the account names. </p> <p>Iv tried the way back machine and other archive websites, but they don\u2019t seem to work very well.</p> <p>Any recommendations of how best to move forward would be greatly appreciated.</p> <p>Cheers</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HonestiSwear\"> /u/HonestiSwear </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsxmle/deleted_social_media_account_abuse/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsxmle/deleted_social_media_account_abuse/\">[comments]</a></span>",
        "id": 3096231,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsxmle/deleted_social_media_account_abuse",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deleted social media account abuse",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/armavirvmqvecano",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T10:37:21.999865+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T09:50:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, hopefully this is the right place to ask this question, and I&#39;m sorry if I phrase it poorly, I&#39;m not entirely familiar with the correct terminology for this stuff. </p> <p>I am trying to save a dreamwidth page to the wayback machine, however this page has a &#39;preliminary&#39; page (not sure the correct term for this) that says &quot;disgression advised&quot;, and on which you have to click &quot;yes I want to view this content&quot; in order to see the content of the page proper. The problem is that this is the page that shows up on the wayback machine when you try to save the link, and when you try to click the &quot;yes I want to view this content&quot; button there (in the way back machine), it gives you an error because it has not saved beyond this point when it saved the URL (as far as I can tell?). </p> <p>I have read threads for similar issues discussing Ao3&#39;s &quot;I agree to see adult content&quot; button, but in that ca",
        "id": 3096229,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsxh0y/how_to_archive_a_dreamwidth_page_with_a_confirm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to archive a Dreamwidth page with a \"confirm you want to view this content\" barrier on the WayBack machine",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Expensive-Award1965",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T10:37:22.166969+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T09:49:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>as far as i can tell i&#39;ll have to capture the whole thing from screen grab.</p> <p>i usually use video downloadhelper on firefox but it&#39;s not working. i have a ticket i just want to watch it later. help!</p> <p>looks like they&#39;re using theoplayer on <a href=\"http://backtothebeginning.com\">backtothebeginning.com</a> from the element classes but can&#39;t find any workarounds</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Expensive-Award1965\"> /u/Expensive-Award1965 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsxgnj/how_can_i_download_the_back_to_the_beginning_live/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsxgnj/how_can_i_download_the_back_to_the_beginning_live/\">[comments]</a></span>",
        "id": 3096230,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsxgnj/how_can_i_download_the_back_to_the_beginning_live",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "how can i download the back to the beginning live stream",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TipmanTips",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T10:37:21.827147+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T09:35:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to archive all my old concert footage and get them on a hard drive rather then have dvds everywhere. Quite a few of them are scratched and my laptop just isn\u2019t able to rip them. </p> <p>Any recommendations as I\u2019m new to this? Thankyou </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TipmanTips\"> /u/TipmanTips </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsx9d3/any_recommendations_for_a_external_disk_reader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsx9d3/any_recommendations_for_a_external_disk_reader/\">[comments]</a></span>",
        "id": 3096228,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsx9d3/any_recommendations_for_a_external_disk_reader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any recommendations for a external disk reader for old scratched dvd-r?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Slowmadism",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T09:32:50.631851+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T09:27:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsx5oq/restoring_a_backblaze_storage_pod_original/\"> <img src=\"https://preview.redd.it/6einaw8038bf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e4c1f3fe954d4e170a12af244e39b2e235a87b5\" alt=\"Restoring a Backblaze Storage Pod (original)\" title=\"Restoring a Backblaze Storage Pod (original)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I\u2019ve just been given an old Backblaze Pod, and would like to restore it back to a functional state.</p> <p>The motherboard is missing, but everything else is complete and working, including the power supplies.</p> <p>Can I simply drop in (pretty much) any motherboard or are there some gotchas I should be aware of? There are 3x storage controller boards, and the case appears to be ATX compatible, so in theory anything ATX compatible with at least 3x full size PCI-E slots would work.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Slo",
        "id": 3095982,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsx5oq/restoring_a_backblaze_storage_pod_original",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/6einaw8038bf1.jpeg?width=640&crop=smart&auto=webp&s=9e4c1f3fe954d4e170a12af244e39b2e235a87b5",
        "title": "Restoring a Backblaze Storage Pod (original)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Obvious_Archer2628",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T09:32:50.986514+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T09:20:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Guys i have been searching for free paid courses on telegram and found and an user providing the huge content , for proof he allowed me to join his private channel where all the huge content was there valid but then he asked for payment of it. so i wanted any technique or idea that would give me accesss or stored the all content before he remove me from the private channel within 10 min also it is not possible to me download the whole content within 10 minutes please help guys!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Obvious_Archer2628\"> /u/Obvious_Archer2628 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsx1od/how_to_access_private_telegram_channel_content/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lsx1od/how_to_access_private_telegram_channel_content/\">[comments]</a></span>",
        "id": 3095983,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsx1od/how_to_access_private_telegram_channel_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HOW TO ACCESS PRIVATE TELEGRAM CHANNEL CONTENT!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mlp2019",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T20:22:56.969487+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T05:34:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>TL;DR: Looking for a reliable 4-bay NAS to consolidate old drives, back up multiple devices, and reduce reliance on Google Drive. Considering Synology DS425+. Open to alternatives.</p> <p>Looking to take advantage of Prime Day (Australia) to finally set up a home NAS. I\u2019m fairly tech-savvy but new to NAS/RAID \u2014 want a plug-and-play option, not building from scratch (yet).</p> <p>I currently use iDrive to back up my main laptop, but want to set up a more robust and centralised solution that fits a proper 3-2-1 backup strategy.</p> <p>Primary goals:</p> <p>Backup and sort data from 8\u201312 old external drives (a mix of personal photos and archived family files)</p> <p>Centralised storage/backup for a 2-person household (around 7 personal devices)</p> <p>Reduce dependence on Google Drive and cloud subscription bloat</p> <p>Secondary/future goals:</p> <p>Possibly run Plex (no 4K for now, mostly archived TV/movie rips from the old drives)</p> <p>Might play wi",
        "id": 3099181,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lstnt7/prime_day_nas_purchase_incoming_ds425_or_better",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Prime Day NAS Purchase Incoming \u2013 DS425+ or Better?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Irinescence",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T04:08:09.327284+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T03:39:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m not really a data hoarder but I thought you all might know more about HDDs than the average bear. </p> <p>I bought a 6tb refurb helium 7200 rpm drive from goharddrive to put in my new pc, just to have some cheap secondary storage (main is a 2tb m.2 ssd partitioned into .3/1.6). I was thinking I&#39;d only use the spinny drive rarely to keep the ssd from filling up or if I downloaded a car or whatever. And that I&#39;d set it to power down after 5 minutes and I&#39;d rarely hear it.</p> <p>But although it&#39;s not loud I hear it always. I looked in process monitor and although there&#39;s nothing on the drive yet, perfmon.exe and powershell.exe are checking it constantly. (maybe svchost.exe and chrome.exe and nvidia overlay too, but I don&#39;t understand what process monitor is telling me). I got windows defender to leave it alone.</p> <p>I can&#39;t figure out how to tell windows to chill out about my E drive and let it go to sleep. The ",
        "id": 3094968,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lsrr6t/new_build_trying_to_figure_out_how_to_allow_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New Build: Trying to figure out how to allow a secondary HDD to spin down and go to sleep.",
        "vote": 0
    }
]