[
    {
        "age": null,
        "album": "",
        "author": "/u/Turd_Burgla",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T18:52:38.131918+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T18:47:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was just looking for some help in finding a proxy. What kind of proxy works best if i&#39;m simply looking to avoid the million captchas and IP site blocks i get everyday. My main priority is privacy and anonymity.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Turd_Burgla\"> /u/Turd_Burgla </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt8rlb/proxy_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt8rlb/proxy_help/\">[comments]</a></span>",
        "id": 3098743,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lt8rlb/proxy_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Proxy Help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Illustrious-Gate3426",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T18:52:38.299692+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T18:41:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone have a scraper that just collects documentation for coding and project packages and libraries on GitHub?</p> <p>I&#39;m looking to start filling some databases with docs and API usage, to improve my AI assistant with coding.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Illustrious-Gate3426\"> /u/Illustrious-Gate3426 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt8lws/github_docs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt8lws/github_docs/\">[comments]</a></span>",
        "id": 3098744,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lt8lws/github_docs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "GitHub docs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/truthseekinfinite",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T18:52:38.465121+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T16:05:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Want to create a product that I can package and sell using Amazon public data.</p> <p>Questions: </p> <p>\u2022 Is it legal to scrape Amazon? \u2022 How would one collect historical data, 1-5 years? \u2022 what\u2019s the best way to do this that wouldn\u2019t bite me in the ass legally? </p> <p>Thanks. Sorry if these are obvious, I\u2019m new to scraping. I can build scraper, had started scraping Amazon, but didn\u2019t realise even public basic data was so legally strict. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/truthseekinfinite\"> /u/truthseekinfinite </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt4uj5/helpadvice_regarding_amazon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt4uj5/helpadvice_regarding_amazon/\">[comments]</a></span>",
        "id": 3098745,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lt4uj5/helpadvice_regarding_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help/advice regarding Amazon",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Theredeemer08",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T16:42:33.498833+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T15:50:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi All,</p> <p>I am scraping using twikit and need some help. It is a very well documented library but I am unsure about a few things / have run into some difficulties. </p> <p>For all the twikit users out there, I was wondering how you deal with rate limits and so on? How do you scale basically? As an example, I get hit with 429s (rate limits) when I scrape get replies from a tweet even once every 30s (well under the documented rate limit time). </p> <p>I am wondering how other people are using this reliably or is this just part of the nature of using twikit?</p> <p>I appreciate any help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Theredeemer08\"> /u/Theredeemer08 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt4hpg/twikit_help_calling_all_twikit_users_how_do_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt4hpg/twikit_help_calling_all_t",
        "id": 3098068,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lt4hpg/twikit_help_calling_all_twikit_users_how_do_you",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Twikit help: Calling all twikit users, how do you use it reliably?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Big_Rooster4841",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T15:38:35.824546+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T15:24:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;ve been thinking about saving bandwidth on my proxy and was wondering if this was possible.</p> <p>I use playwright for reference.</p> <p>1) Visit the website with a proxy (this should grant me cookies that I can capture?)</p> <p>2) Capture and remove proxies for network requests that don&#39;t really need a proxy.</p> <p>Is this doable? I couldn&#39;t find a way to do this using network request capturing in playwright <a href=\"https://playwright.dev/docs/network\">https://playwright.dev/docs/network</a></p> <p>Is there an alternative method to do something like this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Big_Rooster4841\"> /u/Big_Rooster4841 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt3utx/selectively_attaching_proxies_to_certain_network/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lt3utx/selectively_attaching_proxies_to_",
        "id": 3097680,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lt3utx/selectively_attaching_proxies_to_certain_network",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\"selectively\" attaching proxies to certain network requests.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Upper_Improvement533",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T12:22:34.285533+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T11:58:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been working on multiple web scraping projects for various clients and am now trying to implement a more structured and scalable workflow using my current tech stack.</p> <p>I&#39;m curious about what your end to end setup looks like. From scraping to processing, do you use notebooks or scripts? What platform are you running on? How do you organize your code and data? Are you using tools like dbt for data validation? Do you run everything locally or in the cloud?, etc.</p> <p>As someone kinda early in this space, here&#39;s what my current workflow looks like:</p> <ul> <li>Python notebooks, split into categories for scraping, processing, and exporting.</li> <li>Mostly selenium, as I deal with JS-heavy pages.</li> <li>DuckDB for handling large datasets, though it&#39;s still embedded within notebooks.</li> <li>Data is exported to Excel files (per client request), though I\u2019m hoping to shift toward a proper database or data warehouse setup.</li>",
        "id": 3096671,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lszh79/what_is_your_workflow_for_web_scraping_at_scale",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is your workflow for web scraping at scale?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/karatewaffles",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-07-06T01:32:31.838272+00:00",
        "date_dead_since": null,
        "date_published": "2025-07-06T00:31:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I decided to give myself a project to learn some coding / web scraping. I have some familiarity with python, regex, bash, command line ... however they&#39;re not tools I use daily, and re-familiarise myself with once or twice a year as a random project pops up. So I was hoping to get some advice as to whether I&#39;m headed in the right direction here.</p> <p>The project is to scrape the entries on one of YouTube&#39;s free movies pages - extracting movie title, year, genre, runtime, thumbnail, and link - and end up with a spreadsheet containing this data.</p> <p>My plan of attack so far has been:</p> <ul> <li>fetch the html</li> <li>figure out the unique, repeated patterns that identify each piece of data I&#39;m trying to extract</li> <li>build a regex pattern to match for each element</li> <li>get these into an array</li> <li>save the array as a .csv file</li> </ul> <p>Where I&#39;ve gotten to is:</p> <ul> <li>I&#39;ve learned that the html",
        "id": 3094605,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lsoddf/scraping_noob_advice_youtube_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "scraping noob advice (YouTube project)",
        "vote": 0
    }
]