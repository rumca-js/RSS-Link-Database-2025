[
    {
        "age": null,
        "album": "",
        "author": "/u/labubuche",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T20:07:50.683966+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T19:59:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all,</p> <p>So I have a python script that manages to scrape (via Selenium/Selenium-wire) a e-commerce page protected by datadome, when using my own IP address (for testing only)</p> <p>When I configure selenium to go through a residential IP proxy (ProxyEmpire), it gets flagged right away by the website. I know the residential IP proxy is not the problem, as manually using proxyEmpire chrome extension configure with the residential IP I don&#39;t get blocked.</p> <p>Could it be possible that selenium with proxy gets flagged somehow, even though it&#39;s working fine without proxy ? </p> <p>I can manage to reach lots of differents e-commerce website with Selenium+Proxy, so I believe this particular datadome protected site can find out when Selenium is being used with a proxy.</p> <p>Any idea to point me towards the right direction ?</p> <p>Thanks all</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/labubuche",
        "id": 4139227,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p42nae/selenium_gets_detected_only_when_used_with_proxies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Selenium gets detected only when used with proxies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Much-Journalist3128",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T19:07:15.585010+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T18:38:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve made a post about this issue before, I think I posted it yesterday.</p> <p>Anyway it&#39;s Saturday and my code is the exact same (except for the cron scheduling logic because I originally wrote it for Windows and the github hosted runners run Ubuntu so I had to change it accordingly), line for line, method for method, etc, the only difference is that it&#39;s the weekend now.</p> <p>This is a grocery delivery webshop. They do operate on weekends as well, for them it&#39;s normal working hours M-S. </p> <p>I&#39;ve noticed that while M-F my github &quot;version&quot; bot gets blocked at least 80-90% of the time (so basically unless I change this, it&#39;s futile to run it via github actions), today it&#39;s Saturday and out of 20 times it&#39;s run today, it only got blocked 2x.</p> <p>Is this normal for bot detection systems in general? Because I don&#39;t think (might be wrong) that their website traffic is considerably smaller on the weeke",
        "id": 4138856,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p40p67/akamai_not_blocking_or_barely_blocking_my_bot_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AKAMAI not blocking or BARELY blocking my bot on the weekends?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/straightup920",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T17:04:08.686934+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T16:45:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Theoretically if someone wants to scrape a website that specifically forbids scraping data are there any proxies that would allow you to scrape regardless and not try to blacklist you? I know some bigger companies will block you from their service if you use it in that way</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/straightup920\"> /u/straightup920 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p3xum8/what_proxies_have_loose_tos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p3xum8/what_proxies_have_loose_tos/\">[comments]</a></span>",
        "id": 4138136,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p3xum8/what_proxies_have_loose_tos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What proxies have \u201cloose\u201d tos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Fickle-Distance-7031",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T17:04:08.987333+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T16:31:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m looking for patterns or best practices for building low-maintenance scrapers. Right now it feels like every time a website updates its layout or class names, the scraper dies and I have to patch selectors again.</p> <p>Are there reliable techniques people use? (Avoiding fragile class names, relying on structure, fuzzy matching, ML extraction, etc.?) Any good guides on this?</p> <p>Also curious how <strong>companies</strong> handle this. Some services depend heavily on scraping (e.g., flight trackers like Kiwi). Do they just have engineers on call to fix things instantly? Or do they have tooling to detect breakages, diff layouts, fallback extractors, etc.?</p> <p>Basically: <strong>how do you turn scrapers into actual reliable infrastructure instead of something constantly on fire?</strong></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fickle-Distance-7031\"> /u/Fickle-Distance-7031 </a> <br/> <span><a href=\"",
        "id": 4138137,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p3xi6d/how_do_companies_keep_important_scrapers_reliable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do companies keep important scrapers reliable?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MentalAssumption1498",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T16:02:59.218392+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T15:19:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I feel like a reddit webscraper can now be relevant since the reddit api is not accessible that easy anymore (<a href=\"https://www.reddit.com/r/redditdev/comments/1oug31u/introducing%5C_the%5C_responsible%5C_builder%5C_policy%5C_new/?share%5C_id=wmzZcSYT7IMuW5G-G5-HA&amp;utm%5C_medium=ios%5C_app&amp;utm%5C_name=ioscss&amp;utm%5C_source=share&amp;utm%5C_term=1\">https://www.reddit.com/r/redditdev/comments/1oug31u/introducing\\_the\\_responsible\\_builder\\_policy\\_new/?share\\_id=wmzZcSYT7IMuW5G-G5-HA&amp;utm\\_medium=ios\\_app&amp;utm\\_name=ioscss&amp;utm\\_source=share&amp;utm\\_term=1</a>)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MentalAssumption1498\"> /u/MentalAssumption1498 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p3vrej/is_a_reddit_webscraper_relevant_now/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p3vrej/is_a_reddit_webscraper_relevant",
        "id": 4137659,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p3vrej/is_a_reddit_webscraper_relevant_now",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is a reddit webscraper relevant now?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fingerprinthater",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T13:00:49.926483+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T11:58:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It might be funny for some to see someone who fails miserably at everything.</p> <p>First off, I have to say that I&#39;m a complete noob when it comes to programming, and I&#39;m working my way through all these topics, mostly with the help of AI and Reddit. I&#39;ve had a side project for a few years now where I create several hundred multi-accounts per week.</p> <p>Anyway, for about six months now, I&#39;ve been constantly running into problems/deletion waves and can&#39;t seem to get a &quot;secure&quot; system at all.</p> <p>Even without automation, the whole thing goes wrong. Currently, I&#39;m trying to do it manually and focusing on the setup. I used to use many multiloginbrowsers or antidetect browsers with scripts together, but nothing works if you scale just a bit up.</p> <p>The only thing that works for me, but is far too cumbersome, is a VM-based system. Of course, it&#39;s not possible to generate a high number of accounts per day with t",
        "id": 4136725,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p3ribd/see_me_suffering_at_multiaccounting",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "see me suffering at multiaccounting",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Wicked_Python",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T06:40:11.960359+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T06:03:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I\u2019m exploring a project idea and want feedback:</p> <p><strong>Idea:</strong></p> <ul> <li>Collect data from <strong>SEC filings</strong> (10\u2011Ks, 8\u2011Ks, etc.) as well as other <strong>public records</strong> on companies\u2019 real estate and assets worldwide (land, buildings, facilities).</li> <li>Extract structured info (addresses, type, size, year) and geocode it for a <strong>dynamic, interactive map</strong>.</li> <li>Use a pipeline (possibly with LLMs) to clean, organize, and update the data as new records appear.</li> <li>Provide references to sources for verification.</li> </ul> <p><strong>Questions:</strong></p> <ul> <li>Where can I reliably get this kind of data in a <strong>standardized format</strong>?</li> <li>Are there APIs, databases, or public sources that track corporate properties beyond SEC filings?</li> <li>Any advice on building a system that can keep this data <strong>ever-evolving</strong> and accurate?</li> </ul> </div>",
        "id": 4135162,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p3ltus/mapping_companies_properties_from_sec_filings",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mapping Companies\u2019 Properties from SEC Filings & Public Records, Help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MathematicianNice290",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-22T11:55:16.080060+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-22T04:59:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As I was working on a digital marketing project, I came across webscraping and was astounding by the potential webscraping has to my work. I have compiled social media urls for 42 businesses in the same industry and listed them in a google sheet. I&#39;m looking for a tool that can take the url and source data such as total likes, shares, comments, audience demographic, etc. from the major social media apps. Any info would be very helpful!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MathematicianNice290\"> /u/MathematicianNice290 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p3kor2/college_student_new_to_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p3kor2/college_student_new_to_scraping/\">[comments]</a></span>",
        "id": 4136376,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p3kor2/college_student_new_to_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "College Student New to Scraping",
        "vote": 0
    }
]