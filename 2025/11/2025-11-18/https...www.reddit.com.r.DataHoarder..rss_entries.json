[
    {
        "age": null,
        "album": "",
        "author": "/u/tutebo88",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T23:11:10.725025+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T22:58:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, </p> <p>I have an old NAS (2-bay) and a few leftover old hard drives lying around, so I intend to build an additional remote backup at a remote location (over the internet). </p> <p>Redundancy (RAID 1) isn&#39;t really needed for this second-level backup, and OTOH I need to somehow pool the old drives into a bigger volume to achieve a sufficiently large volume. This leaves JBOD and RAID 0 as choices. While I have a pretty clear picture how RAID 0 works, JBOD is a bit of a gray area for me.</p> <p>The main disadvantage of JBOD is that it would probably be a bit slower than RAID 0, but this is not an issue in my case. I couldn&#39;t make use of bigger speeds over this connection anyway.</p> <p>Possible advantages of JBOD I could figure out are: </p> <p>1) You can combine different types of drives (size, speed, etc).</p> <p>2) While in RAID 0 all drives are spinning concurrently all the time, in JBOD there may be only one drive in use at a time. ",
        "id": 4104413,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0qu2i/advantagesdisadvantages_of_jbod_vs_raid_0_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advantages/disadvantages of JBOD vs RAID 0 (for additional backup)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/brain-power",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T21:09:53.679011+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T21:00:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello!</p> <p>Might be an oddball question here... but here goes:</p> <p>Question 1:</p> <p>**Is there a way to control how much time the user has to connect the second of two RAID 1 external drives before MacOS connects/shows the first drive and then inevitably rebuilds the second drive upon connection?**</p> <p>The scenario: I have qty: 2 external HDDs. I have them software configured (using Disk Utility) to RAID1. I eject them both. Disconnect both drives. Then reconnect drive 1 of 2. *wait X number of seconds*. Then reconnect drive 2 of 2.</p> <p>My cursory tests conclude the following: If you reconnect drive 2 of 2 before X seconds, the RAID disk pops up as expected - no rebuilding needed. If you wait longer than X seconds before connecting drive 2 of 2, drive 1 of 2 pops up as usable. Then, if you connect drive 2 of 2 any time after, the RAID begins to rebuild itself (I had it set to auto-rebuild - not sure if that matters). X seems to be about ",
        "id": 4103562,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0nsbi/macos_raid1_drive_connection_timing_requirements",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "MacOS RAID1 - drive connection timing requirements (qty: 2 external drives)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cnidaria45",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T21:09:53.280777+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T20:26:55+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0mwnm/icebergchartscom_is_shutting_down/\"> <img src=\"https://preview.redd.it/fnbgidhqc22g1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=498d50652630abe1de03a5832448c9a0c7f0d505\" alt=\"IcebergCharts.com is Shutting Down\" title=\"IcebergCharts.com is Shutting Down\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"http://IcebergCharts.com\">IcebergCharts.com</a> is shutting down, with editing planning to go down in the next month and the site itself planned to shut down sometime in 2026.</p> <p>Full Explanation from Coda the Admin: <a href=\"https://icebergcharts.com/\">https://icebergcharts.com/</a></p> <p>Original Reddit Thread: <a href=\"https://www.reddit.com/r/IcebergCharts/comments/1owyr3l/no_icebergchartscom_is_apparently_gonna_be_shut/\">https://www.reddit.com/r/IcebergCharts/comments/1owyr3l/no_icebergchartscom_is_apparently_gonna_be_shut/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a ",
        "id": 4103561,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0mwnm/icebergchartscom_is_shutting_down",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/fnbgidhqc22g1.jpeg?width=640&crop=smart&auto=webp&s=498d50652630abe1de03a5832448c9a0c7f0d505",
        "title": "IcebergCharts.com is Shutting Down",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/UrStockDaddy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T20:09:21.819955+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T20:02:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, </p> <p>Black Friday coming up - anyone know any good large cloud storage solutions?</p> <p>Sync.com has a promo for $45 a month</p> <p>Unlimited drive at $30 a month</p> <p>Any other solutions and what\u2019s the best out there?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UrStockDaddy\"> /u/UrStockDaddy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0m9d1/cloud_storage_solutions/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0m9d1/cloud_storage_solutions/\">[comments]</a></span>",
        "id": 4103057,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0m9d1/cloud_storage_solutions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cloud Storage Solutions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/alex97in",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T19:08:40.968449+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T18:55:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I&#39;m not sure if this is the best sub to ask. I recently had the chance to try a Blu-ray and was impressed by the quality (both audio and video).</p> <p>I&#39;d like to start a collection while spending as little as possible. What would be the best option? Store everything on various hard drives? Burn discs and maybe buy cases and print covers? Buy the original Blu-rays directly? (I&#39;ve seen that some used movies are cheap in my area, from 3 to 5 euros, but some out-of-print ones can be very expensive).</p> <p>If it helps, I don&#39;t have 4K available at the moment, although I would like to buy a 4K TV and a decent audio system in the future. Thanks in advance to anyone who can help me.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/alex97in\"> /u/alex97in </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0kgy8/i_just_discovered_bluray_remux_files_what_is_the/\">[link]</",
        "id": 4102562,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0kgy8/i_just_discovered_bluray_remux_files_what_is_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I just discovered Bluray Remux files. What is the most cheapest way to store them?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Original-Tackle988",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T19:08:41.656739+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T18:53:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am thinking on what my backup strategy should be for all the \u201cLinux ISOs\u201d that I have. Currently, the only thing I backup is to sync to cloud my configurations, such as docker. Since my mentality is \u2026 from there, if I get data failure, the internet is my backup and I can always redownload everything providing I know where to find them.</p> <p>I\u2019m curious if people feel the same? Or when do you decide a specific ISO deserves a backup?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Original-Tackle988\"> /u/Original-Tackle988 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0kfho/do_you_back_up_your_linux_isos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0kfho/do_you_back_up_your_linux_isos/\">[comments]</a></span>",
        "id": 4102564,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0kfho/do_you_back_up_your_linux_isos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you back up your \u201cLinux ISOs\u201d?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dioxitanium",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T19:08:41.822018+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T18:28:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oi1vhq/halloween_giveaway_share_your_halloween_memory/\">Halloween Giveaway</a> ended over a week ago, but I haven\u2019t seen an announcement - nothing from <a href=\"/u/UgreenNASync\">/u/UgreenNASync</a> on their post or in this subreddit, nor on their Discord server that they asked us to join.</p> <p>Have the winners been contacted yet?</p> <p>I\u2019d have expected to see an announcement from the hosts within a day or two, given that it was a random selection rather than a competition that needed to be judged.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dioxitanium\"> /u/Dioxitanium </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0jqke/meta_has_the_halloween_giveaway_been_resolved/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0jqke/meta_has_the_halloween_giveaway_been_resolved/",
        "id": 4102565,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0jqke/meta_has_the_halloween_giveaway_been_resolved",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[Meta] Has the Halloween giveaway been resolved?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/shimoheihei2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T19:08:41.405245+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T18:17:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>To change a bit from the usual &#39;what&#39; do you hoard, I was curious &#39;how much&#39; data everyone hoards on a regular basis. So the question is:</p> <p>How much data (in GB or TB) do you typically save/archive per week?</p> <p>Feel free to add comments on whether it&#39;s mostly video, audio, software or what if you&#39;d like. For my part, I&#39;ll start with saying I&#39;m on the lighter side with around 100 GB per week on average, mostly from YouTube.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shimoheihei2\"> /u/shimoheihei2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0jg8x/how_much_do_you_hoard/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0jg8x/how_much_do_you_hoard/\">[comments]</a></span>",
        "id": 4102563,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0jg8x/how_much_do_you_hoard",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How much do you hoard?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Aka_Erus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T18:08:07.900130+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T18:01:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been running a server for some time, it is the backup of my main computer, the laptop and phone as well. So photos work related stuff.<br/> It is my media server (jellyfin) as well and I&#39;m seeding from it (transmission).<br/> Everything runs in dockers .<br/> I have a 12 TB where everything is mostly backed up stuff.<br/> The main drive is 512GB ssd and got all the dockers running.<br/> And then the main media is on two 18 TB drives<br/> I got the first 18 TB got it full then instead of having a plan, there was a sale and got another one to quickly divide my media collection into the two drives. </p> <p>That created a mess, especially seeding with transmission as some stuff is not on the same volume so no hard link possible. Now both 18 TB a chuck full. </p> <p>My plan at the start was when I got enough to put into it, I&#39;ll get 5 drives to make a zfs vdev ( 2 drives parity ?) and like that I can grow whenever I want with more drive as",
        "id": 4101938,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0j0pk/help_getting_everything_in_order",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[help] Getting everything in order",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AlexStoldt",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T18:08:07.372709+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T17:27:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I am searching the web for small hard cases like this:<br/> <a href=\"https://www.thomann.de/de/peli_m40_black.htm\">https://www.thomann.de/de/peli_m40_black.htm</a><br/> which you can lock with a small combination-lock.<br/> Preferably cheaper than the peli-case and shippable to the EU/Germany.<br/> Any recommendations?<br/> Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AlexStoldt\"> /u/AlexStoldt </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0i2bs/searching_for_small_lockable_hardcases_for_small/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0i2bs/searching_for_small_lockable_hardcases_for_small/\">[comments]</a></span>",
        "id": 4101937,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0i2bs/searching_for_small_lockable_hardcases_for_small",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Searching for small lockable hardcases for small SSD's",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/clemwo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T18:08:06.623376+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T17:23:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,<br/> it&#39;s quite funny. Reading stories on here of people backing up whole youtube channels, I often think that people are overly paranoid of stuff disappearing. Until it finally happens to a channel you personally like.</p> <p>That being said, the german youtube channel <a href=\"https://www.youtube.com/@zqnce/videos\">zqnce</a> recently deleted all videos of the famous series &#39;Shore, Stein, Papier&#39; (translates to Heroin, Stone, Paper).</p> <p>The series was quite simple but amazing. The main protagonist &#39;Sick&#39;, sitting at a kitchen table, retold his whole life beginning in his youth where he started consuming Heroin, Cocaine and various other drugs and spiralled deeper into addiction and crime to finance his consumption. The series was amazing, very emotional, highly entertaining and a warning against consuming hard drugs. It&#39;s been quite a few years since I last watched it but it was quite famous in Germany, many o",
        "id": 4101936,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0hz1w/german_youtube_channel_zqnce_deleted_all_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "German youtube channel 'zqnce' deleted all videos of famous biographical web series 'Shore, Stein, Papier' where ex-addict told true stories of 25 years of drug addiction and crime",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/moshgrrrl",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T18:08:09.061991+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T17:21:19+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0hwo1/guys/\"> <img src=\"https://preview.redd.it/ueyrtlcpu12g1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aeae2e373e42171f0067851de3ae4ffa365f7178\" alt=\"Guys\u2026\" title=\"Guys\u2026\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I\u2019m feeling the consequences of my actions right about now</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/moshgrrrl\"> /u/moshgrrrl </a> <br/> <span><a href=\"https://i.redd.it/ueyrtlcpu12g1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0hwo1/guys/\">[comments]</a></span> </td></tr></table>",
        "id": 4101939,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0hwo1/guys",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/ueyrtlcpu12g1.jpeg?width=640&crop=smart&auto=webp&s=aeae2e373e42171f0067851de3ae4ffa365f7178",
        "title": "Guys\u2026",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/patermortis",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T17:07:36.476148+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T17:00:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am wanting to build out a media server. I have 8TB mirrored. Will this be enough, or should I consider larger drives or a different raid format?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/patermortis\"> /u/patermortis </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0hbn8/media_server_storage_needs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0hbn8/media_server_storage_needs/\">[comments]</a></span>",
        "id": 4101350,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0hbn8/media_server_storage_needs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Media Server: Storage Needs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Carlyone",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T17:07:35.850620+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T16:59:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0hb5l/stacks_an_annas_archive_download_manager/\"> <img src=\"https://external-preview.redd.it/MDNkZzd2aWJxMTJnMULLQhQehhO8gF6UbLaMF5yu3AjeDSpmutUCzCnFr99d.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4ce8ebbafb99a6c90678bedabb1c26afdeeed24\" alt=\"Stacks - An Anna's Archive Download Manager\" title=\"Stacks - An Anna's Archive Download Manager\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I finally got tired of all the clicking when downloading (click, wait, click again, and then click again if you&#39;re out of fast downloads) plus the constant babysitting between files. JDownloader kind of helped, but honestly it caused me just as many headaches as it solved. So... I built something better.</p> <p>Introducing Stacks: a proper download manager for Anna&#39;s Archive.</p> <p>It&#39;s made of two pieces: a lightweight Docker service and a Tampermonkey userscript. The script adds a &quot;Download&quot; butt",
        "id": 4101349,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0hb5l/stacks_an_annas_archive_download_manager",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/MDNkZzd2aWJxMTJnMULLQhQehhO8gF6UbLaMF5yu3AjeDSpmutUCzCnFr99d.png?width=640&crop=smart&auto=webp&s=f4ce8ebbafb99a6c90678bedabb1c26afdeeed24",
        "title": "Stacks - An Anna's Archive Download Manager",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EvilAlbinoid",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T17:07:36.719318+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T16:52:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0h4d2/having_trouble_wrapping_my_head_around_storage/\"> <img src=\"https://b.thumbs.redditmedia.com/YK5Q3iSmC6T8VmQekiTiTTTm9zznEc9yTpLfKv8xKqU.jpg\" alt=\"Having trouble wrapping my head around Storage Spaces/columns and extending a data drive\" title=\"Having trouble wrapping my head around Storage Spaces/columns and extending a data drive\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/l29vybt1n12g1.png?width=1577&amp;format=png&amp;auto=webp&amp;s=ed6cfe07fb900403acbcf63e4909f4d9f49913c4\">https://preview.redd.it/l29vybt1n12g1.png?width=1577&amp;format=png&amp;auto=webp&amp;s=ed6cfe07fb900403acbcf63e4909f4d9f49913c4</a></p> <p><a href=\"https://preview.redd.it/5xx2kd0kn12g1.png?width=378&amp;format=png&amp;auto=webp&amp;s=34f4525966d72e9b19e29fc46eb5bd21de1023cd\">https://preview.redd.it/5xx2kd0kn12g1.png?width=378&amp;format=png&amp;auto=webp&amp;s=34f4525966d72e9b19e29fc46eb5b",
        "id": 4101351,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0h4d2/having_trouble_wrapping_my_head_around_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/YK5Q3iSmC6T8VmQekiTiTTTm9zznEc9yTpLfKv8xKqU.jpg",
        "title": "Having trouble wrapping my head around Storage Spaces/columns and extending a data drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Renotron",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T17:07:36.890168+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T16:38:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0gq5y/ssd_to_ssd_speeds_over_sata_on_a_decent_computer/\"> <img src=\"https://b.thumbs.redditmedia.com/hLXa5Mu-2TZPZu4lRfb1NJsFJAAW5m1UQiJ5AOVrOxY.jpg\" alt=\"SSD to SSD speeds over SATA on a decent computer\" title=\"SSD to SSD speeds over SATA on a decent computer\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/8vfiegi3m12g1.png?width=526&amp;format=png&amp;auto=webp&amp;s=b335244b60cd9cc109ce1f7a69009b4ec5a9009a\">https://preview.redd.it/8vfiegi3m12g1.png?width=526&amp;format=png&amp;auto=webp&amp;s=b335244b60cd9cc109ce1f7a69009b4ec5a9009a</a></p> <p>Hello fellow hoarders. I love to tinker with hardware but I&#39;m too lazy to properly study all the rules and info about data transfers, my background is not IT or CompSci (I wish I was!) </p> <p>In my naivety, I thought my Samsung 8tb SSD to SSD transfer (QLC, but I guess that has nothing to do with it) of about 250 GB of videos o",
        "id": 4101352,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0gq5y/ssd_to_ssd_speeds_over_sata_on_a_decent_computer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/hLXa5Mu-2TZPZu4lRfb1NJsFJAAW5m1UQiJ5AOVrOxY.jpg",
        "title": "SSD to SSD speeds over SATA on a decent computer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Anxious_Ad909",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T15:06:21.982718+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T14:46:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I purchased a 28TB HDD and lost important files around 2 weeks later. I&#39;m wondering what my options could be to back up large amounts of data with relative reliability. I&#39;m a novice, but I usually learn fast. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Anxious_Ad909\"> /u/Anxious_Ad909 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0dt2w/reliable_options_for_20tb_of_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0dt2w/reliable_options_for_20tb_of_storage/\">[comments]</a></span>",
        "id": 4099854,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0dt2w/reliable_options_for_20tb_of_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Reliable Options For >20TB of Storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/3IIeu1qN638N",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T15:06:22.182319+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T14:32:54+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0dh04/can_i_connect_a_sff8087to_4_sff8482_to_sata_drive/\"> <img src=\"https://preview.redd.it/7uy281kb012g1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=75e2ba5b874aa6585ed58dc2f2389335141c5cbf\" alt=\"can I connect a SFF-8087to 4 SFF-8482 to SATA drive? The connector shape seems similar (except for the notch on the SATA drive) and the number of pins seems the same. Thanks\" title=\"can I connect a SFF-8087to 4 SFF-8482 to SATA drive? The connector shape seems similar (except for the notch on the SATA drive) and the number of pins seems the same. Thanks\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/3IIeu1qN638N\"> /u/3IIeu1qN638N </a> <br/> <span><a href=\"https://i.redd.it/7uy281kb012g1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0dh04/can_i_connect_a_sff8087to_4_sff8482_to_sata_drive/\">[comments]</a></span> </td></tr></table>",
        "id": 4099855,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0dh04/can_i_connect_a_sff8087to_4_sff8482_to_sata_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/7uy281kb012g1.png?width=320&crop=smart&auto=webp&s=75e2ba5b874aa6585ed58dc2f2389335141c5cbf",
        "title": "can I connect a SFF-8087to 4 SFF-8482 to SATA drive? The connector shape seems similar (except for the notch on the SATA drive) and the number of pins seems the same. Thanks",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jasonumd",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T14:05:47.488173+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T13:52:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0chlf/grateful_dead_jerry_garcia_music_archive_tagging/\"> <img src=\"https://external-preview.redd.it/PMNndZbTyyTzNxpKH1a6aqg2kCpC0kSkB3UHGih8y3w.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c0527a0ed311db015c87f35562b7d65a8617d3a\" alt=\"Grateful Dead Jerry Garcia Music Archive Tagging\" title=\"Grateful Dead Jerry Garcia Music Archive Tagging\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Was curious if this community would be interested in my music tagging effort I recently dialed in.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jasonumd\"> /u/jasonumd </a> <br/> <span><a href=\"https://www.reddit.com/r/JasonUMD/comments/1ocgc8p/grateful_dead_jerry_garcia_music_archive_tagging/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0chlf/grateful_dead_jerry_garcia_music_archive_tagging/\">[comments]</a></span> </td></tr></table>",
        "id": 4099362,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0chlf/grateful_dead_jerry_garcia_music_archive_tagging",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/PMNndZbTyyTzNxpKH1a6aqg2kCpC0kSkB3UHGih8y3w.png?width=640&crop=smart&auto=webp&s=2c0527a0ed311db015c87f35562b7d65a8617d3a",
        "title": "Grateful Dead Jerry Garcia Music Archive Tagging",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sufficient_Side_6836",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T14:05:47.821714+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T13:23:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, does anyone know a 4K downloader for Rednote? Most of what I\u2019ve tried kills the quality. Any recos? Tysm!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sufficient_Side_6836\"> /u/Sufficient_Side_6836 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0bt6z/4k_rednote_downloader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0bt6z/4k_rednote_downloader/\">[comments]</a></span>",
        "id": 4099363,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0bt6z/4k_rednote_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "4K Rednote Downloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/6_clover",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T16:06:51.987243+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T12:52:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi. As mentioned above, I am trying to download a website using httrack website copier (for example, this site <a href=\"http://www.asite.com\">www.asite.com</a>), but I am encountering some problems. These problems are mainly as follows:</p> <p>1) I&#39;m trying to configure the settings to download only sublinks starting with <a href=\"http://www.asite.com\">www.asite.com</a> (e.g., subdomains like <a href=\"http://www.asite.com/any/sub/link\">www.asite.com/any/sub/link</a>). However, I keep seeing another site in the list of links being downloaded (e.g., <a href=\"http://www.anothersite.com\">www.anothersite.com</a> is also being scanned and attempted to download). Or, the Facebook link on the <a href=\"http://www.asite.com\">www.asite.com</a> site is also counted as a sublink and is being attempted to be downloaded.</p> <p>2) Media (photos, videos, GIFs, etc.) on the <a href=\"http://www.asite.com\">www.asite.com</a> site may be pulled from another site (for ",
        "id": 4100640,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0b4qk/how_do_i_customize_the_settings_to_download_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I customize the settings to download a website with httrack website copier?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MADMADS1001",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T16:06:52.064950+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T12:34:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve recorded an audio/video podcast-style documentary and I have ADHD, so things got pretty messy. Id like to get an overview and dates to organize a timeline on whats copied and whats not. </p> <p>I have tons of files: WAVs (inside parent folders with varying naming conventions after I changed system language), M4A, MP4, etc. Most of it is on an external SSD.</p> <p>Some files have a sensible creation date, others have a media creation date, others seem to inherit the parent folder date, and some probably have other kinds of date info attached.</p> <p>I need to sort and organize everything, and also figure out what I still need to download from backups in OneDrive and Google Photos.</p> <p>As you might guess, I\u2019m not very organized. I tried using ChatGPT to get a PowerShell command for this, but it keeps hallucinating and giving me broken stuff.</p> <p>Can someone help me with a non-destructive command or script that will produce a file list with at",
        "id": 4100641,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0ar7i/help_for_simple_command_or_script_to_extract_all",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help for simple command or script to extract all possible date info from ~1000 audio/video files for a documentary that I\u2019ve stored on an external SSD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/helloimjah",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T12:04:45.928170+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T11:27:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I have almost finished my game, but it is missing balanced and tested economy, because game have lot of variables that directly effects economy it would take way too much time for me alone to test even just 1 scenario, usually formula can be made for this and no much of testing would be needed but due to unique aspects of game and variables in it such formula is just too difficult for me to make (Im not even sure it&#39;s possible in my case) and I am not so good at math too so I am testing best case scenarios and are adjusting economy based on that.</p> <p>I am making post on this subreddit because people who use this sub would be main target audience for the game and should be intuitionally familiar with game and concepts rather than someone who plays idle games and is not familiar with data hoarding/technology to who I would need to explain whole concept from 0.</p> <p>So I am looking for volunteers that enjoy idle games and would be willing",
        "id": 4098422,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p09hei/im_making_idle_game_about_data_hoarding_and_need",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm making idle game about data hoarding and need help with testing.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AleksHop",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T11:04:16.997129+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T10:26:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Key features:</p> <ul> <li><strong>Algorithms</strong>: MD5, SHA-1, SHA-2/3, BLAKE2/3, xxHash3/128</li> <li><strong>Optional Fast Mode</strong>: Quick hashing for large files (samples 300MB) for edge cases</li> <li><strong>Flexible Input</strong>: Files, stdin, or text strings</li> <li><strong>Wildcard Patterns</strong>: Support for <code>*</code>, <code>?</code>, and <code>[...]</code> patterns in file/directory arguments</li> <li><strong>Directory Scanning</strong>: Recursive hashing with parallel processing</li> <li><strong>Verification</strong>: Compare hashes against stored database</li> <li><strong>Database Comparison</strong>: Compare two databases to identify changes, duplicates, and differences</li> <li><strong>.hashignore</strong>: Exclude files using gitignore patterns</li> <li><strong>Formats</strong>: Standard, hashdeep, JSON</li> <li><strong>Compression</strong>: LZMA compression for databases</li> <li><strong>Cross-Platform</strong>: Li",
        "id": 4098017,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p08gdy/made_a_fast_tool_to_verify_backups_hashrs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Made a fast tool to verify backups (hash-rs)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Luciana936",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T16:06:52.189887+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T08:30:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Curious how you all manage bookmarks in a way that doesn\u2019t turn into a giant pile of messy posts across all apps. Do you keep everything in the browser? Or save everything in separate platforms like YT playlists, X posts? Tag things and sort them by topic? Or do you just rely on search and not organize by yourself?</p> <p>I&#39;m now trying to collect my bookmarks scattered everywhere on different broswers and apps, and I find it quite time consuming and draining. So I&#39;m wondering is there a better system to do so?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Luciana936\"> /u/Luciana936 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p06o63/how_do_you_use_your_bookmarks/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p06o63/how_do_you_use_your_bookmarks/\">[comments]</a></span>",
        "id": 4100642,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p06o63/how_do_you_use_your_bookmarks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you use your bookmarks?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AleksHop",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T08:01:55.891686+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T07:54:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Made a tool to verify 30TB+ backups in hours instead of days</strong></p> <p>Key features:</p> <ul> <li><strong>Fast mode</strong>: Samples 300MB (start/middle/end) instead of full hash - 100x faster</li> <li>Directory scanning with database storage</li> <li>Verify backups against saved hashes</li> <li>Compare two snapshots to find changes/duplicates</li> <li>Support for 15+ algorithms including BLAKE3 (1-3 GB/s)</li> </ul> <p>Open source, cross-platform, Rust-based.</p> <p>GitHub: <a href=\"https://github.com/vyrti/hash-rs\">https://github.com/vyrti/hash-rs</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AleksHop\"> /u/AleksHop </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p063r9/made_a_tool_to_verify_30tb_backups_in_hours/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p063r9/made_a_tool_to_verify_30tb_backups_in_hours/\">[comments]</a><",
        "id": 4097105,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p063r9/made_a_tool_to_verify_30tb_backups_in_hours",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Made a tool to verify 30TB+ backups in hours instead of days (hash-rs)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Clive1792",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T07:01:17.986182+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T06:32:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Not sure where to ask this but I&#39;m curious how bad other people let their files get &amp; how long it took to sort it all.</p> <p>If you were organised from day 1 then this question isn&#39;t for you :)</p> <p>I have literally thousands &amp; thousands, 10s or 100s of thousands I don&#39;t even know, of files just scattered absolutely everywhere over numerous folders &amp; disks. Some will be many duplicates of others where I&#39;ve just copied something I wasn&#39;t sure if it was backed up or not &amp; they&#39;re all just scattered everywhere. Drives in PCs, portables in desks, internals in boxes on shelves.</p> <p>It&#39;s a mess.</p> <p>So this is going to be a file-by-file process &amp; I hate to think how long this is going to take me. I can use Auslogics or similar to find duplicates &amp; cut things down that way but even then I&#39;m going to be left with sooooooo much to organise. </p> <p>A very daunting task. Will take years to do prop",
        "id": 4096816,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p04sv4/how_long_did_it_take_you_to_organise_all_your",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How long did it take you to organise all your files from being a mess?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Yarplay11",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T04:58:41.157071+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T04:19:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;m thinking of throwing a hdd into my PC which currently has a 1tb Samsung 870 EVO, because I started running out of space and need some cold data storage. Are WD Purple drives worth for it, considering i often do torrents on my pc and need a fairly reliable but inexpensive storage for backups and stuff that doesnt need high speed like films</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yarplay11\"> /u/Yarplay11 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p02ex6/wd_purple_46tb_for_backups/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p02ex6/wd_purple_46tb_for_backups/\">[comments]</a></span>",
        "id": 4096288,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p02ex6/wd_purple_46tb_for_backups",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WD Purple 4/6TB for backups?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rizzrizzzz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T07:01:18.561184+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T04:06:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My WD HDD got bad clusters. I want to buy an external SSD hard drive, basically to store my phone backups and media files for long term with not so frequent accessing. I am currently looking into Crucial X10 Pro, which is better affordable, and SanDisk Extreme Pro, which seems to be more durable designed. Any recommendations between these, or any other better one out there?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rizzrizzzz\"> /u/rizzrizzzz </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p025o4/ssd_for_long_term_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p025o4/ssd_for_long_term_storage/\">[comments]</a></span>",
        "id": 4096818,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p025o4/ssd_for_long_term_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SSD For Long Term Storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MastusAR",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T02:57:24.241002+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T02:45:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As we learned just a week ago about the Ann Arbor hoard, I was thinking what the future looks like? If I got it correctly, Ann Arbor was taped all the way up to 2009, which is very very late for VHS. I myself stopped recording in VHS around 2004-2005, and have first digital TV recordings from 2006 or so. </p> <p>So, what does the future hoards look like medium-wise? Are we facing huge NAS systems with no way to access them? That could be the case when dealing with an estate, and the password has been somehow lost/not found? And then they would probably not be &quot;free to pick up for archivists&quot; as if it&#39;s being kept current, there is monetary value in the medium.</p> <p>Or are facing grandmama&#39;s big ol&#39; LTO tape collection or grandpapa&#39;s optical hoard? What do you think?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MastusAR\"> /u/MastusAR </a> <br/> <span><a href=\"https://www.reddit.com/r",
        "id": 4095803,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p00g3c/future_of_hoards",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Future of hoards",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/harbourhunter",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T07:01:18.296549+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T02:18:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozzvdn/thousands_of_rare_american_recordings_some_100/\"> <img src=\"https://external-preview.redd.it/Gz0a0bqvZfTOoj5CMKZmiN-tvoHKGfT80x1X1MgJhLI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f695a3db8fbd59ec2a8aa21cd51b167df36a6201\" alt=\"Thousands of rare American recordings \u2014 some 100 years old \u2014 go online for all to enjoy\" title=\"Thousands of rare American recordings \u2014 some 100 years old \u2014 go online for all to enjoy\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/harbourhunter\"> /u/harbourhunter </a> <br/> <span><a href=\"https://laist.com/news/arts-and-entertainment/thousands-of-rare-american-recordings-go-online-for-all-to-enjoy\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozzvdn/thousands_of_rare_american_recordings_some_100/\">[comments]</a></span> </td></tr></table>",
        "id": 4096817,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ozzvdn/thousands_of_rare_american_recordings_some_100",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Gz0a0bqvZfTOoj5CMKZmiN-tvoHKGfT80x1X1MgJhLI.jpeg?width=640&crop=smart&auto=webp&s=f695a3db8fbd59ec2a8aa21cd51b167df36a6201",
        "title": "Thousands of rare American recordings \u2014 some 100 years old \u2014 go online for all to enjoy",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AliShibaba",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T02:57:24.577511+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T02:18:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone encountered an issue like this before?</p> <p>I got a Toshiba Enterprise Drive recently and had to run a surface test on it.</p> <p>When I ran the Read+Write Test, it gave me about bad 500+ sectors during the read test.</p> <p>When I ran the Repair Test and then the Read Test, it showed no bad sectors.</p> <p>Finally ran the Read+Write Test again, and the bad sectors returned.</p> <p>Using HD Sentinel for the test.</p> <p>Edit: Sorry I meant No Bad Sectors on Read Test on the title.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AliShibaba\"> /u/AliShibaba </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozzut1/surface_test_bad_sectors_appears_on_readwrite_but/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozzut1/surface_test_bad_sectors_appears_on_readwrite_but/\">[comments]</a></span>",
        "id": 4095804,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ozzut1/surface_test_bad_sectors_appears_on_readwrite_but",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Surface Test - Bad Sectors appears on Read+Write but No Bad Sectors on Write Test",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/caamt13",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T01:55:13.197338+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T01:35:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Podcast. Currently subbed, they have many episodes going back years. Wanna grab em&#39; all - yt-dlp doesn&#39;t really play nice and user friendly doing this, cookies and session id&#39;s and stuff are kind of janky for substack at least when i&#39;ve tried to rip paywalled stuff. in addition, they way they have their page of old/archive stuff is some endless scroll bullshit which makes it really really hard to just scrape all the links or whatever. nothing&#39;s playing nice.</p> <p>any ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/caamt13\"> /u/caamt13 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozyxbu/easiest_way_to_scrape_a_substuck/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozyxbu/easiest_way_to_scrape_a_substuck/\">[comments]</a></span>",
        "id": 4095559,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ozyxbu/easiest_way_to_scrape_a_substuck",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Easiest way to scrape a substuck?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/qwer1627",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-18T00:54:37.329106+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-18T00:36:10+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ozxlux/epstein_files_knowledgebase_any_interest/\"> <img src=\"https://b.thumbs.redditmedia.com/hm7fZg_B5vgKBP_8Kt61va_yeH_19xew9UASqCsHFvk.jpg\" alt=\"Epstein Files knowledgebase - any interest?\" title=\"Epstein Files knowledgebase - any interest?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I converted ~500 docs from EF DOJ dump into embeddings, threw them into Milvus - with HyDE on top. </p> <p>I am debating on the next steps - either converting the rest of the files to embeddings, or calling it good here. My personal interest in this pile of shame is close to zero, I feel dirty just touching them. </p> <p>The future of this project depends on whether the community has interested in a vector-store version of the dump. I may have to cut this initiative if the cost of conversion gets too high, if you want to continue this work (I am using cheapo Bedrock embedding models)</p> <p>What artifacts would you like t",
        "id": 4095236,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ozxlux/epstein_files_knowledgebase_any_interest",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/hm7fZg_B5vgKBP_8Kt61va_yeH_19xew9UASqCsHFvk.jpg",
        "title": "Epstein Files knowledgebase - any interest?",
        "vote": 0
    }
]