[
    {
        "age": null,
        "album": "",
        "author": "/u/Aggravating-Tooth769",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-04T19:41:09.997277+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-04T16:51:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello!<br/> I&#39;m developing a project of web analytics centered around the housing situation in Spain, and the first step towards the analysis is scraping these housing portals. My main objective was to scrap Fotocasa and Idealista since they are the biggest portals in Spain; however, I am having problems doing it. I also followed the robot.txt guidelines and requested access for the Idealista API, but as far as I know, it is legal to do it in Fotocasa. Does someone know any solution updated to 2025, that allows me to scrap from their webs directly?<br/> Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Aggravating-Tooth769\"> /u/Aggravating-Tooth769 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ooce8w/web_scraping_fotocasa_idealista_and_other_housing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ooce8w/web_scraping_fotocasa_idealista",
        "id": 3983975,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ooce8w/web_scraping_fotocasa_idealista_and_other_housing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Scraping Fotocasa, Idealista, and other Housing Portals",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/madredditscientist",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-04T14:34:31.415853+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-04T14:19:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Because I couldn&#39;t find any, I\u2019ve started working on a benchmarking suite for web scraping tools and would love some input from the community.</p> <p>The goal is to measure scraping reliability and performance across different tools and use cases, using reproducible test cases (SSR, SPA, iframes, etc.).</p> <p>So far I&#39;m thinking of evaluating metrics like:</p> <ul> <li>Precision: missing data, hallucinated data, wrong selectors, type/format errors</li> <li>Completeness/Recall: how much of the target data is correctly captured</li> <li>Runtime: start-to-finish latency</li> <li>Anti-blocking: % of blocked requests on sites like Indeed or Cloudflare-protected domains</li> </ul> <p>Test data would cover different domains like news, jobs, ecommerce, crawling depth.</p> <p>What else would you include in such a benchmark? E.g. other metrics, test sites, or ways to ensure fairness across tools?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a hre",
        "id": 3981046,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oo8ck9/how_would_you_design_an_objective_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How would you design an objective web scraping benchmark?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-04T13:33:47.730302+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-04T13:01:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 3980583,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oo6g0q/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/doodlydidoo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-04T11:32:24.509838+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-04T10:45:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There&#39;s a certain popular website from which I&#39;m trying to scrape profiles (including images and/or videos). It needs an account and using a certain VPN works. </p> <p>I&#39;m aware that people here primarily use proxies for this purpose but the costs seem prohibitive. Residential proxies are expensive in terms of dollars per GB, especially when the task involves large volume of data. </p> <p>Are people actually spending hundreds of dollars for this purpose? What setup do you guys have?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/doodlydidoo\"> /u/doodlydidoo </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oo48js/using_proxies_to_download_large_volumes_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oo48js/using_proxies_to_download_large_volumes_of/\">[comments]</a></span>",
        "id": 3979660,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oo48js/using_proxies_to_download_large_volumes_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using proxies to download large volumes of images/videos cheaply?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TraditionClear9717",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-04T06:26:12.544181+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-04T04:41:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>How to automatically detect which school website URLs contain \u201cNews\u201d pages?</strong></p> <p>I\u2019m scraping data from <strong>15K+ school websites</strong>, and each has multiple URLs.<br/> I want to figure out which URLs are <strong>news listing pages</strong>, not individual articles.</p> <p>Example (Brighton College):</p> <pre><code>https://www.brightoncollege.org.uk/college/news/ \u2192 Relevant https://www.brightoncollege.org.uk/news/ \u2192 Relevant https://www.brightoncollege.org.uk/news/article-name/ \u2192 Not Relevant </code></pre> <p>Humans can easily spot the difference, but how can a <strong>machine</strong> do it automatically?</p> <p>I\u2019ve thought about:</p> <ul> <li>Checking for repeating \u201ccard\u201d elements or pagination But those aren\u2019t consistent across sites.</li> </ul> <p>Any ideas for a reliable <strong>rule, heuristic, or ML approach</strong> to detect news listing pages efficiently?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"h",
        "id": 3978085,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1onyek5/automatically_detect_pages_urls_containing_news",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automatically detect pages URLs containing \"News\"",
        "vote": 0
    }
]