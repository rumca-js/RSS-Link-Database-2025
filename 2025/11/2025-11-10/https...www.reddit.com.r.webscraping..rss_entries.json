[
    {
        "age": null,
        "album": "",
        "author": "/u/jpcoder",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-10T21:53:31.374106+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-10T16:39:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been able to successfully scrape Walmart&#39;s product pages using SeleniumBase but I want to be able to get the store specific aisle information which as far as I can tell requires a location cookie to be set from the server. Does anyone know how to trigger this cookie to be set? Or is there another path to do this easier?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jpcoder\"> /u/jpcoder </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1otinv5/scraping_walmart_store_specific_aisle_data_for_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1otinv5/scraping_walmart_store_specific_aisle_data_for_a/\">[comments]</a></span>",
        "id": 4033807,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1otinv5/scraping_walmart_store_specific_aisle_data_for_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Walmart store specific aisle data for a product",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CurrencyPristine8323",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-10T13:23:05.749723+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-10T09:34:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How do you handle compliance and structure?</p> <p>I\u2019ve been exploring healthcare data extraction lately, things like clinical trial databases, hospital listings, and public health portals. One major challenge I\u2019ve faced is maintaining data accuracy and compliance (especially when dealing with PII or HIPAA-sensitive information).</p> <p>Curious how others in this space approach it:</p> <ul> <li>Do you rely more on open APIs or build custom crawlers for structured datasets?</li> <li>How do you handle schema variations and regional compliance?</li> </ul> <p>I\u2019ve seen some interesting approaches using AI-based normalization to make the data usable for analytics, but I would love to hear real-world experiences from this community.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CurrencyPristine8323\"> /u/CurrencyPristine8323 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ot9h73/anyone_here_wo",
        "id": 4030904,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ot9h73/anyone_here_working_on_healthcare_data_extraction",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone here working on healthcare data extraction",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dependent_Tap_2734",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-10T09:18:23.097965+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-10T09:09:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I have a project where I scrape data to find offers online for customers. This involves filling in quite standard but time consuming forms accross several sites. </p> <p>However, when one is found I want to programatically apply for them only if they approve. Therefore, the idea would be to forward the accept button along with the captcha. </p> <p>I tried to send the pre-filled form as an alternative but this is not supported by most of the sites.</p> <p>Is there anyway to forward them the captcha? The time consuming part is filling in all the fields, so this would already be a great help for the end user.</p> <p>I am using Scrapy+Selenium if that is of any relevance.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dependent_Tap_2734\"> /u/Dependent_Tap_2734 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ot93qi/forwarding_captcha_to_enduser/\">[link]</a></span> &#32;",
        "id": 4029378,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ot93qi/forwarding_captcha_to_enduser",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Forwarding captcha to end-user",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Johnerwish",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-10T05:12:07.517374+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-10T02:56:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m just getting started in web scraping. I need birth dates, death dates, photo capture times, and corresponding causes of death for deceased individuals listed on Google Encyclopedia.</p> <p>Here&#39;s my approach: I first locate the web structural elements containing the data I need to scrape. Then instruct the program to scrape them. If there are 400 pages of content, I crawl one page at a time. After completing a page, I simulate clicking the \u201cnext page\u201d button to continue crawling similar web structural elements. Is this method correct? Because it&#39;s very slow, requiring me to test each element&#39;s location within the Java structure individually.</p> <p>However, the cause of death and other underlying causes are difficult to determine.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Johnerwish\"> /u/Johnerwish </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ot2p88/hi_guys_im",
        "id": 4028437,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ot2p88/hi_guys_im_just_getting_started_using_a_very",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hi guys I'm just getting started using a very clunky crawling method",
        "vote": 0
    }
]