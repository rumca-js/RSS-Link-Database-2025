[
    {
        "age": null,
        "album": "",
        "author": "/u/cryptoteams",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-24T22:02:08.622595+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-24T21:46:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I manage ~100 scrapers and the thing that really helped me, was using sessions that record/discard IP, cookies, fingerprints and browsers.</p> <p>What are you running into that would help you with getting your data?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryptoteams\"> /u/cryptoteams </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5u834/what_is_the_most_annoying_thing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5u834/what_is_the_most_annoying_thing/\">[comments]</a></span>",
        "id": 4154831,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p5u834/what_is_the_most_annoying_thing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the most annoying thing?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Few_Response_7028",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-24T22:02:08.840262+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-24T21:02:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can someone help me find the JSON data on <a href=\"https://crashviewer.nhtsa.dot.gov/ciss/details/21312/crash-summary-document\">this site</a>? The website was recently reworked.</p> <p>Using my old method, it should be located <a href=\"https://crashviewer.nhtsa.dot.gov/api/case/CaseOverviewTreeResult?caseID=21312\">here</a>, but i&#39;m getting a 405 error</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Few_Response_7028\"> /u/Few_Response_7028 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5t3e4/help_finding_json_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5t3e4/help_finding_json_data/\">[comments]</a></span>",
        "id": 4154832,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p5t3e4/help_finding_json_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help finding JSON data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ki-_-rito",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-24T14:38:14.804862+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-24T14:11:41+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1p5i24n/derja_smart_scraper_build_ai_for_tunisian_other/\"> <img src=\"https://preview.redd.it/lf68431cq73g1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42acdabfb432c1eec7a3909ab11aa5a5f5f4c6eb\" alt=\"Derja Smart Scraper: Build AI for Tunisian &amp; Other Arabic Dialects\" title=\"Derja Smart Scraper: Build AI for Tunisian &amp; Other Arabic Dialects\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I just released something I wish existed months ago.</p> <p>While building a native Tunisian AI model, I hit a major roadblock: there was no clean, large-scale Derja (Tunisian Arabic) dataset. Collecting it manually was slow, inconsistent, and painful.</p> <p>So I built Derja Smart Scraper \u2014 an open-source tool that automatically collects structured, deduplicated, high-quality Tunisian text from the web. It\u2019s fast, clean, scalable, and can easily be adapted to collect other Arabic dialects or regional languages, ma",
        "id": 4150911,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p5i24n/derja_smart_scraper_build_ai_for_tunisian_other",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/lf68431cq73g1.jpeg?width=640&crop=smart&auto=webp&s=42acdabfb432c1eec7a3909ab11aa5a5f5f4c6eb",
        "title": "Derja Smart Scraper: Build AI for Tunisian & Other Arabic Dialects",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CreepyCondition2314",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-24T15:39:18.351913+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-24T14:10:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anti-Scraping Nightmare: Successfully Bypassed DevTools Block, but CDN IP Blocked Final Download on <a href=\"http://anikai.to\">anikai.to</a> </p> <p>Hey everyone,</p> <p>I recently spent several hours attempting to automate a simple task\u2014retrieving the M3U8 video stream URL for episodes on the anime site anikai.to. This website presented one of the most aggressive anti-scraping stacks I&#39;ve encountered, and it led to an interesting challenge that I&#39;d like to share for community curiosity and learning.</p> <p><strong>The Core Challenges:</strong></p> <p>Aggressive Anti-Debugging/Anti-Inspection: The site employed a very strong defense that caused the entire web page to go into an endless refresh loop the moment I opened Chrome Developer Tools (Network tab, Elements, Console, etc.). This made real-time client-side analysis impossible.</p> <p><strong>Obfuscated Stream Link:</strong> The final request that retrieves the video stream link did not re",
        "id": 4151460,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p5i1bs/antiscraping_nightmare_anikaito",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anti-Scraping Nightmare: anikai.to",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/chichuchichi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-24T13:37:20.128534+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-24T12:56:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to use chrome-profile with &#39;printing.print_preview_sticky_settings.appState&#39; to make duplex: 0</p> <p>But it does not work. It just uses the whatever setting is set on Chrome? Is there any way that I can change the setting?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/chichuchichi\"> /u/chichuchichi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5gc2d/puppeteer_windowprint_how_to_force_single_side/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5gc2d/puppeteer_windowprint_how_to_force_single_side/\">[comments]</a></span>",
        "id": 4150458,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p5gc2d/puppeteer_windowprint_how_to_force_single_side",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "puppeteer window.print() <-- how to force single side page?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GeobotPY",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-24T10:26:26.392614+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-24T09:57:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Currently using a multi-agent system based on LLMs where I have created a system where I can insert any e-commerce url and extract all product links and corresponding titles, prices etc.</p> <p>Ive tested it and it and it is pretty stable on almost all e-com sites. I would assume such a software is valuable to certain businesses, but not sure where to start from a business perspective. Does anyone have any advice?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GeobotPY\"> /u/GeobotPY </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5d3nh/monetize_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p5d3nh/monetize_scraping/\">[comments]</a></span>",
        "id": 4149047,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p5d3nh/monetize_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Monetize scraping?",
        "vote": 0
    }
]