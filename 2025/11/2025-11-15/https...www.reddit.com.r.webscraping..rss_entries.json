[
    {
        "age": null,
        "album": "",
        "author": "/u/pmp1321",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-15T23:15:21.099530+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-15T22:38:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hi everyone, im building a project where im trying to match news stories with expert opinions/quotes about that news topic.</p> <p>i already have the news data but im looking for help on the best way to scrape the quotes.</p> <p>The quotes will come from social media (likely youtube or X or podcasts etc, or maybe theres another source?) </p> <p>Do yall have any ideas on how to best do this, i already have a process that retrieves youtube videos posted from channels then passes the transcript into LLM for summarization but I&#39;m not sure that can work with the news headlines</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pmp1321\"> /u/pmp1321 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oy52tn/scraping_expert_opinions_on_news_headlines/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oy52tn/scraping_expert_opinions_on_news_headlines/\">[comments]<",
        "id": 4079595,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oy52tn/scraping_expert_opinions_on_news_headlines",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping expert opinions on news headlines?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hopefull420",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-15T18:12:06.742460+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-15T18:11:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been doing web scraping for a while, but mostly static sites or small-scale stuff. Recently, I\u2019ve been working on a much bigger project where we need to fully exhaust certain categories/keywords across multiple websites. A few of these sites are dynamic, and that\u2019s where things are getting messy.</p> <p>These spiders are running at scale, a <em>lot</em> of keywords, and the current setup is costing way too much in residential proxy data usage. Right now it\u2019s burning around 2.5 - 4 MB per listing/record, and that\u2019s just not sustainable when you multiply it across hundreds of thousands of businesses + their reviews.</p> <p>The stack is Python, Scrapy, Scrapy-Playwright</p> <p>I\u2019m not great at optimizing dynamic scrappers, havent really done scraping to this scale.</p> <p>The client\u2019s highest-priority requirements are:</p> <ol> <li>Basic business info (cannot miss)</li> <li>All reviews (absolutely cannot miss)</li> </ol> <p>If anyone has advice on r",
        "id": 4078164,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oxym17/need_help_on_optimizing_dynamic_scrapyplaywright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help on optimizing dynamic Scrapy-Playwright spiders",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mickspillane",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-15T14:08:45.010136+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-15T13:42:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My setup: I scrape content from 10 different Amazon locales via curl_cffi and playwright. My target data sits behind a login wall, so I have a different Amazon account in each locale. All my IPs are in the locale I scrape from.</p> <p>Situation: I get detected regularly in Amazon CA and Amazon DE, but not in the others.</p> <p>What I&#39;ve tried: I&#39;ve tried changing IP pools for these locales to no avail.</p> <p>What I&#39;m thinking: I&#39;m thinking my headers are giving me away. Right now, I&#39;m using the same set of headers for all locales. Amazon UK doesn&#39;t give problems. But Amazon CA does. Are some headers locale dependent?</p> <p>Any other suggestions? Thx.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mickspillane\"> /u/mickspillane </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oxs0p6/amazon_detects_my_bot_for_some_locales_only/\">[link]</a></span> &#32; <span><a hre",
        "id": 4076686,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oxs0p6/amazon_detects_my_bot_for_some_locales_only",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Amazon detects my bot for some locales only",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Accurate-Ad6361",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-15T11:07:20.571619+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-15T10:49:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Playwright, selenium, what ever, nothing works. Cloudflare blocks every attempt.</p> <p>I need to scrape some details about companies from a publicly accessible database.<br/> Every company in Italy has a unique VAT-ID such as &quot;03316600604&quot;. The URL looks like that:</p> <p>it&#39;s possible to look up the company through the vat-ID on the company registry website (<a href=\"https://www.ufficiocamerale.it/\">https://www.ufficiocamerale.it/</a>) leading to the full profile:</p> <p><a href=\"https://www.ufficiocamerale.it/7183/connessioni-immobiliari-50-societa-a-responsabilita-limitata-semplificata\">https://www.ufficiocamerale.it/7183/connessioni-immobiliari-50-societa-a-responsabilita-limitata-semplificata</a></p> <p>So at the beginning I thought: easy, /7183/ must follow some pattern, but now, random number. The sitemap is not complete, but seems to drop pages when reaching a certain link quantity. Right now only ~1.4m links are in the sitemap ",
        "id": 4075791,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oxorlk/meet_your_master_i_have_tried_everything",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Meet your master, I have tried everything: ufficiocamerale.it",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/superhero_io",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-15T01:57:43.714995+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-15T01:45:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m exploring automation tools like n8n, but many workflows still rely heavily on controlling webpages. Using APIs can be expensive, so I&#39;m wondering if anyone has tried using Python or other software to automate web interactions directly. In my tests, many sites seem to detect and block automated browsers. Has anyone found effective methods to get around these detection issues?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/superhero_io\"> /u/superhero_io </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oxf3qn/question_about_web_automation_methods/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oxf3qn/question_about_web_automation_methods/\">[comments]</a></span>",
        "id": 4073929,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oxf3qn/question_about_web_automation_methods",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question About Web Automation Methods",
        "vote": 0
    }
]