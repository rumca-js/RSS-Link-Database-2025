[
    {
        "age": null,
        "album": "",
        "author": "/u/Open_Bother_6935",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T21:32:47.598285+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T20:49:56+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1oul3am/i_built_my_own_socialmedia_media_extractor/\"> <img src=\"https://external-preview.redd.it/6ld4YU9AO68fLJNpmFVq69CJ-KHFda_wHmT6KjgFd7Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=850c90b88cf96145450ffd28baad8eda345f3df1\" alt=\"I built my own social-media media extractor because all the existing sites are full of ads.\" title=\"I built my own social-media media extractor because all the existing sites are full of ads.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Right now:</p> <p>\u2022 Instagram and Twitter/X work reliably.</p> <p>\u2022 Clean interface, no ads, no tracking.</p> <p>\u2022 Still missing a lot of features, but it\u2019s kinda usable.</p> <p>yt-dlp started blocking some of my IPs, so I\u2019m temporarily routing requests through a small proxy library. </p> <p>It works, but it\u2019s unstable \u2014 definitely looking for a better approach. </p> <p>I\u2019m planning to expand support for more platforms and improve stability ",
        "id": 4043676,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oul3am/i_built_my_own_socialmedia_media_extractor",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/6ld4YU9AO68fLJNpmFVq69CJ-KHFda_wHmT6KjgFd7Q.png?width=640&crop=smart&auto=webp&s=850c90b88cf96145450ffd28baad8eda345f3df1",
        "title": "I built my own social-media media extractor because all the existing sites are full of ads.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BusinessBitter5076",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T23:43:51.091134+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T18:48:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m scraping <a href=\"http://theresanaiforthat.com\">theresanaiforthat.com</a> to get all ~42,000 AI products across different categories. </p> <p>Current results: Getting 38K products but missing ~4K (5-10 products per category)</p> <p>Site structure:</p> <p>- Main categories with pagination (/task/ads/, /task/ads/page/2/)</p> <p>- Subcategories within each main task (/task/ad-optimization/)</p> <p>- Some products appear hidden behind &quot;Show more&quot; buttons</p> <p>- Using BeautifulSoup + lxml parser</p> <p>What I&#39;m doing:</p> <ol> <li><p>Crawling main category pages with pagination</p></li> <li><p>Extracting subtask URLs and crawling those</p></li> <li><p>Using `find_all(&#39;li&#39;, class_=&#39;li&#39;, attrs={&#39;data-id&#39;: True})`</p></li> </ol> <p>Problem: Still missing 5-10 products per category. Suspects:</p> <p>- Products hidden with CSS/JavaScript (display:none?)</p> <p>- Lazy loading not triggering</p> <p>- Pagination not ",
        "id": 4044641,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ouht41/missing_4k_tools_when_scraping_42k_ai_tools",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Missing ~4k tools when scraping 42k+ AI tools - hidden element issue?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/_internal_function",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T23:43:51.186667+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T18:44:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, all, first time poster!</p> <p>I am not a super experienced web scraper but I have a SaaS application that leverages a well known scraping API. Essentially the scraping portion of my application is triggered when a client forwards a social media post that they want analyzed. </p> <p>The issue that I\u2019m facing is that the API I am using is not always reliable. There often seems to be a glitch or issue with gathering data from the API or it takes way too long to return results. Clients expect results as fast as possible. In addition to this, it\u2019s costing me $0.0015/post. </p> <p>I\u2019m not sure what I steps I should take next. The scraper is only a minor component of my SaaS, and this is a side project so I cannot commit all of my time to the scraping portion. </p> <p>Note that I\u2019m not constantly scraping posts, only when a client sends it to my API. I\u2019m not sure if this would trigger the social medias anti-bot blocking but I\u2019ve heard that they\u2019re ge",
        "id": 4044642,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ouhpeu/fast_reliable_cheap",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fast, Reliable, Cheap",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DetailedLife",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T18:25:47.663407+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T17:29:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, </p> <p>I scrape professionally and in a large quantity but I\u2019m looking to expand my skills. Very comfortable with finding API in the code, using requests in dev tab to find different access points or urls to use, using both python and js with my scrapers. However, I\u2019ve been looking into the more advanced detections and how to actually start understanding the bypass of Akamai and CloudFlare. </p> <p>Obviously the cookies come into play and each cookie verification as you move across the website. What I don\u2019t understand is the actual bypass or spoofing of the cookies to bypass the security. </p> <p>My current guess is that they are using a Akamai or CF cookie that is on a low level security site and using that on high security sites, but I\u2019m probably entirely wrong. With both they fingerprint heavily, so you need to make sure you aren\u2019t identified as a bot(not a new browsing session, not headless, passing the correct cookies, etc) which isn\u2019t",
        "id": 4042183,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oufmtq/advanced_scraping_methods",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advanced Scraping Methods",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IRipTooManyPacks",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T17:22:31.111288+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T17:11:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! </p> <p>So I\u2019ve been building my own scrapers with playwright or and basic HTTP, etc. The problem is, I\u2019m trying to scrape 16,000 websites. </p> <p>What is a better way to do this? Tried scrapy as well. </p> <p>It does the job currently, but takes HOURS. </p> <p>My goal is to extract certain details from all of those websites for data collection. However some sites are heavy JS and causing issues. Scraper is also having false positives. </p> <p>Any information on how to get accurate data would be awesome. Or any information on what scraper to use would be amazing. Thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IRipTooManyPacks\"> /u/IRipTooManyPacks </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ouf4kq/bulk_scrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ouf4kq/bulk_scrape/\">[comments]</a></span>",
        "id": 4041677,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ouf4kq/bulk_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bulk Scrape",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GardenHistorical2593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T16:21:22.679625+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T16:17:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m searching for an existing tool or library to help with my machine learning project. I want to programmatically collect all Medium articles published by the people I follow.</p> <ul> <li><strong>Website URL:</strong> Medium user profiles, for example: <code>https://medium.com/@username</code></li> <li><strong>Data Points:</strong> Article titles, full text content, images, tags, author details, and publish dates.</li> <li><strong>Project Description:</strong><br/> I need to extract the complete post history for several Medium users I follow, not just recent articles. Medium RSS feeds only return a limited number of recent posts, and unofficial APIs I\u2019ve found require querying each username individually. I want to avoid building my own scraper\u2014if a robust, maintained tool already exists, I\u2019d love recommendations. Compatibility with pagination and respectful scraping practices are important for me.</li> </ul> <p>Has anyone used or built a ready-made ",
        "id": 4041021,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oudnvh/looking_for_a_tool_to_scrape_all_medium_posts_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a tool to scrape all Medium posts of people I follow",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GarrixMrtin",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T23:43:51.281284+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T13:11:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I built a production scraper that gets past modern multi-layer anti-bot defenses (fingerprinting, behavioral biometrics, TLS analysis, ML pattern detection).</p> <p>What worked:</p> <ul> <li>B\u00e9zier-curve mouse movement to mimic human motor control</li> <li>Mercator projection for sub-pixel navigation precision</li> <li>12 concurrent browser contexts with bounded randomization</li> <li>Leveraging mobile endpoints where defenses were lighter</li> </ul> <p>Result: harvested large property datasets with broker contacts, price history, and investment gap analysis. </p> <p>Technical writeup + code:<br/> \ud83d\udcdd <a href=\"https://medium.com/@2.harim.choi/modern-anti-bot-systems-and-how-to-bypass-them-4d28475522d1\">https://medium.com/@2.harim.choi/modern-anti-bot-systems-and-how-to-bypass-them-4d28475522d1</a><br/> \ud83d\udcbb <a href=\"https://github.com/HarimxChoi/anti_bot_scraper\">https://github.com/HarimxChoi/anti_bot_scraper</a><br/> Ask me anything about architecture, re",
        "id": 4044643,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ou92ee/built_a_production_web_scraper_that_bypasses",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Built a production web scraper that bypasses anti-bot detection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T13:15:57.239387+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T13:01:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 4039453,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ou8u1d/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TickleTipson_11",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-11T10:11:48.596795+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-11T10:09:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TickleTipson_11\"> /u/TickleTipson_11 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ou5q24/is_there_a_way_to_use_a_profile_picture_to_scrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ou5q24/is_there_a_way_to_use_a_profile_picture_to_scrape/\">[comments]</a></span>",
        "id": 4038290,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ou5q24/is_there_a_way_to_use_a_profile_picture_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a way to use a profile picture to scrape for spotify users?",
        "vote": 0
    }
]