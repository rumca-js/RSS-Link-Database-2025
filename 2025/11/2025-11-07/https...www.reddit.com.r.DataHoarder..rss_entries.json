[
    {
        "age": null,
        "album": "",
        "author": "/u/TeamNathanFTW",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T23:22:04.968710+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T22:58:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I keep getting 504 timeouts - someone must be attacking it. I need it to read books freely online wo having to download just to read.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TeamNathanFTW\"> /u/TeamNathanFTW </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or9339/does_anyone_know_whats_going_on_with_welib/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or9339/does_anyone_know_whats_going_on_with_welib/\">[comments]</a></span>",
        "id": 4014211,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or9339/does_anyone_know_whats_going_on_with_welib",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone know what\u2019s going on with Welib?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gnomesenpai",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T23:22:05.141949+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T22:38:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, both in my personal side and business side finding data is quite frankly a PITA. Does another know of an indexing solution thats free so I can stop relying on windows indexing?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gnomesenpai\"> /u/gnomesenpai </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or8m4s/indexingcatalog_folder_structurefiles/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or8m4s/indexingcatalog_folder_structurefiles/\">[comments]</a></span>",
        "id": 4014212,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or8m4s/indexingcatalog_folder_structurefiles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "indexing/catalog folder structure/files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bizarresolitudes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T23:22:05.335923+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T22:16:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1or81s9/spotify_apple_music_migration_script_api/\"> <img src=\"https://preview.redd.it/8wmiqilatwzf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f30becaa31a8f595733615e1062f20be0702e12e\" alt=\"Spotify \u2192 Apple Music migration script / API cockblock? Playlisty throws &quot;curator doesn't permit transfers.&quot;\" title=\"Spotify \u2192 Apple Music migration script / API cockblock? Playlisty throws &quot;curator doesn't permit transfers.&quot;\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been with Apple Music for years now and I\u2019ve had enough, and I\u2019m exhausted from trying every so-called transfer method out there. I love Apple Music \u2014 hate its algorithm. I love Spotify \u2014 hate its audio quality. Even with lossless, my IEMs confirm it\u2019s still inferior.</p> <p>So I tried Playlisty on iOS. Looked promising, until I hit this:</p> <p>\u201cThe curator of that playlist doesn\u2019t permit transfers to other services.\u201d (sc",
        "id": 4014213,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or81s9/spotify_apple_music_migration_script_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/8wmiqilatwzf1.jpeg?width=640&crop=smart&auto=webp&s=f30becaa31a8f595733615e1062f20be0702e12e",
        "title": "Spotify \u2192 Apple Music migration script / API cockblock? Playlisty throws \"curator doesn't permit transfers.\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/humzay",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T22:21:28.278560+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T22:09:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for enclosure for my 18tb ironwolf Seagate drives, I&#39;m currently using ORICO, it works fine but sometimes it gets disconnected, Thinking to get another one, single drive enclosure, not multiple or raid setups, any suggestions? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/humzay\"> /u/humzay </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or7vu9/hdd_enclosure_sata_for_18tb_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or7vu9/hdd_enclosure_sata_for_18tb_drive/\">[comments]</a></span>",
        "id": 4013793,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or7vu9/hdd_enclosure_sata_for_18tb_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hdd enclosure sata for 18tb drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PeakAppropriate8395",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T23:22:05.599610+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T22:02:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently got a Seagate 22TB External HDD on the cheap that I use for my Plex media. It currently sits on the desk in my gaming room, which is really the only practical place for me to have it.</p> <p>However, the vibration (thumping) of the thing is making me go insane. At first it was so bad that I got the &#39;broom to the ceiling&#39; treatment from my downstairs neighbor, and I&#39;ve since placed it on a wrapped washcloth that has made it bearable but still very annoying.</p> <p>Does anyone have any recommendations for vibration damping pads or any other solutions that might fit my usecase?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PeakAppropriate8395\"> /u/PeakAppropriate8395 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or7pr8/vibration_damping_for_seagate_22tb_external_hdd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or7pr8/vibra",
        "id": 4014214,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or7pr8/vibration_damping_for_seagate_22tb_external_hdd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Vibration damping for Seagate 22TB External HDD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Over-Half-8801",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T22:21:28.050898+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T21:29:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve got like 7-8 Micro SD Cards ranging from 128GB to 512GB. Any idea what type of content I should store on there or anything cool I can do (like bridge them together to form 1TB SHARED or something). </p> <p>Also what&#39;s the lifespan on these things? What kind of data would you prefer keeping on these? I&#39;m thinking of using these to store Plex movies and TV shows on reserve</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Over-Half-8801\"> /u/Over-Half-8801 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or6vuh/what_to_do_with_micro_sd_cards/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or6vuh/what_to_do_with_micro_sd_cards/\">[comments]</a></span>",
        "id": 4013792,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or6vuh/what_to_do_with_micro_sd_cards",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What to do with micro SD cards?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AUFairhope1104",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T21:21:07.370828+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T20:53:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Best method to convert entire website to pdf, including all levels, on macOS?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AUFairhope1104\"> /u/AUFairhope1104 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or5yrt/save_entire_website_as_pdf/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or5yrt/save_entire_website_as_pdf/\">[comments]</a></span>",
        "id": 4013436,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or5yrt/save_entire_website_as_pdf",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Save entire website as pdf?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kraziebomb",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T21:21:07.593906+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T20:33:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I got an incredible NAS device for free with hard drives. It has some age on them (3-4 years on the drives maybe?) all 24 disks were 10tb drives, but for now I unplugged 12 of them to save power. This QNAP was running the normal software that came with it. It cannot use QuTS Hero. It had 4gb ECC memory (2x2) and I upgraded it to 20Gb memory (added 2x8 ECC sticks)</p> <p>My next project is to scan a massive amount of family pictures. The kinds of things that cannot be replaced in any way. Due to the size of this scanning project I only ever want to do it once. Also, the physical copies will only degrade with time so making sure I don&#39;t lose these is very important. </p> <p>I don&#39;t need all the space to be at maximum protection, but a slice of it needs to be. </p> <p>My research leads me to believe that the best setup would be ZFS with 2 disk parity (RAIDZ2 ?)<br/> There are many different use cases and setups and I&#39;m starting to get confuse",
        "id": 4013437,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or5geq/best_way_to_structure_some_of_my_nas_for_maximum",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to structure some of my NAS for maximum data protection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mrcrashoverride",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T20:20:38.049880+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T20:06:37+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mrcrashoverride\"> /u/mrcrashoverride </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1or4olz\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4rr3/which_supermicro_box_and_backplane_would_be_best/\">[comments]</a></span>",
        "id": 4012905,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or4rr3/which_supermicro_box_and_backplane_would_be_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which Supermicro box and backplane would be best\u2026",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/raafayawan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T20:20:38.263011+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T20:06:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just curious to know who has the biggest collection of items like ebooks, comics, movies, tv series, audiobooks etc and how much storage does it take?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/raafayawan\"> /u/raafayawan </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4rg1/who_has_the_biggest_collection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4rg1/who_has_the_biggest_collection/\">[comments]</a></span>",
        "id": 4012906,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or4rg1/who_has_the_biggest_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Who has the biggest collection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FivePlyPaper",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T20:20:38.456105+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T20:01:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I know this has probably been asked a ton before but I am desperate.</p> <p>I\u2019m in Canada and I just NEED more space. I have been using a 2Tb RAID with 500GB cold spares while waiting for serverpartdeals prices to drop but they just get higher..</p> <p>I just wanted to buy like 5 12tb drives while not spending over $1000 but it looks like those days are long gone. Any suggestions, from one hoarder to another?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FivePlyPaper\"> /u/FivePlyPaper </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4n3i/where_to_buy_harddrives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4n3i/where_to_buy_harddrives/\">[comments]</a></span>",
        "id": 4012907,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or4n3i/where_to_buy_harddrives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where to buy Harddrives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/riveraedge",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T20:20:39.385956+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T19:49:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As in, what on earth is Data Hoarding</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/riveraedge\"> /u/riveraedge </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4bmv/i_just_joined_this_sub_in_one_scentence_can_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or4bmv/i_just_joined_this_sub_in_one_scentence_can_you/\">[comments]</a></span>",
        "id": 4012908,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or4bmv/i_just_joined_this_sub_in_one_scentence_can_you",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I just joined this sub, in one scentence can you describe what i've just gotten myself into?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/flameboi900",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T19:19:13.720172+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T18:56:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I recently have gotten into data hoarding (a.k.a the worst financial hobby) and I have files of various types collected over the years that go back to as far as 2008 on a variety of external and internal hard drives that are all mixed totals about 20tb. I was wondering how I might go keeping a backup of all this data better than the current solution I use. I have only a single 14tb HGST hard drive backup of mostly all the data but not quite complete due to it being 20tb total. I really want to use the 3 2 1 backup rule of local, offsite and cloud but currently all I have is local. I have considered blackblaze but there are some people saying it isn&#39;t the best solution, and there is also people suggesting I buy two drives that are the same or more capacity than the data and make it a redudent raid so if I lose a drive, at least I can recover some of the data. If anyone has some good answers for a really terrible first time beginner into the ",
        "id": 4012311,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or2x2j/help_with_backup_of_my_main_data_hoard",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with backup of my main data hoard",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Neros_Cromwell",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T19:19:13.945580+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T18:36:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to build a database from DVD&#39;s, Cd&#39;s et cetera, using my macbook pro, it doesn&#39;t come with a disk drive so i&#39;m trying to order an external one from amazon, it seems the overall top pick that shows up is like 20$ but it doesn&#39;t mention blu-ray, the first one that mentions blu-ray is $81 but the cheapest is 39.99 from a brand &quot;APPINESSEY&quot; what should i be looking for? how can i be certain I&#39;m getting what i need? thank you for any help.<br/> EDIT: I also want to rip Blu-ray&#39;s</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Neros_Cromwell\"> /u/Neros_Cromwell </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or2eib/cheapest_trustable_bluray_external_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or2eib/cheapest_trustable_bluray_external_drive/\">[comments]</a></span>",
        "id": 4012312,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or2eib/cheapest_trustable_bluray_external_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cheapest trustable Blu-ray external drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Legal_Airport6155",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T19:19:13.050489+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T18:14:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>started archiving a news site in march. kept noticing they&#39;d edit or straight up delete articles with zero record. with all the recent talk about data disappearing, figured it was time to build my own archive.</p> <p>runs every 6 hours, grabs new stuff and checks if old ones got edited. dumps to postgres with timestamps. sitting at 48k articles now, about 2gb text + 87gb images.</p> <p>honestly surprised how stable its been? used to run scrapy scripts that died every time they changed layout. this has been going 8 months with maybe 2 hours total maintenance. most of that was when the site did a major redesign in august, rest was just spot checks.</p> <p>using simple schema - articles table with url, title, body, timestamp, hash for detecting changes. found some wild patterns - political articles get edited 3x more than other topics. some have been edited 10+ times. tracked one that got edited 7 times in a single day.</p> <p>using a cloud scraping ",
        "id": 4012308,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or1tdw/been_archiving_a_news_site_for_8_months_caught",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "been archiving a news site for 8 months: caught 412 deleted articles and 3k edits",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/adent07",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T23:22:06.258321+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T17:56:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve tried various browsers and removing all extensions etc and nothing I&#39;ve tried so far gets me to download faster than about 5.5mbps, which is infuriating for 200GB+ files.</p> <p>My computer is connected via a hardline to my gig fiber and I see realized download speeds of 600-800mbps on steam so I know its not my connection. </p> <p>Every now and then I&#39;ll get about 50mbps from a gdrive download but its totally random and not very often. I&#39;m getting ready to pull my hair out here but thought I&#39;d check to see if anyone else had this issue and was able to fix it. Thanks all!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/adent07\"> /u/adent07 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or1bt6/why_is_google_drive_throttling_me/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or1bt6/why_is_google_drive_throttling_me/\">[comment",
        "id": 4014215,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or1bt6/why_is_google_drive_throttling_me",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why is google drive throttling me?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/WinningAllTheSports",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T18:08:40.384745+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T17:11:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1or04wc/why_is_there_pen_lines_on_the_underneath_of_my/\"> <img src=\"https://preview.redd.it/gh0ipx5xavzf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b525f1fbfd2719cf7a790643a7b7ace50c41d7b\" alt=\"Why is there pen lines on the underneath of my Ironwolf 8TB drives?\" title=\"Why is there pen lines on the underneath of my Ironwolf 8TB drives?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>The pen traces why look to be scratches? Bought new from amazon. It\u2019s the same on all 3 drives I bought </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WinningAllTheSports\"> /u/WinningAllTheSports </a> <br/> <span><a href=\"https://i.redd.it/gh0ipx5xavzf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1or04wc/why_is_there_pen_lines_on_the_underneath_of_my/\">[comments]</a></span> </td></tr></table>",
        "id": 4011780,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1or04wc/why_is_there_pen_lines_on_the_underneath_of_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/gh0ipx5xavzf1.jpeg?width=640&crop=smart&auto=webp&s=3b525f1fbfd2719cf7a790643a7b7ace50c41d7b",
        "title": "Why is there pen lines on the underneath of my Ironwolf 8TB drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AshleyAshes1984",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T16:57:33.867634+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T16:27:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqyyxz/using_a_smart_power_strip_that_slaves_the_other/\"> <img src=\"https://external-preview.redd.it/ZWxnYTU2bWYydnpmMdyOs4zIKQv-oLxFk6oKYRwLv2_4ENDcWOsrMBVjkL7-.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8122bf3e4c449a61889fc25dffc2ad8acc5eb945\" alt=\"Using a 'smart' power strip, that slaves the other outlets to the load on the 'main' one, I can now get my disk shelf with no remote power functions to power up or off with the server. So I can power off in a blackout without the disk shelf using all the UPS power and also power up with WOL.\" title=\"Using a 'smart' power strip, that slaves the other outlets to the load on the 'main' one, I can now get my disk shelf with no remote power functions to power up or off with the server. So I can power off in a blackout without the disk shelf using all the UPS power and also power up with WOL.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>The NetApp disk shelf",
        "id": 4011109,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqyyxz/using_a_smart_power_strip_that_slaves_the_other",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/ZWxnYTU2bWYydnpmMdyOs4zIKQv-oLxFk6oKYRwLv2_4ENDcWOsrMBVjkL7-.png?width=640&crop=smart&auto=webp&s=8122bf3e4c449a61889fc25dffc2ad8acc5eb945",
        "title": "Using a 'smart' power strip, that slaves the other outlets to the load on the 'main' one, I can now get my disk shelf with no remote power functions to power up or off with the server. So I can power off in a blackout without the disk shelf using all the UPS power and also power up with WOL.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cyno01",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T16:57:34.277449+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T16:00:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My old PC died and it was running a spare server for something with three drives hooked up. Id like to replace it with a basic mini-pc and a drive enclosure, but looking at amazon theres only one or two brand names that sound at all familiar and the rest are the cheap sounding gibberish. </p> <p>I need 4 bays, toolless is a must, hot swap is preferred, but i dont need RAID or anything, the drives are already formatted and organized, its just gotta be E:\\ F:\\ and G:\\ in windows again. Nothing fancy. I was looking at those drop in dock things, but in my experience active cooling makes all the difference for drive longevity so i want something with a fan. </p> <p>Im leaning towards the Mediasonic one at $140. <a href=\"https://www.amazon.com/gp/product/B078YQHWYW\">https://www.amazon.com/gp/product/B078YQHWYW</a></p> <p>But this TERRAMASTER one came up on some lists too for $190. <a href=\"https://www.amazon.com/dp/B0CTTL9R7Z\">https://www.amazon.com/dp/B0CT",
        "id": 4011111,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqy8in/best_cheap_4bay_das_w_usbc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best cheap 4bay DAS w USBC?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hot-Significance2075",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T16:57:33.696172+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T15:49:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve realized I\u2019m sitting on terabytes of old backups, photos, downloads, and random \u201cjust in case\u201d files that I\u2019ll probably never touch again. It\u2019s not that I need them it\u2019s more like I can\u2019t bring myself to delete them. Every drive feels like a time capsule. </p> <p>At the same time, I\u2019ve been trying to simplify and live more intentionally with my tech. I only keep one banking app, one security app, one storage app yet my drives are chaos. </p> <p>How do you all handle the mental tug-of-war between wanting a clean, minimal setup and the urge to keep everything \u201cjust in case\u201d? Is there a middle ground that doesn\u2019t feel like losing data forever?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hot-Significance2075\"> /u/Hot-Significance2075 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqxy2f/trying_to_find_balance_between_being_a_data/\">[link]</a></span> &#32; <span><a href=\"https://www.",
        "id": 4011108,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqxy2f/trying_to_find_balance_between_being_a_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to find balance between being a data hoarder and wanting digital minimalism",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Express-Obj3ct",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T16:57:34.415454+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T15:21:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I found some refurbished 12tb HGSTs for a decent price (finally), but they have about 7 years of power on hours, so almost since the manufacturing date. One year warranty from the seller</p> <p>Thoughts on maybe getting 2 or 3 of them for the backup?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Express-Obj3ct\"> /u/Express-Obj3ct </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqx7n0/7_year_old_12tb_hgst/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqx7n0/7_year_old_12tb_hgst/\">[comments]</a></span>",
        "id": 4011112,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqx7n0/7_year_old_12tb_hgst",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "7 year old 12tb HGST",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zukic80",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T15:49:52.203792+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T15:17:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>im trying to configure a goodsync job which excludes files that have the offline attribute.</p> <p>these files have been cool tiered by azure file sync. These files exist on the destination server only.<br/> the source server does not have any cool tiered files. </p> <p>I need to copy data from the source server to the destination server<br/> folders on the destination server contain files that have been cool tiered.<br/> during analysis i need it to pick up any new files that need to be copied over from the source to the destination. The analysis must ignore files that are cool tiered (have the offline attribute/reparse file) and not download them back from the the cloud.</p> <p>at the moment i cannot get it to work.<br/> ive configured a test job with the isoffline filter but when the analysis of the folder finishes, it downloads the file from the cloud and the cool tiered icon is gone.<br/> the file size goes from 0 on disk to the full file size.</",
        "id": 4010487,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqx3hh/goodsync_filters_trying_to_use_isoffline",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "goodsync filters - trying to use isoffline",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IT_ISNT101",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T15:49:52.893911+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T15:06:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey Everyone, </p> <p>I just thought this may be interesting... I had a Hitachi Ultrastar HE10 fail yesterday.... Just tens of errors </p> <p><code>[108016.536763] ata2.00: status: { DRDY SENSE ERR }</code></p> <p><code>[108016.536768] ata2.00: error: { ICRC ABRT }</code></p> <p><code>[108016.536776] ata2: hard resetting link</code></p> <p><code>[108016.843331] ata2: SATA link up 6.0 Gbps (SStatus 133 SControl 310)</code></p> <p><code>[108016.864974] ata2.00: configured for UDMA/33</code></p> <p><code>[108016.865033] ata2: EH complete</code></p> <p><code>[108016.885461] ata2.00: sense data available but port frozen</code></p> <p><code>[108016.885475] ata2.00: exception Emask 0x11 SAct 0x20000 SErr 0x0 action 0x6 frozen</code></p> <p><code>[108016.885493] ata2.00: irq_stat 0x48000008, interface fatal error</code></p> <p><code>[108016.885502] ata2.00: failed command: WRITE FPDMA QUEUED</code></p> <p><code>[108016.885508] ata2.00: cmd 61/40:88:98:c9:27/0",
        "id": 4010488,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqwteh/hgst_10_tb_recertified_from_amazon_failed_shocked",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HGST 10 TB recertified from Amazon failed... Shocked by outcome",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LesturTheMolester",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T14:40:35.623715+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T14:28:50+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LesturTheMolester\"> /u/LesturTheMolester </a> <br/> <span><a href=\"/r/data/comments/1oqvupq/good_reliable_sources/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqvv8b/good_reliable_sources/\">[comments]</a></span>",
        "id": 4009959,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqvv8b/good_reliable_sources",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Good reliable sources",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AshleyAshes1984",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T16:57:34.028529+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T14:27:45+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqvu9c/why_have_a_large_offline_media_hoard_why_to_make/\"> <img src=\"https://preview.redd.it/lzcfzu01huzf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc8d714e8fb1552ac5b47358000546f55bd69936\" alt=\"Why have a large offline media hoard? Why to make the ultimate Christmas playlist of course.\" title=\"Why have a large offline media hoard? Why to make the ultimate Christmas playlist of course.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Oh sure, anyone can just watch Christmas movies. I built a Smart Playlist in Kodi to filter for everything containing &#39;Christmas&#39;. Then cherry picked things from that to filter out holiday cooking shows, other stuff and false positives, to get a resulting playlist of &#39;Christmas Episodes&#39; from cartoons and sitcoms mostly, shuffled it entirely and am just slowly going to spin through all 218 items between now and December 25th. This is what no streaming app ",
        "id": 4011110,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqvu9c/why_have_a_large_offline_media_hoard_why_to_make",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/lzcfzu01huzf1.png?width=640&crop=smart&auto=webp&s=dc8d714e8fb1552ac5b47358000546f55bd69936",
        "title": "Why have a large offline media hoard? Why to make the ultimate Christmas playlist of course.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ClimateOk3100",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T16:57:34.800566+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T12:48:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, for a bit of explanation, I&#39;d consider myself a novice Python programmer (and computer programmer in general). Over the course of the past few months, I would&#39;ve crafted small scripts that are personally useful for me (such as a script that clones an .iso image of what I hope are most storage media like flash drives--improved with the help of ChatGPT--or one that retrieves JSON weather data from a free API); at least as of now, I&#39;m not going to be building the next cybersecurity system, but I&#39;m pretty proud of how far I&#39;ve gotten for a novice. So, for the sake of a possible programming idea, could any knowledgeable individuals give me some information concerning how audiovisual disc-decryption software (such as DVDFab&#39;s Passkey or Xreveal) <em>works</em>? Thanks! Note: This request is only for making backup copies of DVDs and Blu-rays I <strong><em>legally own</em></strong> <strong><em>and nothing else.</em></strong></p> </",
        "id": 4011113,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqtikn/discdecryption_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Disc-decryption help.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DoctorFosterGloster",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T11:18:12.106065+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T10:35:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been using Handbrake and MakeMKV to rip old family videos off of DVDs. All have gone okay except for one. I&#39;m wondering if anyone can give suggestions to next steps?</p> <p>Here are the symptoms:</p> <p>- The disc had 2 small (ant sized) scratches but I buffed them out with toothpaste. The rest of the disc is fine. The disc is a imitation CD-R from about 2008 and has been in a case since then.</p> <p>- I can open the disc in file explorer but can&#39;t copy them to my PC.</p> <p>- I can *sometimes* watch the video on VLC player. Sometimes VLC doesn&#39;t want to load the video at all and it gets stuck loading. No error messages... just endless loading. When it plays it stutters a little bit to load, but stops at about 25% through.</p> <p>- I can load the DVD into Handbrake. HB sometimes loads the DVD and the previews (i&#39;ve set it to 50, and it loads them fine from beginning to end of the video). However, when I go to encode, it gets a",
        "id": 4008412,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqr0ob/disc_drive_cant_read_dvd_makes_stun_gun_sounds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Disc drive can't read dvd. Makes stun gun sounds. Help!?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thereal_redditer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T10:15:53.984924+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T09:54:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqqcpb/aliexpress_big_sale_exclusive_20_off_for_us/\"> <img src=\"https://preview.redd.it/tujecgey4tzf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=923e8a9ab65bf6ec7e9e728c3393948c0b88d58b\" alt=\"AliExpress Big Sale! Exclusive 20% off for US customers. Great chance to save on your purchase\" title=\"AliExpress Big Sale! Exclusive 20% off for US customers. Great chance to save on your purchase\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thereal_redditer\"> /u/thereal_redditer </a> <br/> <span><a href=\"https://i.redd.it/tujecgey4tzf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqqcpb/aliexpress_big_sale_exclusive_20_off_for_us/\">[comments]</a></span> </td></tr></table>",
        "id": 4007983,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqqcpb/aliexpress_big_sale_exclusive_20_off_for_us",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/tujecgey4tzf1.png?width=640&crop=smart&auto=webp&s=923e8a9ab65bf6ec7e9e728c3393948c0b88d58b",
        "title": "AliExpress Big Sale! Exclusive 20% off for US customers. Great chance to save on your purchase",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/UgreenNASync",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T09:13:40.160948+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T07:46:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oi1vhq/halloween_giveaway_share_your_halloween_memory/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">https://www.reddit.com/r/DataHoarder/comments/1oi1vhq/halloween_giveaway_share_your_halloween_memory/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UgreenNASync\"> /u/UgreenNASync </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqod47/ugreenlast_72_hours_to_win_wonderful_prizes_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqod47/ugreenlast_72_hours_to_win_wonderful_prizes_for/\">[comments]</a></span>",
        "id": 4007515,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqod47/ugreenlast_72_hours_to_win_wonderful_prizes_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "UGREEN-Last 72 hours to win wonderful prizes for free!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Severe_Tale_4704",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T07:17:03.201663+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T07:01:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Thinking Raid 1, on a JBOD Yottamaster 5 Bay.</p> <p>Using software to mirror 2x 24TB drives, Pull and clone to a 3rd 24TB every 6 months..</p> <p>Already aquired 2 Yotta master 5 bay banks</p> <p>1 x hardware offline cloner 2x HDD bay</p> <p>4 x 24TB for Mirrors 2 x 16TB (0% full) 2 x 12 TB (Half Full) 4 x 4TB (all full) 2 x 4TB (Failed)</p> <p>Be savage, Grill me.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Severe_Tale_4704\"> /u/Severe_Tale_4704 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqnnmv/raid_1_clone_every_6_mnths/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqnnmv/raid_1_clone_every_6_mnths/\">[comments]</a></span>",
        "id": 4007225,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqnnmv/raid_1_clone_every_6_mnths",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Raid 1 + Clone every 6 mnths",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Vivid_Stock5288",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T10:15:54.791390+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T06:51:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So It all began with one brand\u2019s listing. Then another. Now I\u2019ve got months of product pages old images, changing prices, specs that disappeared. It\u2019s oddly satisfying, like watching digital fossils form. I&#39;m thinking of compressing and timestamping them weekly, maybe release snapshots later. Does anyone here archive structured data this way? Or is everyone focused on full HTML dumps?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vivid_Stock5288\"> /u/Vivid_Stock5288 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqni3k/i_have_recently_started_archiving_product_pages/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqni3k/i_have_recently_started_archiving_product_pages/\">[comments]</a></span>",
        "id": 4007984,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqni3k/i_have_recently_started_archiving_product_pages",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I have recently started archiving product pages like they\u2019re postcards from the internet.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/agent_smith88",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T05:14:33.071810+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T04:16:44+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqkqoc/ya_win_some_ya_lose_some/\"> <img src=\"https://b.thumbs.redditmedia.com/qLM0F0R4bJZHhW4aiR3cNLDU1dr3XvqeCRZ-V7Lvbqg.jpg\" alt=\"Ya win some, ya lose some\" title=\"Ya win some, ya lose some\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Our university has a resale program, and I like to stop in every now and again. They had some HGST 8TB for $50, which seems pretty cheap. Well they\u2019ve got.. just a couple hours on them to say the least! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/agent_smith88\"> /u/agent_smith88 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1oqkqoc\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqkqoc/ya_win_some_ya_lose_some/\">[comments]</a></span> </td></tr></table>",
        "id": 4006670,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqkqoc/ya_win_some_ya_lose_some",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/qLM0F0R4bJZHhW4aiR3cNLDU1dr3XvqeCRZ-V7Lvbqg.jpg",
        "title": "Ya win some, ya lose some",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Zoods_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T04:13:49.812266+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T03:32:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to download all the posts I have saved, they&#39;re mainly memes and I have 1000s of them, I&#39;m not moving to a new account or anything, but I just solely want to download them, any way I can do this fast?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Zoods_\"> /u/Zoods_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqjv3h/downloading_all_instagram_posts_on_the_saved_list/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqjv3h/downloading_all_instagram_posts_on_the_saved_list/\">[comments]</a></span>",
        "id": 4006446,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqjv3h/downloading_all_instagram_posts_on_the_saved_list",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "downloading all Instagram posts on the \"saved\" list",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lawrencewil1030",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T04:13:49.626005+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T03:13:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If your budget does not allow quality offsite backup, you should not be guilt-tripped into buying a sketchy service. If you do, it may be worse than just doing 3-3-0 or 2-2-0 (Both does not require offsite) as your data could be read, you may lose data, literally anything could happen. And look, ANY backup is better than no backup. Even to the same drive.</p> <p>And before anyone says something along the lines of &quot;It&#39;s just a few dollars,&quot; it may be hard to understand, but there are people who can&#39;t afford &quot;just a few dollars.&quot;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lawrencewil1030\"> /u/lawrencewil1030 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqjhzw/dont_be_guilttripped_into_321_backup_if_your/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqjhzw/dont_be_guilttripped_into_321_backup_if_your/\">[comments]</",
        "id": 4006445,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqjhzw/dont_be_guilttripped_into_321_backup_if_your",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Don't be guilt-tripped into 3-2-1 backup if your budget only allows sketchy services (Please read the rest before giving opinions)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No-Combination-2991",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T02:09:13.991291+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T02:04:21+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Combination-2991\"> /u/No-Combination-2991 </a> <br/> <span><a href=\"/r/Hardcore/comments/1oqhtva/this_is_gonna_be_a_long_shot_but_here_we_go_does/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqi1gx/this_is_gonna_be_a_long_shot_but_here_we_go_does/\">[comments]</a></span>",
        "id": 4005900,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqi1gx/this_is_gonna_be_a_long_shot_but_here_we_go_does",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "This is gonna be a long shot but here we go, does anyone on here know if there is like an archive of old school hardcore bands ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tookerken",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T01:07:51.669236+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T00:34:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is it or is it not possible to rip my physical media that is 4k and 4k ultra as well as all the others?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tookerken\"> /u/tookerken </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqg2qn/making_backups_my_my_4k_ultra_disks/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oqg2qn/making_backups_my_my_4k_ultra_disks/\">[comments]</a></span>",
        "id": 4005657,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oqg2qn/making_backups_my_my_4k_ultra_disks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Making backups my my 4k Ultra disks",
        "vote": 0
    }
]