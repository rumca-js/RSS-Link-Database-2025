[
    {
        "age": null,
        "album": "",
        "author": "/u/IronicallyIdiotic",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T20:47:52.110549+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T19:55:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all. As you can see from the flair, I am getting just getting started. I am not unfamiliar with programming (started out with C++, typically use Python for ease of use), so I&#39;m not a complete baby, I just need a push in the right direction. </p> <p>I am attempting to build a program -- probably in python -- that will search for the chat widget and automatically fill it out with a designated question, or if it can&#39;t find the widget. search for the customer service email and send it that way. The email portion I think I can handle, I&#39;ve written scripts to send automated emails before. What I need help with is the browser automation with the chat widget.</p> <p>In my light Googling, I of course came across Selenium and Playwright. What is the general consensus on when to use which framework? </p> <p>And then when it comes to searching for the chat widget, it&#39;s not like they are all going to helpfully be named the same thing. I&#39;m su",
        "id": 4013202,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1or4h7y/writing_a_script_to_fill_out_the_chat_widget_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Writing a script to fill out the chat widget on retail websites.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/armanfixing",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T21:50:04.756592+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T17:56:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/webscraping\">r/webscraping</a>,</p> <p>Posted here about 3 weeks ago when I first shipped httpmorph. It was rough. Like, really rough.</p> <p>What actually changed:</p> <p>The fingerprinting works now. Not &quot;close enough&quot; - actually matching Chrome 142. I tested it against suip.biz and other fingerprint checkers, and it&#39;s showing perfect JA3N, JA4, and JA4_R matches. That was the whole point, so I&#39;m relieved.</p> <p>HTTP/2 is in. Spent too many nights with nghttp2, but it&#39;s there. You can switch between HTTP/1.1 and HTTP/2.</p> <p>Async support with AsyncClient. Uses epoll/kqueue, so it&#39;s actually async, not just wrapped blocking calls.</p> <p>Proxy support with auth. Works now.</p> <p>Connection pooling, persistent cookies, SSL verification, redirect tracking. The basics that should&#39;ve been there from day one.</p> <p>Works with <em>some</em>-protected sites now (Brotli and Zlib certificate compression).</p",
        "id": 4013664,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1or1bwh/httpmorph_update_chrome_142_http2_async_and_proxy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "httpmorph update: Chrome 142, HTTP/2, async, and proxy support",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rrdein",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T17:31:25.357958+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T17:08:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone know a site that with Cloudflare that is hard to bypass, to test a bypass solution?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rrdein\"> /u/rrdein </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1or02gb/cloudflareprotected_site_with_high_security_level/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1or02gb/cloudflareprotected_site_with_high_security_level/\">[comments]</a></span>",
        "id": 4011517,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1or02gb/cloudflareprotected_site_with_high_security_level",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cloudflare-protected site with high security level, for testing?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TraditionClear9717",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T13:04:29.123645+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T11:17:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We run a scraper that returns <strong>200 locally</strong> but <strong>403 from our DC VM</strong> (target uses nginx). No evasion (Just Kidding, We can perform evasion \ud83d\ude08), want a clean fix.</p> <p>We are using AWS EC2 Instance for Ubuntu server and also have a secondary ubuntu server on Vultr.</p> <p>Looking for:</p> <ul> <li>Key logs/evidence to collect for an appeal (headers, timestamps, traceroute, sample curl).</li> <li>Tips for working with our DC provider to escalate false positives.</li> <li>Alternatives if access is denied (APIs, licensed feeds, third-party aggregators).</li> </ul> <p>If you reply, please flag whether it\u2019s ops/legal/business experience. I&#39;ll post sanitized curl/headers on request.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TraditionClear9717\"> /u/TraditionClear9717 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oqrqot/dchosted_scraper_returning_403_work",
        "id": 4009266,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oqrqot/dchosted_scraper_returning_403_works_locally",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DC-hosted scraper returning 403 (works locally), seeking outreach-tip",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Upstairs-Public-21",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T10:42:14.268615+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T09:56:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m looking to use <strong>AI for web scraping automation</strong> and could use some advice. I want to make the scraping process smarter, faster, and more efficient. Here are a few things I\u2019m curious about:</p> <ul> <li>What are the best <strong>AI-powered scraping tools</strong> for handling complex sites (JavaScript, anti-bots)?</li> <li>How can I use AI to <strong>structure and clean</strong> scraped data automatically?</li> <li>Tips for <strong>scaling</strong> scraping operations without getting blocked?</li> <li>Any tools or techniques to bypass <strong>CAPTCHAs</strong> effectively?</li> </ul> <p>I\u2019ve tried a few basic scraping tools, but I\u2019m really interested in taking it to the next level with <strong>AI</strong>. Would love to hear your recommendations or experiences!</p> <p>Thanks in advance! \ud83d\ude4f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Upstairs-Public-21\"> /u/Upstairs-Public-21 </a> <br/> <span>",
        "id": 4008266,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oqqdlo/seeking_advice_on_ai_automation_for_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seeking Advice on AI Automation for Web Scraping Tools",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jrsevern",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T05:42:09.096463+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T05:32:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been getting into LLM-based scraping, but bot detection is a nightmare. I feel like I\u2019m constantly battling captchas and IP bans. </p> <p>I\u2019ve tried rotating IPs and all that, but it still feels like I\u2019m walking a tightrope. How do you guys manage to scrape without getting caught? Any tips or tools you swear by?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jrsevern\"> /u/jrsevern </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oqm547/how_do_you_handle_bot_detection_when_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oqm547/how_do_you_handle_bot_detection_when_scraping/\">[comments]</a></span>",
        "id": 4006892,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oqm547/how_do_you_handle_bot_detection_when_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you handle bot detection when scraping websites?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/papeloneo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-07T04:41:35.887330+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-07T04:31:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m new to web scraping, and I&#39;m doing a project in which I need to scrape listings of airbnbs along with locations to create a heat map (in Datawrapper) of airbnbs in a given area. This would mean ultimately getting a CSV file. I&#39;d like something like the information and map t<a href=\"https://app.airdna.co/data/us/airdna-411/listings?lat=28.883607&amp;lng=-82.566221&amp;zoom=7.95\">hat AirDNA has</a>, but I can&#39;t really pay 400$ for access to their info. If i achieve this, it&#39;d be cool to get pricings as well, but that&#39;s a nice plus right now. Listing locations are my main priority.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/papeloneo\"> /u/papeloneo </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oql16r/scraping_airbnb_listing_locations/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oql16r/scraping_airbnb_listing_locati",
        "id": 4006584,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oql16r/scraping_airbnb_listing_locations",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping AirBnB listing locations?",
        "vote": 0
    }
]