[
    {
        "age": null,
        "album": "",
        "author": "/u/SubjectC",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T23:37:22.330190+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T22:42:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Don&#39;t they want to make money? It costs nothing to throw it up on bandcamp.</p> <p>I&#39;m working on creating an offline music collection and its astonishing how hard it is to find files for a lot of the music I want. Often here will be a CD release but no download, so I&#39;m on discords ordering CDs just so I can rip them.</p> <p>Its crazy how much no one cares about anything anymore. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SubjectC\"> /u/SubjectC </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1mchl/its_crazy_to_me_and_really_sucks_that_a_lot_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1mchl/its_crazy_to_me_and_really_sucks_that_a_lot_of/\">[comments]</a></span>",
        "id": 4114387,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1mchl/its_crazy_to_me_and_really_sucks_that_a_lot_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Its crazy to me and really sucks that a lot of artists aren't even releasing their music to download anymore.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/petr_bena",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T21:36:18.669666+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T21:14:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Long time ago I bought some Eurocase tower case with 4x 5 inch bay, these used to be used for CD drives back in the day, later on when I was refurbishing it (in fact I still use it for my home PC, many motherboards later), I replaced those drives with hotswap bays, some of them are really nice (6x 2.5 bays in single 5 inch bay, so easily 24 hot-swap disks in tower case).</p> <p>I liked this setup so much I bought 3 more these cases and built CEPH cluster from them with many disks. Now after about a decade I was considering upgrade to some modern case (I had various issues with these Eurocase boxes, mostly build quality).</p> <p>To my surprise it seems those 5 inch bays are pretty much extinct. Nobody is manufacturing them anymore. Best I could find were some supermicro cases, which were nice, but very expensive, but even those didn&#39;t have more than 2 of them. They did come with built-in hot-swap bays though, but they are built-in, non replaceable,",
        "id": 4113421,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1k2gq/are_5_inch_bays_extinct_how_do_people_build",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are 5 inch bays extinct? How do people build serviceable hot-swap systems these days?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/__rainmaker",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T21:36:18.898237+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T21:07:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi All</p> <p>Hoping someone can help me figure this out as I am relatively new to burning/copying DVDs. I have some media on a DVD that I&#39;d like to make a copy of, but I don&#39;t wanna digitize it I just want to copy it to another DVD. I have a DVD drive that I plug into my Mac via usb, and the disc will show up on my finder window but when I burn the DVD and put it in the player it tells me it&#39;s not readable media.</p> <p>Thanks in advance for any insight :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/__rainmaker\"> /u/__rainmaker </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1jvwo/copying_dvds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1jvwo/copying_dvds/\">[comments]</a></span>",
        "id": 4113422,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1jvwo/copying_dvds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Copying DVDs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JayGarza675",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T20:30:18.095678+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T20:25:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>there was a website called <a href=\"https://thedirectorscommentary.tumblr.com/\">the directors commentary</a> but none of the mega links work anymore and i&#39;m not seeing an archive, streaming websites don&#39;t have the commentary tracks which is unfortunate</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JayGarza675\"> /u/JayGarza675 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1is9c/does_anyone_know_where_i_can_find_dvd_commentaries/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1is9c/does_anyone_know_where_i_can_find_dvd_commentaries/\">[comments]</a></span>",
        "id": 4112903,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1is9c/does_anyone_know_where_i_can_find_dvd_commentaries",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "does anyone know where i can find dvd commentaries?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GearEarly4411",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:36:48.858568+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T19:39:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Archive is ~25TB and growing. DAS will be read only, editing on separate SSD.</p> <p><strong>Plan</strong> </p> <ol> <li>5-bay RAID enclosure that supports hardware RAID 5 (TerraMaster D5\u2011310?)</li> <li>Install 3 15TB WD Red Pro drives (1 for parity)</li> <li>Configure hardware RAID 5 </li> <li>Format, APFS (Mac)</li> <li>Keep 2 empty bays for future expansion</li> </ol> <p><strong>Questions</strong></p> <ol> <li>Is it overkill to replace 1 drive every 3\u20135 years to avoid simultaneous failure? </li> <li>Is 4:1 drives for storage:parity sufficient? Or should I aim for 3:2?</li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GearEarly4411\"> /u/GearEarly4411 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1hj1h/das_noob_review_my_plan/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1hj1h/das_noob_review_my_plan/\">[comments]</a></span>",
        "id": 4113910,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1hj1h/das_noob_review_my_plan",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DAS noob, review my plan",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ShelZuuz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T20:30:17.213837+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T19:38:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1hi4g/is_this_a_normal_write_profile_for_a_samsung_870/\"> <img src=\"https://preview.redd.it/9lx7jcepn92g1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9f2727e7845f388088230bbd192db6f35a18e93\" alt=\"Is this a normal write profile for a SAMSUNG 870 QVO 8TB? I use a pair of those striped to stage backups but copying data to those drives have a pretty dramatic drop-off in perf after the first 8 minutes. This shape is 100% repeated on every backup, no matter the data. Source is CD8P PCIe5 NVMEs.\" title=\"Is this a normal write profile for a SAMSUNG 870 QVO 8TB? I use a pair of those striped to stage backups but copying data to those drives have a pretty dramatic drop-off in perf after the first 8 minutes. This shape is 100% repeated on every backup, no matter the data. Source is CD8P PCIe5 NVMEs.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ShelZuuz\"> /u/ShelZuuz </a> <br/> <spa",
        "id": 4112902,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1hi4g/is_this_a_normal_write_profile_for_a_samsung_870",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/9lx7jcepn92g1.png?width=640&crop=smart&auto=webp&s=a9f2727e7845f388088230bbd192db6f35a18e93",
        "title": "Is this a normal write profile for a SAMSUNG 870 QVO 8TB? I use a pair of those striped to stage backups but copying data to those drives have a pretty dramatic drop-off in perf after the first 8 minutes. This shape is 100% repeated on every backup, no matter the data. Source is CD8P PCIe5 NVMEs.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CAPT4IN_N00B",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:36:49.164198+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T18:41:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi!</p> <p>I&#39;m currently in the process of trying to backup all my files. I have an old Toshiba external hard drive (spinning disk) which is a few years old. I have many important files on this drive, which are sadly the only place they have been stored. Recently I have learnt about data corruption, bit-rot and so on, which is why I bought a NAS with ZFS file system to protect my data. But before I backup the data from the drive to the NAS, I wanna make sure its in as good health as possible. So my question is, what is the best way to find corrupt files or files that have been affected by bit-rot? I have ran a full diagnosis of the hard drive using Toshibas own diagnostictool software, which it passed and the SMART results looked good. Other things which I have seen recomended to check are if the filesystem is correct in disk managment, which it is, check for problems on the drive in the file explorer drive properites, which there are non. I&#39;m",
        "id": 4113911,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1fxsb/is_it_possible_to_find_corrupt_or_bitrot_affected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it possible to find corrupt or bit-rot affected files on external HDD (and fixing them?)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cowpunk52",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T18:27:54.413712+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T17:59:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It&#39;s composed of local Albuquerque and national news coverage of the event, including about 20 minutes of commercials, both national and local. There is also news broadcast recordings of other stories pertinent to the time. I&#39;ve already separated out the commercials into their own package, although I have the full original sequence as well. What&#39;s the best thing to do with this footage? Internet Archive? Youtube?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cowpunk52\"> /u/cowpunk52 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1er3k/ive_just_digitized_3_hours_of_television/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1er3k/ive_just_digitized_3_hours_of_television/\">[comments]</a></span>",
        "id": 4111830,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1er3k/ive_just_digitized_3_hours_of_television",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've just digitized 3 hours of television recordings from Pope John Paul's visit to Denver in 1993. What to do with footage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BrightResearch18",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:36:48.329258+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T17:57:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone else feeling the pressure of how fast \u201ccold storage\u201d keeps becoming\u2026 not so cold anymore? I swear, every year my setup grows, and suddenly the drives that used to feel massive now barely cover new backups, media dumps, and random datasets I \u201cmight need someday.\u201d</p> <p>The real struggle for me lately has been balancing long-term archiving with cloud redundancy. Local storage is reliable, but off-site gets expensive fast, especially once you start hitting egress fees or want to sync larger collections. I\u2019ve seen a few people in <a href=\"/r/OrbonCloud\">r/OrbonCloud</a> digging into alternative ways to handle off-site backups without the usual surprise costs, which got me thinking more about how everyone here is managing their hybrid setups.</p> <p>So I\u2019m curious \u2014 what\u2019s your current strategy for long-term hoarding?</p> <p>Do you:</p> <ul> <li>Go all-in on local storage?</li> <li>Use cloud purely as a last-resort backup?</li> <li>Or have you foun",
        "id": 4113909,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1eov0/anyone_else_feeling_the_pressure_of_how_fast_cold",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone else feeling the pressure of how fast \u201ccold storage\u201d keeps becoming\u2026 not so cold anymore?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/InternationalLoan470",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:36:49.438656+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T17:36:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This may be a dumb question but Im new to NAS storage. I have over 300 Gb and growing of TV shows, Ive been naming them and putting them in file folders so far, should I use jelly fin to make my life easier. (File explorer is starting to take to much time to open the one folder containing all the sub folders)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/InternationalLoan470\"> /u/InternationalLoan470 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1e4y8/jellyfin_should_i_use_it/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1e4y8/jellyfin_should_i_use_it/\">[comments]</a></span>",
        "id": 4113912,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1e4y8/jellyfin_should_i_use_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Jellyfin Should I use it",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Clear_Extent8525",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:36:48.206110+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T17:20:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m hitting the 400TB mark on my &#39;deep cold&#39; archive (stuff I genuinely hope to never look at again) and I&#39;m facing the same existential choice: <strong>Buy-once LTO setup (LTO-9) or commit fully to the Cloud Deep Archive tiers (AWS Glacier Deep, etc.)?</strong></p> <p>The per-TB storage cost is the easy math: LTO (factoring in the tape/drive CAPEX over 5 years) is still marginally cheaper than the $0.99/TB/month cloud services. That&#39;s the part we all celebrate.</p> <p>But the hidden financial monster is the <strong>Retrieval Cost</strong>.</p> <p>In a hypothetical worst-case scenario (e.g., house burns down, NAS is gone, I need to restore 100TB of archival data immediately to a remote server):</p> <ul> <li><strong>LTO:</strong> The cost is the CAPEX of the second off-site drive, the media cost, and the shipping/labor to physically retrieve and mount the tapes. Time-to-Restore (RTO) is high (days/weeks). Retrie",
        "id": 4113908,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1dpc7/lto_vs_glacier_deep_archive_when_the_cost_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LTO vs. Glacier Deep Archive: When the Cost of Retrieval Kills the Initial $/TB Savings",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SnooJokes8831",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T18:27:55.402749+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T17:19:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>so i thought i can download those episodes with yt dlp and archive it on youtube for personal use are there any scrambler software i can use to doge content id, im asking does tools for scrambling exist</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SnooJokes8831\"> /u/SnooJokes8831 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1dnv0/can_i_archive_old_local_dubbed_anime_on_youtube/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1dnv0/can_i_archive_old_local_dubbed_anime_on_youtube/\">[comments]</a></span>",
        "id": 4111832,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1dnv0/can_i_archive_old_local_dubbed_anime_on_youtube",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "can i archive old local dubbed anime on youtube , poemon india said they will delete indigo league on 12th jan",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/WolfyGaming18",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:36:49.936298+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T16:50:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guysl,</p> <p>I\u2019m nearing the 200TB mark on my personal archive, and my biggest pain point right now isn&#39;t the electricity bill, it\u2019s the nagging, sinking fear that my Tier 2 (Cold Storage) is quietly degrading into useless noise.</p> <p>I\u2019ve been a religious follower of the 3-2-1 rule, but applying a full integrity check across my 100TB offline array (drives spun up only quarterly) is becoming an all-day, stressful event. I run a mix of ZFS on the active NAS, but the cold side is a bunch of staggered, encrypted BTRFS pools, plus an Amazon Glacier Deep Archive layer for the &quot;1.&quot;</p> <p>My current check process involves:</p> <ol> <li>Spinning up the cold array.</li> <li>Running a recursive <code>rclone check --checksum</code> on the bulk of the BTRFS data against its manifest.</li> <li>Spot-checking some key archives with <code>par2</code> before spinning down.</li> </ol> <p>The reality is that this is quickly becoming unsustainable. ",
        "id": 4113913,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1cvkn/the_great_checksum_fatigue_how_do_you_handle",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The Great Checksum Fatigue: How do you handle cold-storage integrity without losing your mind (or your data)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/This_Environment_472",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T18:27:54.825618+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T16:48:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><h1>My university has these courses files and pdfs on sharepoint and I need to download them, not access them. Downloading is forbidden but I already paid for my files. I need a way to downlaod them.</h1> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/This_Environment_472\"> /u/This_Environment_472 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1ctx9/sharepoint/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1ctx9/sharepoint/\">[comments]</a></span>",
        "id": 4111831,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1ctx9/sharepoint",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SharePoint",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dennis0162",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T16:25:29.210167+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T15:38:31+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p1axai/please_advisement_on_disk_upgrades/\"> <img src=\"https://b.thumbs.redditmedia.com/vSei0qsaJC4ui5v94fgluYrMxZVHLIlXG4iY4bmKdLc.jpg\" alt=\"Please advisement on disk upgrades\" title=\"Please advisement on disk upgrades\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello fellow Datahoarders,</p> <p>as the title already gave away my Synology NAS are getting full (again \ud83d\ude05) and I am looking to upgrade them, I did some research already but before I purchase looking for some advisement. I am located in the Netherlands.</p> <p>I want to do a serious upgrade of my system because this is the third time already first from 2 to 4 bays and now a disk upgrade.<br/> I was first looking for other disk but looked to it today and the prices are raised again..</p> <p>I have now 4 WD HDD 3.5&quot; 6TB S-ATA3 256MB WD60EFAX Red. I can buy them on my company so that makes a big difference.</p> <p>Are the disk that I selected ",
        "id": 4110529,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p1axai/please_advisement_on_disk_upgrades",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/vSei0qsaJC4ui5v94fgluYrMxZVHLIlXG4iY4bmKdLc.jpg",
        "title": "Please advisement on disk upgrades",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dr_Merf",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T14:24:02.270785+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T14:14:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Newegg has a 20 TB WD Red Pro NAS drive listed for $350 right now, but it says the sale ends tonight.</p> <p>Most Black Friday deals kick into full gear tomorrow, so does this mean I should wait it out? Or is it unlikely that they\u2019ll drop the price even more than it\u2019s at right now.</p> <p>The price is the same on Western Digital\u2019s site with no indication of an ending sale, and about $30 more on Amazon with no indication of it either.</p> <p>Don\u2019t want to jump the gun and buy it at nearly $18/TB if there\u2019s a chance it drops closer to $15 or $16. Not sure how the sales have looked the past couple years, someone please fill me in, thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dr_Merf\"> /u/Dr_Merf </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p18rna/buy_wd_red_pro_now_or_wait/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p18rna/buy_wd_red_",
        "id": 4109389,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p18rna/buy_wd_red_pro_now_or_wait",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Buy WD Red Pro now or wait?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chrelled",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T13:23:30.930439+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T12:41:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Alright, I&#39;ll be real with you guys. I&#39;m scraping multiple e-commerce sites and news portals for a research project, and I&#39;m hitting walls everywhere. Even with proper delays and rotating user agents, my IPs keep getting nuked after a few days.</p> <p>I&#39;ve burned through three different proxy services this month alone. Either the IPs are already blacklisted, the speeds are unusable, or the pricing gets ridiculous when you actually need decent bandwidth.</p> <p>Recently stumbled upon SimplyNode residential proxy while searching for alternatives. Their 8M+ IP pool looks promising, and the traffic-based billing might actually save me money compared to these &quot;unlimited&quot; plans that throttle you into oblivion.</p> <p>But I&#39;ve been burned before. So I&#39;m asking the experts:</p> <p>How do you actually evaluate if a residential proxy provider isn&#39;t selling you recycled datacenter IPs?</p> <p>What&#39;s your experience with ",
        "id": 4108851,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p16n7p/residential_proxies_for_web_scraping_anyone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Residential proxies for web scraping - anyone actually found something that works long-term?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Appropriate-Look-875",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T12:23:09.350207+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T11:58:05+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p15s45/i_put_together_a_small_tool_for_managing_saved/\"> <img src=\"https://preview.redd.it/sjzkhd5xd72g1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a84f9e410925f8f227ee129d216f5218a7f0b761\" alt=\"I put together a small tool for managing saved Reddit comment threads. I\u2019m looking for feedback if you have a moment.\" title=\"I put together a small tool for managing saved Reddit comment threads. I\u2019m looking for feedback if you have a moment.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Appropriate-Look-875\"> /u/Appropriate-Look-875 </a> <br/> <span><a href=\"https://i.redd.it/sjzkhd5xd72g1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p15s45/i_put_together_a_small_tool_for_managing_saved/\">[comments]</a></span> </td></tr></table>",
        "id": 4108411,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p15s45/i_put_together_a_small_tool_for_managing_saved",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/sjzkhd5xd72g1.png?width=640&crop=smart&auto=webp&s=a84f9e410925f8f227ee129d216f5218a7f0b761",
        "title": "I put together a small tool for managing saved Reddit comment threads. I\u2019m looking for feedback if you have a moment.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Challenge_3038",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T12:23:09.468766+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T11:56:03+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Challenge_3038\"> /u/Ok_Challenge_3038 </a> <br/> <span><a href=\"https://king-kibugenza.web.app/chat_viewer.html?fw=2P3b0j5k0C2o6J7z\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p15qrj/i_wanted_to_keep_old_whatsapp_chats_offline/\">[comments]</a></span>",
        "id": 4108412,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p15qrj/i_wanted_to_keep_old_whatsapp_chats_offline",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I wanted to keep old WhatsApp chats offline without storing them anywhere as my data... a tool to help viewing",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gabriel8577",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T15:24:52.497270+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T07:45:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Not sure why people keep saying \u201cjust move to Dropbox Advanced\u201d after Google tightened limits again. Dropbox already walked back its \u201cunlimited\u201d promises, and neither Google nor Dropbox is offering the reliability, speed, or long-term guarantees people think they are.</p> <p>Cloud services change their rules whenever they want\u2014throttling, caps, policy shifts, random sync issues\u2014and most users still don\u2019t have great upload speeds anyway (looking at you, Comcast).</p> <p>At this point, going back to <strong>local storage</strong> makes more sense, especially with HDD capacities skyrocketing. <strong>32TB drives are already shipping</strong>, and even bigger ones are coming soon. A solid NAS setup is starting to look like the only truly future-proof option.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gabriel8577\"> /u/gabriel8577 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p11g4z/you_",
        "id": 4109972,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p11g4z/you_cannot_fully_trust_the_cloud_local_storage_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "You cannot fully trust the cloud, local storage is the way",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/basedbot200000",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T05:16:35.466170+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T04:58:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0ylkg/psa_reddit_public_chat_channels_are_in_readonly/\"> <img src=\"https://preview.redd.it/o7voigswa52g1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45ecaa9a569cc6f52f6a58c2a513f015db13180e\" alt=\"PSA: Reddit public chat channels are in read-only mode\" title=\"PSA: Reddit public chat channels are in read-only mode\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Public reddit chats are now in read-only mode and they will be gone soon. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/basedbot200000\"> /u/basedbot200000 </a> <br/> <span><a href=\"https://i.redd.it/o7voigswa52g1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0ylkg/psa_reddit_public_chat_channels_are_in_readonly/\">[comments]</a></span> </td></tr></table>",
        "id": 4106143,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0ylkg/psa_reddit_public_chat_channels_are_in_readonly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/o7voigswa52g1.png?width=320&crop=smart&auto=webp&s=45ecaa9a569cc6f52f6a58c2a513f015db13180e",
        "title": "PSA: Reddit public chat channels are in read-only mode",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GSDavisArt",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T04:15:55.998852+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T04:05:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there, I&#39;ve read through a lot of older posts here and I&#39;m not sure I&#39;m getting the best answer. I had a drive that I thought was dying a few years back, so I bought an SSD and set the HDD as a slave (Effectively) and forgot about it. It&#39;s now been 5 years, and I have a massive 16TB external that I&#39;m moving everything over to so I can sort stuff once it&#39;s in one place.</p> <p>So, tonight I&#39;m working with this old C drive (It used to be my main). I really haven&#39;t used it at all since 2020, but It has an old copy of Windows on it and a bunch of Steam installations. I just want to get anything that might be personal off of it before I nuke it (Pictures on the Desktop, etc). Do I just copy out the &quot;User&quot; directories? Is that all I need to worry about? Like I said, this copy of Windows hasn&#39;t been in use since 2020, so I don&#39;t need to save any configuration stuff, I just want to save the things the norm",
        "id": 4105937,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0xl5a/clearing_out_an_old_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Clearing out an old drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Great_Phoenix",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T07:19:16.838411+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T03:09:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve noticed that my \u201cnotes\u201d have slowly turned into an actual multi-gigabyte archive: PDFs, screenshots, text files, session notes, timelines, research clippings, emails, and all the little fragments that accumulate if you\u2019ve been collecting information for years.</p> <p>At some point it stops being casual and starts feeling like digital preservation.</p> <p>I know many people here rely on straight folder hierarchies, Obsidian vaults, Zotero setups, local wikis, or plain Markdown. I\u2019ve been experimenting with a few approaches myself, including an offline tool called VaultBook that can store everything locally and search inside attachments like PDFs or images without touching the cloud. It\u2019s been handy for keeping related notes connected, but I\u2019m sure some of you have far more varied setups.</p> <p>So I\u2019m curious:</p> <p>How do you manage a personal archive when it spans years of files and notes? \u2022 Do you tag everything? \u2022 Keep a strict directory stru",
        "id": 4106679,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0wf3t/how_do_you_all_organize_longterm_personal_notes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you all organize long-term personal notes and documents when the archive becomes huge?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mydogmuppet",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T07:19:17.520086+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T01:47:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been a PC builder and amateur data hoarder (35TB) for some time.</p> <p>I monitor my HDDs with Crystal Disk and have always tended to replace or monitor closely from 10,000 power on hours or the 5 year anniversary. Bitter experience forces a Grandfather - Father - Son backup protocol. No data loss these days.</p> <p>I am rigourous about avoiding heat which over 20+ years seems to be the #1 hdd killer. I switch off HDDs when not in use. </p> <p>With the advent of AI I would have thought the HDD monitoring utilites would have become exponentially better at impending HDD failure.</p> <p>Are there any tools out there offering real world predictive facilities these days ? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mydogmuppet\"> /u/mydogmuppet </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0up1e/ai_and_predictive_hdd_failure/\">[link]</a></span> &#32; <span><a href=\"https://www.",
        "id": 4106680,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0up1e/ai_and_predictive_hdd_failure",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI and predictive hdd failure",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Separate-Effort3640",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T01:12:37.471816+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T01:06:04+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1p0ts2h/animate_archive_is_temporarily_back/\"> <img src=\"https://b.thumbs.redditmedia.com/fdbV_1heUSGksCwFyDkj0w0xRzcS1RVuTQAMcZ190eQ.jpg\" alt=\"ANIMATE ARCHIVE IS TEMPORARILY BACK!\" title=\"ANIMATE ARCHIVE IS TEMPORARILY BACK!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Though it&#39;s called the &#39;Animator Archive&#39;, somebody seems to be trying to get the files working!</p> <p><a href=\"https://preview.redd.it/zv78x5ny442g1.png?width=820&amp;format=png&amp;auto=webp&amp;s=17c08a52bfa507076616a6b2f643af6a39df7ecc\">https://preview.redd.it/zv78x5ny442g1.png?width=820&amp;format=png&amp;auto=webp&amp;s=17c08a52bfa507076616a6b2f643af6a39df7ecc</a></p> <p><a href=\"https://animatorarchive.neocities.org/\">https://animatorarchive.neocities.org/</a></p> <p>Before it can get copyright striked by that stupid corporation Adobe again, I&#39;m trying to download all the files and compile them into a ZIP for a playl",
        "id": 4105167,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1p0ts2h/animate_archive_is_temporarily_back",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/fdbV_1heUSGksCwFyDkj0w0xRzcS1RVuTQAMcZ190eQ.jpg",
        "title": "ANIMATE ARCHIVE IS TEMPORARILY BACK!",
        "vote": 0
    }
]