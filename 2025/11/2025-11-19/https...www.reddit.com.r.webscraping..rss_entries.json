[
    {
        "age": null,
        "album": "",
        "author": "/u/6_clover",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T22:01:35.470353+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T21:43:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi. As mentioned above, I am trying to download a website using httrack website copier (for example, this site www.asite.com), but I am encountering some problems. These problems are mainly as follows:</p> <ol> <li>I&#39;m trying to configure the settings to download only sublinks starting with www.asite.com (e.g., subdomains like www.asite.com/any/sub/link). However, I keep seeing another site in the list of links being downloaded (e.g., www.anothersite.com is also being scanned and attempted to download). Or, the YT link on the www.asite.com site is also counted as a sublink and is being attempted to be downloaded.</li> <li>Media (photos, videos, GIFs, etc.) on the www.asite.com site may be pulled from another site (for example, photos on the www.asite.com/any/sub/link link may be pulled from www.image.com). When I set the settings to only pull data from www.asite.com, these photos are not downloaded.</li> </ol> <p>The sites and media (photos, video",
        "id": 4113689,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p1ktvi/how_do_i_customize_the_settings_for_httrack",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I customize the settings for Httrack Website Copier?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Pleasant-Hair5267",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T23:02:37.679879+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T20:20:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Sometimes when I am trying to reverse engineer a website, some responses are encrypted. </p> <p>An example:<br/> <a href=\"https://www.oddsportal.com/football/england/premier-league/burnley-chelsea-Eivnz6xJ/#ah;2;0.25;0\">https://www.oddsportal.com/football/england/premier-league/burnley-chelsea-Eivnz6xJ/#ah;2;0.25;0</a></p> <p>I know that the odds data on the website are obtained from this request:<br/> <a href=\"https://www.oddsportal.com/match-event/1-1-Eivnz6xJ-5-2-e65192954ed1df3d65428dc9393757e9.dat\">https://www.oddsportal.com/match-event/1-1-Eivnz6xJ-5-2-e65192954ed1df3d65428dc9393757e9.dat</a></p> <p>However, the response is encrypted. How should I find the codes for decrypting the responses from the JS files? Instead of going through the JS files one by one, are there quicker ways to find the keywords to search to get to the relevant code?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pleasant-Hair5267\"> ",
        "id": 4114128,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p1inmg/how_to_decrypt_encrypted_responses_from_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to decrypt encrypted responses from a website's API?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Advanced-Citron8111",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T17:52:21.051648+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T17:09:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m new to web scraping, I\u2019m using scrapy. I\u2019m very experienced in Python and decided to start scraping. The coding is very easy for me and I can have all the code very fast, but all my bugs come from entering the wrong data from HTML. </p> <p>Whats the best way to know what data is correct? I feel like there are no patterns and it\u2019s very frustrating because as soon as a think I understand, the next element has it organized entirely different. Do I need to learn HTML? Will that actually solve this problem? Because as of right now I\u2019ve been trying to just learn patterns and to just know what to do look for\u2026 but that\u2019s not really working.. any advice?</p> <p>I expected the coding and bypassing to be the hard part.. not struggling to pull the correct data\u2026</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Advanced-Citron8111\"> /u/Advanced-Citron8111 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/commen",
        "id": 4111497,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p1denz/best_way_to_learn_how_to_pull_the_correct_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to learn how to pull the correct data from HTML?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SemperPistos",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T10:46:28.211678+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T10:02:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Did someone find a way to bypass WordFence anti bot protection on WordPress sites when using crawl4ai or something else? </p> <p>It randomly kicks me out and tries to not allow as many pages as it can during the scrape <a href=\"https://paste.ofcode.org/DcmSHUbwhDez3yJzHq82wc\">https://paste.ofcode.org/DcmSHUbwhDez3yJzHq82wc</a></p> <p>Neither crawl4ai stealth or magic parameters work.</p> <p>The site I&#39;m scraping is owned by the company I work for but as the maintainers charge for any interaction we decided to scrape it and not get from the database. </p> <p>I&#39;ve had great success before with crawl4ai but I can&#39;t figure it out. I also need to scrape paywalled articles so I added the session id from the cookie of a premium account. </p> <p>Thanks guys.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SemperPistos\"> /u/SemperPistos </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p",
        "id": 4107776,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p13nbn/did_someone_find_a_way_to_bypass_wordfence_anti",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Did someone find a way to bypass WordFence anti bot protection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lucky-Zebra9235",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T03:40:23.585039+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T02:36:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been working on an API that uses a dedicated username and password for <a href=\"http://www.conflictnations.com\">www.conflictnations.com</a>. It\u2019s a mobile warfare game. The API logs in and scrapes the website for new games being posted with less than 10 players and then the game code for that server is posted to a discord chat. </p> <p>I seem to be running into an issue with tokens/cookies and headers. I couldn\u2019t get the API to capture dynamic tokens, so I coded Playwright to login and navigate to the getGames API URL and perform a HAR capture of the session tokens and try to hand off to aiohttp to continue polling. </p> <p>The problem is that as soon as aiohttp connects after handoff, even though I have fresh session cookies/tokens, I get a not authorized. It\u2019s almost like the token session expires, even though I ensure to have aiohttp connect before playwright closes. </p> <p>Is there anything I can do to improve my token capture and handling? ",
        "id": 4105828,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p0vqiz/conflictnations",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Conflictnations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Agreeable_Mix_930",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-19T01:39:27.977861+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-19T01:02:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to help my dad&#39;s car dealership he works for increase sales since they don&#39;t really have a marketing team or dedicated person. Idea: I want to take all the inventory data from the dealership&#39;s website (used &amp; new cars), including things like photos, VINs, prices, features, etc., and organize it into a clean Excel/Google Sheet. From there, I want to automatically generate social media posts for things like &quot;Car of the Day,&quot; new arrivals, price drops, or just regular inventory posts. I&#39;ve been experimenting with some websites to scrape the website from Google to automate the data, but I keep running into issues with the workflow and pagination. I&#39;m still figuring it out. I just want to create a simple automated marketing system that the dealership can use daily without hiring a full-time marketing employee. Has anyone done something similar or have ideas on the best tools/process to automate this? Any adv",
        "id": 4105316,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p0toyx/beginner",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Beginner",
        "vote": 0
    }
]