[
    {
        "age": null,
        "album": "",
        "author": "/u/StarLimp877",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T21:26:50.767718+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T20:50:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, today I have been working with virtual machine where I installed docker yesterday and part of today it works well, but after to install wireguard in the same VM and try to up a docker-compose.yml show the next messasge: </p> <p>docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error closing exec fds: get handle to /proc/thread-self/fd: unsafe procfs detected: openat2 /proc/thread-self/fd/: function not implemented</p> <p>Try to up only container but is the same message, I not sure why happend.</p> <p>Anybody have idea to solve this problem?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/StarLimp877\"> /u/StarLimp877 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1p39ydr/my_server_with_docker_not_work_cause_by_openat2/\"",
        "id": 4132719,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p39ydr/my_server_with_docker_not_work_cause_by_openat2",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My server with docker not work cause by openat2 proc.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dangerous-Piece4895",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T16:07:14.765534+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T16:00:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello there! This is my first time on this subreddit, sorry if this is a worn-out topic. But I&#39;m looking for the official best practice for something and I can&#39;t seem to find it.</p> <p>What&#39;s the best way to include *safe* package updates in a Dockerfile (i.e. minor and patch versions)? Our security scanner is constantly getting angry with us about distro-level vulnerabilities, OpenSSL type stuff. I&#39;ve found that a lot of the packages that are getting flagged as having CVEs already have fixed versions, but our base images haven&#39;t included them yet. I&#39;d like to figure out how to either:</p> <ol> <li>Get base images that update these packages more often, or</li> <li>Upgrade the packages safely within our Dockerfile to pull in these patch versions</li> </ol> <p>For what it&#39;s worth, our backend base image is python:3.12.11-slim and our frontend is node:22-alpine.</p> <p>If you have any official sources for your answer that wou",
        "id": 4130045,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p32esr/security_updates_in_dockerfiles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Security updates in Dockerfiles",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PeterHickman",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T16:07:14.929822+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T15:58:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When using Docker it hooks itself into the firewall (iptables in this case). What I want to do is block a specific ip address. I have tried this with ufw but where ufw puts the deny is outside the flow that docker has set up. More correctly the docker chains will accept the packet before returning the flow back to where the ufw chains could handle it</p> <p>I&#39;m thinking creating a new chain BLACKLIST and adding the ip address there with a RETURN if the rule does not match and having the FORWARD chain routing through BLACKLIST before it all dives into the docker chains</p> <p>Does this seem the right approach and is it likely to survive a restart of either the system or docker?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PeterHickman\"> /u/PeterHickman </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1p32crq/blocking_an_ip_address_with_iptables/\">[link]</a></span> &#32; <span><a href=\"http",
        "id": 4130046,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p32crq/blocking_an_ip_address_with_iptables",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Blocking an ip address with iptables",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/New_Cartographer1813",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T12:56:56.283063+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T12:01:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I got this error when pulling images using docker-compose file, what causes this issue, I have tried using other networks, and even other device, but the error still exists</p> <p><code> [+] Running 2/2 ! postgres Interrupted 15.4s \u2718 minio Error Get &quot;https://registry-1.docker.io/v2/&quot;: context deadline exceeded 15.4s Error response from daemon: Get &quot;https://registry-1.docker.io/v2/&quot;: context deadline exceeded </code></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/New_Cartographer1813\"> /u/New_Cartographer1813 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1p2wsim/error_when_pulling_images/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1p2wsim/error_when_pulling_images/\">[comments]</a></span>",
        "id": 4128409,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p2wsim/error_when_pulling_images",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Error when pulling images",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gearsofschwar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T11:56:19.578081+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T11:41:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, wondering if anyone had any understanding on permissions using syncthing through docker. I&#39;m running a container on docker for plex without any permission issues, but no matter what I do syncthing doesn&#39;t seem to have permission to see any of the folders in the drives. I can&#39;t figure out if I installed syncthing and set it up without proper perms, or if it&#39;s something to do with the installation of docker itself?</p> <p>Basically, the folder I wanna share is in &quot;user folder&quot;, but it only seems to be able to see things in the &quot;shared folder&quot;. Even when sharing things from there, it still doesn&#39;t allow permissions. Anyone have any idea where to start digging on what&#39;s stopping syncthing from being allowed to see or do anything?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gearsofschwar\"> /u/gearsofschwar </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/co",
        "id": 4127932,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p2wfbj/having_trouble_with_permissions_for_syncthing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Having trouble with permissions for syncthing through docker on Ugreen OS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jaytrade21",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T10:43:44.837787+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T10:19:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I deleted my GPG key by mistake and now I can&#39;t sign into docker desktop nor can I download portainer.</p> <p>I am a newbie to Linux so any help would be helpful.</p> <p>Running Kubuntu 25.10 here</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jaytrade21\"> /u/jaytrade21 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1p2v24w/deleted_gpg_key_by_mistake/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1p2v24w/deleted_gpg_key_by_mistake/\">[comments]</a></span>",
        "id": 4127396,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p2v24w/deleted_gpg_key_by_mistake",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deleted GPG key by mistake",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pandawooper",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-21T09:27:57.104430+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-21T09:08:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Long story short, am I able to migrate a docker container with all its data, volume, container, postgres, etc. from an individual VM into a VM that has portainer? </p> <p>I plan to migrate all of my docker containers into portainer. As I have it now, I am running Immich in the separate docker VM and uploaded photos to it. I took the compose.yaml file and put it into portainer, mounted it to my external TrueNAS storage with NFS sharing/sata passthrough, and it&#39;s able to work.</p> <p>However it is like a new instance where all the login info/users are gone and I cant see any photos. I still sees the space taking up 380GB right now though but I do not see the photos or videos. It&#39;s as if the storage is being used up by something else. I still have the original Immich VM up and mounted. </p> <p>The first attempt I only copied over .yaml and .env which makes sense why data wasn&#39;t copied over. The 2nd attempt I used scp postgres and other data b",
        "id": 4127031,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1p2txam/migrate_entire_vm_to_another_vm_with_portainer_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Migrate entire VM to another VM with portainer to manage it?",
        "vote": 0
    }
]