[
    {
        "age": null,
        "album": "",
        "author": "/u/That_Ferret_9199",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-27T19:46:57.859798+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-27T18:56:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi yall, </p> <p>I kept this project as free as possible, meaning you don&#39;t have to pay a cent, i&#39;ve built this tool that literally will scrap any sources of your choice and draft it in you inbox (Telegram), summarized of what happened and with a link as well, for more details.</p> <p>Why i&#39;ve built it?</p> <p>Recently i&#39;ve seen one but, i keep hiting the quota, limits and i have to pay, so i&#39;ve collected bunch of tools and sources to build the free version</p> <p>The best part? You can listen to it, i made a simple feature that convert this draft into an audio with ai so you can listen to it using elevenlabs (free version)</p> <p>I&#39;ve documented the installation process, end to end and a Dimo Video of the final result, and i would love to hear your guys thoughts, additional features, or fixes to make this tool helpful for everybody.</p> <p>Star this repo if you find it somewhat helpful. share it to everyone, that would be gold",
        "id": 4181831,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p8a2yp/scrape_you_your_favorite_new_with_ai_and_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scrape you your favorite new with AI and Python - techNews",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/KeyPhrase2074",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-27T14:14:01.273131+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-27T13:45:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>is there any way to use proxy in undetected-chromedriver<br/> , zendriver, nodriver</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KeyPhrase2074\"> /u/KeyPhrase2074 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p82kog/setup_proxy_in_browser_automation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p82kog/setup_proxy_in_browser_automation/\">[comments]</a></span>",
        "id": 4179204,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p82kog/setup_proxy_in_browser_automation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "setup proxy in browser automation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Adorable-Pickle2798",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-27T09:45:08.950480+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-27T06:12:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This might not be the right spot but I figured I\u2019d ask. I\u2019m trying to automate some documents</p> <p>Stripe-&gt;zapier-&gt;program to auto generate document with signature-&gt; email form-&gt; once completed send second auto generated recipt </p> <p>What programs can do this? Tried panda doc and signnow but their pride especially over the monthly limit </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Adorable-Pickle2798\"> /u/Adorable-Pickle2798 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p7utsk/document_automation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p7utsk/document_automation/\">[comments]</a></span>",
        "id": 4177389,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p7utsk/document_automation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Document automation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Imaginary_Set_7296",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-27T02:26:19.673476+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-27T01:14:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I&#39;m looking for someone to make me a scraper of Telegram people that directly transfers people from one Telegram group to another of mine. Can someone do it, I&#39;ll pay you!!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Imaginary_Set_7296\"> /u/Imaginary_Set_7296 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p7p7dv/im_looking_for_someone_to_do_a_job_for_me/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p7p7dv/im_looking_for_someone_to_do_a_job_for_me/\">[comments]</a></span>",
        "id": 4175484,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p7p7dv/im_looking_for_someone_to_do_a_job_for_me",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm looking for someone to do a job for me",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JustMyPoint",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-27T01:18:18.764702+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-27T00:59:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How would one go about extracting the full-resolution image found on the following webpage? <a href=\"https://maps.app.goo.gl/fyVMSXLEVEAATu1A8\">https://maps.app.goo.gl/fyVMSXLEVEAATu1A8</a></p> <p>I tried using both Dezoomify and web developer tools but couldn&#39;t find the zoomable, full-resolution image.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JustMyPoint\"> /u/JustMyPoint </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p7ow6a/extracting_full_resolution_images_from_google/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p7ow6a/extracting_full_resolution_images_from_google/\">[comments]</a></span>",
        "id": 4175148,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p7ow6a/extracting_full_resolution_images_from_google",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Extracting full resolution images from Google Maps reviews",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Diego2196",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-27T13:09:36.891785+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-27T00:07:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been scraping product data from various B2B competitors for about a year. Some require login, some don\u2019t. Since these are B2B shops, accounts usually need resale numbers or other verification.</p> <p>By luck, I managed to get one account approved and have been using it for months. The issue: this account is locked to a specific US state, and this competitor uses server-side dynamic pricing based on the state the account was created in. To see prices for State X, you need an account registered in State X. VPNs or proxies don\u2019t change anything, and updating the address requires contacting an account manager, which I want to avoid.</p> <p>The site uses HubSpot as its CRM, so I\u2019m assuming the state assignment and price logic happen server-side.</p> <p>My question: Is there any way to access the dynamic prices for other US states when the webshop handles location entirely server-side and ties it to the account\u2019s stored state?</p> </div><!-- SC_ON --> ",
        "id": 4178709,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p7nsnb/scraping_dynamic_b2b_pricing_when_its_locked_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Dynamic B2B Pricing When It\u2019s Locked to Account US State?",
        "vote": 0
    }
]