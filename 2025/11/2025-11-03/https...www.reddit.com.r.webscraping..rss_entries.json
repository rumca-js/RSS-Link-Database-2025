[
    {
        "age": null,
        "album": "",
        "author": "/u/waddaplaya4k",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T22:18:09.767762+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T21:20:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, i search for a Tool or Software, to Download a Website from web-archiv (<a href=\"https://web.archive.org/\">https://web.archive.org/</a>) with all sub-pages.</p> <p>Thanks all</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/waddaplaya4k\"> /u/waddaplaya4k </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1onoem2/how_can_i_download_a_webarchiv_link/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1onoem2/how_can_i_download_a_webarchiv_link/\">[comments]</a></span>",
        "id": 3975635,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1onoem2/how_can_i_download_a_webarchiv_link",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can i Download a web-archiv Link?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NoArmadillo4122",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T22:18:09.987384+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T20:46:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello y&#39;ll,<br/> I am trying to understand the inner workings of CAPTCHA, and wanted to know what browser fingerprinting information do most of the CAPTCHA services capture and use that data for bot detection later. Most captcha providers use js postMessage communication to make bi-directional communication between the iframe and parent, but I am excited to know more about what specific information do these captcha providers capture. </p> <p>Is there any resource or anyone understand better what specific user data is captured and also is there a way to tamper that data? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NoArmadillo4122\"> /u/NoArmadillo4122 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1onnhw8/understanding_captcha_working/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1onnhw8/understanding_captcha_working/\">[comments]</a></span>",
        "id": 3975636,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1onnhw8/understanding_captcha_working",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Understanding captcha working",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/qaji101",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T18:13:57.540574+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T17:59:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi folks recently I have been searching any tools that can give me restaurant&#39;s menu and home service business&#39; service along with image and review. I have found a cheap scraper that provides every other data of a business except the additional ones that I need.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/qaji101\"> /u/qaji101 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oniwo9/restaurant_menu_and_services_scraping_needed/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oniwo9/restaurant_menu_and_services_scraping_needed/\">[comments]</a></span>",
        "id": 3973794,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oniwo9/restaurant_menu_and_services_scraping_needed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Restaurant \"Menu and Services\" Scraping Needed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/qaji101",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T18:13:57.681425+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T17:59:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi folks recently I have been searching any tools that can give me restaurant&#39;s menu and home service business&#39; service along with image and review. I have found a cheap scraper that provides every other data of a business except the additional ones that I need.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/qaji101\"> /u/qaji101 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oniw91/restaurant_menu_and_services_scraping_needed/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oniw91/restaurant_menu_and_services_scraping_needed/\">[comments]</a></span>",
        "id": 3973795,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oniw91/restaurant_menu_and_services_scraping_needed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Restaurant \"Menu and Services\" Scraping Needed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vroemboem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T16:12:14.193348+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T15:52:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I want to scrape an API endpoint. Preferably, I&#39;d store those response as JSON responses and then ingest the JSON in a SQL database. Any recommendations on how to do this? What providers should I consider?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vroemboem\"> /u/vroemboem </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1onfe87/best_database_setup_and_providers_for_storing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1onfe87/best_database_setup_and_providers_for_storing/\">[comments]</a></span>",
        "id": 3972559,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1onfe87/best_database_setup_and_providers_for_storing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best database setup and providers for storing scraped results?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Repulsive_Pomelo_746",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T11:02:07.527406+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T08:38:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I want to compile information about international film festivals into a google sheets document that updates the deadline dates, competitions, call for entries/industry instances and possible schedule changes. I tried using filmagent, filmfreeway, festhome and other similar websites. I&#39;m a complete newbie when it comes to scraping and just found out it was a whole thing today, i tried puppeteer but keep getting an error with the &quot;newpage&quot; command that i&#39;m not understanding -I tried all the solutions I found online but Ive yet to solve it myself-.</p> <p>I was wondering whether you had any suggestions as to how to approach this project, or if there are any (ideally free) tools that could help me out! Or if this is either impossible or would be very expensive, I&#39;m honestly so lost lmao. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Repulsive_Pomelo_746\"> /u/Repulsive_Pomelo_746",
        "id": 3970149,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1on6ka6/website_to_updateable_excelsheets",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Website to updateable excel/sheets",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Careless-Party-5952",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T08:00:36.566091+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T07:57:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, can you tell me how do you bypass websites protected with cloudflare, I would really appreciate your advice. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Careless-Party-5952\"> /u/Careless-Party-5952 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1on5yf8/how_do_you_handle_cloudflare/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1on5yf8/how_do_you_handle_cloudflare/\">[comments]</a></span>",
        "id": 3969235,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1on5yf8/how_do_you_handle_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you handle CloudFlare?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GeobotPY",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T08:00:36.923668+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T07:47:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi good folks! </p> <p>I am scraping an e-commerce page where the contents are lazyloaded (load on scroll). The issue is that some product category pages has over 2000 products and at a certain point my headess browser runs into memory exhaustion. For context: I run a dockerized AWS lambda function for the scraping.</p> <p>My error looks like this:<br/> [ERROR] 2025-11-03T07:59:46.229Z 5db4e4e7-5c10-4415-afd2-0c6d17 Browser session lost - Chrome may have crashed due to memory exhaustion</p> <p>Any fixes to make my scraper less memory intensive?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GeobotPY\"> /u/GeobotPY </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1on5szl/error_chrome_may_have_crashed_due_to_memory/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1on5szl/error_chrome_may_have_crashed_due_to_memory/\">[comments]</a></span>",
        "id": 3969236,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1on5szl/error_chrome_may_have_crashed_due_to_memory",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[ERROR] Chrome may have crashed due to memory exhaustion",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jjzman",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T02:57:32.341116+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T02:07:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve used scrappy, playwright, and selenium. All sent to be detected regularly. I use a pool of 1024 ip addresses, different cookie jars, and user agents per IP.</p> <p>I don\u2019t have a lot of experience with Typescript or Python, so using C++ is preferred but that is going against the grain a bit.</p> <p>I\u2019ve looked at potentially using one of these:</p> <p><a href=\"https://github.com/ulixee/hero\">https://github.com/ulixee/hero</a></p> <p><a href=\"https://github.com/Kaliiiiiiiiii-Vinyzu/patchright-nodejs\">https://github.com/Kaliiiiiiiiii-Vinyzu/patchright-nodejs</a></p> <p>Anyone have any tips for a persons just getting into this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jjzman\"> /u/jjzman </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1omzqst/scraping_best_practices_to_antibot_detection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1omzqst/s",
        "id": 3968091,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1omzqst/scraping_best_practices_to_antibot_detection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping best practices to anti-bot detection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Grigoris_Revenge",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-03T01:57:02.853337+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-03T01:03:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ll start by saying I&#39;m not a programmer. Not even a little. My background is hardware and some software integration in the past. </p> <p>I needed a tool and have some free time on my hands so I&#39;ve been building the tool with the help of Ai. I&#39;m pretty happy with what I&#39;ve been able to do but of course this thing is probably trash compared to what most people are using, but I&#39;m ok with that. I&#39;ll keep chipping away at it and will get it a little more polished as I keep learning what I&#39;m doing wrong. </p> <p>Anyway. I want to integrate Crawl4ai as one of my scan modes. Any thoughts on using it? Any tips? I&#39;m doing everything in python currently (running windows). </p> <p>I&#39;m able to scrape probably 75% of the sites I&#39;ve tried using the multiple scan modes I have setup. It&#39;s the Javascript (edited to correct my ignorance) heavy sites that can sometimes give me issues. I wrote some browser extensions that ",
        "id": 3967808,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1omydv6/scaper_project_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scaper project - python",
        "vote": 0
    }
]