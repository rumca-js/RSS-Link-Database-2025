[
    {
        "age": null,
        "album": "",
        "author": "/u/_mackody",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T22:07:34.257123+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T22:05:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Scraping Next.js sites is way easier than most people think. A lot of them expose internal data APIs that power their pages, and you can hit those endpoints directly without touching the rendered HTML. </p> <p>If the site uses getStaticProps or getServerSideProps, chances are the JSON it fetches is sitting one request away. </p> <p>Open your network tab, filter for fetch or XHR, and you\u2019ll usually find the exact API the frontend is calling. Once you have that, scraping becomes a simple matter of requesting structured data instead of parsing the page.</p> <p>Example:</p> <p>\u2018\u2019 import fetch from &quot;node-fetch&quot;;</p> <p>async function scrape() { const url = &quot;<a href=\"https://example.com/api/products\">https://example.com/api/products</a>&quot;; // found in network tab const res = await fetch(url, { headers: { &quot;User-Agent&quot;: &quot;Mozilla/5.0&quot; } });</p> <p>if (!res.ok) throw new Error(&quot;Request failed&quot;);</p> <p>const data",
        "id": 4164293,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p6q1jo/nextjs_golden_tip",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NextJS Golden Tip",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/_mackody",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T22:07:34.359964+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T22:00:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys I created a powerful eBay scraper and am using Playwright to handle the scraping. </p> <p>I have the front-end deployed to Vercel and the database is serverless Postgres. I\u2019m getting around to the backend and I was able to deploy to Railway but I\u2019m running into so many issues with memory within the container.</p> <p>I have a bunch of Railway credits would prefer to keep the deployment there, what is a good set up for chrome in a container, needs to also be relatively durable?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_mackody\"> /u/_mackody </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6pwj2/deploying_playwright_scrapers/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6pwj2/deploying_playwright_scrapers/\">[comments]</a></span>",
        "id": 4164294,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p6pwj2/deploying_playwright_scrapers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deploying Playwright Scrapers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No-Associate-6068",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T22:07:34.466657+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T21:37:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I built ORION to map career data.</p> <p>Instead of using BS4 to parse HTML or Selenium to render the page, I reverse-engineered the .json endpoints for subreddit threads. It makes the scraping about 10x faster and lighter on resources.</p> <p>I implemented a 2-second delay logic to stay within the polite part tier of rate limiting.</p> <p>Link here: <a href=\"https://mrweeb0.github.io/ORION-tool-showcase/\">https://mrweeb0.github.io/ORION-tool-showcase/</a></p> <p>Curious how others handle the new rate limits on the JSON endpoints?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Associate-6068\"> /u/No-Associate-6068 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6pbc9/i_built_an_opensource_reddit_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6pbc9/i_built_an_opensource_reddit_scraper/\">[comments]</a></span>",
        "id": 4164295,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p6pbc9/i_built_an_opensource_reddit_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I built an open-source Reddit scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Much-Journalist3128",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T22:07:34.568770+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T21:25:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I run the bot via Github Actions. I stick to the library, don&#39;t modify the code. If I run the bot via my PC, I don&#39;t have failures. </p> <p>I&#39;ve had the bot (via GA) visit <a href=\"https://www.browserscan.net/bot-detection\">BrowserScan - Robot Detection/WebDriver | BrowserScan</a> and take screenshots of the entire page, and according to the screenshots, my bot passed.</p> <p>The webshop uses AKAMAI. Should I just give up on github actions? Should I just get a rasbperry pi or mini PC and call it a day? I want to run the bot 2x an hour from 7AM to 7PM (so 1x every 30 minutes)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Much-Journalist3128\"> /u/Much-Journalist3128 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6p0q0/im_using_zendriver_for_my_bot_but_still_fail/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6p0q0/im_using_zendriver_",
        "id": 4164296,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p6p0q0/im_using_zendriver_for_my_bot_but_still_fail",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm using zendriver for my bot but still fail",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/breakslow",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T21:06:28.240509+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T20:10:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been working on an inventory/price tracker and after digging around for the least painful way to use curl-impersonate from node.js, I stumbled upon this library - <a href=\"https://www.npmjs.com/package/cuimp\">https://www.npmjs.com/package/cuimp</a>. It&#39;s nothing special, but it looks to be the most &quot;complete&quot; wrapper for curl-impersonate for node.js (after trying a bunch of other options).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/breakslow\"> /u/breakslow </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6n1e1/curlimpersonate_wrapper_for_nodejs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p6n1e1/curlimpersonate_wrapper_for_nodejs/\">[comments]</a></span>",
        "id": 4163899,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p6n1e1/curlimpersonate_wrapper_for_nodejs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "curl-impersonate wrapper for Node.js",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T13:35:51.819326+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T13:00:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 4159836,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p6bu45/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Firstboy11",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T06:13:55.148636+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T05:17:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I am scraping a few websites and set up a container app job on Azure. While it&#39;s slower than my spare device, the main issue is that the Azure container app is starting to cost a lot. I decided to run the scrapers on my spare device instead. Getting residential proxies, which I already have, is way cheaper. </p> <p>The API I am scraping from needs a POST request to provide me with the data. That&#39;s how it works in the network tab. I am using a popular proxy service, but it&#39;s not letting me make POST requests. I don&#39;t mind paying for residential proxies that will allow me to make POST requests. Any advice will be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Firstboy11\"> /u/Firstboy11 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1p643cd/good_proxy_that_allows_you_to_make_post_requests/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit",
        "id": 4157248,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p643cd/good_proxy_that_allows_you_to_make_post_requests",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Good proxy that allows you to make POST requests",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/the_bigbang",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-11-25T02:10:24.139927+00:00",
        "date_dead_since": null,
        "date_published": "2025-11-25T02:07:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just published a deep dive on what\u2019s happened to DIY Google SERP crawlers between 2023\u20132025 \u2014 JS walls everywhere, TLS/browser fingerprinting, poisoned datacenter IP reputation, and why \u201cjust send an HTTP request and parse HTML\u201d is basically dead. The article walks through how we got here (LLMs treating Google as a free knowledge firehose) and what changed technically under the hood.</p> <p>I\u2019d love to hear from folks still running their own SERP stacks: did you double down on anti-bot engineering, or decide to call it and move to APIs instead? And how do you think the open web will look in the AI era?</p> <p>Feel free to check it out here: <a href=\"https://tonywang.io/blog/google-serp-crawler-after-2025\">https://tonywang.io/blog/google-serp-crawler-after-2025</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/the_bigbang\"> /u/the_bigbang </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/",
        "id": 4156301,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1p60bej/after_2025_can_your_google_serp_crawler_still",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "After 2025, Can Your Google SERP Crawler Still Survive?",
        "vote": 0
    }
]