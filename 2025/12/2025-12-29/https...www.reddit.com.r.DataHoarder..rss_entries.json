[
    {
        "age": null,
        "album": "",
        "author": "/u/Myfirstreddit124",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T23:44:23.276715+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T22:58:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an external ExFAT drive.</p> <p>I use Mac to create Folder 1 and Windows to create Folder 2.</p> <p>My Mac only sees Folder 1. Windows sees both Folders 1 and 2. I ran chkdsk and created Folder 2 again, but my Mac still can&#39;t see it.</p> <p>On Mac Terminal, <code>ls</code> does see an invalid Folder 2. Finder does not see it at all. It is not hidden.</p> <p>~ <code>% ls -la &quot;/Volumes/Drivename&quot;</code><br/> <code>ls: Folder 2: Invalid argument</code></p> <pre><code>~ % diskutil verifyVolume /Volumes/Drivename The volume /dev/rdisk5s1 with UUID X appears to be OK File system check exit code is 0 </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Myfirstreddit124\"> /u/Myfirstreddit124 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pz10m4/mac_cannot_see_files_i_create_on_windows/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments",
        "id": 4431715,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pz10m4/mac_cannot_see_files_i_create_on_windows",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mac cannot see files I create on Windows",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Illustrious_Cash1325",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T23:44:23.455344+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T22:51:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking to build a machine that can read data from pretty much every type of media that was consumer/prosumer grade over the last say 35 years. Don&#39;t really know where to start, so looking for suggestions on everything from the mobo on up. </p> <p>I realize it is going to be difficult finding stuff like zip/etc drives. Anyways, if you were going to tackle this kind of a project, where would you start? What media would you want to have covered? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Illustrious_Cash1325\"> /u/Illustrious_Cash1325 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pz0uhd/data_receptacle_build_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pz0uhd/data_receptacle_build_help/\">[comments]</a></span>",
        "id": 4431716,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pz0uhd/data_receptacle_build_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data receptacle build help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/demigod987",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T23:44:23.738501+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T22:44:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought some 3.5&quot; SAS drives at a good price, planning to install them in some standard Dell/HP desktop PCs with SATA connections. I have PCIe LSI HBA cards for the data connections I bought some SAS to SATA converters, and I put tape over the first 3 pins of the power connector on the SAS drives.</p> <p>But the drives will not power up. When I plug in the power to the drives I hear a quick high pitched electronic chirp and then nothing.</p> <p>I&#39;ve done a lot of troubleshooting. I thought this was going to &quot;just work&quot;. Turns out I can&#39;t get any SAS drive to power up at all.</p> <p>1 - I tried putting the tape on the first 3 pins on the SAS-&gt;SATA converter instead of the drive, same problem</p> <p>2 - I have an old 512GB SAS drive that I don&#39;t care about, and I removed the first 3 pins with some tiny pliers, it chirps but won&#39;t power up.</p> <p>3 - I have an old desktop that I don&#39;t care about, I removed the oran",
        "id": 4431717,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pz0nvg/cannot_power_up_sas_drives_with_sata_power_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cannot Power Up SAS Drives With SATA Power With 3.3v Pin Reset",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ageek",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T22:39:05.822801+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T21:44:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I recently bought this second hand Seagate exos 8TB HDD, and after a while of being turned on, it starts making this constant noise, at the 23 second mark, I put the HDD to sleep and that&#39;s why the noise stops until the end, also the noise usually stops when there&#39;s read/write activity.</p> <p>I suspected the case, the mounting bracket and the PSU, since everything was second hand, but after replacing everything, the noise is still there.</p> <p>I ran the official seagate diagnostics and it comes out clean.</p> <p>Has anyone heard noise like this before? maybe this is normal for seagate exos (which is loud from what I read), I&#39;m okay with the noise, just worried it might mean something bad.</p> <p>Thank you in advance</p> <p><a href=\"https://reddit.com/link/1pyz61i/video/x2kutot6q7ag1/player\">https://reddit.com/link/1pyz61i/video/x2kutot6q7ag1/player</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.redd",
        "id": 4431411,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyz61i/contant_noise_from_seagate_exos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Contant noise from Seagate Exos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/skip_the_tutorial_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T21:29:43.147717+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T20:38:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need a very large amount (~500k) of transcripts from youtube videos. Most existing APIs that I found so far have very low batch size limits or they charge a lot. I wouldn&#39;t mind paying a bit of money but obviously the price quickly gets very high when you have to pay a few cents for each transcript and you&#39;re requesting so many. </p> <p>The official youtube api does not have an endpoint for transcripts and I got ip banned very quickly when I tried to scrape the transcripts.</p> <p>Are any of you guys familiar with any possible solutions? It&#39;s for a NLP related project.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/skip_the_tutorial_\"> /u/skip_the_tutorial_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyxgiu/wherehow_to_get_large_amounts_of_youtube_video/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyxgiu/wherehow_to_get_large_a",
        "id": 4430998,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyxgiu/wherehow_to_get_large_amounts_of_youtube_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where/how to get large amounts of youtube video transcripts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/That-Way-5714",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T20:21:57.207946+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T20:14:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My use case is a Plex Server. I am running out of storage. I currently am using my old desktop as storage, connected via SMB to a miniPC that is running the Plex server. Seagate still has their external drives on pretty good sale (~$11/TB for the 22TB and 24TB models). I would plan to buy 2 and connect one to my desktop and one to the miniPC, so that I can rip from CD/DVD using my desktop, then create a simultaneous copy to the drive connected to the miniPC.</p> <p>The other option would be to buy recertified/-furbished SAS drives and build a purpose built NAS. Obviously this would be more expensive. But would it be worth the extra time and expense?</p> <p>The only near-future thing I might add is NVR for exterior surveillance cameras.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/That-Way-5714\"> /u/That-Way-5714 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pywu6b/external_drives_or_",
        "id": 4430561,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pywu6b/external_drives_or_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "External Drives or NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/portiaboches",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T20:21:57.964468+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T19:54:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It has to be an mp4 for at least as an option, just trust me on this.</p> <p>Say i have a pristine FLAC album (folder containing the albums tracks each in FLAC), what do i do to get them all to mp4 preserving as much fidelity as possible?</p> <p>I&#39;ve come across suggestions there is a hacky/elliptical way to do it with ffmpeg but I dont have a source or solid reference for that contention, as attractive as it seems</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/portiaboches\"> /u/portiaboches </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pywanj/what_is_the_optimal_way_of_converting_a_flac_into/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pywanj/what_is_the_optimal_way_of_converting_a_flac_into/\">[comments]</a></span>",
        "id": 4430562,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pywanj/what_is_the_optimal_way_of_converting_a_flac_into",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the optimal way of converting a FLAC into mp4 without losing quality or minimizing the amount lost if it must be so?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/John-diySSD",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T20:21:56.484645+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T19:16:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I saw this at the Best Buy site today. Is this for real?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/John-diySSD\"> /u/John-diySSD </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyvad8/wd_black_2tb_sn8100_nvme_ssd_80350/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyvad8/wd_black_2tb_sn8100_nvme_ssd_80350/\">[comments]</a></span>",
        "id": 4430560,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyvad8/wd_black_2tb_sn8100_nvme_ssd_80350",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WD_BLACK 2TB SN8100 NVMe SSD @ $803.50",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Milo_007",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T19:05:16.453721+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T18:50:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyukhp/wus721010ale6l4_power_disable_feature_related/\"> <img src=\"https://b.thumbs.redditmedia.com/05gCQYgrRoQ5DGtTiWvUs-1D5lNreiZX453jYF7jQXE.jpg\" alt=\"WUS721010ALE6L4 - Power Disable Feature Related Query\" title=\"WUS721010ALE6L4 - Power Disable Feature Related Query\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I bought a new Western Digital Ultrastar DC HC330 (10TB) [WUS721010ALE6L4] and tried to initialize the disk on windows which failed with an error stating &quot; The request couldn&#39;t be performed because of an I/O device error&quot;. </p> <p>Event viewer shows entries with event IDs 10 and 153. </p> <p>I read some earlier posts where the power disable feature in enterprise disks can be a problem in desktop windows environments and the 3.3 V power supply to the 3rd pin in the SATA power cable needs to be blocked in order to make the drive work out. </p> <p>My question is : Is this an issue in th",
        "id": 4430094,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyukhp/wus721010ale6l4_power_disable_feature_related",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/05gCQYgrRoQ5DGtTiWvUs-1D5lNreiZX453jYF7jQXE.jpg",
        "title": "WUS721010ALE6L4 - Power Disable Feature Related Query",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/phyrooo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T17:57:28.641316+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T16:58:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I&#39;d like to share a small project of mine that I thought, given that there have been discussions about the Internet Archive, some members of this community might appreciate. The main idea is to &quot;label&quot; videos that have not been AI manipulated in a trust-minimized way by timestamping them before massive AI edits become too cheap, which we&#39;re not far from. It&#39;s a way to protect historical videos against rewrites and thus manipulation. The project is an open archive of such timestamp proofs, which can be verified by anyone and contains proofs for a bit more than 2M Internet Archive identifiers that had the &quot;movies&quot; media type. The software also allows for checking which files were timestamped from a given identifier. It would be good if the archive replicas were spread around, so if you find 1GB of free disk space, consider cloning the repository. This can be done by visiting the page below and clicking on the",
        "id": 4429580,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyrice/ohara_an_open_archive_of_verifiably_timestamped",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ohara: An open archive of verifiably timestamped video hashes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/spooogey",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:04.605302+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T15:55:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been messing with getting sonarr/radarr up and running for the last month. I&#39;ve just had some issues with data corruption that I don&#39;t know how to fix.</p> <p>Right now I just have the one pc running all the *arrs with 2 harddrives(one as a backup) in a <a href=\"https://www.canadacomputers.com/en/hdd-docking-stations/255906/vantec-jx-usb-3-2-gen1-dual-bay-dock-for-sata-drive-clone-function-nst-d258s3-bk.html\">Vantec Dual Bay Dock</a>. Now we&#39;ve had some brownouts a handful of times in the last month because of snow storms. Everytime this happens and the power goes out a harddrive corrupts. Luckily it hasn&#39;t knocked out both so I can restore it. I was about to send back one of the drives since I suspected it was the harddrive. But this morning the same thing happened with a new drive.</p> <p>What can I do to stop this from happening? Is it because of the enclosure I&#39;m using? Or is it because the *arrs are usually in the mid",
        "id": 4429011,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyptvu/probablem_with_data_corruption",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Probablem with Data Corruption.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/korba_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T15:41:44.297589+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T14:32:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m trying to build a backup NAS in a 10&quot; rack to host at a secondary location. I need 50Tb of usable storage so using 2.5&quot; drives seems like an issue. I&#39;m thinking about something like the Icy Dock FatCage MB155SP-B.</p> <p>Has anyone had any success mounting this in a 10&quot; rack directly or with a 3d printed enclosure?</p> <p>Any other recommendations?</p> <p>Thanks!! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/korba_\"> /u/korba_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pynqbd/what_enclosure_for_35x_35_drives_in_a_10_rack/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pynqbd/what_enclosure_for_35x_35_drives_in_a_10_rack/\">[comments]</a></span>",
        "id": 4428489,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pynqbd/what_enclosure_for_35x_35_drives_in_a_10_rack",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What enclosure for 3-5x 3.5\" drives in a 10\" rack?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Such-Bench-3199",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T14:29:46.111910+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T14:23:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I see the ads all the time, so misleading. They never say how much the actual product is, let alone how much the storage is.</p> <p>I have seen the ads for the tiny NVME Sharge. Looks amazing, until you realise the 2-3TB NVME is, at least for me, super expensive.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Such-Bench-3199\"> /u/Such-Bench-3199 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pynikn/anyone_else_have_products_from_orico_or_sharge/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pynikn/anyone_else_have_products_from_orico_or_sharge/\">[comments]</a></span>",
        "id": 4428014,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pynikn/anyone_else_have_products_from_orico_or_sharge",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone else have products from orico or sharge?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Speckz5701",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:04.912071+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T14:16:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to do deep research on specific topics and want to find a tool or website that allows me to pull only articles from specific outlets (like AP News, Reuters, maybe Financial Times) and filter by exact date ranges, for example, \u201conly articles about [Topic X] from January 2025.\u201d</p> <p>Google News and some databases kind of get close, but they\u2019re either not granular enough or include way too many irrelevant sources. I\u2019m looking for something where I can really hyper-focus by:</p> <p>\u2022 Topic or keyword</p> <p>\u2022 Publication (e.g. only AP, only Reuters, etc.)</p> <p>\u2022 Date or date range (e.g. Jan 1 to 31, 2025)</p> <p>It doesn\u2019t have to be free. I\u2019d be open to paid tools or platforms (research databases, news aggregators, etc.) as long as they\u2019re reliable and searchable in that way.</p> <p>Any suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Speckz5701\"> /u/Speckz5701 </a> <br/> <span><a href=\"ht",
        "id": 4429012,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyncvo/looking_for_a_website_that_lets_me_pull_articles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a website that lets me pull articles by topic, publication, and specific date range.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Round_Ad4755",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:04.436058+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T14:14:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was just wondering if there is a possibility of finding someone\u2019s deleted videos on TikTok or is it just permanently gone</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Round_Ad4755\"> /u/Round_Ad4755 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pynb5e/deleted_tiktok_videos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pynb5e/deleted_tiktok_videos/\">[comments]</a></span>",
        "id": 4429010,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pynb5e/deleted_tiktok_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deleted TikTok videos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kh3t",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T14:29:46.267364+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T13:31:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I\u2019m going a bit crazy trying to keep up with all the price spikes and stock availability (I\u2019m in the EU).</p> <p>I\u2019m currently using a single 4 TB WD external drive, which is now about 90% full. I don\u2019t have a backup copy, so I feel the need to upgrade and add more disks.</p> <p>I\u2019m planning to buy an Icy Box 5-bay enclosure (IB-3805-C31) soon. From what I understand, this is the EU equivalent of the Sabrent 5-bay. I typically use my drives about once a week, either to write data for long-term storage or to access memories and documents, but most of the time the enclosure stays offline.</p> <p>My plan is to start with:</p> <ul> <li><strong>2 HDDs</strong> in the first two bays: <ul> <li>1st drive: long-term storage for personal data (family photos, documents, music, movies)</li> <li>2nd drive: backup copy I will also keep a third copy on a separate 4 TB WD external HDD.</li> </ul></li> <li><strong>1 SSD (&gt;4 TB)</strong> in the third ba",
        "id": 4428015,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pymcmw/please_shill_me_the_best_disks_for_a_5bay_das_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Please shill me the best disks for a 5-bay DAS for these needs (EU based)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Legal-Ad296",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T13:22:30.873466+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T13:06:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>can this site be downloaded for offline usage? <a href=\"https://mitxela.com/plotterfun/\">https://mitxela.com/plotterfun/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Legal-Ad296\"> /u/Legal-Ad296 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyltty/can_this_type_of_website_be_downloaded/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyltty/can_this_type_of_website_be_downloaded/\">[comments]</a></span>",
        "id": 4427574,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyltty/can_this_type_of_website_be_downloaded",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can this type of website be downloaded?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/perecastor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T13:22:30.392609+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T12:44:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyldnm/zero_loss_compress_reduce_photo_library_size/\"> <img src=\"https://external-preview.redd.it/g6nLmbzlbE-1edxQkvfrbe4sS0NMyenCDiBnlxswFBw.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e8427b0f62fdeec5c3be6e85a3a98b0d9b13652\" alt=\"Zero Loss Compress: Reduce Photo Library Size Without Data Loss!\" title=\"Zero Loss Compress: Reduce Photo Library Size Without Data Loss!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;m the developer of the app. Please ask any questions. Here is an FAQ: <a href=\"https://fractale.itch.io/zero-loss\">https://fractale.itch.io/zero-loss</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/perecastor\"> /u/perecastor </a> <br/> <span><a href=\"https://apps.apple.com/us/app/zero-loss-compress/id6738362427\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyldnm/zero_loss_compress_reduce_photo_library_",
        "id": 4427573,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyldnm/zero_loss_compress_reduce_photo_library_size",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/g6nLmbzlbE-1edxQkvfrbe4sS0NMyenCDiBnlxswFBw.jpeg?width=640&crop=smart&auto=webp&s=2e8427b0f62fdeec5c3be6e85a3a98b0d9b13652",
        "title": "Zero Loss Compress: Reduce Photo Library Size Without Data Loss!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DrGonzo3000",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:05.293985+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T11:50:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an external 20TB HDD that has a max SMART temperature of 68\u00b0C recorded (it was in summer, sun shone on top of it, no fan. I know it was dumb). The drive has been working flawlessly for 3 months since, but it constantly was over 50\u00b0 (I have a fan now, the new 26TB drive sits at 40\u00b0 max). The drive is full of data, but I\u2019ve already copied everything to the new 26TB HDD. </p> <p>I\u2019m planning to retire the 20TB drive and use it as cold storage, basically just sitting in a drawer, disconnected, and only accessed if the new drive fails (and then only to copy the data to new drive).</p> <p>Are there any concerns with keeping it as a cold backup given that max temp? Or is it fine as long as it\u2019s not powered on regularly?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DrGonzo3000\"> /u/DrGonzo3000 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pykcwd/thoughts_on_keeping_a_20tb_hdd_with_68c",
        "id": 4429013,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pykcwd/thoughts_on_keeping_a_20tb_hdd_with_68c_max_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Thoughts on keeping a 20TB HDD with 68\u00b0C max in SMART as cold storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MrPotatoMan85",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T12:18:04.829521+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T11:08:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Ive been looking for a HDD that prioritizes reliability and longevity. I wanna use it for storing lots of old mp4 files and photos. Currently i have been eyeing WD Elements 10 TB Desktop External HDD, but i still want to hear other peoples opinion that have more knowledge on this topic.<br/> I plan on getting 2, one for general use and one for backup.</p> <p>Are there any better choices for long term storage? Ive looked into M-DISC Blu-ray but that seemed to like too much trouble for what its worth.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MrPotatoMan85\"> /u/MrPotatoMan85 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyjn93/is_the_wd_elements_10_tb_desktop_external_hdd_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyjn93/is_the_wd_elements_10_tb_desktop_external_hdd_a/\">[comments]</a></span>",
        "id": 4427201,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyjn93/is_the_wd_elements_10_tb_desktop_external_hdd_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is the WD Elements 10 TB Desktop External HDD a good choice for long term storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TurnipOk1269",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:05.652099+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T10:54:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there</p> <p>Newbie to the group here. I currently have my backups split between an external Samsung SSD drive, an old Synology 411 slim and Amazon S3 and trying to get myself a little better organised. Fortunately I don&#39;t have tons of data that I need to &#39;properly&#39; protect (around 2TB that is important) alongside ripped media (CDS) which I want to &#39;lightly&#39; protect given it&#39;s a pain to recreate the rips (I have all the original media) but not the end of the world if I had to. </p> <p>My thinking (based on reading a lot of helpful posts on here!) is to follow one of two plans:</p> <p>Plan A-</p> <p>i) Buy a Synology DS225+ (DS725+) with 2 x 6 or 8TB drives in Raid 1 and use this as a single place where I can pull everything together and organise mirrors of my current important data and periodic backups or historical data. I would be treating this as a more reliable &#39;single&#39; drive, although I am interested in explorin",
        "id": 4429014,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyje6m/twixmas_data_organisation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Twixmas Data Organisation!!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Revolutionary_Age794",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:05.988857+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T10:10:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I usually use Gramtra on desktop because it lets me save multiple images from a post at once, which is super convenient for archiving.</p> <p>I\u2019m curious though \u2014 what tools or methods do you guys use for saving Instagram posts, especially when there are a lot of images?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Revolutionary_Age794\"> /u/Revolutionary_Age794 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyinh9/what_do_you_use_to_save_or_archive_instagram_posts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyinh9/what_do_you_use_to_save_or_archive_instagram_posts/\">[comments]</a></span>",
        "id": 4429015,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyinh9/what_do_you_use_to_save_or_archive_instagram_posts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What do you use to save or archive Instagram posts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sufficient-Set2644",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T10:01:28.919354+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T09:58:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyig6g/urgently_need_advice_on_data_recovery_a/\"> <img src=\"https://external-preview.redd.it/aWZtN25wNHg4NGFnMTIUE3JvWgwCHCebL_NIeKlB3nIYFRO4XOH7g4iN64Zj.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=003dfe58123d0fef8954603a8e0183770822933b\" alt=\"Urgently need advice on data recovery. A nightmarish Christmas experience.\" title=\"Urgently need advice on data recovery. A nightmarish Christmas experience.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>What happened: My Toshiba Canvio 2tb had contact with liquid from a pet&#39;s pee ( for not too long or too much) but enough for it to not work properly at first (no light, weird disk sound) on the 25th. After taking the drive out of case and do general cleaning on it (blower + Iso alchohol) after a day, it started connecting again. I was in the process of copying everything and the video I posted is during this time (about 30-40% was already backed up to a new",
        "id": 4426451,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyig6g/urgently_need_advice_on_data_recovery_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/aWZtN25wNHg4NGFnMTIUE3JvWgwCHCebL_NIeKlB3nIYFRO4XOH7g4iN64Zj.png?width=640&crop=smart&auto=webp&s=003dfe58123d0fef8954603a8e0183770822933b",
        "title": "Urgently need advice on data recovery. A nightmarish Christmas experience.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NalgeneEnjoyer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T16:50:06.213535+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T08:54:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I read a bunch of threads on this and just cannot find the parts I am looking for in Denmark, so will appreciate any help and I apologise if its been asked a million times over.</p> <p>I am repurposing an old PC into a backup server and need the cheapest, reliable way to attach 8 x 3.5&quot; SATA HDDs. </p> <p>It will be used as a backup server, so I do not need proper cooling of the drives or anything like that at this point. I can always 3D-print a tower and blow some air on it if needed. Think of it as cold storage. </p> <p>I\u2019m running Proxmox + TrueNAS/ZFS in a VM, so disks should be presented individually (HBA/IT mode, not hardware RAID). From reading the subreddit I think I should avoid USB DAS (want stable links + SMART).</p> <p>I have looked for HBAs, Raid Controllers, and JBODs and they all seem overpriced for what I want to do. Maybe I am missing something. If my motherboard just had 8 sata connections with power I would have done that.</p> ",
        "id": 4429016,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyhe24/cheap_backup_server_with_8_x_35_sata_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cheap Backup Server with 8 x 3.5\" SATA HDDs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/One-Kaleidoscope7571",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T08:53:03.635109+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T08:47:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i just ran a faceseek myself and it was a massive wake up call for why we hoard data locally. it found photos of me from a defunct hobbyist forum that i thought was wiped from the web years ago.</p> <p>it proves that even if a site goes dark, the crawlers have already indexed the biometric data and linked it to your current identity. it made me realize that unless your data is on a cold-storage drive in your desk, it\u2019s basically public metadata for any ai to find. is anyone here working on a local facial-recognition index for their own archives?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/One-Kaleidoscope7571\"> /u/One-Kaleidoscope7571 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyhaib/the_faceseek_proving_that_deleted_sites_never/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyhaib/the_faceseek_proving_that_deleted_sites_never/\">[comments]",
        "id": 4426196,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyhaib/the_faceseek_proving_that_deleted_sites_never",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The faceseek proving that deleted sites never actually die.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Construction4430",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T08:53:04.133195+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T08:40:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys,</p> <p>A few years ago, I started to build a nice homelab for my own use that I wanted quiet as hell and as low power as possible. I invested in a JCVD 12S4 case with 12 slots that I populated over time with 8TB SATA SSDs and been using them with TrueNAS Scale (passed to a VM through Proxmox and a dedicated HBA). Everything is backed up on a 2nd NAS with mechanical HDDs. Yesterday, I ordered the 12th SSD meaning the enclosure is now full since I opened my Plex server to my family and friends.</p> <p>Since I don&#39;t see 16TB SATA SSD being sold at large scale and no hint that they will in the future, I am questioning myself about how to continue adding storage to my homelab while keeping my initial quiet+lowpower quest in sight.</p> <p>What would you recommend ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Construction4430\"> /u/Ok_Construction4430 </a> <br/> <span><a href=\"https://www.reddit.com/r",
        "id": 4426197,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyh60w/storage_strategy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Storage strategy",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/blud97",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T08:53:04.545381+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T08:19:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have a pc in a case that supports 2 3.5\u201d drives. However I have 6 sata ports and as of right now 5 drives. There are mounts I can tell for 2.5 drives and some empty space in the case. For context the case is 011AM-G from power spec. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blud97\"> /u/blud97 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pygtc9/are_there_any_ways_to_add_more_drives_to_a_case/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pygtc9/are_there_any_ways_to_add_more_drives_to_a_case/\">[comments]</a></span>",
        "id": 4426198,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pygtc9/are_there_any_ways_to_add_more_drives_to_a_case",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are there any ways to add more drives to a case that is at max capacity?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pmjm",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T07:48:57.646195+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T06:56:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Basically title. I&#39;d like something I could set up to run automatically that will take a snapshot of a subreddit, and archive the threads and comments from the first page of that subreddit at that moment in time (sorted by &quot;hot&quot; or whatever the default reddit sorting method is), then puts it into some kind of browsable archive.</p> <p>Any suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pmjm\"> /u/pmjm </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyfeqh/best_way_to_take_daily_snapshots_of_various/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyfeqh/best_way_to_take_daily_snapshots_of_various/\">[comments]</a></span>",
        "id": 4425931,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyfeqh/best_way_to_take_daily_snapshots_of_various",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to take daily snapshots of various subreddits?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RelativeCitron",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T05:43:30.420087+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T05:08:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Where is the best place to aqquire IDM nowadays? The steamrip one does not work anymore..</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RelativeCitron\"> /u/RelativeCitron </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyddew/idm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyddew/idm/\">[comments]</a></span>",
        "id": 4425459,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyddew/idm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "IDM",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/WorriedBlock2505",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T05:43:29.952314+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T05:06:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pydc6d/what_are_these_flat_black_circles_for/\"> <img src=\"https://preview.redd.it/jld3ycixs2ag1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b20dbd6e1afdd2c111a2358715fed9bf97d87f72\" alt=\"What are these flat black circles for?\" title=\"What are these flat black circles for?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WorriedBlock2505\"> /u/WorriedBlock2505 </a> <br/> <span><a href=\"https://i.redd.it/jld3ycixs2ag1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pydc6d/what_are_these_flat_black_circles_for/\">[comments]</a></span> </td></tr></table>",
        "id": 4425458,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pydc6d/what_are_these_flat_black_circles_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/jld3ycixs2ag1.jpeg?width=640&crop=smart&auto=webp&s=b20dbd6e1afdd2c111a2358715fed9bf97d87f72",
        "title": "What are these flat black circles for?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Next_Sheepherder_152",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T05:43:30.591874+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T04:42:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for a cloud service where I can upload a video and share it with a password protected link. </p> <p>View only access, no download option, just watch/stream. Free or paid, both are fine. Any suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Next_Sheepherder_152\"> /u/Next_Sheepherder_152 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pycuhn/looking_for_cloud_storage_to_share_videos_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pycuhn/looking_for_cloud_storage_to_share_videos_with/\">[comments]</a></span>",
        "id": 4425460,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pycuhn/looking_for_cloud_storage_to_share_videos_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for cloud storage to share videos with password protection (view-only, no download)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Present_Albatross557",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T05:43:30.710976+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T03:55:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently bought a new drive to expand my spanned volume server, however I hit a wall when I tried to add the drive onto my existing span. Th extend volume option is greyed out. I have already set the drive i want to add to be a &#39;dynamic drive&#39; however the option remains greyed out. I am not great with pc&#39;s I should add I get the basics and built pcs in the past however setting up drives is something alien to me any help would be greatly appreciated. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Present_Albatross557\"> /u/Present_Albatross557 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pybwcq/disk_utility_troubles_unable_to_add_new_drive_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pybwcq/disk_utility_troubles_unable_to_add_new_drive_to/\">[comments]</a></span>",
        "id": 4425461,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pybwcq/disk_utility_troubles_unable_to_add_new_drive_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Disk Utility Troubles: Unable to add new drive to existing spanned volume pls help cheers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RobbyThomas2525",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T03:33:25.464508+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T03:22:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>does anyone know how to hard reset or reset the password on a thecus n4200 i can&#39;t seem to be able to log into it even though i don&#39;t remember putting a password in i looked online and i can&#39;t any reset button on it or anything that would reset it</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RobbyThomas2525\"> /u/RobbyThomas2525 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyb75e/question_about_a_thecus_n4200/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pyb75e/question_about_a_thecus_n4200/\">[comments]</a></span>",
        "id": 4425045,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyb75e/question_about_a_thecus_n4200",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about a thecus n4200",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ExtensionGo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T03:33:24.814414+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T02:51:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have this old CD-RW that I used to backup my files when I was a kid. It had stories I wrote, homework, photographs of family and friends, and music. Life got busier as I got older and I forgot all about this backup.</p> <p>It wasn&#39;t until a few years ago, I remembered it and tried to view the files, but it took my computer a long time to read it and sometimes not all files would appear. When I took the disc out and tried again, File Explorer couldn&#39;t read it at all. If I right-clicked and viewed the properties, it showed the disc contents as 0 bytes. Multiple, subsequent attempts all failed.</p> <p>I think I might have actually posted a thread here or maybe a tech support forum about this problem. I learned that different brands of CD-RW have different lifespans, that humidity, temperature, the dyes, all played a role, and eventually the disc would degrade. As it was unreadable, I was certain it was dead. Despite this, I couldn&#39;t throw a",
        "id": 4425044,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pyaj1o/a_holiday_miracle_my_cdrw_works_again",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A Holiday Miracle - My CD-RW Works Again!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hayt_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T02:32:42.302612+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T02:28:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>tldr; Brand new 2.5 external 4 TB Seagate Expansion HDD vs 3.5 external 4 TB Seagate Expansion Desktop Drive from 2021. Which is better to use for storing some stuff I don&#39;t want to lose and keeping it (mostly) unplugged? More info below.</p> <p>____________</p> <p>Hello,</p> <p>I&#39;d like to get a big storage solution in the future but it&#39;s not going to happen overnight (due to the cost, research etc). I&#39;ve always kept my stuff on a variety of external drives which I am sure is something this community balks at. Sorry, haha, I&#39;m hoping to change that.</p> <p>My short term goal is to put some important (not life critical) stuff on a few of these drives until I can get a proper NAS or similar running hopefully in a year or two. At the moment I have two available HDDs to use. I won&#39;t need access to it frequently so I was going to just have it on a drive which I will spin up a few times I year but otherwise keep unplugged (in a cool",
        "id": 4424850,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pya18l/which_of_these_two_external_drives_should_i_use",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which of these two external drives should I use for \"cold\" storage while I work towards affording a proper NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DesperantibusOmnibus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T02:32:41.689632+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T02:28:22+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pya14y/best_pcie_sata_expansion_card/\"> <img src=\"https://preview.redd.it/ialw5elr02ag1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3907020396b6316656d8644a0235a10737c91d16\" alt=\"Best PCI-E Sata Expansion Card?\" title=\"Best PCI-E Sata Expansion Card?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Aspiring data hoarder here. Stupid question and I have a lot to learn. </p> <p>I&#39;m looking for a simple way to get more sata ports into my PC. I see there&#39;s a lot of options out there.<br/> I actually would like to have each drive appear individually instead of having to combine. </p> <p>Not sure if the card in the pic here would make all the drives appear as just four, or if this is the best option for what I&#39;m looking for. </p> <p>I&#39;d like to have a decent amount of speed available, data in my case is video for editing.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.r",
        "id": 4424849,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pya14y/best_pcie_sata_expansion_card",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/ialw5elr02ag1.jpeg?width=640&crop=smart&auto=webp&s=3907020396b6316656d8644a0235a10737c91d16",
        "title": "Best PCI-E Sata Expansion Card?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/enkisissy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T02:32:42.838442+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T02:16:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to scrape patreon for a few creators. I like their content, and they&#39;re small, but I don&#39;t want to keep paying for it monthly, so I don&#39;t really want to use Kemono.</p> <p>I have successfully used (PatreonDownloader)[<a href=\"https://github.com/AlexCSDev/PatreonDownloader%5C\">https://github.com/AlexCSDev/PatreonDownloader\\</a>] tool, but I chickened out before downloading too much. I don&#39;t have a VPN, or access to static IPs and I care about my current account. I am willing to get a patreon that I am willing to let be banned, but I don&#39;t want my IP blocked.</p> <p>I&#39;m curious what people think I would need to do this safely. I&#39;m aware that it will likely cost a little more to create another account and pay for creators. I&#39;m not sure I want to pay a monthly VPN fee for too long or pay too much for IPs. I&#39;m curious what the most cost effective way to do this would be.</p> </div><!-- SC_ON --> &#32; submitted by",
        "id": 4424851,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1py9ri6/how_to_scrape_patreon_while_avoiding_a_ban",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape Patreon while avoiding a ban",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Electrical_Count5311",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-29T04:38:59.285973+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-29T02:05:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been scraping discord for around 5 years now, I was wondering if anyone else has the same interest or would like to work with me on this or even take a peek into what I have created :) </p> <p>I have archived attachments, messages, statuses, almost everything you could imagine. I plan to release a site for this quite soon, message me if you&#39;re interested or would like to see progress. I do have a contact which is my discord</p> <p>discord username: unc938312</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electrical_Count5311\"> /u/Electrical_Count5311 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1py9iki/ive_been_scraping_discord_for_around_5_years_now/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1py9iki/ive_been_scraping_discord_for_around_5_years_now/\">[comments]</a></span>",
        "id": 4425244,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1py9iki/ive_been_scraping_discord_for_around_5_years_now",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've been scraping discord for around 5 years now",
        "vote": 0
    }
]