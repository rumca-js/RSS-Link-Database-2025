[
    {
        "age": null,
        "album": "",
        "author": "/u/Complete-Increase936",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-24T11:33:47.257119+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-24T11:30:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I&#39;m trying to get wheat data off this website <a href=\"https://www.usda.gov/about-usda/general-information/staff-offices/office-chief-economist/commodity-markets/wasde-report\">https://www.usda.gov/about-usda/general-information/staff-offices/office-chief-economist/commodity-markets/wasde-report</a> but I&#39;m struggling to know how to get the information off the link. Each time I try to hit the endpoint I&#39;m getting timeout error. Any Suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Complete-Increase936\"> /u/Complete-Increase936 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pulb23/how_do_you_scrape_data_from_a_link_that_downloads/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pulb23/how_do_you_scrape_data_from_a_link_that_downloads/\">[comments]</a></span>",
        "id": 4397227,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pulb23/how_do_you_scrape_data_from_a_link_that_downloads",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you scrape data from a link that downloads a excel file?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Friendly_Article_429",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-24T07:11:02.066610+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-24T06:52:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i&#39;ve been scraping with web scraper for a long time now, off of ao3. it&#39;s been working perfectly well, whether i needed it to scrape ten or hundreds of pages, it did the job just as well. these past couple days, however, i&#39;ve been struggling with it. i&#39;d start a classic scraping job, it&#39;ll scrape quite a good amount of pages, then i&#39;m hit with the &#39;&#39;retry later&#39;&#39; error message. (if you&#39;re not familiar with ao3, it means you went too fast too quickly, and the website puts you on time out)</p> <p>it happened once again just now, when i scraped a couple hundred pages. like i said, i scraped enough to know that the web scraper never gets hit with this time out. i started this job about a hour after i woke up, so i didn&#39;t visit ao3 in hours.</p> <p>typing this out made me think i could make the delay between two pages longer, so i&#39;m gonna do that, but in the meantime, if you encountered this issue and sol",
        "id": 4396098,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pugyl9/webscrapping_struggles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "webscrapping struggles",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Urten",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-24T06:08:23.805051+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-24T05:11:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, Just as the title - has anyone ever build any autohealing scrapper, there are few github libraries but they don&#39;t seem to be working or inaccurate, if the api changes the scraper breaks. So I want to ask if anyone had any luck building a fully functional autohealing scraper. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Urten\"> /u/Urten </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers/\">[comments]</a></span>",
        "id": 4395857,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1puf7hb/autohealing_crawlersscrappers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AutoHealing Crawlers/Scrappers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wowitsalison",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-24T04:05:09.266675+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-24T03:16:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m still pretty new to web scraping, and so far all my experience has been with BeautifulSoup and Selenium. I just built a super basic scraper with BeautifulSoup that downloads the PGNs of every game played by any chess grandmaster, but the website I got them from seems to have a pretty low request limit and I had to keep adding sleep timers to my script. I ran the script yesterday and it took almost an hour and a half to download all ~500 games from a player. Is there some way to get around this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wowitsalison\"> /u/wowitsalison </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pud2hq/getting_around_request_limits/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pud2hq/getting_around_request_limits/\">[comments]</a></span>",
        "id": 4395466,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pud2hq/getting_around_request_limits",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting around request limits",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/New_Reception2726",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-24T01:51:29.767276+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-24T00:05:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working on developing a crawler for Google Display Ads across different websites. The challenge I\u2019m facing is that I can\u2019t find or create a unique ID for each ad that remains consistent across multiple sites. Has anyone come across a solution for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/New_Reception2726\"> /u/New_Reception2726 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pu961b/has_anyone_developed_a_google_display_ads_crawler/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pu961b/has_anyone_developed_a_google_display_ads_crawler/\">[comments]</a></span>",
        "id": 4394984,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pu961b/has_anyone_developed_a_google_display_ads_crawler",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone developed a Google Display ads crawler?",
        "vote": 0
    }
]