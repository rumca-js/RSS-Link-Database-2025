[
    {
        "age": null,
        "album": "",
        "author": "/u/Rurouni_Phoenix",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T22:19:40.553284+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T21:26:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, </p> <p>I&#39;m the founder of <a href=\"/r/AcademicQuran\">r/AcademicQuran</a>, an academically based subreddit which explores the Quran, early Islamic history and Islamic Studies in general from an historical critical perspective. Our sub is nearly 5 years old and there are many high quality posts discussing a variety of academic topics that have been made over the years, and I am interested in finding a way in preserving the content of these posts , if not the data for the entire subreddit. </p> <p>What steps would need to be taken in order to preserve some of the better posts on the sub in a way that would be legal and not in violation of any Terms of Service on Reddit? What would have to be done in order to preserve the data of the entire subreddit (even though if I had my choice it would be the higher quality posts whose data would be preserved only)?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Rur",
        "id": 4423903,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1py2ze9/need_advice_for_preserving_subreddit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need advice for preserving subreddit posts/subreddit data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ASatyros",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T21:14:21.802123+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T21:05:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>Just wanna share my simple work flow for handling audio converting, maybe someone will find it useful.</p> <p>Also it&#39;s parallel - uses all cores of CPU, so it&#39;s much faster.</p> <p>Parallel works only in powershell version 7 and up, so you need to get that before running the script.</p> <p><code>cd</code> to directory where you have files, converts recursively every file in every folder bellow.</p> <p>copy-paste from notepad (to clear formatting) to run it</p> <h2>.wav to .flac:</h2> <p><code>powershell Get-ChildItem -Recurse -Filter *.wav | ForEach-Object -Parallel { $outfile = Join-Path $_.DirectoryName &quot;$($_.BaseName).flac&quot; ffmpeg -y -i $_.FullName -c:a flac -compression_level 12 $outfile } </code></p> <h2>.flac to .opus (160K is enough for &quot;transparency&quot; XD)</h2> <p><code>PowerShell Get-ChildItem -Recurse -Filter *.flac | ForEach-Object -Parallel { $outfile = Join-Path $_.DirectoryName &quot;$($_.BaseName).o",
        "id": 4423623,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1py2hoz/audio_converting_guide_ffmpeg_powershell_7",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Audio converting guide (ffmpeg, powershell 7, windows, parallel and recursive)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Scardigne",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T21:14:21.902096+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T20:46:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone have any suggestions here please?</p> <p>Will be storing the media copied across both drives if that&#39;s recommended.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Scardigne\"> /u/Scardigne </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1py1zyf/two_basic_1tb_external_hdd_storages_for_family/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1py1zyf/two_basic_1tb_external_hdd_storages_for_family/\">[comments]</a></span>",
        "id": 4423624,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1py1zyf/two_basic_1tb_external_hdd_storages_for_family",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Two basic 1tb external HDD storages for family media longterm?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mightbeathrowawayyo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T20:09:17.145202+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T19:04:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Spec&#39;ing a new system and copilot is telling me that <a href=\"https://www.newegg.com/p/pl?d=Fractal+Design+Define+7\">this case</a> can easily hold 5 3.5&quot; SATA drives. I don&#39;t see it. I&#39;m confused. Is it hallucinating or am I missing something. Just don&#39;t want to trust it blindly but despite looking over the product information it looks to me like it will hold at most two drives (without getting creative) Can anyone confirm?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mightbeathrowawayyo\"> /u/mightbeathrowawayyo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxzgjg/fractal_design_define_7_drive_capacity/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxzgjg/fractal_design_define_7_drive_capacity/\">[comments]</a></span>",
        "id": 4423346,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxzgjg/fractal_design_define_7_drive_capacity",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fractal Design Define 7 drive capacity",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/benignsalmon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T19:00:34.120375+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T18:25:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I saw a comment on a post talking about how automation tools are getting better, and so I ask you, what do you use? I use musicbrainz Picard and renamemytvseries (I think?). But I find myself vibe coding tools to do specific things and I wonder if there isn&#39;t a better, safer way.. or at least a more mainstream tool that already exists. And if not, what Software tools do you use that you or someone you know have made? I&#39;m thinking like renamers, audio level normalizers, artwork fetchers, API checkers, folder structure normalizers, dedupers, etc.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/benignsalmon\"> /u/benignsalmon </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxyfzy/what_software_tools_are_we_using/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxyfzy/what_software_tools_are_we_using/\">[comments]</a></span>",
        "id": 4423053,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxyfzy/what_software_tools_are_we_using",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What Software Tools are we using?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fuckAraZobayan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T18:00:00.878809+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T17:47:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>In the past like a year or two ago I used serverpartdeals and goharddrive and got crazy deals on 14 TB and 12 TB drives that were manufacturer refurbished or recertified, now that I&#39;m back in the market, I checked out their websites for the first time in a year and it seems that their prices have gone up way high. A year ago from goharddrive I was able to get a 12tb Ironwolf with 3 years warranty for like $110.</p> <p>Are there any alternatives?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fuckAraZobayan\"> /u/fuckAraZobayan </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxxggj/best_place_to_buy_high_capacity_hard_drives_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxxggj/best_place_to_buy_high_capacity_hard_drives_for/\">[comments]</a></span>",
        "id": 4422703,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxxggj/best_place_to_buy_high_capacity_hard_drives_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "best place to buy high capacity hard drives for the low/cheap?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No_Crazy_2442",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T18:00:01.973990+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T17:01:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This is (sort of) a follow-up to my <a href=\"https://old.reddit.com/r/DataHoarder/comments/1pkykjd/how_to_preserve_guitar_girl_2022_an_onlineonly/\">previous post on preserving Guitar Girl</a>.</p> <p>Extracting the APK was easy. (I used <a href=\"https://github.com/alexcmgit/kanade\">Kanade</a>.) Making a packet capture has proven to be much harder.</p> <p>After connecting my device to mitmproxy and installing the CA certificate, all apps, except for the browser, refuse to connect to the internet. Apparently, apps won&#39;t accept custom CA certificates.</p> <p>This can be worked around if the device is rooted (I think, <a href=\"https://docs.mitmproxy.org/stable/howto/install-system-trusted-ca-android/\">the official guide</a> is hard to parse), but rooting is not a possibility for me as I have banking apps installed on my device.</p> <p>Another possibility would be to <a href=\"https://medium.com/@williamxyz/monitoring-network-on-android-with-mitmproxy-b",
        "id": 4422706,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxwa8p/how_do_i_make_a_packet_capture_of_an_android_app",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I make a packet capture of an Android app without rooting my device?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/WarMinister23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T18:00:01.446467+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T15:46:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been having trouble the last couple days archiving Twitter links on archive.is/archive.ph. It\u2019s the only place that can capture x.com URLs since Elon switched the domain, so this is irritating. It just keeps loading indefinitely. The thing is I\u2019m able to archive other stuff still, just not Twitter links. Is anyone else having similar issues? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WarMinister23\"> /u/WarMinister23 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxuexy/is_archivetoday_slow_for_anyone_else/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxuexy/is_archivetoday_slow_for_anyone_else/\">[comments]</a></span>",
        "id": 4422704,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxuexy/is_archivetoday_slow_for_anyone_else",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Archive.today slow for anyone else?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ChuChuPoppy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T15:44:46.257427+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T14:54:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I have just bought a few Nintendo Dream mags &amp; I intend to scan them in &amp; archive them online, likely on archive.org. My local library does not support color scans. I am checking around for printing services/Staples/maybe some of the local colleges for other digitizing options, but I&#39;ll Eventually need a scanner anyhow. I am not super familiar with digitizimg beyond scanning in my personal pencil &amp; pen art, &amp; digitizing in general (but especially magazines) is a new area for me. A very select amount of scans of the Nindoris I have bought do exist online (issue 204 from 2011 has like 30 pages scanned, for instance) but they&#39;re lacking in quality &amp; do not carry &gt;1/2 the pages. As I intend to have at least 1 of the pages partially translated, the lacking quality of the scans available are a bit of a bottleneck -- the text is not clear at all &amp; it makes the characters hard to read even for my translator. So! I nee",
        "id": 4421942,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxt70d/looking_to_scan_some_old_nindori_magazines",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to scan some old Nindori magazines, preferably without damaging the spines?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dragofers",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T13:31:13.583068+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T13:23:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I run a homelab and have a NAS which stores both archival data (i.e. photo galleries, movies) and files I work with on a regular basis (i.e. documents) in a zfs pool consisting of mirrored zdevs. I let my NAS sync files to my PCs so that they can access and work on them locally without delay or compatibility issues.</p> <p>However, it occurred to me that having several synced copies of the dataset raises the chances that one of the copies gets corrupted (mainly due to bad sectors on a harddrive) and synced to all the other copies. </p> <p>My first idea was that I could keep checksums of my data and watch for spontaneous changes, but I don&#39;t really see an easy way for a program to distinguish this from the case where a user has edited the data. The other would be to run regular scans of all drives to check for bad blocks.</p> <p>As far as I can see, the safest and simplest way to protect the data would be to have my PCs work with a network share, b",
        "id": 4421287,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxr9a8/syncing_without_corruption",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Syncing without corruption?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Oiwowa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T13:31:14.269761+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T13:08:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m reworking my storage strategy after realizing I had no real archive or backup system for my photo/video data (mostly irreplaceable family photos).</p> <p>Current storage layout (external HDDs, USB-C Enclosure, cold storage): - MacBook internal SSD (1 TB) as a working set - Seagate SkyHawk 4 TB as an archive: finished projects, RAWs, exports - Seagate IronWolf 8 TB as backup target: Time Machine (Mac) and a manual copy of the archive (not intended to be backed up again)</p> <p>I know Time Machine does not back up data stored on the same volume, and I\u2019m not expecting it to. Volumes would be separated and plenty of free space maintained.</p> <p>Question: Is the general recommendation against storing other data on a Time Machine disk mainly about human error / workflow confusion or are there actual technical downsides (TM reliability, snapshot management, restore edge cases) even with clean volume separation?</p> <p>Looking for real-world experiences ",
        "id": 4421288,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxqyo7/time_machine_manual_archive_copy_on_same_disk",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Time Machine + manual archive copy on same disk (separate volumes) \u2013 actual risk or just best practice?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tethercat",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T13:31:13.012655+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T13:04:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>C&#39;mon, you&#39;re all better than this.</p> <p>I thought this community was nice.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tethercat\"> /u/tethercat </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxqw2s/venting_i_dislike_how_upvoted_comments_crap_on_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxqw2s/venting_i_dislike_how_upvoted_comments_crap_on_a/\">[comments]</a></span>",
        "id": 4421286,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxqw2s/venting_i_dislike_how_upvoted_comments_crap_on_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Venting: I dislike how upvoted comments crap on a person's efforts but don't offer helpful suggestions.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/manzurfahim",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T09:10:12.243306+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T08:54:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I downloaded a 4K UHD disc and before offloading it from my main storage, I archived it using winrar. I tested it and it worked fine. I copied it to two different 20TB drives (One Seagate Exos, One WD Ultrastar). This was about a month ago. The archive was split into multiple 1GB files.</p> <p>Today I needed the files for seeding, so I tried to extract it. It stopped at part11.rar saying the archive is corrupt. It was fine when I tested it before copying to the drives. Luckily, I had two recovery volumes created, so I deleted the corrupted file, and the recovery volumes reconstructed the file.</p> <p>Then I tried to extract it from the other 20TB drive (WD), and it extracted fine. No corrupt files. </p> <p>So, I think the Seagate Exos had a silent bit error ??</p> <p>The drive health is showing 100%, running a full surface read test now.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/manzurfahim\"> /u/manzurfahim",
        "id": 4420294,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxmrii/just_had_a_bit_rot_i_think_experience",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Just had a bit rot (I think) experience!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AngelOfDeath6-9",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T09:10:12.562002+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T08:13:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to download and archive some biggest packs (25GB+) on Freesound. downloading them via browser is impossible because of timeouts. API is very limited and in my case it allows downloading only 200 sound a day. you have to be logged in in order to download any sound. some python scripts don\u2019t work, they download corrupted files. there are some repos on github but they are so old they don\u2019t work (anymore?)</p> <p>how do I download these packs? is it possible?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AngelOfDeath6-9\"> /u/AngelOfDeath6-9 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxm55g/downloading_large_freesound_packs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxm55g/downloading_large_freesound_packs/\">[comments]</a></span>",
        "id": 4420295,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxm55g/downloading_large_freesound_packs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading large Freesound packs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/brando2131",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T09:10:11.944411+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T08:04:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Current setup 2x 6TB (zfs mirror), 80% full.</p> <p>Bought 2x 12TB deciding what to do with them... What I&#39;m thinking, please let me know if I&#39;m not considering something, and what would you do?</p> <ul> <li>Copy everything to a new 12TB mirror, but continue using the 6TB mirror as my main and delete all the less used items to free space (like any large backups not needed to be accessed frequently). Downsides would be managing two pools, I currently run them as external drives lol which would mean 4 external drives, and possibly outgrowing the space again on the 6TB main. I don&#39;t want to end up placing new files in both places.</li> <li>Copy everything to a new 12TB mirror, use that as the main, nuke the 6TBs. Maybe a (6+6) stripe, and use it as an offline backup/export of the 12TB mirror? Or I could go (6+6)+12TB mirror with the 12TB offline backup/export, but would still need to rebuild the (6+6) stripe.</li> </ul> </div><!-- SC_ON --> &",
        "id": 4420293,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxm0fz/upgrading_from_2x_6tb_to_2x_12tb_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Upgrading from 2x 6TB to 2x 12TB storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Large_Cost4726",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T08:03:11.279097+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T07:56:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have a pc with 3 drives totalling 5.5tb</p> <p>But I have so many videos and pictures from years ago that I dont even look at but I can&#39;t delete them or only rarely need to look them up for something. I need a single central place (instead of anywhere across 3 drives and countless directories) but I don&#39;t need a 24/7 nas or server. Maybe I should buy a large drive and put them on there? Plug in only when needed? But then i also am going to run out of space on my desktop and id rather clean it up and reorganize so I&#39;ll probably not have the originals on my computer. So in that case I&#39;ll be trusting on this other drive \u200bwhich isnt great. Should I buy two drives and put one in a safe or give to a family meneber? My computer is 6 years old and I know hdd and ssds dont last forever. I dont like spending money but I figure the stress and even devastation if something happened makes whatever the cost worth it.</p> </div><!-- SC_ON",
        "id": 4420108,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxlvbt/how_should_i_back_up_my_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How should I back up my media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Apizzleg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T08:03:11.589400+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T07:54:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Struggling to Export Your Snapchat Data? Here\u2019s What I Learned</strong></p> <p>If you\u2019ve tried exporting Snapchat recently, you\u2019ve probably noticed:<br/> \u2013 <strong>Tiny downloads instead of what you expected \u2013 photos &amp; videos</strong><br/> \u2013 <strong>Some files aren\u2019t readable</strong><br/> \u2013 <strong>HTML files that don\u2019t show anything</strong><br/> \u2013 <strong>Exports that finish but return incomplete data</strong></p> <p>Last year this was a <strong>simple 3\u2011step process</strong>, but now things have changed and <strong>most tutorials online are outdated</strong>.</p> <p>After testing different export options, request types, and timelines, here\u2019s what I noticed:<br/> \u2013 <strong>Snapchat exports in unreadable files</strong><br/> \u2013 <strong>Not all data types are included in every export</strong><br/> \u2013 <strong>Certain requests only return partial results</strong><br/> \u2013 <strong>Some steps require a deeper understanding of how the export works<",
        "id": 4420109,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxltyn/snapchat_memories_export_fix",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Snapchat Memories Export Fix",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TopNew7830",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T18:00:02.330019+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T07:50:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Simple question, how can I download videos from contribee? Screen recording is out of this question, just audio is also not good enough.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TopNew7830\"> /u/TopNew7830 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxlrs4/downloading_videos_from_contribee/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxlrs4/downloading_videos_from_contribee/\">[comments]</a></span>",
        "id": 4422707,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxlrs4/downloading_videos_from_contribee",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading videos from contribee",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bogdanoff_enjoyer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T06:59:56.588103+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T06:16:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m in the process of backing up many .RAW and .JPEG photos I took for this whole year from my DSLR and I thought about burning them onto an M-DISC. I usually use ImgBurn for CDs and DVDs but I figured for 25GB and 50GB M-DISCs it may be a lot more different, especially because I want to burn these photos on as reliably as possible. </p> <p>In addition, outside of Verbatim are there any other M-DISC manufacturers? I usually use TY (or nowadays CMC Pro) instead of Verbatim as their quality since CMC bought them out has been rather questionable outside of DataLifePlus.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bogdanoff_enjoyer\"> /u/bogdanoff_enjoyer </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxk6vt/software_for_burning_photos_on_mdisc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxk6vt/software_for_burning_photos_on_mdisc/\">[comment",
        "id": 4419944,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxk6vt/software_for_burning_photos_on_mdisc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Software for burning photos on M-DISC?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MirageBamboozling",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T06:59:56.747868+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T06:06:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a personal harddrive. I have two purposes one is for storing all the photos/videos etc and another is to store all the important documents(which I won&#39;t be frequently be needing, just as a backup). I&#39;m looking for buyitforlife kinda drive. Which is the best option for me, I&#39;m looking for more longevity </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MirageBamboozling\"> /u/MirageBamboozling </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxk030/best_external_harddrive_for_personal_use/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxk030/best_external_harddrive_for_personal_use/\">[comments]</a></span>",
        "id": 4419945,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxk030/best_external_harddrive_for_personal_use",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best external harddrive for personal use",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RomanaFinancials",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T04:51:03.271292+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T04:31:41+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxi7sq/testing_early_stages_of_media_server/\"> <img src=\"https://preview.redd.it/wk02vcruhv9g1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=01851ead4e0bf7d490905937e98fa202847dddd4\" alt=\"Testing early stages of media server\" title=\"Testing early stages of media server\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have 24TB in these cheap $175 external hard drives. I have another 4TB in SSD\u2019s on my desktop. These are the early stages of a very elaborate media server. I\u2019m storing 4K / 1080p files and various films/memories. The goal is to transfer everything to a 40TB external hard drive, those are anywhere from $1,000 - $3,000 for a state of the art one.</p> <p>Thoughts and any tips/suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RomanaFinancials\"> /u/RomanaFinancials </a> <br/> <span><a href=\"https://i.redd.it/wk02vcruhv9g1.jpeg\">[link]</a></span> ",
        "id": 4419596,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxi7sq/testing_early_stages_of_media_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/wk02vcruhv9g1.jpeg?width=640&crop=smart&auto=webp&s=01851ead4e0bf7d490905937e98fa202847dddd4",
        "title": "Testing early stages of media server",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AncestralAngel",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T04:51:05.412057+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T03:50:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I read that Annas Archive hosts Spotify&#39;s music list now, where is the option to download some music albums form Annas Archive?</p> <p>Thx in advanced.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AncestralAngel\"> /u/AncestralAngel </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxhecq/how_can_i_download_music_from_annas_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxhecq/how_can_i_download_music_from_annas_archive/\">[comments]</a></span>",
        "id": 4419597,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxhecq/how_can_i_download_music_from_annas_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can I download music from Annas Archive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/waterzexplus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T03:45:32.970762+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T02:53:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Bought Daft Punk on CD and planning to archive it for the more cheap pirates but I wanna scan everything from the booklet to the disc. Not the back cover I&#39;ve got no idea how to disassemble jewel cases. Anyways how do you scan disc art </p> <p>I have a 2017 Intel Mac running Ventura. My printer/scanner is a flatbed HP. Not sure which model right now but it&#39;s post-2018 afaik.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/waterzexplus\"> /u/waterzexplus </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxg8gw/how_do_you_scan_disc_art/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxg8gw/how_do_you_scan_disc_art/\">[comments]</a></span>",
        "id": 4419412,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxg8gw/how_do_you_scan_disc_art",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HOW do you scan disc art.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ethan_0100",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T01:35:05.477346+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:46:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I moved the NAND backups to a different location from the SD card and I deleted the ones from the SD card, is it bad? Either way, is there a solution to fix this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ethan_0100\"> /u/ethan_0100 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxdjyc/the_laptop_i_used_to_mod_my_3ds_broke_and_my_nand/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxdjyc/the_laptop_i_used_to_mod_my_3ds_broke_and_my_nand/\">[comments]</a></span>",
        "id": 4419024,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxdjyc/the_laptop_i_used_to_mod_my_3ds_broke_and_my_nand",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The laptop I used to mod my 3DS broke and my NAND backups are in it",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheCoxer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T01:35:04.389866+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:42:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxdhe5/question_about_hdd_sata_interface/\"> <img src=\"https://b.thumbs.redditmedia.com/aow2sm21ORPptoX6HduiBOqaBOb1Lx-EWDpxZjEItZM.jpg\" alt=\"Question about HDD SATA interface\" title=\"Question about HDD SATA interface\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/gxhcyu6hbu9g1.png?width=3705&amp;format=png&amp;auto=webp&amp;s=873748b4118df1f1a1421cda465ed5f55eabfde4\">old DC HC550 vs new DC HC550</a></p> <p>I recently bought a new manufacturer refurbished <a href=\"https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc550-wuh721818al5201-0f38352-18tb-7-2k-rpm-sas-12gb-s-sed-3-5-recertified-hard-drive\">HC550</a> from serverpartdeals (pictured on the bottom) and noticed that the sata interface is different from the standard 3.5&quot; HDD. I compared it to another one of my HC550 (pictured on top). I&#39;m thinking it&#39;s a manufacturer&#39;s mistake, but can some",
        "id": 4419023,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxdhe5/question_about_hdd_sata_interface",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/aow2sm21ORPptoX6HduiBOqaBOb1Lx-EWDpxZjEItZM.jpg",
        "title": "Question about HDD SATA interface",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Veloxxx_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T01:35:03.878230+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:41:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t really care too much about longevity (obviously they should still be ok for a bit) but I struggle to find any hardware for cheap here.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Veloxxx_\"> /u/Veloxxx_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxdgph/any_websites_that_sell_cheap_hdds_uk/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxdgph/any_websites_that_sell_cheap_hdds_uk/\">[comments]</a></span>",
        "id": 4419022,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxdgph/any_websites_that_sell_cheap_hdds_uk",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any websites that sell cheap HDDs? (UK)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chimiku",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T00:27:29.373658+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:24:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Wondering are there other good looking cases that are similar to the Jonsbo N4, similar dimension and able to hold at least 6 drives. Size really matter the most since I want it to fit into a cubby next to my networking equipment.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Chimiku\"> /u/Chimiku </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxd3jp/good_looking_compact_case_6_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxd3jp/good_looking_compact_case_6_drives/\">[comments]</a></span>",
        "id": 4418795,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxd3jp/good_looking_compact_case_6_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Good looking compact case (6 drives)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ironmansuperhero69",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T00:27:29.100679+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:20:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have a M2 iMac and looking for an external HD for backup solution (Time Machine) and extra space for media. HD on Mac is 2TB. </p> <p>I was running a couple externals and both failed. Looking at possibly a DAS or NAS but have never run either, or maybe just a couple externals run in a RAID setup with the Mac software. I have never used either and have no experience. </p> <p>I also have off site backup with Backblaze. </p> <p>Suggestions please, cost is definitely high on the list. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ironmansuperhero69\"> /u/ironmansuperhero69 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxd0m2/backup_and_extra_space_for_media_on_mac/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxd0m2/backup_and_extra_space_for_media_on_mac/\">[comments]</a></span>",
        "id": 4418794,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxd0m2/backup_and_extra_space_for_media_on_mac",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup and extra space for media on Mac",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RecursionIsRecursion",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T00:27:29.623084+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:18:57+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1pxcz1r/had_to_take_this_boy_out_not_because_he_was_bad/\"> <img src=\"https://b.thumbs.redditmedia.com/AEI6UAESmM4FgT4OQAgh7T055FLhrShyn4uC7KhLXCg.jpg\" alt=\"Had to take this boy out - not because he was bad, but because he was too small =(\" title=\"Had to take this boy out - not because he was bad, but because he was too small =(\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/n4p8aoam8u9g1.png?width=1094&amp;format=png&amp;auto=webp&amp;s=ac76d819893fe8edf0db81ca289fd275dc2cc036\">https://preview.redd.it/n4p8aoam8u9g1.png?width=1094&amp;format=png&amp;auto=webp&amp;s=ac76d819893fe8edf0db81ca289fd275dc2cc036</a></p> <p>I could have waited until he had been on for exactly 4 years (35040 hours) but...I needed the space =(</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RecursionIsRecursion\"> /u/RecursionIsRecursion </a> <br/> <span><a href=\"ht",
        "id": 4418796,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxcz1r/had_to_take_this_boy_out_not_because_he_was_bad",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/AEI6UAESmM4FgT4OQAgh7T055FLhrShyn4uC7KhLXCg.jpg",
        "title": "Had to take this boy out - not because he was bad, but because he was too small =(",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ChopNorris",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-28T00:27:29.769615+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-28T00:01:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good evening,</p> <p>I&#39;m looking for HDDs for my first Unraid server in a Jonsbo N3 case. I&#39;ve found some WD180EDGZ at a good price, but after some research, I&#39;ve discovered they are &quot;shucked&quot; disks that might not work unless a mod with Kapton tape is applied.</p> <p>I&#39;m quite new to this, but supposedly the Jonsbo backplane does not use the 3.3V rail and therefore should not have any issues. Can someone confirm if this is true?</p> <p>I&#39;ve checked several websites and haven&#39;t found a clear answer. Are these disks a good option? Could I still run into issues with the 3.3V pin? Second hand they are 290\u20ac each for 18TB, and might get some additional discount if bought in a pack, which is quite a good deal given the prices here.</p> <p>Any other advice when buying second hand drives ir welcome too.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ChopNorris\">",
        "id": 4418797,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1pxcl69/are_wd180edgz_safe_to_buy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are WD180EDGZ safe to buy?",
        "vote": 0
    }
]