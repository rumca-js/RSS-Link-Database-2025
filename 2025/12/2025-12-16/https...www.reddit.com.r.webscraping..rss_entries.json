[
    {
        "age": null,
        "album": "",
        "author": "/u/that-sewer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T23:44:35.465478+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T23:05:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi people (who are hopefully better than me at this)!</p> <p>I\u2019m working on an assignment built on transport data sourced from a site (I mistakenly thought they\u2019d have JSON file I could download) and if anyone has any ideas/guidance, I\u2019d appreciate it. I also might seem like I have no clue what I\u2019m on about and that\u2019s because I don\u2019t.</p> <p>I\u2019m trying to make a spreadsheet based on the logs from a cities bus (allowed in fair use, and I\u2019m a student so it isn\u2019t commercial) over three months. I can successfully get four of the five catagories I need (Date, Time, Start, Status) but there is a fifth bit I need that I can only access by clicking each little blue \u201ci\u201d that is next to the status. I\u2019m tracking 5 buses and there\u2019s between 2000-3000 entries on each so manual is out of the question, I\u2019ve already pitched the concept so I can\u2019t pivot. I\u2019ve downloaded two software scrapers and a browser, completed all the tutorials and been stumped at the i each tim",
        "id": 4338795,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pogk8g/little_blue_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Little blue \u201ci\u201ds",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MouseProfessional935",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T20:28:14.284414+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T16:20:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,<br/> I hope this is the right place to ask, if not, feel free to point me to a more appropriate subreddit.</p> <p>I\u2019m a researcher and I need to collect all posts published on a specific subreddit (it\u2019s a relatively young one, created in 2023). The goal is academic research.</p> <p>I\u2019m not very tech-savvy, so I\u2019ve been looking into existing scrapers and tools (including paid ones), but everything I\u2019ve found so far seems to cap the output at around 1000 posts.</p> <p>I also tried applying for access to the Reddit API, but my request was rejected.</p> <p>My questions are:</p> <ul> <li>Are there tools that allow you to scrape more than 1000 posts from a subreddit?</li> <li>Alternatively, are there tools that keep the post limit but allow you to run multiple jobs by timeframe (e.g. posts from 2024-01-01 to 2024-01-31, then the next month, etc.)?</li> <li>If tools are not the right approach, are there coding-based methods that I could realisti",
        "id": 4337410,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1po64w7/scraping_all_posts_from_a_subreddit_beyond_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping all posts from a subreddit (beyond the 1,000 post limit)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/WiseSucubi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T17:04:47.518758+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T16:10:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi I wan&#39;t to make projects with real world data unfortunately often i don&#39;t find an api for it or the api costs me my soul . I used to do basic web scraping back in 2020 but now days even my simple scripts with bs4 and request get blocked by google, cloud flare , wafs... etc . in yt space people are promoting llm based web scraping but that doesn&#39;t solves my problem ether if it doesn&#39;t brings more problems what should I do ? is it even possible or should I put my life saving on big data center proxies and some voodo magic llm + aws multi undocumented github frameworks solutions ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WiseSucubi\"> /u/WiseSucubi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1po5vsf/is_web_scraping_dead/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1po5vsf/is_web_scraping_dead/\">[comments]</a></span>",
        "id": 4335500,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1po5vsf/is_web_scraping_dead",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is web scraping dead ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GiganteColosso",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T17:04:47.152823+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T16:08:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to set up a scraping bot for JustWatch, but I&#39;m getting really frustrated because the titles don&#39;t load automatically. They only load when I manually click the carousel buttons for each streaming service and scroll down the page.</p> <p>For my scraping bot to work, I need to somehow force the site to show all titles (at least from the last 24\u201348 hours), so I can identify them. I&#39;ve tried many approaches without success.</p> <p>I&#39;ve also tried using GraphQL, but it didn&#39;t work because I need the data specifically from this page: <a href=\"https://www.justwatch.com/br/novo\">https://www.justwatch.com/br/novo</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GiganteColosso\"> /u/GiganteColosso </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1po5uec/how_to_force_justwatch_to_load_all_titles_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/w",
        "id": 4335499,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1po5uec/how_to_force_justwatch_to_load_all_titles_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to force justwatch to load all titles on screen?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T13:49:06.989754+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T13:01:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 4333659,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1po1clp/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kerrie_mariah",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T13:49:07.196907+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T12:53:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>a friend of mine is trying to save as much money as possible for his family and noticed that sometimes publix has cheaper chicken than walmart or aldis. I was thinking I could make him an app that would scrape the prices of these three places and give him a list each week of where to get the cheapest items on his grocery list. I have the webapp finished (with dummy data) but I hadn&#39;t realised that getting the actual data might be difficult. I wanted to ask a couple of questions:</p> <p>- is there an easy way to get the pricing data for these three stores? Two are on instacart which has some scraping protections </p> <p>- the online price seems to differ from the in person price randomly, sometimes by 2%, sometimes by 19% without any obvious rhyme or reason</p> <p>I&#39;m assuming the difficulty in scraping and the variation in price online vs in person is on purpose, and I&#39;ve hit some deadends. Thought I&#39;d ask here just in case! </p> </div",
        "id": 4333660,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1po16mz/is_it_possible_to_scrape_publix_item_prices",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it possible to scrape publix item prices?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DealyingDebugger",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T13:49:07.365410+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T12:22:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>Just wanted to check if I\u2019m the only one seeing this or if others are experiencing the same thing.</p> <p>Over the past few days/weeks, I\u2019ve noticed a drop in CAPTCHA solving success rates when using 2Captcha and CapSolver (mainly reCAPTCHA). Tasks seem to fail more often, take longer, or return incorrect tokens compared to earlier.</p> <p>Nothing major has changed on my end in terms of:</p> <ul> <li>Implementation</li> <li>Proxy quality</li> <li>Request volume</li> <li>Target sites</li> </ul> <p>So I\u2019m wondering:</p> <ul> <li>Are others seeing similar issues?</li> <li>Could this be due to changes from Google?</li> <li>Any noticeable difference between the two services for you?</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DealyingDebugger\"> /u/DealyingDebugger </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1po0llu/lower_captcha_solving_success_with_2captcha/",
        "id": 4333661,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1po0llu/lower_captcha_solving_success_with_2captcha",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Lower CAPTCHA solving success with 2Captcha / CapSolver lately?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Icy_Can_4652",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T12:45:49.284212+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T11:44:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, I used WfDownloader for saving all my pinterest boards, but recently it shopped working, something about pinterest changing the json files to null. Anyway, I found this chrome extension on github <a href=\"https://github.com/rrokutaro/pinterest-board-downloader\">https://github.com/rrokutaro/pinterest-board-downloader</a></p> <p>I would like to know if it is safe to use, or if someone could do a tutorial on it, that would be great. I also found an extension in the store <a href=\"https://chromewebstore.google.com/detail/pinsnap-pinterest-image-d/dopngnmhcmnfefbjpcednhkecmbcopik\">https://chromewebstore.google.com/detail/pinsnap-pinterest-image-d/dopngnmhcmnfefbjpcednhkecmbcopik</a></p> <p>but Im not sure about that one either. I really need to save my boards, so if anyone has any tips or ideas that would be great, thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Icy_Can_4652\"> /u/Icy_Can_465",
        "id": 4333117,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pnzwoc/is_it_safe_to_use_this_github_extension",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it safe to use this github extension?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bolinhadegorfe56",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T07:16:40.782658+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T06:35:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>im done and lazy, i doenst even know if here is the right place for this type of question, but whatever</p> <p>i\u2019ll use translate:</p> <p>I&#39;m dealing with a very specific problem and AI was doing well</p> <p>Now this crap has gone crazy and I&#39;ve reached the limit of technology (and my stupidity and dishonor as a \u201cdev\u201d)</p> <p>Basically, I&#39;m trying to intercept an array of HTML links but it&#39;s encrypted in b64 and xor 3:1 inside a div with data-v and data-x (split into several parts)</p> <p>To make matters worse, it deletes this div through an obfuscated js script (just below) with millions of characters (making it impossible to understand what&#39;s really happening) and I can&#39;t intercept the function calls with the decryption keys that happen during the process due to stupidity, ignorance and naivety of how to do things</p> <p>I already tried adding breakpoint, running with violentmonkey, going to the arm and nothing</p> <p>In the ",
        "id": 4331365,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pnv4p0/i_need_some_tips_for_a_specific_problem",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "i need some tips for a specific problem",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Tetrix_Texxar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-16T00:57:22.713566+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-16T00:34:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am working on a project which involves building a database of many different pieces of scientific equipment across the higher education institutions in a particular US state. For example, a list of every confocal, electron, or other large microscope at a Michigan college or university (not my actual goal).</p> <p>Obviously each higher education institution has its own website where the equipment they list is in a unique spot for each website. Due to time limitations I would like to automate some aspect of the crawling of these large websites to build a (mostly) comprehensive list. </p> <p>I understand pure web scraping is not exactly the right tool for the job. I am asking, however, in your experience as developers or scraping enthusiasts, what the best tool or process would be to start building this comprehensive list? Has anyone worked on a similar project to this and could give me advice?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=",
        "id": 4329731,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pnnz3f/process_for_building_large_database_with_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Process for building large database with web scraping (and crawling)",
        "vote": 0
    }
]