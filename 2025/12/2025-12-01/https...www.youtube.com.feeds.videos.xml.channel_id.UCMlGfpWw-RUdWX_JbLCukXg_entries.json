[
    {
        "age": null,
        "album": "",
        "author": "CppCon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T15:53:03.190547+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T15:07:07+00:00",
        "description": "https://cppcon.org\u200b\n---\n\nOptimize Automatic Differentiation Performance in C++ - Steve Bronder - CppCon 2025\n---\n\nReverse\u2011mode automatic differentiation (AD) powers everything from back\u2011propagation that trains trillion\u2011parameter large language models to the Stan programming language's Bayesian inference engines. Performance tricks like arena allocators, expression\u2011templates, SIMD friendly data structures, and template meta-programming finds its way inside C++ AD libraries. Milliseconds saved per gradient compound can turn into hours of wall\u2011time wins.\n\nThis session dissects the engineering behind those performance wins, showcasing improvements across different C++ AD libraries over time. Attendees will see how contemporary C++ AD techniques can make AD so fast that there is rarely a need to add hand-written derivatives to your program. The talk assumes familiarity with modern C++ but no prior exposure to automatic differentiation.\n\n---\n\nSlides: https://github.com/CppCon/CppCon2025/blo",
        "id": 4209431,
        "language": "en",
        "link": "https://www.youtube.com/watch?v=_YCbGWXkOuo",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 420,
        "source_url": "https://www.youtube.com/feeds/videos.xml?channel_id=UCMlGfpWw-RUdWX_JbLCukXg",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://i4.ytimg.com/vi/_YCbGWXkOuo/hqdefault.jpg",
        "title": "Optimize Automatic Differentiation Performance in C++ - Steve Bronder - CppCon 2025",
        "vote": 0
    }
]