[
    {
        "age": null,
        "album": "",
        "author": "/u/Bartrader",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T22:22:39.498906+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T22:14:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been pulling data from real estate sites lately, and the biggest headaches weren\u2019t the selectors \u2014 it was the constant CAPTCHAs, random redirects, and IP blocks. Even simple projects like \u201ccollect listings into Excel\u201d turned into a mini-battle.</p> <p>I shifted to a setup where a <a href=\"https://github.com/crawlbase\">crawler API</a> fetches the rendered HTML for me, and then I parse everything with Cheerio + export with ExcelJS. No proxy juggling, no browser automation, no retry loops.</p> <p>Here\u2019s the basic idea using two sites (Estately and Re/Max):</p> <ul> <li>Request the fully rendered page through the API</li> <li>Load the HTML with Cheerio</li> <li>Extract price, beds, baths, sqft, and address</li> <li>Sort by price</li> <li>Save the results into a timestamped Excel file</li> </ul> <p>Both scripts follow the same pattern, and once you get one working, adding new cities or platforms is basically copy-paste with new selectors.</p> <p>If yo",
        "id": 4212650,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pbqmea/how_i_automated_real_estate_data_extraction",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How I Automated Real Estate Data Extraction",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Internal_Ad_472",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T23:25:56.760693+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T21:56:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We are looking for a specific type of Data Scientist\u2014someone who is bored by standard corporate ETL pipelines and wants to work on the messy, chaotic, and cutting-edge frontier of AI Search and Web Data.</p> <p>We aren&#39;t just looking for model tuning; we are looking for massive-scale data retrieval and synthesis. We are building at the intersection of AI Citations (GEO), Programmatic SEO, and Linkbuilding automation.</p> <p>The Challenge: If you have experience wrestling with Common Crawl, building robust scraping pipelines that survive anti-bot measures, and integrating Linkbuilding APIs to manipulate the web graph, we want to talk to you.</p> <p>What we are looking for:</p> <ul> <li>2+ Years of Experience: Real-world experience.</li> <li>The Scraper&#39;s Mindset: You know your way around Puppeteer/Playwright, rotating proxies, and handling CAPTCHAs.</li> <li>Big Data Handling: You aren&#39;t scared of the size of Common Crawl datasets.</li> <li",
        "id": 4212997,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pbq5qr/hiring_data_scientist_engineer_common_crawl",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[HIRING] Data Scientist / Engineer | Common Crawl & Technical SEO",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abdullah-shaheer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T20:16:10.537010+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T19:22:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I hope you people are fine and good. I am stucked in a problem, my goal is to get the names of subreddits (maximum). I have tried a lot but I cannot get all the results. If I could have names of all the subreddits, I will manage to get the other data and apply filters. I know that it&#39;s practically impossible to get every subreddit name as they keep on increasing every minute. I am looking to have more than a Million records, so that after applying filters, I could have 200k plus subreddit names having 5k+ subscribers. Any advice or experience is highly appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abdullah-shaheer\"> /u/abdullah-shaheer </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pbm314/getting_the_list_of_names_of_all_the_subreddits/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pbm314/getting_the_list_of_names_o",
        "id": 4211676,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pbm314/getting_the_list_of_names_of_all_the_subreddits",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting the list of names of all the subreddits",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ki-_-rito",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T20:16:10.317558+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T19:13:42+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1pblumz/free_source_siteforge_live_websites_export/\"> <img src=\"https://preview.redd.it/p2zufxfl6n4g1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7594dd21ce6b69f7516ed6f9bec738b26fb4daa8\" alt=\"Free source : SiteForge : live websites export\" title=\"Free source : SiteForge : live websites export\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Just launched a tool I\u2019ve been dreaming of building for a while: SiteForge.</p> <p>Ever wanted to take a live website and instantly generate a ready-to-run project without relying on AI or external services? That\u2019s exactly what SiteForge does.</p> <p>SiteForge is a client-side Chrome extension that captures the HTML, CSS, assets, and layout of any page and exports it as:</p> <ul> <li>Next.js 14 + Tailwind static app<br/></li> <li>WordPress theme (PHP + theme.json)<br/></li> <li>Experimental multi-page Next.js app<br/></li> </ul> <p>All exports are deterministic, mean",
        "id": 4211675,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pblumz/free_source_siteforge_live_websites_export",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/p2zufxfl6n4g1.jpeg?width=640&crop=smart&auto=webp&s=7594dd21ce6b69f7516ed6f9bec738b26fb4daa8",
        "title": "Free source : SiteForge : live websites export",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/echno1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T20:16:10.796410+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T13:14:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m looking for someone who understands request-based browser automation, session handling, and antibot behaviour. Mainly need help with structuring requests properly, dealing with cookies, proxies, fingerprints, and making sure the flows don\u2019t trigger bot checks. Prefer someone in UK/UAE timezone who\u2019s worked with this stuff before. If you\u2019ve got experience with tricky login flows or bypassing strict request validation, message me \u2014 just need some guidance.</p> <p>Can pay up to $50 per hour for your time</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/echno1\"> /u/echno1 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pbcgog/need_help_with_some_requestbased_browser/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pbcgog/need_help_with_some_requestbased_browser/\">[comments]</a></span>",
        "id": 4211677,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pbcgog/need_help_with_some_requestbased_browser",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help with some request-based browser automation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-01T03:55:12.046217+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-01T03:01:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello and howdy, digital miners of r/webscraping!</p> <p>The moment you&#39;ve all been waiting for has arrived - it&#39;s our once-a-month, no-holds-barred, show-and-tell thread!</p> <ul> <li>Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you&#39;ve just unleashed on the world?</li> <li>Maybe you&#39;ve got a ground-breaking product in need of some intrepid testers?</li> <li>Got a secret discount code burning a hole in your pocket that you&#39;re just itching to share with our talented tribe of data extractors?</li> <li>Looking to make sure your post doesn&#39;t fall foul of the community rules and get ousted by the spam filter?</li> </ul> <p>Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!</p> <p>Just a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let&#39;s get ",
        "id": 4205075,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pb1j3z/monthly_selfpromotion_december_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Monthly Self-Promotion - December 2025",
        "vote": 0
    }
]