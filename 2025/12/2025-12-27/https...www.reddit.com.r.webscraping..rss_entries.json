[
    {
        "age": null,
        "album": "",
        "author": "/u/Fragrant_Ad3054",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-27T22:38:41.973787+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-27T20:35:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m currently measuring power consumption while a Python program is running.</p> <p>I&#39;m creating a table to record my results, and that&#39;s where I&#39;m encountering the problem...</p> <p>Actually, I&#39;m creating a simple web scraping program that makes a request every 30 seconds.</p> <p>The thing is, I&#39;m not just scraping the page; I&#39;m also retrieving specific information.</p> <p>It takes my program about 3 seconds to retrieve the information.</p> <p>So my question is:</p> <p>When you read &quot;scraping a web page every 30 seconds,&quot; do you understand:</p> <p>\u2022 \u2060that the request occurs every 30 seconds, taking into account the time it takes to process the information?</p> <p>OR</p> <p>\u2022 \u2060that the request occurs every 30 seconds, without taking into account the time it takes to process the information (30 seconds + 3 seconds)?</p> <p>Thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/",
        "id": 4418428,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1px7vrn/question_d\u00e9lai_entre_requ\u00eate",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question d\u00e9lai entre requ\u00eate",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/artnote1337",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-27T19:17:50.720646+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-27T18:41:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, I&#39;m scraping data from a website that has a paywall on some of its data, BUT the endpoint that returns this data was easily found in the source code and does not require any special cookies besides the ones from a free account. Its data from census from a country that were digitalized, the census itself is public but the way this data is being provided may not be I guess. I&#39;m using proxy, a few accounts and browsers to scrape the data using this found endpoint (respecting 429s). Will/Can I be in trouble? What are your opinion on the moral/ethics in this sort of scraping?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/artnote1337\"> /u/artnote1337 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scrap",
        "id": 4417551,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1px553f/legal_implications_of_this_sort_of_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Legal implications of this sort of scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Round_Method_5140",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-27T00:53:43.929068+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-27T00:53:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This is a project where I learned some basics through self teaching and generative assistance from Antigravity. I started by sniffing network on their web pages. Location search, product search, etc. It was all there. Next was understanding the most lightweight and efficient way to get information. Using curl cffi was able to directly call the endpoints repetitively. Next was refinement. How can I capture all stores with the least number of calls? I&#39;ll look incorporate stores and products from iheartjane next.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Round_Method_5140\"> /u/Round_Method_5140 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000/\">[comments]</a></span>",
        "id": 4413306,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pwk7xw/i_deployed_a_side_project_scraping_5000",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I deployed a side project scraping 5000 dispensaries.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/YouthFinancial8591",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-27T00:53:44.135194+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-27T00:33:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need a bulk scrapping tool that allows me to download all the videos and images from a Skool group, as well as the files and text written in the description. Are there any extensions or people who can help me?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/YouthFinancial8591\"> /u/YouthFinancial8591 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pwjrsx/skool_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pwjrsx/skool_scraping/\">[comments]</a></span>",
        "id": 4413307,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pwjrsx/skool_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SKOOL SCRAPING",
        "vote": 0
    }
]