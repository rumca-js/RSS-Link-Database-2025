[
    {
        "age": null,
        "album": "",
        "author": "/u/SalamanderEuphoric82",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T16:10:35.562001+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T16:04:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Using docker for years now, i believe at least 10 years.<br/> Im started to organise it nicer/better.</p> <p>This is how its organised after a lot changes last weeks: I catagorised several containers i several subfolders:</p> <p>In my MAIN docker-compose.yaml at the root: i have a include state:</p> <pre><code>include: - path: protocols/govee2mqtt/govee2mqtt.yaml env_file: protocols/govee2mqtt/govee2mqtt.env - path: protocols/mosquitto/mosquitto.yaml env_file: protocols/mosquitto/mosquitto.env - path: cinema/cinema.yml env_file: cinema/cinema.env - path: dashboards/dashboards.yml env_file: dashboards/dashboards.env - path: diagnostics/diagnostics.yml env_file: diagnostics/diagnostics.env - path: download_clients/download_clients.yml env_file: download_clients/download_clients.env - path: network/network.yml env_file: network/network.env - path: protocols/protocols.yml env_file: protocols/protocols.env - path: security/security.yml env_file: security/s",
        "id": 4410719,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw7q74/open_question_about_multiple_compose_files_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open Question about multiple compose files and improvement",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EntrepreneurWaste579",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T16:10:35.956751+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T15:43:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m running two services with Docker Compose (2.36.0)</p> <p>The <strong>first service (WAHA)</strong> needs about <strong>120 seconds</strong> to start. During that time I also need to manually log in so it can initialize its sessions. Only after those 120 seconds can it be considered <strong>ready</strong>.</p> <p>The <strong>second service</strong> must <strong>not start until the first service explicitly signals that it\u2019s ready</strong>.</p> <pre><code>services: waha: image: devlikeapro/waha restart: unless-stopped ports: - &quot;3000:3000&quot; environment: WAHA_API_KEY: ${WAHA_API_KEY} WAHA_DASHBOARD_USERNAME: ${WAHA_DASHBOARD_USERNAME} WAHA_DASHBOARD_PASSWORD: ${WAHA_DASHBOARD_PASSWORD} WHATSAPP_SWAGGER_USERNAME: ${WHATSAPP_SWAGGER_USERNAME} WHATSAPP_SWAGGER_PASSWORD: ${WHATSAPP_SWAGGER_PASSWORD} kudos: image: kudos restart: unless-stopped environment: WAHA_URL: http://waha:3000 </code></pre> <p>How can I do this?</p> </div><!-- SC_ON --> &#32;",
        "id": 4410720,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw78il/how_to_make_a_docker_compose_service_wait_until",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to make a Docker Compose service wait until another signals ready (after 120s)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ZenithNomad43",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T13:56:23.697317+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T13:41:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Docker Compose works great when you have one or two projects. The friction starts when a single host runs many stacks.</p> <p>On a typical server, each Compose project lives in its own directory, with its own compose file. That design is fine, but over time it creates small operational costs:</p> <ul> <li>You need to remember where each project lives</li> <li>You constantly <code>cd</code> between folders</li> <li>You repeat <code>docker compose ps</code> just to answer basic questions</li> <li>You manually map ports, container IDs, and health states in your head</li> </ul> <p>None of this is difficult. It is just noisy.</p> <p>The real problem is not Docker Compose, but the lack of a host-level view. There is no simple way to ask:</p> <ul> <li>What Compose projects are running on this machine?</li> <li>Which ones are healthy?</li> <li>What services and ports do they expose?</li> </ul> <p>People usually solve this with shell scripts, aliases, or notes",
        "id": 4409981,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw4k4s/managing_multiple_docker_compose_stacks_is_easy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Managing multiple Docker Compose stacks is easy, until it isn\u2019t",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NathLWX",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T12:53:22.938728+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T12:34:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For context, I&#39;m using a certain Docker container (Jellyfin) with a few external ssd&#39;s directories mapped to the Docker volume via the Docker compose file, if I&#39;m not mistaken.</p> <p>I have an external SSD where the files (videos) for Jellyfin libraries are located (because my laptop has limited storage).</p> <p>Since my Jellyfin library&#39;s directory is set to that Docker volume, whenever my SSD got unplugged/unmounted, then mounted it again, it got connected with different directory with different partition name (/dev/sdb0 instead of /dev/sda0), since the sda0&#39;s directory is currently being used by the Docker container and can&#39;t be removed when unplugged.</p> <p>I can manually stop the container, then remount the external drive, then start the container again. But I sometimes forgot to stop the container before remounting it.</p> <p>I thought it&#39;d be easier to automatically stop the Docker container when I unmount it, if t",
        "id": 4409600,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw3a8c/is_it_possible_to_automatically_stop_a_container",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it possible to automatically stop a container if I unmount/unplug my external drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FinishCreative6449",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T12:53:22.642643+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T12:00:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I built a tool that analyzes Docker image sizes directly from container registries \u2014 without pulling the images.</p> <p>I needed to compare 50 versions of a Docker image to find when it got bloated. Pulling all tags would be slow and use gigabytes of bandwidth. Instead, I got all the data in seconds using registry metadata APIs.</p> <h1>The Key Insight</h1> <p>When you <code>docker pull</code>, the client fetches a manifest, then a config, then downloads all layers. The manifest and config are tiny JSON files (~10KB total). The layers are gigabytes.</p> <p>But the manifest already contains layer sizes:</p> <pre><code>{ &quot;layers&quot;: [ {&quot;size&quot;: 29536818, &quot;digest&quot;: &quot;sha256:af6eca94...&quot;}, {&quot;size&quot;: 1841029843, &quot;digest&quot;: &quot;sha256:c46e201c...&quot;} ] } </code></pre> <p>And the config contains the Dockerfile commands that created each layer:</p> <pre><code>{ &quot;history&quot;: [ {&quot;created_by",
        "id": 4409599,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw2p24/analyzing_docker_images_without_downloading_them",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Analyzing Docker Images Without Downloading Them",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/qlabb01",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T11:47:28.880879+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T11:17:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey!</p> <p>I would like to persist Docker volumes between OS reinstalls for some services (mail, databases, etc.). My idea would be to use a separate filesystem (for example, a dedicated disk or partition) and mount it after reinstalling the OS.</p> <p>Ideally, I would just have to mount the filesystem after installing the OS and start up my docker compose files, which contains the named volume definition, e.g.:</p> <pre><code>services: myservice: volumes: volume1:&lt;path-to-data&gt; ... volumes: volume1: type: none device: /mnt/d/myservice-data o: bind </code></pre> <p>Is this a valid approach/are there any drawbacks? Or are there better ways to achieve what I want?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/qlabb01\"> /u/qlabb01 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1pw205r/persisting_volumes_between_os_reinstalls/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com",
        "id": 4409293,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw205r/persisting_volumes_between_os_reinstalls",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Persisting volumes between OS reinstalls",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/9YearOldKobe",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T11:47:29.086250+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T11:06:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, </p> <p>im a very new guy to docker and basically just learned about it previous week at university. I understand the basics, containerization, and what the benefits are, debugging, consistency and so forth. But im a bit confused as to when should i compose my project in docker. We are doing a microservice project for this specific class, there are 7 microservices i have developed, but its important to note that 1. Some need modifications still and 2. 3 arent developed yet as im waiting for my teammate to do them. And because of this I am wondering, do I create a docker image now? Or do I need to have all microservices finished and THEN i start with docker. Or is it possible to add the microservices and update them in docker later?</p> <p>Thank you in advance</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/9YearOldKobe\"> /u/9YearOldKobe </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1pw",
        "id": 4409294,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw1u2c/when_in_the_development_cycle_to_use_docker",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "When (in the development cycle) to use docker?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rebellion_unknown",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T10:38:51.364228+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T09:56:24+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rebellion_unknown\"> /u/rebellion_unknown </a> <br/> <span><a href=\"/r/nextjs/comments/1pw0opg/404_after_build_completes/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1pw0qi4/404_after_build_completes/\">[comments]</a></span>",
        "id": 4408949,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw0qi4/404_after_build_completes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "404 after build completes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SnappieRT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T10:38:51.502169+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T09:54:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there! For my final project, I will most likely be asked to create a MariaDB database using Docker. The problem is that in class, I use an external hard drive with Ubuntu (ext4) because we aren&#39;t allowed to bring our own laptops.</p> <p>The real issue is that at home I work on a Mac, and connecting that external drive is impossible. I&#39;m looking for a way to have the container on both systems and work on it while keeping the data synced across both. Gemini recommended using an external drive with an APFS partition and using <code>apfs-fuse</code> on Linux to mount it and store the container there.</p> <p>Any recommendations would be a huge help, as paying for the Paragon software for Mac is not an option for me right now.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SnappieRT\"> /u/SnappieRT </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1pw0pli/crossplatform_docker_workflow_managi",
        "id": 4408950,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pw0pli/crossplatform_docker_workflow_managing_mariadb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cross-platform Docker Workflow: Managing MariaDB between macOS and Linux (Ubuntu)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/no0bmaster_690",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T05:30:07.725661+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T05:12:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We started with docker compose when we had like 5 services. It was great, super simple, everyone could understand it. Fast forward 18 months and we&#39;re at 20+ services and docker compose is making everything harder not easier.</p> <p>Things started breaking in production that worked fine on our laptops. Services couldn&#39;t find each other properly and stuff would randomly fail under real traffic. We were doing weird workarounds with config files that got messy. We couldn&#39;t see what was happening, when something broke we had no idea which service was causing the problem or why. Everything just showed up as containers and that tells you nothing useful when you have 20 of them talking to each other.</p> <p>Someone suggested we needed orchestration tools and after trying a few things we switched to something more solid. The migration was a shitty proccess, took weeks and we had some scary deploys. but we can see what&#39;s happening in our system",
        "id": 4407867,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pvw9vh/docker_compose_hit_a_limit_at_20_microservices",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Docker compose hit a limit at 20 microservices, had to change everything",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PrestigiousZombie531",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T05:30:07.934652+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T04:55:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><ul> <li>Postgres is running inside a docker container named postgres_server.development.ch_api</li> <li>Express is running inside another docker container named express_server.development.ch_api</li> <li>I am trying to setup self signed SSL certificates for PostgeSQL using openssl</li> <li>This is taken from the documentation as <a href=\"https://www.postgresql.org/docs/18/ssl-tcp.html#SSL-CERTIFICATE-CREATION\">per PostgreSQL here</a></li> <li>If CN is localhost, the docker containers of express and postgres are not able to connect to each other</li> <li>If CN is set to the container name, I am not able to connect psql from my local machine to the postgres server because same thing CN mismatch</li> <li>How do I make it work at both places?</li> </ul> <p>```</p> <h1>!/usr/bin/env bash</h1> <p>set -e</p> <p>if [ &quot;$#&quot; -ne 1 ]; then echo &quot;Usage: $0 &lt;postgres-container-name&gt;&quot; exit 1 fi</p> <h1>Directory where certificates will be sto",
        "id": 4407868,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pvvy48/if_cnlocalhost_docker_containers_cannot_connect",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "If CN=localhost, docker containers cannot connect to each other, if CN=<container-name> I cannot connect to postgres docker container from local machine for verify-full SSL mode with self signed openssl certificates between Express and postgres",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JWill018",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-26T02:16:16.440941+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-26T01:28:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Merry christmas guys, </p> <p>I&#39;ve been working on this for 2 days and still cannot find a solution for this use case. My main issue being that I can not figure out how to translate the .env file in Techhut&#39;s tutorial for Airvpn into an actual working instance for PIA(Private Internet Access). If anyone has gotten this working or can give me a good work around you would be much appreciated. I would really like to use PIA because I already have the subscription. </p> <p>Mind you, I dont think PIA with wireguard is compatible with gluetun (if it is its very convoluted). </p> <p>This is the .env file</p> <p># General UID/GIU and Timezone</p> <p>TZ=America/Chicago</p> <p>PUID=1000</p> <p>PGID=1000</p> <p># Input your VPN provider and type here</p> <p>VPN_SERVICE_PROVIDER=airvpn</p> <p>VPN_TYPE=wireguard</p> <p># Mandatory, airvpn forwarded port</p> <p>FIREWALL_VPN_INPUT_PORTS=port</p> <p># Copy all these varibles from your generated configuration ",
        "id": 4407320,
        "language": "en",
        "link": "https://www.reddit.com/r/docker/comments/1pvs2ks/getting_gluetun_to_work_with_pia_ft_techhut",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting Gluetun to work with PIA ft. Techhut Server Tutorial",
        "vote": 0
    }
]