[
    {
        "age": null,
        "album": "",
        "author": "/u/Kind_Contact_3900",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-04T21:17:59.607166+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-04T20:49:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been thinking a lot about browser automation lately\u2014tools like Selenium and Playwright are powerful, but they often mean diving straight into code for even simple tasks. What do you all use for repetitive web stuff as testing flows, data pulls, or multi-step interactions? Ever wish for something more visual?</p> <p><strong>Loopi and Playwright are both open-source tools for browser automation, but they cater to different user needs.</strong> Playwright is a robust, code-based library primarily designed for end-to-end testing and web scraping across multiple browsers, with broad language support. Loopi, on the other hand, is a newer desktop application focused on visual, no-code workflow building for local Chromium-based automations, making it more accessible for non-developers tackling repetitive tasks.</p> <p><strong>When to Choose Which?</strong></p> <ul> <li><strong>Choose Playwright</strong> if you&#39;re a developer needing flexible, cro",
        "id": 4240936,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pebdur/visual_browser_automation_code_vs_nocode",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Visual browser automation: Code vs. no-code approaches?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ZealousidealMark6535",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-04T14:05:17.611354+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-04T11:48:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I\u2019m working on a robotics automation project and trying to learn how people collect B2B data for outbound research.</p> <p>I\u2019m looking to understand:</p> <p>How to scrape or collect public data to identify companies that may need automation (e.g. restaurants, hospitals, construction)</p> <p>What kinds of web sources are commonly used (public sites, directories, job pages, maps, government portals, etc.)</p> <p>What APIs or public datasets are available for company-level or role-level data</p> <p>Best practices for ethical and compliant scraping (rate limits, public data only, etc.)</p> <p>The goal is research and outreach learning, not promotion or selling here.</p> <p>If you\u2019ve done something similar or have technical insights, I\u2019d appreciate some direction.</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ZealousidealMark6535\"> /u/ZealousidealMark6535 </a> <br/> <span><a href=\"https://www.redd",
        "id": 4236795,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pdy04l/how_to_collect_b2b_data_using_web_scraping_or_apis",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to collect B2B data using web scraping or APIs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Critical033",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-04T09:00:42.915172+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-04T08:43:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For background, for my job we need time to time to check what is media feedback on some topics (internal usage). In the real past we used to spend hours watching videos, then I started scrapping captions to search faster. That created an internal small database we used to search quickly.</p> <p>Then I was using a deprecated API from YouTube that would allow me to easily scrape its captions; since a few years that got deprecated and only custom solutions are available to scrape this captions (also failing frequently). Last year this got even stronger and most libraries are not working anymore. I also found some demand from YouTube to a private company (millions fine) for scraping or sth similar (couldn&#39;t really catch exactly the case due to legales language).</p> <p>My main question, if we continue scraping (we stopped since official API was deprecated) for this kind of internal usage are we risking getting a demand from YouTube?</p> <p>There is an",
        "id": 4234738,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pdv17l/is_youtube_captions_scrapping_legal_or_some_way",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is YouTube Captions Scrapping Legal (or some way to get the data)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SantiPG14",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-04T09:00:43.170745+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-04T06:42:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to figure out whether it&#39;s possible to scrape <strong>only the sponsored results (Google Ads)</strong> from a regular Google Search results page.</p> <p>I&#39;m not interested in the organic results, just the ads that appear at the top or bottom. </p> <p>Doing it manually is extremely slow, especially because the second page may contain sponsored results that don\u2019t appear on the first one, and the same happens with the following pages.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SantiPG14\"> /u/SantiPG14 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pdt4j5/is_it_possible_to_scrape_only_google_ads_from/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1pdt4j5/is_it_possible_to_scrape_only_google_ads_from/\">[comments]</a></span>",
        "id": 4234739,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pdt4j5/is_it_possible_to_scrape_only_google_ads_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it possible to scrape only Google Ads from search results?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Captain_Dawn013",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-12-04T07:59:25.745028+00:00",
        "date_dead_since": null,
        "date_published": "2025-12-04T06:04:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys! I built an Electron desktop app to handle the UI for our automation project, but right now, the Playwright automation is bundled inside the app.</p> <p>We&#39;re using Electron + React as the frontend and Playwright as our automation backend... but I&#39;m planning to de-couple it from the app so it doesn&#39;t take too much resources from the user&#39;s computer (since it opens the browser context on user&#39;s computer). </p> <p>We have self hosted VMs made possible by Proxmox and I want my electron app to communicate to it...maybe with an API gateway service then I also want to host a shared DB so all our data are consistent.</p> <p>I ask several LLMs about this and they suggested having a &quot;Message Queue&quot; (MQ) system and using technologies like Celery, Redis, RabbitMQ and Django. Of course, this was heavily influence of my experience as a Python Developer and that we are using Python playwright as our automation engine.</p> <p>I",
        "id": 4234265,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1pdsgeu/architecture_help_decoupling_playwright_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Architecture Help: Decoupling Playwright from Electron \u269b\ufe0f\ud83c\udfad",
        "vote": 0
    }
]