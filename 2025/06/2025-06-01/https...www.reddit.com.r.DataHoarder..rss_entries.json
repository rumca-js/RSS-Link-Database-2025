[
    {
        "age": null,
        "album": "",
        "author": "/u/Careless_Ferret_3299",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T23:56:43.949092+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T23:19:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I have one of these enclosures: yes I know they are probably frowned upon in here, but I only have it so I can back up my stuff to a 6TB HDD. </p> <p>Just a quick Q: the fan on the bloody thing is stupid loud, has anyone modded one to get a better fan working in it? I did change the stock fan in it for one of these :</p> <p><a href=\"https://www.amazon.co.uk/dp/B07D74LXBW?ref=ppx_yo2ov_dt_b_fed_asin_title\">https://www.amazon.co.uk/dp/B07D74LXBW?ref=ppx_yo2ov_dt_b_fed_asin_title</a> </p> <p>as these 2 wouldnt fit in the F&#39;ing thing.....</p> <p><a href=\"https://www.amazon.co.uk/dp/B009NQMESS?ref=ppx_yo2ov_dt_b_fed_asin_title\">https://www.amazon.co.uk/dp/B009NQMESS?ref=ppx_yo2ov_dt_b_fed_asin_title</a></p> <p><a href=\"https://www.amazon.co.uk/dp/B008S1HNPS?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1\">https://www.amazon.co.uk/dp/B008S1HNPS?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1</a></p> <p>But alas its still well loud....</p> <p>I know its a ",
        "id": 2824719,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l12ycg/orico_ds200u3_2_bay_hdd_enclosure_fannoise",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Orico - DS200U3 2 Bay HDD enclosure fan/noise question",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cultural-Victory3442",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T23:56:43.462063+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T23:10:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am about to buy a better capacity hard drive for saving my files, because right now I only use 500Gb hard drives that i had along the years</p> <p>So I want to move to a better capacity drive. </p> <p>But I&#39;m not sure on how much $ per TB is a good price.</p> <p>Any suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cultural-Victory3442\"> /u/Cultural-Victory3442 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l12rrf/how_much_per_tb_do_you_pay/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l12rrf/how_much_per_tb_do_you_pay/\">[comments]</a></span>",
        "id": 2824717,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l12rrf/how_much_per_tb_do_you_pay",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How much per TB do you pay?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/saiba_444",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T23:56:43.671989+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T22:54:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m curious about what naming systems, metadata, and folder organization folks use for TV shows and movies. </p> <p>I&#39;m a newbie so I&#39;m still working on mine. For TV shows, I&#39;m currently using the subtitle metadata for the episode number, and tags for the season. I then group by tags and sort by subtitle. I put shows in their own folders, all grouped into one TV show folder in Videos. I don&#39;t own too much physical media yet, so I haven&#39;t been able to add much to my database. I don&#39;t have a philosophy for movies yet. ;;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/saiba_444\"> /u/saiba_444 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l12ety/what_is_your_file_organization_philosophy_for_tv/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l12ety/what_is_your_file_organization_philosophy_for_tv/\">[comments]</a></span>",
        "id": 2824718,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l12ety/what_is_your_file_organization_philosophy_for_tv",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is your file organization philosophy for TV shows and movies?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Yennefer____",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T21:46:44.948516+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T21:43:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just wonder people who are trying to build a brand, how common they use phantombuster(or any other extension)? How many people are aware of the auto-follow/organic grow option?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yennefer____\"> /u/Yennefer____ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l10t0m/how_popular_phantombuster_among_insta/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l10t0m/how_popular_phantombuster_among_insta/\">[comments]</a></span>",
        "id": 2824168,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l10t0m/how_popular_phantombuster_among_insta",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How popular phantombuster among insta brand-building users?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Equivalent-Car-1021",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T21:46:45.126221+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T21:39:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I watched this amazing motorcycle video on a bike and home grown giant killer that was withdrawn for copyright reasons. Is it anywhere to be found in the archives <a href=\"https://youtu.be/zqpY7C41QLU\">https://youtu.be/zqpY7C41QLU</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Equivalent-Car-1021\"> /u/Equivalent-Car-1021 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l10pb7/removed_youtube_masterpiece/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l10pb7/removed_youtube_masterpiece/\">[comments]</a></span>",
        "id": 2824169,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l10pb7/removed_youtube_masterpiece",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Removed youtube masterpiece",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Quantity_5697",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T21:46:44.682889+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T21:37:07+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l10npk/seagates_insane_40tb_monster_drive_is_real_and_it/\"> <img src=\"https://external-preview.redd.it/AAMq7PQnpPv8yXWP5H1W_uscbOlAMoezUvqujjDIaoA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=419ec2c81fb172a86f59634a81778e92d87e35f2\" alt=\"Seagate\u2019s insane 40TB monster drive is real, and it could change data centers forever by 2026!\" title=\"Seagate\u2019s insane 40TB monster drive is real, and it could change data centers forever by 2026!\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Quantity_5697\"> /u/Ok_Quantity_5697 </a> <br/> <span><a href=\"https://www.techradar.com/pro/seagate-confirms-40tb-hard-drives-have-already-been-shipped-but-dont-expect-them-to-go-on-sale-till-2026\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l10npk/seagates_insane_40tb_monster_drive_is_real_and_it/\">[comments]</a></span> </td></tr></table>",
        "id": 2824167,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l10npk/seagates_insane_40tb_monster_drive_is_real_and_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/AAMq7PQnpPv8yXWP5H1W_uscbOlAMoezUvqujjDIaoA.jpg?width=640&crop=smart&auto=webp&s=419ec2c81fb172a86f59634a81778e92d87e35f2",
        "title": "Seagate\u2019s insane 40TB monster drive is real, and it could change data centers forever by 2026!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/geeyoff",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T21:46:44.501868+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T20:54:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>TL;DR - On a single ext4 hdd, can I mimic the cool data protection of ZFS?</p> <p>I have an 8tb hdd connected to an old laptop, and I&#39;m using it as a file server and for self-hosting a few docker apps (navidrome, jellyfin, adguard, etc.) That one hdd is plenty for me, and I keep regular 3-2-1 backups. </p> <p>The hdd is formatted as ext4. Is there a &quot;best practices&quot; configuration or software setup to ensure healthy data retention on that hdd? </p> <p>People here rave about zfs, but they often have more sophisticated setups than I do. I started reading about ZFS, and <em>yikes,</em> my first impression is that, for me, it&#39;s not worth the steep learning curve. (I&#39;m a busy dad to two young energetic kids!) So what could I do with my existing setup to reduce headaches? Alternatively, is ZFS worth it for a humble home server like mine? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/geeyoff\"> /u",
        "id": 2824166,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0zm7x/best_practices_on_ext4_for_someone_too_busy_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best practices on ext4 (for someone too busy to learn ZFS)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/metahades1889_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T20:41:21.083865+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T20:00:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What is the best model for 1 petabyte storage? It&#39;s for personal use, not business use. I&#39;ve seen on this forum that they&#39;re around 200k, but on Amazon I see 10k models. What&#39;s the difference?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/metahades1889_\"> /u/metahades1889_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0yb22/what_is_the_best_model_for_1_petabyte_storage_its/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0yb22/what_is_the_best_model_for_1_petabyte_storage_its/\">[comments]</a></span>",
        "id": 2823818,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0yb22/what_is_the_best_model_for_1_petabyte_storage_its",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the best model for 1 petabyte storage? It's for personal use, not business use. I've seen on this forum that they're around 200k, but on Amazon I see 10k models. What's the difference?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IHateSpamCalls",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T18:31:21.051208+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T18:23:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0vz6q/a_new_ssd_form_factor_can_house_a_staggering/\"> <img src=\"https://external-preview.redd.it/KYZpKwuzO-urKRnGy4TadsCWMEbwpn6OyBmdfD3u07M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f64ea740594c9964f3edede6598bf9cda3d09943\" alt=\"A new SSD form factor can house a staggering 1,000,000 GB of storage\" title=\"A new SSD form factor can house a staggering 1,000,000 GB of storage\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IHateSpamCalls\"> /u/IHateSpamCalls </a> <br/> <span><a href=\"https://www.tomshardware.com/pc-components/ssds/a-new-ssd-form-factor-can-house-a-staggering-1-000-000-gb-of-storage-e2-drives-could-store-11-000-4k-movies-with-80w-power-draw\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0vz6q/a_new_ssd_form_factor_can_house_a_staggering/\">[comments]</a></span> </td></tr></table>",
        "id": 2823081,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0vz6q/a_new_ssd_form_factor_can_house_a_staggering",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/KYZpKwuzO-urKRnGy4TadsCWMEbwpn6OyBmdfD3u07M.jpg?width=640&crop=smart&auto=webp&s=f64ea740594c9964f3edede6598bf9cda3d09943",
        "title": "A new SSD form factor can house a staggering 1,000,000 GB of storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MedelFamily",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T18:31:21.261591+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T18:10:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For those of you renaming media, this was just posted a few days ago. I tried it out and it\u2019s even faster than FileBot. Highly recommend.</p> <p>Thanks <a href=\"/u/Jimmypokemon\">u/Jimmypokemon</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MedelFamily\"> /u/MedelFamily </a> <br/> <span><a href=\"https://www.reddit.com/r/software/s/lFN7C1iGLE\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0vnvk/free_simpler_filebot/\">[comments]</a></span>",
        "id": 2823082,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0vnvk/free_simpler_filebot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Free: Simpler FileBot",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Spartan_213",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T19:36:21.545430+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T17:36:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How can I filter to download only reels of a instagram account. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Spartan_213\"> /u/Spartan_213 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0ut61/instagram_reel_download/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0ut61/instagram_reel_download/\">[comments]</a></span>",
        "id": 2823448,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0ut61/instagram_reel_download",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Instagram reel download",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Screen_6446",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T16:21:22.869262+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T15:31:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0rrw7/m2_sata_enclosure_is_slow_af/\"> <img src=\"https://a.thumbs.redditmedia.com/QQSMPQgYgfLyJGVVMdcI42P8EKuCbc1rueJ1zGg6gD0.jpg\" alt=\"M.2 sata enclosure is slow af\" title=\"M.2 sata enclosure is slow af\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So i got an Amazon basics Usb 3.0 M.2 sata enclosure and the read and write speeds seem to be very low what could be the issue (my system has USB 3.2 gen 1 Type C port) Is this an issue with the SSD or the enclosure?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Screen_6446\"> /u/Ok_Screen_6446 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1l0rrw7\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0rrw7/m2_sata_enclosure_is_slow_af/\">[comments]</a></span> </td></tr></table>",
        "id": 2822390,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0rrw7/m2_sata_enclosure_is_slow_af",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/QQSMPQgYgfLyJGVVMdcI42P8EKuCbc1rueJ1zGg6gD0.jpg",
        "title": "M.2 sata enclosure is slow af",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Educational-Teach315",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T15:16:18.091336+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T15:01:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m not sure why this does not seem to exist, and I wonder if I\u2019m overlooking something. What would seem awesome to me is a NAS which has 1nvme boot drive, then a pool of 3 nvme in raidz1 for fast storage and a pool of 3 or more sata disks for large storage.</p> <p>Why does this not exist? I might DIY it, but wonder if i\u2019m overlooking something obvious, like perhaps its not required if you just use nvme cache or\u2026?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Educational-Teach315\"> /u/Educational-Teach315 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0r1j8/in_search_of_an_nvme_and_sata_combo_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0r1j8/in_search_of_an_nvme_and_sata_combo_nas/\">[comments]</a></span>",
        "id": 2821990,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0r1j8/in_search_of_an_nvme_and_sata_combo_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "In search of an NVMe and SATA combo NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Glen_Garrett_Gayhart",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T15:16:17.872572+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T14:30:26+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0qbd7/i_want_to_save_all_the_urls_for_all_of_the_art/\"> <img src=\"https://preview.redd.it/7ogqapjtsb4f1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=620bbd5b7e4249f6c1c489727f3e41e7a9adf4f7\" alt=\"I want to save all the URLs for all of the art pages, or all of the main image URLs from those pages, from specific Deviantart galleries. How should I automate this? What should I program in?\" title=\"I want to save all the URLs for all of the art pages, or all of the main image URLs from those pages, from specific Deviantart galleries. How should I automate this? What should I program in?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Glen_Garrett_Gayhart\"> /u/Glen_Garrett_Gayhart </a> <br/> <span><a href=\"https://i.redd.it/7ogqapjtsb4f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0qbd7/i_want_to_save_all_the_urls_for_all_of_the_art/",
        "id": 2821989,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0qbd7/i_want_to_save_all_the_urls_for_all_of_the_art",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/7ogqapjtsb4f1.jpeg?width=320&crop=smart&auto=webp&s=620bbd5b7e4249f6c1c489727f3e41e7a9adf4f7",
        "title": "I want to save all the URLs for all of the art pages, or all of the main image URLs from those pages, from specific Deviantart galleries. How should I automate this? What should I program in?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/atakangoek",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T14:11:18.108033+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T13:12:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i\u2019ve always liked the idea of data hoarding, all these DMCAs, copyright strikes, reddit users mass deleting their content for protest, etc. has made me realize that i can\u2019t trust the things i would need to always be there on the internet (for example, i need the firmware for my WD ShareSpace NAS because i had to format it\u2019s hard drives but since WD stopped supporting it and removed the firmware downloads, theres no working ones ive found over a week of research)</p> <p>But my biggest issue is the cost, i have a very strong gaming pc with a 1 TB SSD and a 2 TB Hard Disk, i also have a spare laptop with 126 GB SSD. I also have a WD ShareSpace NAS leftover from my dad with 4 drives, each being 1 TB but i couldn\u2019t get them to work due to the aforementioned problems. </p> <p>I really despise deleting any data or keeping it on the cloud but economically my country is not in a very good place so I don\u2019t have a lot of disposable income, if this is a hobby tha",
        "id": 2821666,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0on1b/how_can_i_start_my_data_hoarding_journey",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can I start my Data Hoarding Journey?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JohnTheFisherman142",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T19:36:21.696628+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T13:05:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I keep hearing how 3.5&quot; go 24, 26, 28TB and soon there&#39;s gonna be 30-- Actually I don&#39;t want any of this.<br/> What I&#39;d like is 2.5&quot; 8TB drives. Plop 8 of those into Z2 or R6. And: with proper power management. I used to run a bunch of Toshiba 3TB desktop drives in raid5 (yes I know) that would spin down via hdparm when the OS did not detected any disk IO in 15minutes. Worked a charm. New Toshis don&#39;t give adamn about what hdparm tells them.<br/> With my setup I could have all my storage no further away than a 5 seconds spin-up and still go easy on the power bill. I don&#39;t want 4x14TB 3.5&quot; in this gen8 microserver running 24/7 now even when nobody&#39;s home.<br/> So-- is there any news that these capacities will come to 2.5&quot; desktop drives?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JohnTheFisherman142\"> /u/JohnTheFisherman142 </a> <br/> <span><a href=\"https://www.redd",
        "id": 2823449,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0oi6y/no_25_hamr_drives_on_the_horizon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "No 2.5\" HAMR drives on the horizon?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sad_Individual_8645",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T19:36:21.902370+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T12:48:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0o5pu/i_believe_maxdigitaldata_labeled_the_wrong_drive/\"> <img src=\"https://a.thumbs.redditmedia.com/9BsRYE_Y0KA892YJ8Is_WZNyA0lNh8xfFhWljO86848.jpg\" alt=\"I believe MaxDigitalData labeled the wrong drive that I was sent, it shows as 7200 RPM with 64MB of cache on my computer but the model is supposed to be 5700 RPM with 32MB\" title=\"I believe MaxDigitalData labeled the wrong drive that I was sent, it shows as 7200 RPM with 64MB of cache on my computer but the model is supposed to be 5700 RPM with 32MB\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Here is the drive that I bought: </p> <p><a href=\"https://preview.redd.it/8b9mur5kab4f1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=58497c7dae945a0975b07d3bcdecb7a14b535976\">https://preview.redd.it/8b9mur5kab4f1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=58497c7dae945a0975b07d3bcdecb7a14b535976</a></p> <p>Here is an image of the sticker on it: </p> <",
        "id": 2823450,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0o5pu/i_believe_maxdigitaldata_labeled_the_wrong_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/9BsRYE_Y0KA892YJ8Is_WZNyA0lNh8xfFhWljO86848.jpg",
        "title": "I believe MaxDigitalData labeled the wrong drive that I was sent, it shows as 7200 RPM with 64MB of cache on my computer but the model is supposed to be 5700 RPM with 32MB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/z_2806",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T13:06:18.029244+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T12:46:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Website name is public.sud.uz and all pdfs are formatted like this </p> <p><a href=\"https://public.sud.uz/e8e43a3b-7769-4b29-8bda-ff41042e12b5\">https://public.sud.uz/e8e43a3b-7769-4b29-8bda-ff41042e12b5</a></p> <p>Without .pdf at the end. How can i download them is there any way to do it automatically? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/z_2806\"> /u/z_2806 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0o4l1/how_do_i_download_all_pdfs_from_this_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0o4l1/how_do_i_download_all_pdfs_from_this_website/\">[comments]</a></span>",
        "id": 2821302,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0o4l1/how_do_i_download_all_pdfs_from_this_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do i download all pdfs from this website?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jonathanweber_de",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T13:06:17.848242+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T12:43:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello!</p> <p>As a cameraman, a lot of my work consists of handling media files, converting videos, rendering, etc... For most cases, I go with the presets the different encoders (I mainly use x265) offer and that is just fine for the individual purpose and &quot;getting the job done&quot; in a reasonable amount of time with a reasonable amount of incompetence in terms of encoder settings ;).</p> <p>But; for the sake of knowing what I am doing I started exploring encoder settings. And after doing that for a few days, I came to the conclusion that having a more fine-grained approach to encoding my stuff (or at least knowing what IS possible) cannot be too bad. I found pretty good settings for encoding my usually grainy movie projects using a decent CRF value, preset slow and tuning aq-mode, aq-strength, psy-rd and psy-rdoq to my likings (even though just slightly compare to the defaults).</p> <p>What I noticed, though, is, that the resulting files have",
        "id": 2821301,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0o215/streamers_method_for_getting_highest_quality_at_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Streamer\u2019s method for getting highest quality at a predictable bitrate \u2013 3-pass encodes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Broad_Sheepherder593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T13:06:18.178087+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T12:20:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>To fellow data hoarders out there by any chance if someone would have the complete series of Greek 2007? Seasons 1 - 4?</p> <p>Thanks!!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Broad_Sheepherder593\"> /u/Broad_Sheepherder593 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0nmax/looking_for_a_tv_show_2007/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0nmax/looking_for_a_tv_show_2007/\">[comments]</a></span>",
        "id": 2821303,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0nmax/looking_for_a_tv_show_2007",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a tv show 2007",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ggekko999",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T13:06:18.326789+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T12:14:32+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0ni5y/easy_shucking/\"> <img src=\"https://b.thumbs.redditmedia.com/dppyhXTC6wVIya34gIsjZPAZr9Y5v1tuImbPFXlLMrI.jpg\" alt=\"Easy shucking\" title=\"Easy shucking\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>For ~ $2 I made a SATA power cable extender that drops the 3rd pin by connecting two sata to molex back to back. No special tape &amp; razor blades, worked first time, zero stress solution :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ggekko999\"> /u/ggekko999 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1l0ni5y\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0ni5y/easy_shucking/\">[comments]</a></span> </td></tr></table>",
        "id": 2821304,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0ni5y/easy_shucking",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/dppyhXTC6wVIya34gIsjZPAZr9Y5v1tuImbPFXlLMrI.jpg",
        "title": "Easy shucking",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zR8gPRtSUS7jJT8e",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T11:45:08.562346+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T11:41:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When it was first released people put effort into figuring out every ending and choose your own adventure story but it\u2019s all gone now </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zR8gPRtSUS7jJT8e\"> /u/zR8gPRtSUS7jJT8e </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0mx6w/did_anyone_archive_every_part_of_black_mirror/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0mx6w/did_anyone_archive_every_part_of_black_mirror/\">[comments]</a></span>",
        "id": 2820845,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0mx6w/did_anyone_archive_every_part_of_black_mirror",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Did anyone archive every part of black mirror: bandersnatch now that Netflix has removed its all interactive media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thomas001le",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T11:45:08.844736+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T11:32:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi.</p> <p>I&#39;ve recently come across <a href=\"https://rustic.cli.rs/\">Rustic</a>. This seems to be an alternative implementation of what Restic does but in Rust. Apart from the apparent Go vs Rust war that I don&#39;t want to go into detail here, Rustic has some pretty interesting feature, most notably, support for cold storage: it supports splitting the repository in a hot and a cold part, where the much smaller hot repository is used for bookkeeping and the cold repository is used to keep the actual data.</p> <p>This is all great, but OTOH Rustic seems to be generally less mature and focus on features instead of stability. There is a pretty<a href=\"https://rustic.cli.rs/docs/comparison-restic.html\"> comprehensive comparison with Restic</a> on their side. The worrying row for me is that while restic has decent test coverage, Rustic claims only 42% coverage *even in their core library*. So over half of the code never runs through tests, but you te",
        "id": 2820846,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0mrs3/any_experience_with_rustic",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any experience with Rustic?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Elegant_Beginning789",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T11:45:09.046746+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T11:10:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Choosing a drive for editing material shot on a small movie set daily. Material is shot, transferred to this drive, and as the rest is being shot editors put together a rough cut for director to see and figure out gaps.</p> <p>Redundancy is key so we have decided to have raid1 setup. Editing stations are all macbook pros with M2 chips.</p> <p>We wanted to get 2 8TB sticks and just make 1 raid with them but realized that it\u2019s significantly cheaper to get 4 4TB sticks. We can just make 2 raid 1 drives out of them and put them all in 1 Acasis enclosure. When connected, 2 4TB drives will show up which for us is fine and has no difference from 1 8TB drive in terms of usability. But some people in our team are worried about having 2 drives show up from 1 enclosure and say it\u2019s better to get the 8TB sticks. No one is very tech savvy so we decided to ask for advice online.</p> <p>Also one more person brought up that SN850x might be an overkill and suggested t",
        "id": 2820847,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0mer6/4x4tb_sn850x_inside_acasis_4bay_for_mac",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "4x4TB SN850X inside Acasis 4-bay for mac?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/catboy519",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T10:39:37.617749+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T10:16:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I must mention I have ADHD which makes this even harder to deal with.</p> <ul> <li>Phone 1: 16/16 GB</li> <li>Phone 2: 128/128 GB</li> <li>Desktop PC: 464/464 GB</li> <li>Laptop: also full.</li> <li>Flash drive: 70/128 GB but I stopped using it because it rarely works due to my phone storage being too full for it to be able to load into the phones memory..</li> </ul> <p>Then I also have some external hard drives 512 GB which I also store stuff on.</p> <p>Now the problem is I have alot of different devices which I store stuff on... and its completely unorganized, its a total chaotic mess. My photos and videos and apps and things are all over the place. I struggle to find anything I need, cause which device is it on? And also I have alot of duplicates of files across my devices.</p> <p>Almost all of my devices are full and even if I move stuff to external drives, its only a matter of days before the device is full again. Sometimes even within 1 day.</p>",
        "id": 2820540,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0lk55/my_data_is_a_mess_i_need_serious_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My data is a mess. I need serious help.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MemeZJaki",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T10:39:37.346189+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T09:51:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone!</p> <p>I\u2019m interested in restoring <a href=\"http://TheSlap.com\">TheSlap.com</a> (yes, the parody site from the TV show <em>Victorious</em>). Unfortunately, I\u2019ve run into a few dead ends:</p> <ul> <li>Archive downloaders (like Wayback Machine Downloader and ArchiveBox) are either failing or returning incomplete pages.</li> <li>The Wayback Machine only has partial captures of the site.</li> <li>I haven\u2019t found a full rip or mirror of it anywhere yet.</li> </ul> <p>Does anyone have experience restoring old Flash-heavy or entertainment-based websites like this?<br/> Any advice on tools, resources, or alternative approaches to get more archived content (e.g., browser-based scraping, wget tricks, etc.) would be incredibly helpful.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MemeZJaki\"> /u/MemeZJaki </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0l",
        "id": 2820539,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0l6oj/restoring_theslapcom_need_help_with_archived_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Restoring TheSlap.com \u2013 Need Help with Archived Data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Eurodance_SouthEast",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T19:36:22.194350+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T09:34:33+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Eurodance_SouthEast\"> /u/Eurodance_SouthEast </a> <br/> <span><a href=\"/r/u_Eurodance_SouthEast/comments/1l09adi/new_nas_build_help_needed_on_os/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0ky78/new_nas_build_help_needed_on_os/\">[comments]</a></span>",
        "id": 2823451,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0ky78/new_nas_build_help_needed_on_os",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New NAS build - help needed on OS!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/luxfc",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T09:33:39.153723+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T09:33:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. I was getting an error code 50 on MacOS when moving some large file folders to EXFAT formated HDDs and decided to finish the job on a windows machine. But the files moved to the HDD using windows are not showing up when I open the drive to MacOS. Any help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/luxfc\"> /u/luxfc </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0kxd8/files_copied_to_exfat_hdd_not_showing_up_on_mac/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0kxd8/files_copied_to_exfat_hdd_not_showing_up_on_mac/\">[comments]</a></span>",
        "id": 2820202,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0kxd8/files_copied_to_exfat_hdd_not_showing_up_on_mac",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Files copied to EXFAT HDD not showing up on Mac",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JadensWebMC",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T19:36:22.399739+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T09:30:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0kw8u/vhs_to_x264_done_by_a_camera/\"> <img src=\"https://b.thumbs.redditmedia.com/eMr_t0W1IcGJZIv_wgEWmJ-afUMr3o5lGEJ5MaPL_Cg.jpg\" alt=\"[VHS to x264] Done by a camera?\" title=\"[VHS to x264] Done by a camera?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m posting here since I lack the 100 karma needed to post on archivists.</p> <p>I think I have a pretty decent way of digitizing and archiving VHS tapes that doesn&#39;t take crap tons of storage for no good reason.</p> <p>First, I somehow just... have an S-VHS VCR which I&#39;ve since learned is kind of rare, but it has S-Video ins and outs, so I decided to try to plug that into my Sony miniDV camcorder which apparently from that I learned that the port on the camera is actually bidirectional. So, I connected it up, and then I connected that camcorder to a 2011 17&quot; MacBook Pro over FireWire, and opened QuickTimePlayer.</p> <p>For the audio (w",
        "id": 2823452,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0kw8u/vhs_to_x264_done_by_a_camera",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/eMr_t0W1IcGJZIv_wgEWmJ-afUMr3o5lGEJ5MaPL_Cg.jpg",
        "title": "[VHS to x264] Done by a camera?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Upstairs_Space_3195",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T09:33:39.307227+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T09:28:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0kux5/help_picking_a_flash_drive_back_up_that_connects/\"> <img src=\"https://preview.redd.it/iig7y02cba4f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=790cb15c251008e2c53eb6d12f88a117b9d17029\" alt=\"Help picking a flash drive back up that connects directly to phone.\" title=\"Help picking a flash drive back up that connects directly to phone.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi I\u2019m new to hoarding kind of. Phone storage is low and I wanted to transfer photos and videos to a drive. I have an iOS and wanted a flash drive that could directly connect to my phone so it\u2019s easier on the go with a good amount of space. Computers confuse me quite a bit so I can\u2019t transfer from a plain usb one.</p> <p>This was the only one I could find on social media, but I\u2019m very skeptical about these things and the reliability. I don\u2019t mind how pricy it would be for a reliable one. </p> <p>Does anyone know of any ",
        "id": 2820203,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0kux5/help_picking_a_flash_drive_back_up_that_connects",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/iig7y02cba4f1.jpeg?width=640&crop=smart&auto=webp&s=790cb15c251008e2c53eb6d12f88a117b9d17029",
        "title": "Help picking a flash drive back up that connects directly to phone.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CandidateStriking843",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T08:27:53.688670+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T08:14:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0jsgi/need_help_sandisk_ultra_32gb/\"> <img src=\"https://preview.redd.it/a50hbbo3y94f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c83357e5c7b0e93fba58ea5321cbbc0a71191ce9\" alt=\"need help- sandisk ultra 32gb\" title=\"need help- sandisk ultra 32gb\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>so little bit of context here, my flashdrive (sandisk ultra) is arround 8 years (still looks good) but randomly, its stuck on lock (currently read-only state). i have tried using mac, it still didn&#39;t work. Next i tried is windows (windows 10) and diskpart from windows. still did not work. Diskpart recognizes it, but explorer does not show the drive. Can someone help me here? all my files are still there. Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CandidateStriking843\"> /u/CandidateStriking843 </a> <br/> <span><a href=\"https://i.redd.it/a50hbbo3y94f1",
        "id": 2819955,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0jsgi/need_help_sandisk_ultra_32gb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/a50hbbo3y94f1.png?width=640&crop=smart&auto=webp&s=c83357e5c7b0e93fba58ea5321cbbc0a71191ce9",
        "title": "need help- sandisk ultra 32gb",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/searchjobs_poster",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T07:15:09.963463+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T07:09:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>guide on downloading youtube playlists:<br/> <a href=\"https://www.reddit.com/r/downr/comments/1l0gi4f/how_to_download_an_entire_youtube_playlist/\">https://www.reddit.com/r/downr/comments/1l0gi4f/how_to_download_an_entire_youtube_playlist/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/searchjobs_poster\"> /u/searchjobs_poster </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0iu3z/how_to_download_an_entire_youtube_playlist/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0iu3z/how_to_download_an_entire_youtube_playlist/\">[comments]</a></span>",
        "id": 2819707,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0iu3z/how_to_download_an_entire_youtube_playlist",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to Download an Entire YouTube Playlist ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Bonesnap1234",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T07:15:10.260365+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T06:29:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can someone explain how to rip dvds in a simple way, I\u2019m not very tech savvy lol</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Bonesnap1234\"> /u/Bonesnap1234 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0i7y1/simplistic_method_to_rip_dvds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0i7y1/simplistic_method_to_rip_dvds/\">[comments]</a></span>",
        "id": 2819708,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0i7y1/simplistic_method_to_rip_dvds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Simplistic method to rip dvds",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Expensive-Let3251",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T19:36:22.662733+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T05:46:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been saving commentary, livestreams, and strange uploads , mostly for audio. I normally do full desktop with yt-dlp or ClipGrab, but needed something less resource-intensive on the road.<br/> Found EsMP3, a browser converter that played pretty smooth. No glitchy redirects, can capture 320kbps, and had no issues with playlists too (with patience).<br/> I still like local tools for high-volume pulls but, for mobile work or infrequent, this one filled the gap better than most I&#39;ve tried. Anyone use browser-based tools in your arsenal, or do you use CLI/batch scripts only?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Expensive-Let3251\"> /u/Expensive-Let3251 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0hk8e/played_around_with_esmp3_as_a_lightweight_utility/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0hk8e/played_around_with_esmp3_as_a_li",
        "id": 2823453,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0hk8e/played_around_with_esmp3_as_a_lightweight_utility",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Played around with EsMP3 as a lightweight utility for capturing audio from YouTube \u2013 surprisingly good",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wade-wei",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T15:16:18.628880+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T05:18:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been using NFS over TCP for a while without issues. The write speed is ~600MB/s with CX3 FDR IB connections in RHEL7/8. I always wanna try NFS over RDMA but a friend of mine who works as tech support warned of its stability.</p> <p>MLNX/NV dropped such support since MLNX_OFED 4.x, despite relatively simple ways to activate this feature. I did give it a shot and write speed is approx. 1.1GB/s, almost doubling that of TCP, which is tempting. I wonder if RDMA is indeed risky as he stated. Has anybody got practical experience with it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wade-wei\"> /u/wade-wei </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0h42k/is_nfs_over_rdma_save_for_datahoarding_in_rhel78/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0h42k/is_nfs_over_rdma_save_for_datahoarding_in_rhel78/\">[comments]</a></span>",
        "id": 2821991,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0h42k/is_nfs_over_rdma_save_for_datahoarding_in_rhel78",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is NFS over RDMA save for data-hoarding in RHEL7/8?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No-Vast-8000",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T05:05:11.623002+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T04:00:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, So I&#39;m having a kind of weird issue. I&#39;ve got a number of drives combined via Stablebit and have been running a tool called MKV Optimizer to strip away extra audio tracks that aren&#39;t needed.</p> <p>If I go and look at a specific file I can see the size reduce, however, for some reason the overall free space doesn&#39;t seem to be updating. I let it run overnight and the drive actually LOST a small amount of free space, when it should have freed up what would have been hundreds of Gigabytes.</p> <p>It just doesn&#39;t seem to be accounting for the filesize changing.</p> <p>I&#39;m not 100% sure this is related to Stablebit but it seems like the most likely culprit to me. </p> <p>Anyone know of a fix for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Vast-8000\"> /u/No-Vast-8000 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0fsdb/issues_with_stablebit_not_re",
        "id": 2819307,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0fsdb/issues_with_stablebit_not_relinquishing_freed_up",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Issues with Stablebit not relinquishing freed up space",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DrDoom229",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-01T02:53:30.635141+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-01T02:41:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0eco6/shucked_2_24tb_seagates/\"> <img src=\"https://preview.redd.it/j9omxzota84f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b4ed5175ef46d79e752454b4c4366b09a234e24\" alt=\"Shucked 2 24tb Seagates\" title=\"Shucked 2 24tb Seagates\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Small gift to myself. Shucked 2 24tb Seagate Expansion drives. Super happy. I am consolidating 3 10tb drives to 2 24tb drives. My media server is growing and happy I caught these on sale. 279.00 a piece</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DrDoom229\"> /u/DrDoom229 </a> <br/> <span><a href=\"https://i.redd.it/j9omxzota84f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0eco6/shucked_2_24tb_seagates/\">[comments]</a></span> </td></tr></table>",
        "id": 2818933,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0eco6/shucked_2_24tb_seagates",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/j9omxzota84f1.png?width=640&crop=smart&auto=webp&s=4b4ed5175ef46d79e752454b4c4366b09a234e24",
        "title": "Shucked 2 24tb Seagates",
        "vote": 0
    }
]