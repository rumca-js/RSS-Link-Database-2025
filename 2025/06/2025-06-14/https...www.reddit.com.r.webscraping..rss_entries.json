[
    {
        "age": null,
        "album": "",
        "author": "/u/volomike",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T19:45:01.611573+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T19:27:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there an API for this? So, we can give a company name and city/state and it can return likely matches, and then we can pull those and get the key decision makers and their listed address info? What about potential email addresses?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/volomike\"> /u/volomike </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lbgw4z/scraping_usa_secretary_of_state_filings/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lbgw4z/scraping_usa_secretary_of_state_filings/\">[comments]</a></span>",
        "id": 2932837,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lbgw4z/scraping_usa_secretary_of_state_filings",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping USA Secretary of State Filings",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/xkiiann",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T15:20:42.255400+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T14:53:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://github.com/xKiian/awswaf\">https://github.com/xKiian/awswaf</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/xkiiann\"> /u/xkiiann </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lbajrc/aws_waf_fully_reverse_engineered_implemented_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lbajrc/aws_waf_fully_reverse_engineered_implemented_in/\">[comments]</a></span>",
        "id": 2931569,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lbajrc/aws_waf_fully_reverse_engineered_implemented_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AWS WAF fully reverse engineered & implemented in Golang and Python",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sea_Put_2759",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T19:45:01.882926+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T14:23:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello</p> <p>I&#39;m working on a scrapper for football data for a data analysis study focused on probability.</p> <p>If this thread don&#39;t fall down, I will keep publishing in this thread the results from this work.</p> <p>Here are some CSV files with some data.</p> <p>- List of <a href=\"https://dpaste.org/8Mh2Q\">links of the all leagues from each country</a> available in Flashscore.</p> <p>- List of <a href=\"https://dpaste.org/nZpuq\">links of tournaments of all leagues from each country by year</a> available in Flashscore.</p> <p>I can not publish the source code, for while, but I&#39;ll publish asap. Everything that I publish here is for free. </p> <p>The next steps are to scrap data from tournaments.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sea_Put_2759\"> /u/Sea_Put_2759 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lb9vrn/flashscore_football_scrapped_data/\">[link]</a></sp",
        "id": 2932838,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lb9vrn/flashscore_football_scrapped_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Flashscore football scrapped data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Educational_Foot3881",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T13:08:42.295464+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T12:34:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m facing an issue when using Puppeteer with the puppeteer-cluster library, specifically encountering the error:<br/> <strong>&quot;Cannot read properties of null (reading &#39;sourceOrigin&#39;)&quot;</strong>,<br/> which happens when using <code>page.setCookie</code>. This is caused by the fact that puppeteer-cluster does not yet support using <code>browser.setCookie()</code>.</p> <p>I\u2019m now planning to try using Crawlee or Playwright. Do you have any good recommendations that would meet the following requirements:</p> <ol> <li>Cluster-based scraping</li> <li>Easy to deploy</li> </ol> <p><strong>Development stack:</strong><br/> Node.js, Docker</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Educational_Foot3881\"> /u/Educational_Foot3881 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lb7o9z/can_you_help_me_decide_whether_to_use_crawlee_or/\">[link]</a></span> &#32; <span><a href=\"https:",
        "id": 2930823,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lb7o9z/can_you_help_me_decide_whether_to_use_crawlee_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can you help me decide whether to use Crawlee or Playwright?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SeamusCowden",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T07:43:44.785467+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T07:12:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all,</p> <p>I am working on a news article crawler (backend) that crawls, discovers articles, and stores them in a database with metadata. I am not very experienced in scraping, but I have issues running into hard paywalls, and webpages have different structures and selectors, making building a general scraper tough. It runs into privacy consent gates, login requirements, and subscription requirements. Besides that, writing code to extract the headline, author, and full text is tough, as websites use different selectors. I use <a href=\"https://docs.crawl4ai.com/\">Crawl4AI</a>, <a href=\"https://trafilatura.readthedocs.io/en/latest/\">Trafilatura</a> and BeautifulSoup as my main libraries, where I use Crawl4AI as much as possible. </p> <p>Would anyone happen to have any experience in this field and be able to give me some tips? All tips are welcome!</p> <p>I really appreciate any help you can provide.</p> </div><!-- SC_ON --> &#32; submitted by &#3",
        "id": 2929394,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lb2nbu/advice_on_news_article_crawling_and_scraping_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advice on news article crawling and scraping for media monitoring",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aoksiku",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T07:43:44.495256+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T07:10:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working on a project to programmatically scrape the entire online records. The `/SWS/properties` API requires an `x-sws-turnstile-token` (Cloudflare Turnstile) for each request, which seems to be single-use and generated via a browser-based JavaScript challenge. This makes pure HTTP requests (e.g., with Axios) tricky without generating a new token for every page of results.</p> <p>My current approach uses Puppeteer to automate browser navigation and intercept JSON responses, but I\u2019d love to find a more efficient, purely API-based solution without browser overhead. Its tedious because the site i need to enter each iteration manually and its paginated page. Im new to scraping.</p> <p>Specifically, I\u2019m looking for:</p> <ol> <li><p>. Alternative endpoints or methods to access the full dataset (e.g., bulk download, undocumented APIs).</p></li> <li><p>Techniques to programmatically handle Turnstile tokens without a full browser (e.g., reverse-engine",
        "id": 2929393,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lb2mfv/how_to_programmatically_scrape_without_perrequest",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to Programmatically Scrape without Per-Request Turnstile Tokens?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/_marcuth",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-14T04:28:51.884133+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-14T03:48:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1lazazx/my_web_scraping_project/\"> <img src=\"https://external-preview.redd.it/nwuTsO-hBG5eGtELj17wwJAZSvl-S3R68-j1J6-OcYE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5490d569f0a25662234fc9355cd74b10c4d2fd25\" alt=\"My Web Scraping Project\" title=\"My Web Scraping Project\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been interested in web scraping for a few years now, and over time I&#39;ve had to deal with common problems of disorganization and architecture... So, taking some ideas from my friends and having my own ideas, I started writing an NPM package that solved common web scraping problems. I recently split it into some smaller packages and licensed them all under the MIT license. I&#39;d like to ask you to take a look and I&#39;m accepting feedback and contributions :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_marcuth\"> /u/_marcuth </a> <br/> <",
        "id": 2928770,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lazazx/my_web_scraping_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/nwuTsO-hBG5eGtELj17wwJAZSvl-S3R68-j1J6-OcYE.png?width=216&crop=smart&auto=webp&s=5490d569f0a25662234fc9355cd74b10c4d2fd25",
        "title": "My Web Scraping Project",
        "vote": 0
    }
]