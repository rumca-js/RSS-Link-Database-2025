[
    {
        "age": null,
        "album": "",
        "author": "/u/felicaamiko",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T23:05:49.854825+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T22:32:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>-f -n works for me in ddrescue, but it&#39;s been kind of crawling at 9% recovered for like a week now. so i am kinda scared to touch it. i was told that as long as i create an img and a log you can make multiple passes with different settings and it won&#39;t retry sections it has already recovered...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/felicaamiko\"> /u/felicaamiko </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l4co3d/is_sudo_ddrescue_fan_good_for_an_initial_first/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l4co3d/is_sudo_ddrescue_fan_good_for_an_initial_first/\">[comments]</a></span>",
        "id": 2860912,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l4co3d/is_sudo_ddrescue_fan_good_for_an_initial_first",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "is sudo ddrescue -fAn good for an initial first pass on external HDD? i was told to do -f -n for a first pass, but considering retrying with -f -n -A.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JKAF3",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T23:05:49.653930+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T22:14:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Ok so I have 2x 2TB intel sas ssds</p> <p>My pc motherboard is a ASUS ROG Strix X670E-F Gaming WiFi</p> <p>How can I access these SSDs so I can use them as storage? I just want to use them as storage and that\u2019s all</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JKAF3\"> /u/JKAF3 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l4c9l8/sas_drive_in_desktop/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l4c9l8/sas_drive_in_desktop/\">[comments]</a></span>",
        "id": 2860911,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l4c9l8/sas_drive_in_desktop",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SAS drive in desktop?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JimFrankenstein138",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T19:51:17.044433+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T19:17:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I partially downloaded a fairly large torrent on a laptop (Sesame Street) and ran out of room. I transferred the data to a large external HD. I then deleted the data from my laptop. I then started downloading the torrent again, this time directing the data to be downloaded on the externalHD. Will the already downloaded data be overwritten or will the 500+ GB data be recognized and only the missing data will be downloaded? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JimFrankenstein138\"> /u/JimFrankenstein138 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l47y3f/torrent_question_for_large_file/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l47y3f/torrent_question_for_large_file/\">[comments]</a></span>",
        "id": 2859579,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l47y3f/torrent_question_for_large_file",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Torrent Question for large file",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TestFlightBeta",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T19:51:16.805267+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T19:12:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am thinking of getting a few Seagate Expansion drives to put in my room since the Seagate Exos I have from ServerPartDeals are too noisy. Is it worth it to shuck the them to put in my JBOD in exchange for the loss in the 3-year warranty?</p> <table><thead> <tr> <th align=\"left\">Feature</th> <th align=\"left\">Seagate Expansion (Shucked)</th> <th align=\"left\">Seagate Expansion (Unshucked)</th> <th align=\"left\">Seagate Exo</th> </tr> </thead><tbody> <tr> <td align=\"left\">Noise</td> <td align=\"left\">Low</td> <td align=\"left\">Low</td> <td align=\"left\">High (twice as much?)</td> </tr> <tr> <td align=\"left\">Price</td> <td align=\"left\">Similar</td> <td align=\"left\">Similar</td> <td align=\"left\">Similar</td> </tr> <tr> <td align=\"left\">Warranty</td> <td align=\"left\">None / Voided</td> <td align=\"left\"><del>3 Years</del> 1 year (!!!)</td> <td align=\"left\">90 Days or 2 Years</td> </tr> <tr> <td align=\"left\">Connectivity</td> <td align=\"left\">Requires JBOD</td> ",
        "id": 2859578,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l47u88/loss_in_warranty_worth_shucking_a_drive_seagate",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Loss in warranty worth shucking a drive? (Seagate Expansion)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/iNebulaiNinjai",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T18:46:50.276129+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T17:58:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve used YouTube. Is there a better site?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iNebulaiNinjai\"> /u/iNebulaiNinjai </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l45x61/where_do_you_go_to_download_files_for_your/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l45x61/where_do_you_go_to_download_files_for_your/\">[comments]</a></span>",
        "id": 2859012,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l45x61/where_do_you_go_to_download_files_for_your",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where do you go to download files for your digital library?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SaggyPig4321",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T18:46:49.827405+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T17:45:40+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l45ltf/is_this_bdr_at_full_capacity_post_burning/\"> <img src=\"https://preview.redd.it/v05ldaymb55f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=174956cf704012c95836e2163f4b94c1486e0fe7\" alt=\"Is this BD-R at full capacity post burning?\" title=\"Is this BD-R at full capacity post burning?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Power2Go 13 shows that it&#39;s just under the max capacity of 24202 MB before burning the BDMV, but once completed, the disc seems to have unused space still. When I look at the properties of the BD-R it shows 0 bytes of used data and around 11GB of free data. Should I try going over what Power2Go is saying will max it out or is this normal for burning BD-R? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SaggyPig4321\"> /u/SaggyPig4321 </a> <br/> <span><a href=\"https://i.redd.it/v05ldaymb55f1.jpeg\">[link]</a></span> &#32; <span><a hr",
        "id": 2859011,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l45ltf/is_this_bdr_at_full_capacity_post_burning",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/v05ldaymb55f1.jpeg?width=640&crop=smart&auto=webp&s=174956cf704012c95836e2163f4b94c1486e0fe7",
        "title": "Is this BD-R at full capacity post burning?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/youre_being_illegal",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T17:42:54.708239+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T17:02:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m not rich, neither am I particularly clever. I was thinking I could learn on an old second hand server. Upgrade when I know what I&#39;m doing and what I need/want.</p> <p>Is this a good/bad idea? I would just want storage and access for music, photo&#39;s. maybe learn how to run a vm. Generally a little digital playground for me to learn something new to me.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/youre_being_illegal\"> /u/youre_being_illegal </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l44gto/is_a_cheap_old_server_from_ebay_a_good_idea_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l44gto/is_a_cheap_old_server_from_ebay_a_good_idea_for/\">[comments]</a></span>",
        "id": 2858383,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l44gto/is_a_cheap_old_server_from_ebay_a_good_idea_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is a cheap old server from Ebay a good idea for me to learn on?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rexyuan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T17:42:54.920043+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T16:59:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.techradar.com/pro/mission-impossible-the-final-reckoning-gets-surprise-guest-appearance-a-revolutionary-360tb-silica-storage-media\">https://www.techradar.com/pro/mission-impossible-the-final-reckoning-gets-surprise-guest-appearance-a-revolutionary-360tb-silica-storage-media</a></p> <p>How far away these alternative material stuff good for cold storage are from coming to the consumer market? And what does it mean for data hoarding?</p> <p>I think it would make the 2 in the 1-2-3 backup principle become 1 copy stored in your usual drive and 1 copy stored in these kinds of specialized cold storage drive</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rexyuan\"> /u/rexyuan </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l44e0m/just_watched_the_new_mission_impossible_movie_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l44e0m/jus",
        "id": 2858384,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l44e0m/just_watched_the_new_mission_impossible_movie_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Just watched the new Mission Impossible movie and found out the \u201c5D\u201d silica drive in it is real and made by Sphotonix",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Badillaboy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T16:35:50.678720+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T16:30:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Debating a 20tb external hard drive or a small nas. I want to store my family\u2019s iCloud back up to reduce the charges for larger cloud data. Also I have about 8tb of pictures and small videos for the life of my iOS history. Also we have two gaming pc so was thinking they can be synced to one nas to have like pictures and just data taking up the high speed memory on the gaming pc. Any recommendations on price or should I build a pi nas. Looking at ideas. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Badillaboy\"> /u/Badillaboy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l43o2e/i_need_data_backup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l43o2e/i_need_data_backup/\">[comments]</a></span>",
        "id": 2857773,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l43o2e/i_need_data_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need data backup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Arcueid-no-Mikoto",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T16:35:50.829405+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T16:21:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I wanted to download this website:</p> <p><a href=\"https://www.mangaupdates.com/\">https://www.mangaupdates.com/</a></p> <p>It&#39;s a very valuable manga database for me, I can always find mangas I&#39;d like to read by filtering for tags etc. And I&#39;d like to keep it if for whatever reason it goes away one day or they change their filtering system which is pretty good now for me.</p> <p>Problem is, there&#39;s a ton of stuff I&#39;m not interested like <a href=\"https://www.mangaupdates.com/forum\">https://www.mangaupdates.com/forum</a><br/> Is there a way I can add like URLs not to download like that one and anything /forum/xxx? </p> <p>Also is HHTrack a good tool? I used it in the past but it&#39;s been a while, so I wonder if there&#39;s better ones by now, seems this was updates last in 2017.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Arcueid-no-Mikoto\"> /u/Arcueid-no-Mikoto </a> <br/",
        "id": 2857774,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l43fwq/downloading_site_with_httrack_can_i_add_url",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading site with HTTrack, can I add url exception?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Illustrious_Crab_146",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T16:35:50.978275+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T15:47:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l42kfl/free_file_sync_hangs_after_100gb_of_transfer/\"> <img src=\"https://preview.redd.it/omkw5ookq45f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=88f872edc54ffacc1f952b1a7722678916fac838\" alt=\"Free file sync hangs after 100gb of transfer\" title=\"Free file sync hangs after 100gb of transfer\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I was restoring backup from my pc to hdd.</p> <p>Tried multiple times everytime when progress reaches 100gb + it just gets to 0.</p> <p>It was fine when I copied everything from hdd to pc.</p> <p>I quick formatted my hdd but still the issue persist.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Illustrious_Crab_146\"> /u/Illustrious_Crab_146 </a> <br/> <span><a href=\"https://i.redd.it/omkw5ookq45f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l42kfl/free_file_sync_hangs_after_100gb_o",
        "id": 2857775,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l42kfl/free_file_sync_hangs_after_100gb_of_transfer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/omkw5ookq45f1.jpeg?width=640&crop=smart&auto=webp&s=88f872edc54ffacc1f952b1a7722678916fac838",
        "title": "Free file sync hangs after 100gb of transfer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/missiletime",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T16:35:51.127334+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T15:25:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Since &quot;robocopy /MIR&quot; is not a &quot;true&quot; incremental backup, meaning it doesn&#39;t store the new, added data separately, is there any reason to periodically do full backups? From my understanding, you want to do a full backup to get a &quot;fresh start&quot; and also because after many incremental backups, recovery time becomes too long. But &quot;robocopy /MIR&quot; basically does a full backup, it just doesn&#39;t copy the files that already exist in the destination + deletes the ones that aren&#39;t present in the source anymore. From my understanding, this is the same as erasing the backup drive and doing a full backup, but faster.</p> <p>Also, before any of you say to not use robocopy, because it&#39;s not a full-fledged backup tool. I have barely 1TB of data I want to backup just in case something happens. I don&#39;t want any convoluted software with too many features.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=",
        "id": 2857776,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l42196/question_about_a_backup_plan_using_windowss",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about a backup plan using Windows's robocopy",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Keeedo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T16:35:51.277285+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T15:20:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently make tons of content via photography and videography and ive recenetly filled up my 12tb (after raid 1) enclosure. Ive thought about making the jump to NAS but its all new to me.</p> <p>Ideally I&#39;d like to future proof the setup, I dont have a precise budget because I don&#39;t know what each price point gets me. To give you an idea, my first option was the G-Raid Project 2, so I was expecting to pay around $1,500 or so for that.</p> <p>Also worth noting, that would be my only backup, because currently my Backblaze account only backs up my local hard drives, does not support a NAS setup (unless that has changed? How does the protection differ from a NAS vs my Raid 1 with BackBlaze backup?</p> <p>This would be solely for storage long term and file accessibility. When I want to work on something from the past, I can just pull it to my computer and then put it back.</p> <p>Apologies for my ignorance, but im out of my realm here.</p> </div",
        "id": 2857777,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l41wmd/ideal_nas_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ideal NAS Setup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/benibonnano",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T15:29:55.318683+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T15:07:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i accidently corrupted a bitlocked drive, i tried recovery software but all i could recover was files that im assuming was on it before being encrypted.</p> <p>is ther a way i can recover the partition if i only have the bitlock password?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/benibonnano\"> /u/benibonnano </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l41k0c/accidently_corrupted_a_bitlock_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l41k0c/accidently_corrupted_a_bitlock_drive/\">[comments]</a></span>",
        "id": 2857101,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l41k0c/accidently_corrupted_a_bitlock_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "accidently corrupted a bitlock drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cultural-Victory3442",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T14:24:08.016986+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T14:23:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m currently thinking about ways to organize my data for easy retrieval of files (given that there&#39;s many &quot;categories of files&quot;, like, old backup stuff, music, movies, etc). </p> <p>I have been searching for a tag-based system that could make this process easier. Like, I would be able to find the soundtrack of a movie, without having to put them next to each other in a folder.. I primarily use Linux and I&#39;m curious about the feasibility of implementing a tagging system like this.</p> <p>Do you think a tag-based approach is practical, maybe using some app for this, or do you just rely on naming conventions and standard file search/tree structure?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cultural-Victory3442\"> /u/Cultural-Victory3442 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l40h5q/about_ways_to_tag_files_for_easy_retrievalindexing/\">[link]</a></span> &#3",
        "id": 2856363,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l40h5q/about_ways_to_tag_files_for_easy_retrievalindexing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "About ways to tag files for easy retrieval/indexing",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Necessary_Isopod3503",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T14:24:08.168996+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T13:53:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This question is for those who are somewhat familiar with burning optical media and/or computer parts and drives in general.</p> <p>I&#39;ve started using optical a few years ago and have recently bought a blu-ray reader/burner alongside some other optical readers to burn BDRs, CDs and DVDs, all used except for an USB external DVD drive I use sometimes.</p> <p>However none of these have been installed internally on my computer, i use them externally with a SATA to USB adapter and additional energy supply from the outlet. They are internal drives, bulky and with that traditional metallic box around it, but being used externally, so far all results of their use are pretty good. I know the SATA to SATA connecting is ideal but it&#39;s not possible with my current PC case, it has no place to install an internal drive in and the front has no exit for a drive, only fans, it&#39;s a roughly strong PC used in gaming and work as well.</p> <p>However I have not",
        "id": 2856364,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3zrcd/do_rising_temperatures_in_dvdcdbdr_drives_during",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do rising temperatures in DVD/CD/BDR drives during burning potentially damage the drive and it's components long term?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/narutonaruto",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T15:29:55.569813+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T13:39:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m an audio engineer and do some video work. I&#39;ve been using two 6tb thunderbolt gdrives for my &quot;2&quot; backups on the 3-2-1 plan but they&#39;re full. I never delete any client work so it&#39;s just going to keep growing. I have done a lot of reading here and it seems like getting some enclosures and using Sata drives would be more sustainable moving forward.</p> <p>I&#39;d like to keep the whole audio backup together as long as possible before segmenting it to multiple drives since I have a lot of returning clients (so keeping track of who is on what backup could become a pain). Video and all that could be on a different drive and make that dream last for a long while.</p> <p>I just wanted to bounce this off people with more experience before pulling the trigger. Not sure what to look for in an enclosure, I&#39;m thinking sticking with thunderbolt would be nice. I&#39;ve read to seek out enterprise level drives. Any and all thoughts w",
        "id": 2857102,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3zfzc/newbie_backup_guidance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Newbie backup guidance",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/injureT2P",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T14:24:08.642354+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T13:00:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to recover an old YouTube video that was deleted about a year ago. I believe the link is <a href=\"https://youtu.be/1Hn1QCrUm-k?si=D5ATJC6G91o3pFbb\">https://youtu.be/1Hn1QCrUm-k?si=D5ATJC6G91o3pFbb</a> </p> <p>The title is John Lindahl - In the dead of night - 432 Hertz</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/injureT2P\"> /u/injureT2P </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3yl98/recover_old_yt_video/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3yl98/recover_old_yt_video/\">[comments]</a></span>",
        "id": 2856366,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3yl98/recover_old_yt_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recover old yt video?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Appropriate-Luck6466",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T14:24:08.317624+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T12:55:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone here collect workprints and or tv cuts? If so, I&#39;m looking for the tv cut of National Lampoon&#39;s Senior Trip with deleted scenes. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Appropriate-Luck6466\"> /u/Appropriate-Luck6466 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3yhfa/national_lampoons_senior_trip_1995_tv_version/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3yhfa/national_lampoons_senior_trip_1995_tv_version/\">[comments]</a></span>",
        "id": 2856365,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3yhfa/national_lampoons_senior_trip_1995_tv_version",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "National Lampoon's Senior Trip (1995) tv version",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/a4955",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T12:45:07.200950+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T12:33:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m someone who&#39;s looking to build a proper expensive NAS eventually, but that&#39;s at least a year away at the moment. I wanna get some kind of better backup than I have now currently (keeping redundant copies of files I care about on my SSD and HDD in my home PC, and occasionally copying to/from my laptop as well). My workplace was throwing out old PCs, and as I was in charge of securely wiping them (used nwipe), and was allowed to keep them after wiping since they were going to ewaste otherwise (nothing was so important on them that they needed to be destroyed). These drives have been running in a server for 5-6 years, then sat on a shelf for another several years. They have around 50,000 power on hours each, however given I know how this office works I suspect there was proportionally far less reads/writes than the average used office hard drive.</p> <p>Should I bother to set up a quick and dirty NAS backup with them? Given the risk I wou",
        "id": 2855614,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3y0sy/ive_come_across_some_decade_old_4tb_hard_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've come across some decade old 4tb hard drives. Do I bother?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MichaelM_Yaa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T12:45:07.016240+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T12:11:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3xltb/testing_wd_red_plus_4tb_is_this_read_curve_normal/\"> <img src=\"https://preview.redd.it/j14ua5c5o35f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f003267c49425130820c3aa1e92254e2ecee0e78\" alt=\"testing WD RED PLUS 4TB - is this read curve normal? (getting slower as test goes on)\" title=\"testing WD RED PLUS 4TB - is this read curve normal? (getting slower as test goes on)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MichaelM_Yaa\"> /u/MichaelM_Yaa </a> <br/> <span><a href=\"https://i.redd.it/j14ua5c5o35f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3xltb/testing_wd_red_plus_4tb_is_this_read_curve_normal/\">[comments]</a></span> </td></tr></table>",
        "id": 2855613,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3xltb/testing_wd_red_plus_4tb_is_this_read_curve_normal",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/j14ua5c5o35f1.jpeg?width=640&crop=smart&auto=webp&s=f003267c49425130820c3aa1e92254e2ecee0e78",
        "title": "testing WD RED PLUS 4TB - is this read curve normal? (getting slower as test goes on)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kommandantredundant",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T10:31:58.631095+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T10:14:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Yeah you read right, not SanDisk. Got it for free with my AliExpress order.</p> <p>I tested it with h2testw. 3.9GB OK, 1.9 TB lost. Well. So what can I do with it now? is it just going into the bin? I know I shouldn&#39;t rely on it whatsoever, but will this thing actually only take 3.9GB of data or can I put more data onto it, but it will be random wether that data gets corrupted?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kommandantredundant\"> /u/kommandantredundant </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3vjsd/i_got_a_free_2tb_micro_sd_from_sandian/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3vjsd/i_got_a_free_2tb_micro_sd_from_sandian/\">[comments]</a></span>",
        "id": 2854535,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3vjsd/i_got_a_free_2tb_micro_sd_from_sandian",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I got a free 2TB micro SD from SanDian",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/timabell",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T10:31:58.390274+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T09:43:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3v28z/github_luxagenrotkraken_longterm_dataintegrity/\"> <img src=\"https://external-preview.redd.it/6Lhv0wZrOixtmhZpqDvxls3L8SmcPuPToINtf3rXlbg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=90154b7058e8f46c838268ec9bdb4eae201a22e7\" alt=\"GitHub - luxagen/rotkraken: Long-term data-integrity tracker\" title=\"GitHub - luxagen/rotkraken: Long-term data-integrity tracker\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>A friend of mine wrote this to store checksums of data in extended-file-attributes. I think that&#39;s a damn neat idea.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/timabell\"> /u/timabell </a> <br/> <span><a href=\"https://github.com/luxagen/rotkraken\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3v28z/github_luxagenrotkraken_longterm_dataintegrity/\">[comments]</a></span> </td></tr></table>",
        "id": 2854534,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3v28z/github_luxagenrotkraken_longterm_dataintegrity",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/6Lhv0wZrOixtmhZpqDvxls3L8SmcPuPToINtf3rXlbg.jpg?width=640&crop=smart&auto=webp&s=90154b7058e8f46c838268ec9bdb4eae201a22e7",
        "title": "GitHub - luxagen/rotkraken: Long-term data-integrity tracker",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Living_Double_1146",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T10:31:58.780675+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T09:42:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>... deleted data from a cdrw disk that is over 15 yo? I may have lost family photos in it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Living_Double_1146\"> /u/Living_Double_1146 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3v1jf/can_one_restore/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3v1jf/can_one_restore/\">[comments]</a></span>",
        "id": 2854536,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3v1jf/can_one_restore",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can one restore...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BrickAndroid",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T08:17:46.922725+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T07:56:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I download a lot of porn pics frequently of the same women and I need to sort them into separate folders. While some of the pics have these women&#39;s names in the filenames, a lot don&#39;t, because they were download from Reddit or Telegram or other places that don&#39;t give meaningful names. So the only option I see is sorting by faces.</p> <p>My Android phone&#39;s Gallery app has a feature like this, but it does so for ALL the pics on the phone, and not just the folders I want.</p> <p>Is there a program like this for PC?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BrickAndroid\"> /u/BrickAndroid </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3tjel/need_to_group_pics_by_face/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3tjel/need_to_group_pics_by_face/\">[comments]</a></span>",
        "id": 2853518,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3tjel/need_to_group_pics_by_face",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need to group pics by face",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TL_TRIBUNAL",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T08:17:47.103377+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T06:15:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So i just want to buy a 500 gb 2.5 sata ssd, and then i saw videos about dram and how cheap ssds dont have this thing. would a dram less ssd affect like my frames and stuff? i have my os and few competetive titles on my m.2 nvme 1tb, and plan on using the new ssd for story based single player games</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TL_TRIBUNAL\"> /u/TL_TRIBUNAL </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3s0ux/layman_in_data_storage_just_need_an_ssd_but_heard/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3s0ux/layman_in_data_storage_just_need_an_ssd_but_heard/\">[comments]</a></span>",
        "id": 2853519,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3s0ux/layman_in_data_storage_just_need_an_ssd_but_heard",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Layman in Data storage, just need an ssd but heard about dram and dram less",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SpareMe01",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T02:53:57.059502+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T02:37:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. </p> <p>I currently have 2x20TB drives set up as JBOD on my primary PC (windows 11), which only store my Plex data</p> <p>Considering the amount of content I have, I am wary of having no form of back up. I don&#39;t have the means to follow the 3-2-1 rule and feel comfortable enough with a single offline backup.</p> <p>My leading thought was to by two more 20TB drives and put them in <a href=\"https://www.amazon.com/TERRAMASTER-D2-320-USB-RAID-Enclosure/dp/B0C8GCZP5K?dib=eyJ2IjoiMSJ9.3joUYmJcBW74Z94DJMrlNYww7E1xNdYu6mw05LVTpwqqx_-X8OiI-mttOM-9RUKRkUjuvAzQu5VhkhZGDkv313CVt19Xsgffs_qvuGxti5y0PkQEOVvI1pT9l6sbZ_bIWCZESzp4FBx-hMog-ZJGgGCyeZdjyyR0gX633Ufn5SMSBYMLQZgwJH_xXrhvArdiK3efHpG1Eg-PmuR3M4pTXM5VASPgJNaB_sgl4vq4h38.blGFRq8-jOJJUg4qiK_Psam2hq0bk0tx5oCVkNmB3HI&amp;dib_tag=se&amp;keywords=2%2Bbay%2Braid%2Benclosure&amp;qid=1748312643&amp;sr=8-4&amp;th=1\">Terramaster D2-320</a> enclosure, and periodically backup the drives on my main PC. Coupl",
        "id": 2852457,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3o9e5/best_method_to_have_single_backup_of_40tb_of_plex",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best method to have single back-up of 40TB of Plex Data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ScaryPineapple5815",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T03:58:53.788628+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T01:56:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Of course I know archive.ph and the other &quot;archive&quot; sites, that removes paywalls just fine. But it does not work for gallery articles with multiple pages. It just saves the first page. </p> <p>Take this<br/> <a href=\"https://ga.de/fotos/bonn/fedcon-2025-in-bonn-bilder%5C_bid-128461233\">https://ga.de/fotos/bonn/fedcon-2025-in-bonn-bilder\\_bid-128461233</a></p> <p>This it the outcome<br/> <a href=\"https://archive.ph/3FQ9V\">https://archive.ph/3FQ9V</a></p> <p>And since every picture has a different random url I can&#39;t even use the direct link to the first picture and change it to see the other pictures. </p> <p>Any better sites? Seems like many news sites have changed their galleries in that way. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ScaryPineapple5815\"> /u/ScaryPineapple5815 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3ngdy/paywall_remover_for_gallery_with_multi",
        "id": 2852689,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3ngdy/paywall_remover_for_gallery_with_multiple_pages",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Paywall Remover for Gallery with multiple pages",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Broad_Sheepherder593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T00:45:52.834253+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T00:30:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, </p> <p>Been reading up on ideal drive temp and would like to check what&#39;s the best setting -</p> <p>My room ambient is 32 deg C in which under normal fan mode, drive temp is 45 deg. If i do set the fan to max, can get it down to 42 deg. </p> <p>No issues with the noise as nobody is in the room so I&#39;m thinking to just max it out permanently? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Broad_Sheepherder593\"> /u/Broad_Sheepherder593 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3lpza/drive_temp/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l3lpza/drive_temp/\">[comments]</a></span>",
        "id": 2851930,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l3lpza/drive_temp",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Drive temp",
        "vote": 0
    }
]