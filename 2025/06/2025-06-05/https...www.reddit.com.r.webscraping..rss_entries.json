[
    {
        "age": null,
        "album": "",
        "author": "/u/Magic-Wasabi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T18:14:51.443656+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T17:46:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, does anyone have an up to date db/scraping program about tennis stats? </p> <p>I used to work with the @JeffSackmann files from github but he doesnt update them oftenly\u2026</p> <p>Thanks in advance :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Magic-Wasabi\"> /u/Magic-Wasabi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l45m5x/tennis_data_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l45m5x/tennis_data_webscraping/\">[comments]</a></span>",
        "id": 2858764,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l45m5x/tennis_data_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tennis data webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BoiWonder95A",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T17:09:54.037658+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T16:45:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Yes we all know how annoying it is to scrape bet365. I was wondering if anyone has successfully scraped/reversed engineered their api. Willing to pay for the solution. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BoiWonder95A\"> /u/BoiWonder95A </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l44221/scraping_bet365_api/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l44221/scraping_bet365_api/\">[comments]</a></span>",
        "id": 2858086,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l44221/scraping_bet365_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "scraping bet365 api",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Embarrassed-Crazy-85",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T18:14:51.591933+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T16:31:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><pre><code>from botasaurus.browser import browser, Driver @browser(reuse_driver=True, block_images_and_css=True,) def scrape_details_url(driver: Driver, data): driver.google_get(data, bypass_cloudflare=True) driver.wait_for_element(&#39;a&#39;) links = driver.get_all_links(&#39;.btn-block&#39;) print(links) scrape_details_url(&#39;link&#39;) </code></pre> <p>Hello guys i&#39;m new at web scrapping and i need help i made a script that bypass cloudflare using botasaurus library here is example for me code but after the cloudflare is bypassed<br/> i got this error botasaurus_driver.exceptions.DetachedElementException: Element has been removed and currently not connected to DOM.<br/> but the page loads and the DOM is visible to me in the browser what can i do ? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Embarrassed-Crazy-85\"> /u/Embarrassed-Crazy-85 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/co",
        "id": 2858765,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l43pcp/detachedelementexception_error",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DetachedElementException ERROR",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DemonforgedTheStory",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T15:00:33.207248+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T14:53:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This app was really useful to me as it was completely local (a chrome extension, with no server) and perfect for low-intensity scraping. However, the creator is no longer selling licenses. </p> <p>Any alternatives?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DemonforgedTheStory\"> /u/DemonforgedTheStory </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l4176n/alternatives_for_tryspidercom/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l4176n/alternatives_for_tryspidercom/\">[comments]</a></span>",
        "id": 2856845,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l4176n/alternatives_for_tryspidercom",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Alternatives for : tryspider.com",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Warm-Line-87",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T09:50:26.955731+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T09:10:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What does google want??</p> <p>Browser-level detection:</p> <ul> <li><strong>(1)</strong> Realistic, Long-lived Browser Fingerprint that stays around??? For example, does google want me to give it a convincing Fingerprint, and then I&#39;m free to make requests using that, as long as I exhibit realistic behavior?</li> <li><strong>(2)</strong> Realistic, Short-lived Browser Fingerpint???? For instance, can I just create a per-request on-the-fly Fingerprint, and then continue to</li> </ul> <p>Server-level detection:</p> <ul> <li><strong>(A)</strong> Rarely Rotating IP&#39;s from different cloud providers + the best option from (1) or (2)?</li> <li><strong>(B)</strong> Constantly Rotating IP&#39;s from different cloud providers + the best option from (1) or (2)?</li> </ul> <p>So what is it?</p> <ul> <li><strong>1 + A</strong> - <strong>Longer</strong>-living Browser Fingerprint, + <strong>Longer</strong>-Living IP request origin?</li> <li><strong>1 + B</",
        "id": 2854362,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l3ul3h/what_exactly_does_google_want",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What, exactly, does Google Want????",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tuduun",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-05T03:15:31.935577+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-05T02:36:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, what is a great library or a tool that identifies fake forms and honeypot forms made for bots? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tuduun\"> /u/tuduun </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l3o8ig/honeypot_formsfake_forms_for_bots/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l3o8ig/honeypot_formsfake_forms_for_bots/\">[comments]</a></span>",
        "id": 2852567,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l3o8ig/honeypot_formsfake_forms_for_bots",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Honeypot forms/Fake forms for bots",
        "vote": 0
    }
]