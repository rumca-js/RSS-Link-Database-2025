[
    {
        "age": null,
        "album": "",
        "author": "/u/Legacy_Media",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T23:57:25.880838+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T23:55:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m wondering if anyone knows of an updated source for old MTV/TRL? Stash of old VHS recordings of MTV maybe? Looking for something specific from 2008, but also just wondering in general how to find vintage MTV. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Legacy_Media\"> /u/Legacy_Media </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2rc9b/looking_for_a_source_for_mtv_trl_episodes/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2rc9b/looking_for_a_source_for_mtv_trl_episodes/\">[comments]</a></span>",
        "id": 2842563,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2rc9b/looking_for_a_source_for_mtv_trl_episodes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a source for MTV TRL episodes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheSkeeper",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T23:57:26.033360+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T23:16:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I hope this kind of question fits the subreddit \u2014 if not, feel free to redirect me.</p> <p>I\u2019m working on a project that involves sharing a high-resolution image (specifically a map) in a Reddit post. This image may receive updates over time (fixes, improvements, etc.), so I need a way to replace or update it without creating a new post every time.</p> <p>Here\u2019s what I\u2019m looking for: \u2022 A platform that allows me to upload and possibly update a high-resolution image (ideally keeping the same link, or at least making it easy to update). \u2022 I\u2019m fine with registering on the platform myself. \u2022 The important part: I want people to be able to view and download the image without logging in or being tracked in any way. \u2022 Likewise, I don\u2019t want viewers to see anything about me \u2014 no account name, no identifying info. \u2022 Basically, anonymous in both directions: I upload the image, others view or download it, and neither of us knows anything about the ot",
        "id": 2842564,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2qia9/looking_for_a_privacyrespecting_way_to_share_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a privacy-respecting way to share and update a high-res image publicly",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Major-Masterpiece-10",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T23:57:26.181215+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T23:07:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, with emulation of iPhoneOS apps slowly on the rise and me trying to remember 32bit era apps, is there a place that kept all App Store data? not looking for the .ipa&#39;s, but for the data like title, icon, description, and most importantly, but likely the hardest part, screenshots.</p> <p>For example, I&#39;m looking for an app that I used to love, it was a clock/calendar and &quot;weather?&quot;, but it was an awesome simulated flip clock wall, even each day of the month was a little flipper, and when you passed your finger over them those would flip and a second after they would cycle to the correct &quot;flip state&quot; again. It may have been Rockifone&#39;s Flipclock HD, but I can&#39;t be sure, as there&#39;s no pictures. I managed to find an ipa for this specific app, and the background PNG in the ipa does look similar but on the other hand appears not to be it either. but this second paragraph is actually beside the point.</p> </div><!--",
        "id": 2842565,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2qakj/list_of_all_iosiphoneos_apps_ever_published_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "List of all iOS/iPhoneOS apps ever published on App Store?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/worldlybedouin",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T22:52:28.144062+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T22:05:11+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/worldlybedouin\"> /u/worldlybedouin </a> <br/> <span><a href=\"https://old.reddit.com/r/homelab/comments/1l2ouy6/backups_are_your_friend/?\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2ovi4/backups_are_your_friend/\">[comments]</a></span>",
        "id": 2842218,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2ovi4/backups_are_your_friend",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backups Are Your Friend",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Azadi_23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T22:52:28.296045+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T21:56:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Any advice welcome! </p> <ol> <li><p>I would like to get all my old files onto one external storage solution from several old hard drives - what\u2019s a good brand/make/model for around 2TB - 4TB? Which ones to avoid? I bought cheap large USBs that worked briefly and then became corrupted so I don\u2019t want to make the same mistake twice! </p></li> <li><p>My newer laptop has a faster processor and can move files very efficiently but cannot read/write from my old external hard drives. My old laptop can access the old drive but is very slow and may crash if I try to put too much on it to transfer from old external HD to new external HD. Any tips? </p></li> <li><p>How can I be sure old storage drives are empty of my data? Once I have transferred everything I will delete all files and would be happy to recycle parts if possible. Is there a recommended safety method to be sure my old files are unrecoverable? They\u2019re mostly photos, videos, songs and work/uni text ",
        "id": 2842219,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2oo64/transfer_and_backup_from_older_to_newer_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Transfer and backup from older to newer storage solutions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Endawmyke",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T21:47:08.987019+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T21:37:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have some of these large thermal paper photos from Chuck E. Cheese\u2019s from like 20+ years ago that I\u2019m wanting to scan.</p> <p>But I have a bad memory from childhood when I tried to scan a NASCAR ticket as a kid and it totally ruined the ticket. I\u2019m guessing the heat of the scanner light was enough to black out the whole thing.</p> <p>And seeing as the Chuck E. Cheese photos are also thermal paper I\u2019m worried running it through the scanner will black it out in the same way.</p> <p>Any advice? </p> <p>I\u2019m using an Epson FastFoto FF-680W btw, and it\u2019s advertised to work with receipts (which I believe are also thermal paper?) but I just wanna make sure with anyone here experienced so I don\u2019t accidentally kill these photos.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Endawmyke\"> /u/Endawmyke </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2o7a6/whats_the_best_way_to_scan_photos_from_the",
        "id": 2841778,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2o7a6/whats_the_best_way_to_scan_photos_from_thermal",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s the best way to scan photos from thermal paper so that they don\u2019t get ruined? Specifically photos from Chuck E. Cheese\u2019s.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CoffeeTrashed",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T21:47:09.497602+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T21:17:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Folks! </p> <p>Hopefully I&#39;m posting this in the right sub, apologies if not. Basically, I currently have a very very low tech Plex server running in my apartment (Dell 3240 Compact running Debian with 12TB of external dumb storage) and would like to expand this to be a little more all encompassing. </p> <p>I&#39;d like to have a database setup that contains my Plex Server stuff (How hard would it be to swap to Jellyfin?), all of my books, music, and a bunch of informational YouTube videos that I&#39;ve downloaded (example: <a href=\"https://www.youtube.com/watch?v=Et5PPMYuOc8\">https://www.youtube.com/watch?v=Et5PPMYuOc8</a>). My goal is to have it setup so that all of these things are accessible via any device on my local network, even if my internet is down. </p> <p>Optionally, I&#39;m also interested in a front end that maybe brings a lot of this together and makes it searchable and looking nicer? I know Plex can technically handle the music ",
        "id": 2841781,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2now1/selfhosting_a_database_for_entertainment_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Self-Hosting a Database for Entertainment and Information",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Vaemorn",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T21:47:09.168750+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T21:14:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2nmdc/i_would_like_to_scandigitize_some_old_hi88_tapes/\"> <img src=\"https://b.thumbs.redditmedia.com/piofCE54QMCpyvFIYw5FJgDID16zALTeJ5Zg6jOGBhE.jpg\" alt=\"I would like to scan/digitize some old hi8(8) tapes onto my pc. How would o go about this\" title=\"I would like to scan/digitize some old hi8(8) tapes onto my pc. How would o go about this\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I found what I believe to be hi8 tapes and would like to scan and digitize some of them, I have found 2 camcorders that will play the tapes back.</p> <p>I bought a FireWire/ DV in/out to usb cable </p> <p>And I downloaded obs </p> <p>What am I missing?</p> <p>I\u2019ve found plenty of help online but I\u2019m not sure if I have the right stuff or I\u2019m doing something wrong ect</p> <p>Any help would be greatly appreciated </p> <p>I\u2019ve attached photos of what I have </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www",
        "id": 2841779,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2nmdc/i_would_like_to_scandigitize_some_old_hi88_tapes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/piofCE54QMCpyvFIYw5FJgDID16zALTeJ5Zg6jOGBhE.jpg",
        "title": "I would like to scan/digitize some old hi8(8) tapes onto my pc. How would o go about this",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ConfusedHomelabber",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T21:47:09.319091+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T20:58:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just wrapped up setting up my NAS. Had to work with a mix of different sized drives, so each one ended up being its own share. Not ideal, but it works for now.</p> <p>I was planning on doing the usual layout\u2014Documents, Photos, Music, etc.\u2014but after seeing a few screenshots floating around here, I realized there\u2019s a lot of different approaches people take to organizing their data.</p> <p>So now I\u2019m curious: what does your file structure look like? How do you handle multiple shares or drives with different capacities? Would love to hear what works for you and why</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ConfusedHomelabber\"> /u/ConfusedHomelabber </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2n7o7/new_nas_setup_with_mixed_drive_sizes_curious_how/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2n7o7/new_nas_setup_with_mixed_drive_sizes_curiou",
        "id": 2841780,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2n7o7/new_nas_setup_with_mixed_drive_sizes_curious_how",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New NAS Setup with Mixed Drive Sizes \u2013 Curious How You All Structure Your Folders",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BlincxYT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T20:42:09.329626+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T20:19:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, the palera1n discord sevrer (ios jailbreak tool: <a href=\"https://palera.in\">https://palera.in</a> ) was just deleted. this sucks cuz it was a very good resource not only for info regarding the jailbreak but also other technical information like jailbreak development etc. </p> <p>did anyone of you possibly back up the discord server while it was still in the archived state? i should have done that and i literally had a tool for that open yesterday but i didnt back it up :(</p> <p>idk, would be epic if someone does have it :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BlincxYT\"> /u/BlincxYT </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2m82u/did_anyone_back_up_the_palera1n_discord_server/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2m82u/did_anyone_back_up_the_palera1n_discord_server/\">[comments]</a></span>",
        "id": 2841295,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2m82u/did_anyone_back_up_the_palera1n_discord_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Did anyone back up the palera1n discord server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Harry_Yudiputa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T20:42:09.002894+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T19:50:59+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2lh3s/unloading_33k_photos_and_videos_from_amazon/\"> <img src=\"https://preview.redd.it/wesj5jwynr4f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd1dcb1e55482a7f46896f0f4cf68fd367af399a\" alt=\"Unloading 33K photos and videos from Amazon photos is actually insane. Hopefully my CPU is ready for this tonight\" title=\"Unloading 33K photos and videos from Amazon photos is actually insane. Hopefully my CPU is ready for this tonight\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Harry_Yudiputa\"> /u/Harry_Yudiputa </a> <br/> <span><a href=\"https://i.redd.it/wesj5jwynr4f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2lh3s/unloading_33k_photos_and_videos_from_amazon/\">[comments]</a></span> </td></tr></table>",
        "id": 2841294,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2lh3s/unloading_33k_photos_and_videos_from_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/wesj5jwynr4f1.png?width=320&crop=smart&auto=webp&s=fd1dcb1e55482a7f46896f0f4cf68fd367af399a",
        "title": "Unloading 33K photos and videos from Amazon photos is actually insane. Hopefully my CPU is ready for this tonight",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SummorumPontificum90",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T18:33:09.408255+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T17:32:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What do you think about exchanging disk space with a friend or a complete stranger as an offsite backup? Is this a thing?? Why or why not??</p> <p>Obviously this backup should be encrypted. It would not be hard to find someone who is interested in such thing in a community like this one.</p> <p>Let\u2019s make an hypotetic example: I let you store a 4 TB encrypted backup in my NAS and you let me do the same thing (and same disk space) on your NAS.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SummorumPontificum90\"> /u/SummorumPontificum90 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2i1mp/offsite_backup_exchange_with_a_stranger/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2i1mp/offsite_backup_exchange_with_a_stranger/\">[comments]</a></span>",
        "id": 2840312,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2i1mp/offsite_backup_exchange_with_a_stranger",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Offsite backup exchange with a stranger",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SureElk6",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T17:26:11.189431+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T16:37:40+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2gmil/hard_disk_drive_failure_analysis_and_prediction/\"> <img src=\"https://external-preview.redd.it/43EPxESD8DaVUa7dj8VUnF_xs6ABKjQseeN7kecAass.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3dce2f1911b837c6d414b3aeb8efb7783aa4df6c\" alt=\"Hard Disk Drive Failure Analysis and Prediction: An Industry View (2023)\" title=\"Hard Disk Drive Failure Analysis and Prediction: An Industry View (2023)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SureElk6\"> /u/SureElk6 </a> <br/> <span><a href=\"https://research.facebook.com/publications/hard-disk-drive-failure-analysis-and-prediction-an-industry-view/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2gmil/hard_disk_drive_failure_analysis_and_prediction/\">[comments]</a></span> </td></tr></table>",
        "id": 2839674,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2gmil/hard_disk_drive_failure_analysis_and_prediction",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/43EPxESD8DaVUa7dj8VUnF_xs6ABKjQseeN7kecAass.jpg?width=640&crop=smart&auto=webp&s=3dce2f1911b837c6d414b3aeb8efb7783aa4df6c",
        "title": "Hard Disk Drive Failure Analysis and Prediction: An Industry View (2023)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zombik327",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T17:26:10.732870+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T16:34:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2gjwp/is_this_normal_packaging/\"> <img src=\"https://preview.redd.it/qg0raoianq4f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=81d63aacde281e40b1d3cb31e4b68dadee8e0d04\" alt=\"Is this normal packaging ?!\" title=\"Is this normal packaging ?!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Few days ago I ordered WD Red Plus 8TB and usb cable and it came packaged like this. No bubble wrap whatsoever, even box is stupidly oversized... Stupid question but should I refund it?! box also had a crease on one side... I think our &quot;local&quot; seller might be playing UPS Ace Ventura... \ud83d\ude12</p> <p>While I&#39;m here, can you recommend me some 8tb hdd that will be ocasionally used ? I want to connect it to pc, fill it up and then &quot;put on a shelf&quot; for unknown amount of time. Don&#39;t care about noise or speed.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zombik327\">",
        "id": 2839672,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2gjwp/is_this_normal_packaging",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/qg0raoianq4f1.png?width=640&crop=smart&auto=webp&s=81d63aacde281e40b1d3cb31e4b68dadee8e0d04",
        "title": "Is this normal packaging ?!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lysander_Au_Lune",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T17:26:10.972189+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T16:19:49+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2g670/whats_the_oldest_25_external_hdd_you_own_with/\"> <img src=\"https://b.thumbs.redditmedia.com/Tb9OavGIfjnNS2bhiDAK5pTqmbMDerZA1Mz27xZD-KA.jpg\" alt=\"What's the oldest 2.5&quot; external HDD you own with zero bad or reallocated sectors?\" title=\"What's the oldest 2.5&quot; external HDD you own with zero bad or reallocated sectors?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Mine is this lil guy from 2010. Served as my OS drive backup for 8 years. Recently rediscovered and still kicking.<br/> It has USB 2.0 micro-B port and is slow af - 35MB/s.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lysander_Au_Lune\"> /u/Lysander_Au_Lune </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1l2g670\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2g670/whats_the_oldest_25_external_hdd_you_own_with/\">[comments]</a></span> </td></tr>",
        "id": 2839673,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2g670/whats_the_oldest_25_external_hdd_you_own_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/Tb9OavGIfjnNS2bhiDAK5pTqmbMDerZA1Mz27xZD-KA.jpg",
        "title": "What's the oldest 2.5\" external HDD you own with zero bad or reallocated sectors?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Exotic_Imagination10",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T22:52:28.532239+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T15:59:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Until recently, tools like `rclone` and `MultCloud` were able to access Google Photos albums using the `photoslibrary.readonly` and `photoslibrary.sharing` scopes.</p> <p>Due to recent Google API changes, these scopes are now deprecated and only available to apps that passed a strict validation process \u2014 which makes it nearly impossible for open-source tools or personal scripts to access your own photos and albums.</p> <p>This effectively breaks any form of automated backup from Google Photos.</p> <p>We&#39;ve just submitted a proposal to Google asking for a new read-only backup scope, something like:</p> <p>`<a href=\"https://www.googleapis.com/auth/photoslibrary.readonly.backup%5C%60\">https://www.googleapis.com/auth/photoslibrary.readonly.backup\\`</a></p> <p>\u2705 Read-only </p> <p>\u2705 No uploads or sharing </p> <p>\u2705 For archival and backup tools only</p> <p>\ud83d\udcec You can support the request by starring or commenting here: </p> <p><a href=\"https://issuetracker",
        "id": 2842220,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2fnnj/google_photos_api_blocks_rclone_access_to_albums",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google Photos API blocks rclone access to albums \u2014 help us ask Google for a read-only backup scope",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VishwjeetChavan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T16:17:20.873168+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T15:30:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I used to take pictures and videos casually, and now I have so many that my phone is barely functioning. Recently, I found a trick where I can upload photos and short videos (under 10 seconds) to Snapchat and use it like cloud storage. The only downside is that videos longer than 10 seconds can&#39;t be uploaded this way.</p> <p>I also use an external hard drive to back up my data, but I&#39;m still worried about it getting corrupted and losing everything.</p> <p>My main question is: Can Snapchat ban me for using it this way? I know millions of people do it, but I&#39;m still nervous.</p> <p>Also, what are some other good ways to store my pictures and videos safely?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VishwjeetChavan\"> /u/VishwjeetChavan </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2ewme/how_reliable_is_snapchat_as_a_cloud_storage/\">[link]</a></span> &#32; <span><a href=\"h",
        "id": 2839002,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2ewme/how_reliable_is_snapchat_as_a_cloud_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How reliable is Snapchat as a cloud storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RoachedCoach",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T16:17:20.099773+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T15:22:15+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2ep6o/petabyte_ssds_for_servers_being_developed_in/\"> <img src=\"https://external-preview.redd.it/OYfsczp1y8Jeg3wyEiyNI77bximXhrk2YmmmLzWnkb4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c0d11135042b85cbcb3170f933b05b13b737a9f\" alt=\"Petabyte SSDs for servers being developed (in German)\" title=\"Petabyte SSDs for servers being developed (in German)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RoachedCoach\"> /u/RoachedCoach </a> <br/> <span><a href=\"https://www.heise.de/news/Neue-Bauweise-soll-Petabyte-SSDs-ermoeglichen-10424097.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2ep6o/petabyte_ssds_for_servers_being_developed_in/\">[comments]</a></span> </td></tr></table>",
        "id": 2839001,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2ep6o/petabyte_ssds_for_servers_being_developed_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/OYfsczp1y8Jeg3wyEiyNI77bximXhrk2YmmmLzWnkb4.jpg?width=640&crop=smart&auto=webp&s=3c0d11135042b85cbcb3170f933b05b13b737a9f",
        "title": "Petabyte SSDs for servers being developed (in German)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DiskBytes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T15:12:05.257096+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T14:48:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve got an issue with mbuffer which has never happened to me before. Basically, the data out is going to tape quicker than it can go in, causing the tape to stop, wait for the buffer to fill, then start again. </p> <p>But mbuffer is supposed to prevent this from happening, very strange as it has always worked well prior to today and I can&#39;t see what I&#39;m doing differently. </p> <p>As I always have, I&#39;m using tar -b 2048 --directory&quot;name&quot; -cvf - ./ | mbuffer -m 6G -L -P 80 -f -o /dev/st0</p> <p>Any ideas? Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DiskBytes\"> /u/DiskBytes </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2duvk/strange_mbuffer_issue/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2duvk/strange_mbuffer_issue/\">[comments]</a></span>",
        "id": 2838313,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2duvk/strange_mbuffer_issue",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Strange mbuffer issue",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vff",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T15:12:05.048183+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T14:18:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>You may be familiar with the book <a href=\"https://en.wikipedia.org/wiki/A_Million_Random_Digits_with_100,000_Normal_Deviates\">A Million Random Digits with 100,000 Normal Deviates</a> from the RAND corporation that was used throughout the 20th century as essentially <em>the</em> canonical source of random numbers.</p> <p>I\u2019m working towards putting together a similar collection, not of one million random decimal digits, but of at least one quadrillion random binary digits (so 128 terabytes). Truly random numbers, not pseudorandom ones. As an example, one source I\u2019ve been using is video noise from a USB webcam in a black box, with every two bits fed into a <a href=\"https://en.wikipedia.org/wiki/Randomness_extractor#Von_Neumann_extractor\">Von Neumann extractor</a>.</p> <p>I want to save everything because randomness is by its very nature ephemeral. By storing randomness, this gives permanence to ephemerality.</p> <p>What I\u2019m wondering is how people sort",
        "id": 2838312,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2d4aw/archiving_random_numbers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Archiving random numbers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MarinatedPickachu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T14:02:44.799989+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T13:34:23+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l2c1lv/why_does_my_new_sandisk_portable_ssd_is_default/\"> <img src=\"https://b.thumbs.redditmedia.com/aSlORUoZnf-l1iJbGNDuCuJUlVrCBO706ZZIeMMStiM.jpg\" alt=\"Why does my new Sandisk Portable SSD is default formatted with a cluster size of 1MB?\" title=\"Why does my new Sandisk Portable SSD is default formatted with a cluster size of 1MB?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I got a 2TB external SSD (SanDisk Portable SSD) and was quite surprised when the 12GB of data I copied onto it consumed 103GB of disk space. Turns out the disk is formated with a cluster size of 1MB and my data consists of lots of small files. Why such a big cluster size? Are there good reasons not to reformat the drive with a smaller cluster size (windows offers me a minimum possible cluster size of 128kb)? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MarinatedPickachu\"> /u/MarinatedPickac",
        "id": 2837616,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2c1lv/why_does_my_new_sandisk_portable_ssd_is_default",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/aSlORUoZnf-l1iJbGNDuCuJUlVrCBO706ZZIeMMStiM.jpg",
        "title": "Why does my new Sandisk Portable SSD is default formatted with a cluster size of 1MB?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pratyathedon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T14:02:45.009766+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T13:16:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m in the market for two large-capacity internal drives (16TB\u201320TB) to use in my home server/Unraid setup.<br/> I\u2019ve been digging through specs and price lists, but I wanted to get some community input before pulling the trigger.</p> <p><strong>The thing is I am not from the US, but will be visiting PA in July, I would like to place an order in the next 2 weeks. SPD seems to be the go-to place where y&#39;all buy HDDs with fewer issues.</strong> </p> <p>May main use case is for storing media and use that for jellyfin, I found several recertified Seagate on SPD that are within my budget. Can someone help me with what drives are the safest bet cause i wont be able to test it till i get back to my home.<a href=\"https://serverpartdeals.com/collections/manufacturer-recertified-drives/products/western-digital-ultrastar-dc-hc530-wuh721414al5201-14tb-7-2k-rpm-sas-12gb-s-512e-3-5-recertified-hard-drive\"></a></p> <p><a href=\"https://serverpartdeals.com/collect",
        "id": 2837617,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2bn01/expanding_my_nas_with_more_tbs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Expanding my NAS with more TBs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/der_pudel",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T12:57:13.380309+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T12:30:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After nearly losing a significant portion of my personal data in a PC upgrade that went wrong (gladly recovered everything), I finally decided to implement proper-ish 3-2-1 strategy backups. </p> <p>My goal is to have an inexpensive (in the sense that I&#39;d like to pay for what I&#39;m actually going to use), maintainable and upgradeable setup. The data I&#39;m going to back up is are mostly photos, videos and other heavy media content with nostalgic value, and personal projects that are not easy to manage in git (hobby CAD projects, proto/video editing, etc.).</p> <p>Setup I came up with so far:</p> <ul> <li>1. On PC side, backups are handled by Duplicati. Not sure how stable/reliable it is long term, but my first impression from it is very positive.</li> <li>2. Backups are pushed to SFTP server hosted by Raspberry Pi with <a href=\"https://radxa.com/products/accessories/penta-sata-hat/\">Radxa SATA Hat</a> and 4x1TB SSD in RAID5 configuration (mdadm",
        "id": 2837079,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2an6l/roast_my_diy_backup_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Roast my DIY backup setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Crazy6440",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T11:50:42.905388+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T11:23:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey DataHoarders! Bit of an oddball situation: My uncle\u2019s old iPhone is stuck behind the iCloud Activation Lock, and we can\u2019t get in (email\u2019s long gone, and no luck with password recovery). We\u2019re not trying to bypass the lock to use the phone just want to see if there\u2019s any chance of pulling photos or voicemails off it.</p> <p>Most recovery software I\u2019ve seen just quits entirely when it hits an Activation Lock, but I\u2019m curious if anyone here has tried using Gbyte Recovery (or anything similar) in this situation? Does Gbyte actually try to dig into the locked data, or is that just marketing talk?</p> <p>I know it\u2019s a long shot, but figured if anyone knows how to get data off an Activation Locked iPhone, it\u2019s someone in here. Appreciate any thoughts or real-world results!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Crazy6440\"> /u/Ok_Crazy6440 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/com",
        "id": 2836609,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l29cvo/can_gbyte_recover_photos_from_an_icloudlocked",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can Gbyte recover photos from an iCloud-locked iPhone? Uncle\u2019s old phone dilemma",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Illustrious_Crab_146",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T11:50:42.696552+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T10:47:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l28pwr/size_while_copying_is_different_by_appx_152_gb/\"> <img src=\"https://b.thumbs.redditmedia.com/QTRdcTwvIOY2U3mQJSkgZlSWiJs8bbxsETk2EFtAJlc.jpg\" alt=\"size while copying is different by appx 152 gb\" title=\"size while copying is different by appx 152 gb\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Windows explorer is telling me the size of files is 360 gb in total on my hard drive win dir stat is tell the same thing.</p> <p>But when copying all of the selected folders to windows the remaining size says 512 Gb. Since my SSD on laptop is 395 gb free i doubt it will fit.</p> <p>What is the issue here? Do I have to backup the files on different laptops due to this which is a hassle.</p> <p>i am thinking of using this hdd to permanently connected to my router via usb for extra space since it&#39;s collecting dust with the unlicensed games and movies it has on it</p> </div><!-- SC_ON --> &#32; submitted by &#",
        "id": 2836608,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l28pwr/size_while_copying_is_different_by_appx_152_gb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/QTRdcTwvIOY2U3mQJSkgZlSWiJs8bbxsETk2EFtAJlc.jpg",
        "title": "size while copying is different by appx 152 gb",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/umataro",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T10:46:39.203475+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T09:47:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I usually delete my files from USB flash drives, SD cards and hard disks with <code>shred -n 1 -u *</code> if they can&#39;t be encrypted but this adds too much wear to flimsy media like SD cards. I would like to be able to just corrupt important headers and insert random data at reasonable intervals to simply make the files unusable before they get <code>unlink</code>-ed. Is there such a thing?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/umataro\"> /u/umataro </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l27rqo/is_there_a_utility_that_corrupts_media_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l27rqo/is_there_a_utility_that_corrupts_media_files/\">[comments]</a></span>",
        "id": 2836084,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l27rqo/is_there_a_utility_that_corrupts_media_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a utility that corrupts media files (pictures+videos) until they're unusable?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tzfld",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T08:36:42.881893+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T07:47:54+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tzfld\"> /u/tzfld </a> <br/> <span><a href=\"https://www.monperrus.net/martin/store-data-paper\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l261qi/how_to_store_data_on_paper/\">[comments]</a></span>",
        "id": 2835277,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l261qi/how_to_store_data_on_paper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to store data on paper?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GuyWhoDiesIn2025_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T22:52:28.856636+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T07:40:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i want to upload a video onto internet achieve automatically. I won&#39;t be able to do it myself because it&#39;s going to be my death video (I have cancer &amp; I qualify for the <em>California End of Life Option Act</em> &amp; I&#39;ll be legally taking pills prescribed to me to end my life <strong>BUT I WANT TO DO IT ON VIDEO &amp; I WANT IT TO STAY ONLINE FOREVER!</strong>) </p> <p>Is there a way I could automatically have it uploaded onto internet archive? I&#39;m going to be recording it with OBS &amp; I already figured out how to get OBS to automatically stop recording after a certain amount of time, but im trying to figure out how to automatically have it uploaded onto internet archive. </p> <p>Can someone please tell me how to do that? Does anyone know of a tutorial or something? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GuyWhoDiesIn2025_\"> /u/GuyWhoDiesIn2025_ </a> <br/> <span><a href=\"https://w",
        "id": 2842221,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l25xle/is_there_a_way_i_can_automatically_have_a_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a way I can automatically have a video uploaded onto internet achieve by having it automatically scan a folder on my computer (I'm only uploading 1 video)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/onelonedatum",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T06:25:39.319610+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T06:08:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Here are a few prompt-driven assistants to generate fully verified <code>yt-dlp</code> commands I recently created.</p> <p>Paste your video/audio URL, answer a few quick prompts (video vs audio, MP4 vs MKV, subs external or embedded, custom output path), and get back a copy-paste CLI snippet validated against the latest <code>yt-dlp</code> docs (FFmpeg required for embedding metadata/subs).</p> <p>Try them here: - <a href=\"https://chatgpt.com/g/g-683e80e555b88191a5d7626f7dced9ff-media-cli-cmd-generator\">ChatGPT Custom GPT (Media \ud835\ude72\ud835\ude7b\ud835\ude78 \ud835\ude8c\ud835\ude96\ud835\ude8d \ud835\udda6\ud835\uddbe\ud835\uddc7\ud835\uddbe\ud835\uddcb\ud835\uddba\ud835\uddcd\ud835\uddc8\ud835\uddcb \ud83c\udfac \u2b07\ufe0f)</a><br/> - <a href=\"https://g.co/gemini/share/f9b448a9a1e2\">Gemini Custom Gem (Media \ud835\ude72\ud835\ude7b\ud835\ude78 \ud835\ude8c\ud835\ude96\ud835\ude8d \ud835\udda6\ud835\uddbe\ud835\uddc7\ud835\uddbe\ud835\uddcb\ud835\uddba\ud835\uddcd\ud835\uddc8\ud835\uddcb \ud83c\udfac \u2b07\ufe0f)</a></p> <hr/> <p>happy to make tweaks as needed, share the underlying prompts, and/or help w/ usage -- just let me know! \ud83e\udd16 \ud83d\ude80</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/onelonedatum\"> /u/onelonedatum </a> <br/> <span><a href=\"https://www.reddit.com/r/Dat",
        "id": 2834691,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l24kkx/ai_chatbot_assistants_for_easy_ytdlp_command",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI chatbot assistants for easy `yt-dlp` command generation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bobwin770",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T06:25:39.471200+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T05:53:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 15 years worth of photos, roughly 10TB of RAW photos. I\u2019m thinking of uploading all RAWS to Amazon Photos as they offer unlimited storage. However Amazon Photos does not allow you to create folders, only albums and ideally I would like images grouped within folders such as Events, Commercial, Personal, etc. This is how I have all my images saved on my external hard drives. </p> <p>Seperate to this I would like to be able to send work to clients as reference and quickly access images for Instagram posts. For this I was thinking of creating a lower res 2mb per image jpeg version of each folder and uploading these to OneDrive which has a proper folder system making it easier to locate quickly and no need for every photo to be its full RAW size for sending to clients or posting on instagram. </p> <p>Does anyone have a better solution to this or currently do something similar? Any help would be greatly appreciated </p> </div><!-- SC_ON --> &#32; sub",
        "id": 2834692,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l24bol/how_to_store_15_year_photo_archive_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to store 15 year photo archive? Help!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Such-Bench-3199",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T05:17:22.322424+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T04:29:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am unsure how many others would take this news, but for those of us who archive everything, especially on Mac, get Podcast Archiver from the app store and get all of WTF now before it is gone.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Such-Bench-3199\"> /u/Such-Bench-3199 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l22y0o/saw_wtf_is_ending_only_if_you_want_read_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l22y0o/saw_wtf_is_ending_only_if_you_want_read_on/\">[comments]</a></span>",
        "id": 2834436,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l22y0o/saw_wtf_is_ending_only_if_you_want_read_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Saw WTF is ending, only if you want read on.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/svper-user",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T04:11:21.246357+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T03:46:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After discovering BTRFS, I was amazed by its capabilities. So I started using it on all my systems and backups. That was almost a year ago. </p> <p>Today I was researching small &quot;UPS&quot; with 18650 batteries and I saw posts about BTRFS being very dangerous in terms of power outages. </p> <p>How much should I worry about this? I&#39;m afraid that a power outage will cause me to lose two of my backups on my server. The third backup is disconnected from the power, but only has the most important part.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/svper-user\"> /u/svper-user </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l226pq/fear_of_btrfs_and_power_outage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l226pq/fear_of_btrfs_and_power_outage/\">[comments]</a></span>",
        "id": 2834167,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l226pq/fear_of_btrfs_and_power_outage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fear of BTRFS and power outage.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/waby-saby",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T04:11:21.543024+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T03:19:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to have a professional level file hosting service. Preferably something that is SOX and HIPAA compliant, but that&#39;s a nice to have. </p> <p>What is required is limiting files to certain people or groups and the ability to track who downloads what. </p> <p>A simple interface that is branded is needed. Is like a way to have the ability to share a file simply with a link for occasional files. </p> <p>This should not be based on per user as that will fluctuate greatly. </p> <p>Any ideas? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/waby-saby\"> /u/waby-saby </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l21os1/looking_for_file_hosting/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l21os1/looking_for_file_hosting/\">[comments]</a></span>",
        "id": 2834168,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l21os1/looking_for_file_hosting",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for File Hosting",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ThirdWaveK",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T03:06:11.524854+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T02:31:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for suggestions on ways to add other forms of media, preferably free or open source, that can be downloaded so it could be completely offline. Best way to maximize storage through different audio/video formats? The overall goal is to have a portable ecosystem that could theoretically run on any hardware from the past, say, 20 years or so. </p> <p>I\u2019m new here, but excited about the prospects. Thanks for any help and input guys!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ThirdWaveK\"> /u/ThirdWaveK </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l20rra/making_a_5tb_portable_hdd_that_hosts_its_own_os/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l20rra/making_a_5tb_portable_hdd_that_hosts_its_own_os/\">[comments]</a></span>",
        "id": 2833914,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l20rra/making_a_5tb_portable_hdd_that_hosts_its_own_os",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Making a 5tb portable HDD that hosts its\u2019 own OS (Lubuntu), a large amount of what\u2019s available on Kiwix, and RetroArch",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lousewort81",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T02:01:08.719424+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T01:54:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been a lurker in this subreddit for a long time, and I\u2019ve noticed\u2014both here and across Reddit\u2014a growing trend of weird pearl-clutching whenever porn gets mentioned. Sometimes people ask for advice or help managing their porn stash. The horror!</p> <p>I\u2019ve lost count of how many times I\u2019ve seen the same tired, corny remarks like \u201cgooner\u201d or \u201ccoomer,\u201d or comments\u2014joking or not\u2014suggesting that the OP has an addiction or is some kind of creep because they mentioned porn in any capacity whatsoever. It happens in nearly every porn-related thread around here.</p> <p>How hard is it to just ignore a post if it\u2019s not your thing? No one needs or cares to read a sermon of your nofap, puritan, or manosphere hang-ups. </p> <p>Personally, I\u2019ve got around 490GB of porn. I wouldn\u2019t call myself a &quot;hoarder,&quot; but like anyone managing a large dataset, I\u2019m curious about how to organize and make use of it\u2014and so are plenty of others here.</p> </div><!-- SC_ON",
        "id": 2833717,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l2015p/maybe_an_unpopular_opinion_but_porn_is_still_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Maybe an unpopular opinion, but porn is still data. Some people are going to seek advice or strategies for hoarding, sorting, or managing it. We don\u2019t need to hear in every NSFW thread that you have a problem with it. If it\u2019s not your thing, just scroll past and move on like an adult.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SimKaiLong",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T03:06:11.760123+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T01:40:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, have recently setup a PC running Proxmox and spun up LXCs to host some media services like the Arr stack, Jellyfin, Nextcloud etc. Also using it to run a VMs for TrueNAS, Immich and a Debian host. </p> <p>I&#39;ve currently got a data pool for 4x12TB disks and am looking to create a backup copy that is not within the same machine/server.</p> <p>I&#39;m aware of the 3-2-1 strategy but would like to keep costs low for now as I&#39;ve just started out. I have 2 extra 12TB drives on hand and plan to have 1 as a cold spare and 1 as a backup for my critical data like family media, which is currently at 1.5TB.</p> <p>Looking to get an enclosure for one of the 12TB drives so I can plug it in occasionally to do a backup. Preferably one that has a fan to keep the drive cool?</p> <p>Other suggestions are welcomed too.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SimKaiLong\"> /u/SimKaiLong </a> <br/> <span><a hre",
        "id": 2833915,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l1zqz2/recommend_me_a_35_hdd_enclosure_with_fan",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommend me a 3.5\" HDD Enclosure with Fan",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/clickbatedubs",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T02:01:08.985882+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T01:34:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to figure out how to completely mirror a version of a site from the Wayback Machine. Basically I want to download the full thing sorta like HTTrack or ArchiveBox does, but using the archived Wayback Machine version instead.</p> <p>I\u2019ve tried wayback-downloader and the Strawberry fork, but neither really worked well for anything large. Best I\u2019ve gotten is a few scattered pages, and a ton of broken links or missing assets that function fine on the actual waybackmachine.</p> <p>Anyone know a good way to actually pull a full, working snapshot of a site from Wayback? Preferably something that works decently with big sites too.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/clickbatedubs\"> /u/clickbatedubs </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l1zmut/how_would_i_fully_mirror_a_site_from_wayback/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoard",
        "id": 2833718,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l1zmut/how_would_i_fully_mirror_a_site_from_wayback",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How would I fully mirror a site from wayback machine??",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Yusei0",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-03T02:01:09.133718+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-03T00:58:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i am mainly looking into downloading the pokemon anime episodes from youtube but i cant figure out how todo it with the german audio track instend of the english one. i keep finding about using youtube dlp but i just cant figure out how to use it for this task, maybe someone can help me. idealy it would be great to have something with a GUI. i got open video downloader installed but i dont think it can download different audiotracks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yusei0\"> /u/Yusei0 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l1yxas/how_to_download_video_from_youtube_that_has/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l1yxas/how_to_download_video_from_youtube_that_has/\">[comments]</a></span>",
        "id": 2833719,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l1yxas/how_to_download_video_from_youtube_that_has",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to Download Video from Youtube that has multilanguage audio",
        "vote": 0
    }
]