[
    {
        "age": null,
        "album": "",
        "author": "/u/Ill_Dare8819",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T21:23:33.834090+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T20:00:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m looking for advice on a very lightweight, fast, and hard-to-detect (in terms of automation) browser (python) that supports async operations and proxies (things like aiohttp or any other http requests module is not my case). Performance, stealth, and the ability to scale are important.</p> <p><strong>My current experience:</strong></p> <ul> <li>I\u2019ve used <code>undetected_chromedriver</code> \u2014 works good but lacks async support and is somewhat clunky for scaling.</li> <li>I\u2019ve also used <code>playwright</code> with <code>playwright-stealth</code> \u2014 very good in terms of stealth and API quality, but still too heavy for my current scaling needs (high resource usage).</li> </ul> <p>Additionally, I would really appreciate advice on where to rent suitable servers (VPS, cloud, bare metal, etc.) to deploy this, so I can keep my local hardware free and easily manage scaling. Cost-effectiveness would be a bonus.</p> <p>Thanks in advance for any suggestions!<",
        "id": 2918347,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9wb67/lightweight_browser_for_scraping_scaling_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Lightweight browser for scraping + scaling & server rental advice?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ArchipelagoMind",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T20:18:43.312905+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T19:42:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you want to login and scrape any sites (most social media sites.) you usually need an email to register. Gmail seem to get picky about creating too many email addresses registered to the same phone number. Proton Email also demanded I had a unique backup email. Are there any good email services where I can simply create a puppet email account for my webscraping needs without the need for other unique phone numbers/email addresses? What are people&#39;s go to?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ArchipelagoMind\"> /u/ArchipelagoMind </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9vuqc/best_email_service_to_use_for_puppet_accounts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9vuqc/best_email_service_to_use_for_puppet_accounts/\">[comments]</a></span>",
        "id": 2917834,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9vuqc/best_email_service_to_use_for_puppet_accounts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Email service to use for puppet accounts",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Independent_Fan_232",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T21:23:34.012550+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T18:41:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i just want to ask is there any method that allow we search in raw source code like google dorks ? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Independent_Fan_232\"> /u/Independent_Fan_232 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9uaxp/can_we_search_code_snippet_directly_from_search/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9uaxp/can_we_search_code_snippet_directly_from_search/\">[comments]</a></span>",
        "id": 2918348,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9uaxp/can_we_search_code_snippet_directly_from_search",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "can we search code snippet directly from search engine ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aliciafinnigan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T15:49:50.289730+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T15:25:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I&#39;m pretty new to web scraping and I ran into something I don&#39;t understand. I am scraping an API of a website, which is being hit around 4 times before actually delivering the correct response. They are seemingly being hit at the same time, same URL (and values), same payload and headers, everything. </p> <p>Should I also hit this endpoint from Python at the same time multiple times, or will this lead me being blocked? (Since this is a small project, I am not using any proxies.) Is there any reason for this website to hit this endpoint multiple times and only deliver once, like some bot detection etc.?</p> <p>Thanks in advance!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/aliciafinnigan\"> /u/aliciafinnigan </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9pakr/api_endpoint_being_hit_multiple_times_before/\">[link]</a></span> &#32; <span><a href=\"https://www.reddi",
        "id": 2915440,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9pakr/api_endpoint_being_hit_multiple_times_before",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "API endpoint being hit multiple times before actual response",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Pitiful-Jeweler4287",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T21:23:34.189048+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T15:19:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Scan any webpage and start a conversation with <a href=\"https://WebLens-AI.vercel.app\">WebLens.AI</a> \u2014 uncover insights, generate ideas, and explore content through interactive AI chat.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pitiful-Jeweler4287\"> /u/Pitiful-Jeweler4287 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9p4zf/weblensai_look_through_the_internet/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9p4zf/weblensai_look_through_the_internet/\">[comments]</a></span>",
        "id": 2918349,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9p4zf/weblensai_look_through_the_internet",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WebLens-AI (LOOK THROUGH THE INTERNET)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ConsistentProject682",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T15:49:50.438765+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T15:00:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey y&#39;all, I&#39;m novice programmer (more analysis than engineering; self-taught) and I&#39;m trying to get some small little projects under my belt. One thing I&#39;m working on is a small script that would check a url if it&#39;s static HTML (for scrapy or BS) or if it&#39;s JS-rendered (for playwright/selenium) and then scrape based on the appropriate tools.</p> <p>The thing is that I&#39;m not sure how to create a distinction in the Python script. ChatGPT suggested a minimum character count (300), but I&#39;ve noticed that JS-rendered texts are quite long horizontally. Could I do it based on newlines (never seen JS go past 20 lines). If y&#39;all have any other way to create a distinction, that would be great too. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ConsistentProject682\"> /u/ConsistentProject682 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9oojf/checking_f",
        "id": 2915441,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9oojf/checking_for_jsrendered_html",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Checking for JS-rendered HTML",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Which_Seaworthiness",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T14:44:51.827665+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T14:22:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi. Can anyone share if they know open source working code that can bypass cloudfare error 403 on indeed? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Which_Seaworthiness\"> /u/Which_Seaworthiness </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9nqx8/error_403_on_indeed/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9nqx8/error_403_on_indeed/\">[comments]</a></span>",
        "id": 2914689,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9nqx8/error_403_on_indeed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Error 403 on Indeed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/51times",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T14:44:51.656213+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T13:38:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://coberturamovil.ift.org.mx/\">https://coberturamovil.ift.org.mx/</a><br/> These are the area of interests for me. How do I scrape them?<br/> I tried the following:<br/> <a href=\"https://coberturamovil.ift.org.mx/sii/buscacobertura\">https://coberturamovil.ift.org.mx/sii/buscacobertura</a> is request URL, taking some payload<br/> I wrote the following code but it just returned the html page back</p> <pre><code>import requests url = &quot;https://coberturamovil.ift.org.mx/sii/buscacobertura&quot; # Simulated form payload (you might need to update _csrf value dynamically) payload = { &quot;tecnologia&quot;: &quot;193&quot;, &quot;estado&quot;: &quot;23&quot;, &quot;servicio&quot;: &quot;1&quot;, &quot;_csrf&quot;: &quot;NL0ES9S8SskuVxYr3NapMovFEpgcbkkaFkqweQIIBlaq7vhjlpxN7tzZ_TOzRWWNwV2CRCA3YAj3mNfm8dkXPg==&quot; } headers = { &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0&",
        "id": 2914688,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9mpwo/is_it_possible_to_scrape_a_maps_based_website_not",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it possible to scrape a maps based website, not related to google?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/musicdimasko",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T14:44:52.001341+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T12:20:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just wondering how many of you are using mobile proxies (like 4G/5G) for scraping \u2014 especially when targeting tough or geo-sensitive sites.</p> <p>I\u2019ve mostly used datacenter and rotating residential setups, but lately I\u2019ve been exploring mobile proxies and even some multi-port configurations.</p> <p>Curious:</p> <ul> <li>Do mobile proxies actually help reduce blocks / captchas?</li> <li>How do they compare to datacenter or residential options?</li> <li>What rotation strategy do you use (per session / click / other)?</li> </ul> <p>Would love to hear what\u2019s working for you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/musicdimasko\"> /u/musicdimasko </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9l1g5/do_you_use_mobile_proxies_for_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9l1g5/do_you_use_mobile_proxies_for_scraping/\">[comments]</",
        "id": 2914690,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9l1g5/do_you_use_mobile_proxies_for_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you use mobile proxies for scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jagaimo-",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T12:00:20.476865+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T10:58:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, so I am designing something that involves logging in to the Google Suite through a Chrome window that Selenium opened via a .py script. </p> <p>That being said, everything is done manually (email entering, 2FA, captcha, all that). I am trying to find a way to get the user at furthest to a 2FA/Passkey screen so that THEY can complete it, but not a necessary feature.</p> <p>Is this an issue? Legally? ToS wise? And what about at scale, is this something that (if it became a nuisance) google could just disable? I am very new to scraping and this isn\u2019t scraping per se, just part of a project and I thought this would be the place to ask\u2026 if you need any clarification, lmk!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jagaimo-\"> /u/jagaimo- </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9jizw/google_signin_via_selenium_window/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit",
        "id": 2913470,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9jizw/google_signin_via_selenium_window",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google sign-in via Selenium Window",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mickspillane",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-12T07:41:59.749502+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-12T06:40:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Curious if there are any open source models out there to which I can throw a list of timestamps and it can give me a % likelihood that the request pattern is from a bot. For example, if I give it 1000 timestamps exactly 5 seconds apart, it should return ~100% bot-like. If I give it 1000 timestamps spanning over several days mimicking user sessions of random length durations, it should return ~0% bot-like. Thanks.</p> <p>edit: ideally a model which is based on real data</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mickspillane\"> /u/mickspillane </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9fm38/frequency_analysis_model/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l9fm38/frequency_analysis_model/\">[comments]</a></span>",
        "id": 2911902,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l9fm38/frequency_analysis_model",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Frequency Analysis Model",
        "vote": 0
    }
]