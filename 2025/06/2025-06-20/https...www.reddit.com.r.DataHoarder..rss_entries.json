[
    {
        "age": null,
        "album": "",
        "author": "/u/RicardoMyBoiii",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T23:20:40.648763+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T23:00:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, I recently bought a Quantum ultrium lto 6-H drive. I installed it in a Dell Powervault for LTO-4 and connected it with a mini sas 8088 cable to a HBA (Lsi 9207-8e) in IT mode. However, every 18 GB / 2 minutes the drive stops briefly and continues writing. Could it be that the enclosure, since it is not designed for lto 6, is causing the shoe shining? And if so, what is the best way to install the drive in my PC to ensure that the connections do not cause a bottleneck? Drivers are all up to date and I have tried Windows ltfs, Linux Ubuntu ltfs and tar etc. but to its always the same. I don&#39;t know what to do. Many thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RicardoMyBoiii\"> /u/RicardoMyBoiii </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lghnue/lto6_shoe_shining_problem/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comment",
        "id": 2982900,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lghnue/lto6_shoe_shining_problem",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LTO-6 shoe shining problem",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lousewort81",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T23:20:40.199592+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T22:53:39+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lghisy/i_think_what_i_really_want_is_to_be_a_hard_drive/\"> <img src=\"https://external-preview.redd.it/Qlcpj4rweDJJB_jCsfEGp0Z7-dJDFhOiwVjkVQyrCNI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=005fb65d2eb3d44f4251885e937071d5cd70823f\" alt=\"I think what I really want is to be a hard drive.\" title=\"I think what I really want is to be a hard drive.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lousewort81\"> /u/lousewort81 </a> <br/> <span><a href=\"https://streamable.com/xjwvn1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lghisy/i_think_what_i_really_want_is_to_be_a_hard_drive/\">[comments]</a></span> </td></tr></table>",
        "id": 2982898,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lghisy/i_think_what_i_really_want_is_to_be_a_hard_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Qlcpj4rweDJJB_jCsfEGp0Z7-dJDFhOiwVjkVQyrCNI.jpeg?width=640&crop=smart&auto=webp&s=005fb65d2eb3d44f4251885e937071d5cd70823f",
        "title": "I think what I really want is to be a hard drive.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ZivH08ioBbXQ2PGI",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T23:20:40.900692+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T22:32:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It&#39;s a low-power temporary FM station in LA promoting a new Slice soda campaign with a retro 80s/90s-sounding countdown of AI music.</p> <p>Has anyone grabbed the stream?</p> <p>Sorry if inappropriate.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ZivH08ioBbXQ2PGI\"> /u/ZivH08ioBbXQ2PGI </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lgh1zm/has_anyone_grabbed_music_from_the_fizz_fm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lgh1zm/has_anyone_grabbed_music_from_the_fizz_fm/\">[comments]</a></span>",
        "id": 2982902,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lgh1zm/has_anyone_grabbed_music_from_the_fizz_fm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone grabbed music from The FIZZ FM?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Narutobi_Sensei",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T23:20:40.403390+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T22:27:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a WD Elements 18tb drive and am upgrading to a 24tb. I want to ensure everything transfers with 0 errors, and the windows file explorer copy paste can be extremely dodgy and crash.</p> <p>Is there a program that out there for this sort of thing? That will create a checksum or something and handle the transfer and ensure everything goes smoothly? Or is there just some better way except Ctrl+C, Ctrl+V 16 TB of data in file explorer between the two drives? There has to be right?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Narutobi_Sensei\"> /u/Narutobi_Sensei </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lggy9j/best_way_to_transfer_several_tb_to_new_hard_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lggy9j/best_way_to_transfer_several_tb_to_new_hard_drive/\">[comments]</a></span>",
        "id": 2982899,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lggy9j/best_way_to_transfer_several_tb_to_new_hard_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to transfer several TB to new hard drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hpinkjetprinter1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T23:20:41.172356+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T22:23:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;d like to use a spare PC i have that has lots of drive bays to make a little home das for hoarding and was wondering if I can connect it to my main PC with a USB type C to access it that way as I&#39;d imagine it would be faster than over the network assuming I get a nice cable, any help or recommendations on what I should do would be great!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hpinkjetprinter1\"> /u/hpinkjetprinter1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lggunb/can_i_connect_two_pcs_together_with_usb_for_one/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lggunb/can_i_connect_two_pcs_together_with_usb_for_one/\">[comments]</a></span>",
        "id": 2982903,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lggunb/can_i_connect_two_pcs_together_with_usb_for_one",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I connect two PCs together with USB for one to act as a das?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HopefullyASilbador",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T20:03:40.486217+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T19:38:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Should it be booted up at least once per year to prevent memory degradation? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HopefullyASilbador\"> /u/HopefullyASilbador </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lgd0c1/i_have_a_4tb_wd_ssd_what_are_some_best_practices/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lgd0c1/i_have_a_4tb_wd_ssd_what_are_some_best_practices/\">[comments]</a></span>",
        "id": 2981636,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lgd0c1/i_have_a_4tb_wd_ssd_what_are_some_best_practices",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I have a 4tb WD SSD. What are some best practices for long term data storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bentancurry",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T18:58:32.803189+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T17:55:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Basically the title is the question. Struggling to do so as none of the Chrome extensions recommend in other reddit posts work, and yt-dlp is proving quite tough to use. Any advice?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bentancurry\"> /u/bentancurry </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lgai2d/how_do_i_download_a_patreon_video_hosted_on_vimeo/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lgai2d/how_do_i_download_a_patreon_video_hosted_on_vimeo/\">[comments]</a></span>",
        "id": 2981211,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lgai2d/how_do_i_download_a_patreon_video_hosted_on_vimeo",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I download a Patreon video (hosted on Vimeo) that I paid for?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Old_Locksmith9254",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T20:03:40.783238+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T16:51:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u00b4ve been looking for a download tool to download insta posts for offline viewing. Specifically my saved posts. </p> <p>But the tools i\u00b4ve found don\u00b4t really do that. I would like to have the same experiance viewing instagram offline as I do online. Thus I want to download all data from a post like caption, links, music, some comments etc. </p> <p>I\u00b4m not looking for just a jpeg or html file. I looking for the whole thing. I\u00b4m sure im not the only one </p> <p>can anyone lead me in that direction? preferably for non-programmers. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Old_Locksmith9254\"> /u/Old_Locksmith9254 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg8wyy/instagram_downloader_but_with_captions_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg8wyy/instagram_downloader_but_with_captions_and/\">[comments]</a></span>",
        "id": 2981637,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg8wyy/instagram_downloader_but_with_captions_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Instagram downloader but with captions and comments",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/0biwan-Kenobi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T20:03:40.985488+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T16:44:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Couldn&#39;t find much online for this, so reaching out here.</p> <p>Looking to use SeaTools to test the hard drives were delivered yesterday. Ultimately just want to make sure there are no bad blocks/sectors and that no damage occurred during shipping before I throw them into the NAS.</p> <p>Trying to understand the difference of the <strong>Long Self Test</strong> vs the <strong>Long Generic Test</strong>, and which might be more applicable for my use case.</p> <p>Thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/0biwan-Kenobi\"> /u/0biwan-Kenobi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg8qn0/seatools_long_self_test_vs_long_generic_test/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg8qn0/seatools_long_self_test_vs_long_generic_test/\">[comments]</a></span>",
        "id": 2981638,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg8qn0/seatools_long_self_test_vs_long_generic_test",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SeaTools - Long Self Test vs Long Generic Test",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/poisonrabbit",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T15:44:19.848528+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T15:04:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>so i&#39;m currently in a dilemma on which storage solution I wanna use for my increasing media collection. and i&#39;m torn between using a NAS(Synology DS423+) or a <a href=\"https://www.nzsale.co.nz/product/Tristar-Online-ICY-BOX-IB-3640SU3-External-4-bay-JBOD-system-for-3-5-Inch-SATA-HDDs/s/RRY5ZffiV0GmB4qOX5NyaA\">JBOD enclosure like this one</a> . I wanna use it for playing and storing medias. not really interested in streaming it outside of my LAN</p> <p>i&#39;ll summarize what I think with pros and cons</p> <p>starting with the most offered solution NAS (in this case i&#39;m interested in Synology DS423+)<br/> <strong>Pros:</strong><br/> -its built exactly for my needs<br/> -has a &#39;system&#39; designed for storing files<br/> -more versatile/options<br/> -RAID option/support is good (atleast according to diff users)</p> <p><strong>Cons:</strong><br/> -more technical than I thought and i&#39;m not a tech savie. I need to figure out a wide asso",
        "id": 2979565,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg69xd/jbod_on_drive_enclosure_or_nas_for_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "JBOD on Drive Enclosure or NAS (for media collection) help decide which based on my pros and cons",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Nairathu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T20:03:41.244780+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T14:36:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Here comes my issue and ideal solution :) </p> <p>Tho I would ask the esteemed gents and gals help if you have experience. </p> <p>I want to have an nvm enclosure, that can read UHS 2 cards have PD and atleast 2 usb C ports.<br/> Now, ideally this is portable too. </p> <p>My questions:<br/> Do you know any that would suffice to this criteria and are actually sold? Found some, but they are not sold atm. </p> <p>Fanxiang and Doccase seems to be the exact copy to one and other, are they actually? (if so than I don&#39;t need to pay the extra price of Doccase) </p> <p>If I chose to have a separate NVMe enclosure (and hub and card reader) than whihc NVMe enclosures could be the best to pick? (I read a lot that the different chipsets results vary in speed, reliability etc)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nairathu\"> /u/Nairathu </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg5k",
        "id": 2981639,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg5kse/nvme_enclosure_that_also_reads_uhs_2_cards_with_pd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NVMe Enclosure that also reads UHS 2 cards with PD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EnvoySass",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T14:37:33.234554+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T14:18:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A podcast i follow has disbanded and i wanting to archive it for posterity i have downloaded all of the Patreon content, the problem is that some of the earlier episodes are on Spotify exclusively. Does anyone know the best way to download these episodes? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EnvoySass\"> /u/EnvoySass </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg55og/spotify_podcast/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg55og/spotify_podcast/\">[comments]</a></span>",
        "id": 2978984,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg55og/spotify_podcast",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Spotify podcast",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Fordtough68",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T14:37:33.438167+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T13:37:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am contemplating converting my two 8 bay nas that currently have 4 tb drives, to 12tb drives. Right now I think I have about 5 or 6 12tb drives, but they are full of data too. My thought is to transfer everything to the cloud, wipe all of my 12tb drives and put them into my 8 bay nas&#39;s and buy s few more to go with them, then cancel the cloud storage. What would be a good way of doing this that wouldn&#39;t take forever to accomplish? I only have 40MB upload speed unfortunately. Whats a good service for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fordtough68\"> /u/Fordtough68 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg477w/temporary_cloud_storage_for_120tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg477w/temporary_cloud_storage_for_120tb/\">[comments]</a></span>",
        "id": 2978985,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg477w/temporary_cloud_storage_for_120tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Temporary cloud storage for 120+tb",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/shartoberfest",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T11:21:52.789918+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T10:40:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m new to this so feel free to delete this post if its not appropriate. </p> <p>I have about 20 years of digital photographs saved (2004-now) and my method of storing them has been to use an external HDD until it fills up in a few years and buy a larger capacity one. Rinse/repeat. I&#39;m currently using a 16TB WD elements external drive and it will be filled up in about 2 years by my estimate. Would it make sense to continue this method and buy a 20TB drive, or should i get a HDD dock and add new drives to expand my capacity? </p> <p>I do edit photos on occasion, but for the most part its just storage. I&#39;m not a professional photographer, these are just my personal photos.</p> <p>I also use a cloud backup as well for redundancy, which is purely for storage/archive.</p> <p>Any help would be appreciated, thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shartoberfest\"> /u/shartoberfest </a> <br/> <s",
        "id": 2977488,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg0sn6/best_method_of_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Method of Backup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Open_Importance_3364",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T10:16:49.371935+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T09:54:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg02b0/michael_the_data_hoarder/\"> <img src=\"https://external-preview.redd.it/Z2V6NmFsbTAxMjhmMTKx-l6jZy4hWikKdWLzC1wt8f3w4G8nZ0QgMKxOzWHA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaa0c39013e94c1095fb568ab3dc59cd9b193824\" alt=\"Michael the Data Hoarder\" title=\"Michael the Data Hoarder\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><em>A hoarder hoarder hoards hoarders.</em> \ud83d\ude05</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Open_Importance_3364\"> /u/Open_Importance_3364 </a> <br/> <span><a href=\"https://v.redd.it/cts4i0p0128f1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lg02b0/michael_the_data_hoarder/\">[comments]</a></span> </td></tr></table>",
        "id": 2977032,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lg02b0/michael_the_data_hoarder",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Z2V6NmFsbTAxMjhmMTKx-l6jZy4hWikKdWLzC1wt8f3w4G8nZ0QgMKxOzWHA.png?width=640&crop=smart&auto=webp&s=aaa0c39013e94c1095fb568ab3dc59cd9b193824",
        "title": "Michael the Data Hoarder",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/poggiaus1542",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T20:03:41.707617+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T09:05:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to keep a work backup of a google drive shared folder. The backup will be done once a week. The real problem is that the folder is quite huge and it&#39;s quite unconfortable to download all the stuff, unpack it, erase the old folder on my NAS and upload all the files.</p> <p>The incremental backup has not to be done with the NAS itself, it can be done with a windows PC (or a linux one if it&#39;s needed).</p> <p>Can it be done?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/poggiaus1542\"> /u/poggiaus1542 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfzc9j/google_drive_shared_folder_on_wd_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfzc9j/google_drive_shared_folder_on_wd_nas/\">[comments]</a></span>",
        "id": 2981640,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfzc9j/google_drive_shared_folder_on_wd_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google drive shared folder on WD nas",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/QLaHPD",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T08:05:33.687644+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T07:55:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Guys, so I&#39;m building a dataset of YouTube comments, I&#39;m trying to be as diverse as possible, taking many types of channels as possible, and, as you can imagine lots and lots of comments are duplicated/spam.</p> <p>I know this topic isn&#39;t only about <a href=\"/r/DataHoarder\">r/DataHoarder</a> but I guess its worth posting here too, should I keep all comments or remove duplication leaving only the first copy of each?</p> <p>I thought on these pros and cons:</p> <blockquote> <p>Pros on keep:<br/> - Spam information, which comes not from the comments content itself, but by meta analysis over a batch of them.</p> <p>Cons on keep:<br/> - Redundant information, more storage usage <del>even if we have about 10% of the world&#39;s storage</del>.</p> <p>- Require more processing later if you want to remove the duplication before usage.</p> </blockquote> <p>So what you guys think?</p> <p>Also I will share it once it&#39;s finished, so if you have a l",
        "id": 2976381,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfyazy/building_a_dataset_of_yt_comments_and_need_your",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Building a dataset of YT comments, and need YOUR help deciding on how to proceed....",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Better-Way-2421",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T08:05:33.854993+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T07:46:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After years of using various NAS setups, I finally found my sweet spot with the TerraMaster F8-SSD Plus. As someone who primarily needs reliable backups + lightweight document sharing for the whole family (no heavy workloads), the non-Plus version was tempting\u2014but I\u2019m thrilled I went with the Plus. Zero regrets.</p> <p>My unconventional SSD choice:</p> <p>I popped in two 2TB Orico D10 NVMe SSDs\u2014not on TerraMaster\u2019s compatibility list. Why? Past positive experiences with Orico. Worst-case scenario, I\u2019d repurpose them elsewhere. Spoiler: They worked flawlessly!</p> <p>Thermals &amp; Hardware:</p> <p>Used Orico\u2019s thermal strips + TerraMaster\u2019s included heatsinks. Even during sustained transfers, SSDs hover at 40-42\u00b0C\u2014absolutely solid. The passive cooling design deserves props.</p> <p>Real-world perks:</p> <p>Silent operation: Tucked away on a shelf, you\u2019ll forget it\u2019s running</p> <p>Future-proof: Slowly populating all 8 bays as storage needs grow</p> <p>",
        "id": 2976382,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfy6ih/i_just_got_my_perfect_home_data_hub_simple",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I Just Got My Perfect Home Data Hub: Simple & Flawless!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Consistent-Camel-499",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T08:05:34.055536+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T07:40:38+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfy3e0/getting_rid_of_the_directors_cut/\"> <img src=\"https://preview.redd.it/zhg3qtfgd18f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d985cee642c7cd22f96a8cb2de71cf34ecd6f8a\" alt=\"Getting rid of the directors cut?\" title=\"Getting rid of the directors cut?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So I\u2019ve got pre-ripped dvds and I\u2019m trying to put the whole Star Wars trilogy on a flash drive I\u2019ve done 5 movies but attack of the clones is a directors cut so I did what I normally would but the audio just has George Lucas talking over the whole movie how do I get rid of him and just keep the movie audio? (New to this btw)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Consistent-Camel-499\"> /u/Consistent-Camel-499 </a> <br/> <span><a href=\"https://i.redd.it/zhg3qtfgd18f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments",
        "id": 2976383,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfy3e0/getting_rid_of_the_directors_cut",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/zhg3qtfgd18f1.jpeg?width=640&crop=smart&auto=webp&s=1d985cee642c7cd22f96a8cb2de71cf34ecd6f8a",
        "title": "Getting rid of the directors cut?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/iamfuturetrunks",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T08:05:34.257691+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T07:20:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So my first question might be pretty easy for some people but I have a Memorex MVD4543. I had to look up the manual cause no idea where that is, if we even still have it. </p> <p>Can&#39;t find anything showing the ability to record from a VHS to a DVD which is what I originally thought I could do. So asking here cause the manual I found was only talking about recording to VHS. Guessing maybe the player doesn&#39;t have a way to record to DVD&#39;s only play them? </p> <p>If this one does work, do I need to play the sound loud for the recording or can it be pretty quiet or will it be to quiet for the DVD to record the audio? Not sure how that works. </p> <p>So if that way doesn&#39;t work im contemplating just taking a bunch of the VHS&#39;s to the big city (hours of driving) to have a &quot;professional&quot; do it for me. Since all the posts I have come across (some on here) talk about getting this or that and do this or that but watch out for this ",
        "id": 2976384,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfxsf9/questions_about_digitizing_old_vhs_tapes_and_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Questions about digitizing old VHS tapes and a Memorex MVD4543",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/True-Entrepreneur851",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T07:00:37.119550+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T06:47:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. I have photos, ebooks and personal documents that I backup on my NAS + send to Cloud backup with current routine : - Data source on my Mac external drive. - Use freesync to send to NAS. - Use rclone on my NAS to send to cloud through 3 scripts in task scheduler.</p> <p>My questions below : - Would it be possible to backup from Mac OS to NAS and Cloud using Rclone but via batch ? I guess so \u2026 but wondering how. - Does it make sense to use 3 separated scripts and is it best option ? How can you state in a script \u00ab process to next line \u00bb ? - how can I encrypt my data going to Cloud ? Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/True-Entrepreneur851\"> /u/True-Entrepreneur851 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfxa7s/what_to_use_for_backup_batch/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfxa7s/what_to_use_for_ba",
        "id": 2976110,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfxa7s/what_to_use_for_backup_batch",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What to use for backup batch",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RushLow9890",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T05:57:40.870805+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T05:36:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been seeing a lot of brands now claiming to have AI powered NAS setups, but it\u2019s been hard to tell what\u2019s legit and what\u2019s just marketing.</p> <p>Things like AI photo tagging, semantic search, OCR... even local LLM built in, like private AI search without going through the cloud. That sounds useful, but how well does it actually work when dealing with my own messy photo libraries, mixed file types, and weird folder naming? Anyone trying out NAS with AI features built in? Curious how it actually holds up with messy, real-life data, not just polished demo examples.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RushLow9890\"> /u/RushLow9890 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfw6ch/anyone_figured_out_whether_ai_features_in_nas_are/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfw6ch/anyone_figured_out_whether_ai_features_in_nas_are",
        "id": 2975863,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfw6ch/anyone_figured_out_whether_ai_features_in_nas_are",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone figured out whether AI features in NAS are actually useful or just hype?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Broad_Sheepherder593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T03:45:14.261535+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T02:51:43+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lftam4/verifying_refurb_drives/\"> <img src=\"https://preview.redd.it/q6mlrcswxz7f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=acc849dd2fa0c071b3ea165bace451e1d397c632\" alt=\"Verifying refurb drives\" title=\"Verifying refurb drives\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>Due to the long ordering process in my area, decided to keep a cold spare just in case. I&#39;m planning to get a manufacturer recertified drive. I do know about the bathtub curve so for me to make sure its indeed working, I&#39;m planning to use this drive continuously for a month? / 1000 hours. If no issues, then will just power this on monthly to check. Would this be an acceptable method?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Broad_Sheepherder593\"> /u/Broad_Sheepherder593 </a> <br/> <span><a href=\"https://i.redd.it/q6mlrcswxz7f1.jpeg\">[link]</a></span> &#32; <span><a h",
        "id": 2975414,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lftam4/verifying_refurb_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/q6mlrcswxz7f1.jpeg?width=640&crop=smart&auto=webp&s=acc849dd2fa0c071b3ea165bace451e1d397c632",
        "title": "Verifying refurb drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LowConcept997",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-20T00:30:16.725334+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-20T00:06:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>\u201cThe Goonie Show\u201d was a popular youtuber with her most video having 12M views, and was many people\u2019s childhoods in 2016-2018. And in September 2023, she decided to delete all of her videos. She had 108 videos, and now only about 80 something are archived on YouTube. This channel was my entire life in 2017, and it breaks my inner child\u2019s heart knowing that some videos are lost. I\u2019m currently praying this works because I\u2019ve drained and wasted the past like 8 months of my life attempting to find her videos. Thank you if you read this \u2764\ufe0f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LowConcept997\"> /u/LowConcept997 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfq28y/can_you_guys_help_me_find_youtube_videos_from_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfq28y/can_you_guys_help_me_find_youtube_videos_from_the/\">[comments]</a></span>",
        "id": 2974733,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfq28y/can_you_guys_help_me_find_youtube_videos_from_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can you guys help me find YouTube videos from \u201cThe Goonie Show\u201d Channel? (It\u2019s titled \u201cRyen Linnea\u201d now)",
        "vote": 0
    }
]