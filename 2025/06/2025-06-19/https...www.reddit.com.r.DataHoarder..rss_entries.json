[
    {
        "age": null,
        "album": "",
        "author": "/u/Homebucket33",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T23:26:48.647054+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T23:24:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am beginning to think I&#39;m a data horder. Music,movies,tv,pictures,video games,programs and even operating systems. I run Windows 11 Pro on a headless server that I maintain from a personal laptop within my network. My question here is about backup. Currently, I use Stablebit Drivepool. I would like to use parity and have considered moving to an Unraid system, but I am comfortable with Windows and its file formats. Is there a way that I can stay on Windows and use parity for my backup? I have read that Storage Spaces can do it, but I have heard bad reviews on it about data loss and corruption. I am hoping to hear some opinions and experience with either staying with Windows or moving to Unraid (or something similar). Thanks in advance. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Homebucket33\"> /u/Homebucket33 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfp73q/backupparity_in",
        "id": 2974421,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfp73q/backupparity_in_windows",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup/parity in Windows",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Onimatus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T23:26:48.832535+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T22:39:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>But they mostly do in person. I&#39;ve had a bunch of other ones such as Pivotal180, Gridlines, F1F9, FinacnialModelOnlilne, WallStreetPrep, WSO, BIWS, CFI, Financial Edge, and a few others (EdBodmer too), but none of them are compete. ForvisMazars/Corality would seem like the holy grail though even if I complete the rest.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Onimatus\"> /u/Onimatus </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfo7st/strange_but_has_anyone_ever_collected_project/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfo7st/strange_but_has_anyone_ever_collected_project/\">[comments]</a></span>",
        "id": 2974422,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfo7st/strange_but_has_anyone_ever_collected_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Strange, but has anyone ever collected Project Finance courses? Was hoping for ForvisMazars and Corality",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PulsedMedia",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T21:15:32.201270+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T20:56:08+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfltnf/density_12x35_hdd_1ru_with_2x_mitx_nodes/\"> <img src=\"https://b.thumbs.redditmedia.com/2rn5XR6dYxDqH45dlkS787dAlq44EZkIAPZ_4t9zg2c.jpg\" alt=\"Density? 12x3.5&quot; HDD @ 1RU with 2x mITX Nodes\" title=\"Density? 12x3.5&quot; HDD @ 1RU with 2x mITX Nodes\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>These just passed CPU stress test and are fully functioning. This is the platform we have been developing over at <a href=\"http://PulsedMedia.com\">PulsedMedia.com</a> for a few years, but now we have been working with the 12x3.5&quot; HDD + 2x mITX nodes instead of 8x mITX/1L MiniPC on 1 rack unit.</p> <p><a href=\"https://reddit.com/link/1lfltnf/video/5lkyfzs34y7f1/player\">https://reddit.com/link/1lfltnf/video/5lkyfzs34y7f1/player</a></p> <p>We share a lot of this process in other forums and in our discord.</p> <p><a href=\"https://preview.redd.it/9ihd38554y7f1.jpg?width=1868&amp;format=pjpg&amp;auto=webp&amp",
        "id": 2973816,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfltnf/density_12x35_hdd_1ru_with_2x_mitx_nodes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/2rn5XR6dYxDqH45dlkS787dAlq44EZkIAPZ_4t9zg2c.jpg",
        "title": "Density? 12x3.5\" HDD @ 1RU with 2x mITX Nodes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wewewawa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T21:15:31.850841+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T20:18:24+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfkxcr/windows_11_user_has_30_years_of_irreplaceable/\"> <img src=\"https://external-preview.redd.it/8KO8bXy-cd1tCrCwPE-zEdzlNOVEMQCbWxsCb0pU_O8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=311fcff32c506d099eef0f0208a201a0caa360db\" alt=\"Windows 11 user has 30 years of 'irreplaceable photos and work' locked away in OneDrive - and Microsoft's silence is deafening\" title=\"Windows 11 user has 30 years of 'irreplaceable photos and work' locked away in OneDrive - and Microsoft's silence is deafening\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wewewawa\"> /u/wewewawa </a> <br/> <span><a href=\"https://www.techradar.com/computing/windows/windows-11-user-has-30-years-of-irreplaceable-photos-and-work-locked-away-in-onedrive-and-microsofts-silence-is-deafening\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfkxcr/windows_11_user_has_30_years_of_irr",
        "id": 2973815,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfkxcr/windows_11_user_has_30_years_of_irreplaceable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/8KO8bXy-cd1tCrCwPE-zEdzlNOVEMQCbWxsCb0pU_O8.png?width=640&crop=smart&auto=webp&s=311fcff32c506d099eef0f0208a201a0caa360db",
        "title": "Windows 11 user has 30 years of 'irreplaceable photos and work' locked away in OneDrive - and Microsoft's silence is deafening",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IActuallyDontAgree",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T20:10:33.730596+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T19:40:04+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfjzv1/legit_or_nah_patriot_p400_lite_2tb_for_50/\"> <img src=\"https://external-preview.redd.it/D8MScy_4EXx1zQg2SRvO9jO2HBhABq88U8Ycr7EWkAA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3b97b6da6885424667f347f6d3ebf33b850dc69\" alt=\"Legit or nah? Patriot P400 lite 2TB for $50?\" title=\"Legit or nah? Patriot P400 lite 2TB for $50?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I found this SSD on Amazon after looking through PCpartpicker (Swedish amazon site) :<a href=\"https://www.amazon.se/dp/B0BTDRJZBX?tag=pcpp-21&amp;linkCode=ogi&amp;th=1&amp;psc=1\">https://www.amazon.se/dp/B0BTDRJZBX?tag=pcpp-21&amp;linkCode=ogi&amp;th=1&amp;psc=1</a></p> <p>For just 500 SEK or about $50 and im wondering if I should buy it. Worst case I can return it within 30 days, best case I get 2TB for $50. Made even stranger is the fact that the smaller capacity drives are more expensive.</p> <p>Its sold by Patriot Sweden which loo",
        "id": 2973419,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfjzv1/legit_or_nah_patriot_p400_lite_2tb_for_50",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/D8MScy_4EXx1zQg2SRvO9jO2HBhABq88U8Ycr7EWkAA.png?width=640&crop=smart&auto=webp&s=c3b97b6da6885424667f347f6d3ebf33b850dc69",
        "title": "Legit or nah? Patriot P400 lite 2TB for $50?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Naernoo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T20:10:34.002683+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T19:16:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi!<br/> I have a large number of images I want to deduplicate. I tried Anti-Twin because it worked out of the box.</p> <p>However, the performance is really bad. I ran a deduplication scan between two folders and it found about 10 GB of duplicates, which I deleted. Then I ran a second scan, and it found another 2 GB. A third scan found 1 GB, and then another found around 500 MB, and so on.</p> <p>It seems like it never catches all duplicates in one go. Why is that? I set all limits really high. </p> <p>Are there better alternatives that don\u2019t have these issues?</p> <p>I tried using Czkawka a few years ago, but ran into permission errors, missing dependencies, and other problems.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Naernoo\"> /u/Naernoo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfjezc/antitwin_performs_poorly_for_deduplication_any/\">[link]</a></span> &#32; <span><a href=\"",
        "id": 2973420,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfjezc/antitwin_performs_poorly_for_deduplication_any",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anti-Twin Performs poorly for deduplication. Any better alternatives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lionsrawrr",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T19:05:38.891383+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T18:24:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>been having trouble finding a bulk post downloader that will work on a mac. tried jdownloader but it did not include the audio portion of videos even tho the original posts do have sound. checked the settings and then read up on it and guess its just something it doesnt always do. Suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lionsrawrr\"> /u/lionsrawrr </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfi3sv/reddit_video_post_downloader_for_mac/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfi3sv/reddit_video_post_downloader_for_mac/\">[comments]</a></span>",
        "id": 2973069,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfi3sv/reddit_video_post_downloader_for_mac",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "reddit video post downloader for mac",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Melodic-Network4374",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T19:05:39.164447+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T18:12:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Yeah, I know about the wiki, it has links to a bunch of stuff but I&#39;m interested in hearing your workflow.</p> <p>I have in the past used wget to mirror sites, which is fine for just getting the files. But ideally I&#39;d like something that can make WARCs, singlefile dumps from headless chrome and the like. My dream would be something that can handle (mostly) everything, including website-specific handlers like yt-dlp. Just a web interface where I can put in a link, set whether to do recursive grabbing and if it can follow outside links.</p> <p>I was looking at ArchiveBox yesterday and was quite excited about it. I set it up and it&#39;s soooo close to what I want but there is no way to do recursive mirroring (<code>wget -m</code> style). So I can&#39;t really grab a whole site with it, which really limits its usefulness to me.</p> <p>So, yeah. What&#39;s your workflow and do you have any tools to recommend that would check these boxes?</p> </div",
        "id": 2973070,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfhtty/what_do_you_use_for_website_archiving",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What do you use for website archiving?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tyrandemain",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T18:02:14.888652+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T17:59:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>USBs can be lost, drives can break, and some cloud storages are eager to terminate your account if you don&#39;t log in for a certain period of time. I&#39;m looking for an option to store a handful of tiny files (mostly text documents with notes, configs etc, but some media as well: screenshots a short clips), where I can upload those files to, and be sure that if I only need them in 5-10 years time, they will still be there?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tyrandemain\"> /u/tyrandemain </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfhglo/best_cold_storage_solution_for_small_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfhglo/best_cold_storage_solution_for_small_files/\">[comments]</a></span>",
        "id": 2972650,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfhglo/best_cold_storage_solution_for_small_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best cold storage solution for small files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nothing-counts",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T18:02:15.051565+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T17:57:57+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfhfm6/i_built_air_delivery_share_files_instantly/\"> <img src=\"https://external-preview.redd.it/l6dLM6ME8Ugc0k_jHN6RKXqBC2cY4Ix19-O_NobSrlo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=96575e3b2e46894a69a7aa991c4344e6dd7141c3\" alt=\"I built Air Delivery \u2013 Share files instantly. private, fast, free. ACROSS ALL DEVICES\" title=\"I built Air Delivery \u2013 Share files instantly. private, fast, free. ACROSS ALL DEVICES\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nothing-counts\"> /u/nothing-counts </a> <br/> <span><a href=\"https://airdelivery.site\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfhfm6/i_built_air_delivery_share_files_instantly/\">[comments]</a></span> </td></tr></table>",
        "id": 2972651,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfhfm6/i_built_air_delivery_share_files_instantly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/l6dLM6ME8Ugc0k_jHN6RKXqBC2cY4Ix19-O_NobSrlo.png?width=640&crop=smart&auto=webp&s=96575e3b2e46894a69a7aa991c4344e6dd7141c3",
        "title": "I built Air Delivery \u2013 Share files instantly. private, fast, free. ACROSS ALL DEVICES",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/grn_frog",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T18:02:15.215184+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T17:38:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to dig through the mountains of different options and suggestions hasn&#39;t provided a straightforward answer.</p> <p>I do hobby photography and am looking for recommendations for photo backup. </p> <p>I have a MacBook Air running Lightroom, and I import all my photos to an external drive that I work off of. I have a separate NVMe M.2 external drive that I wanted to use purely as a backup device for my photos. Ideally, I&#39;d like it to automatically back up my external drive containing my photos and Lightroom catalog once a month. From doing some reading, people have recommended ChronoSync and Carbon Copy Cloner for this use case. I&#39;ve read and gotten into the weeds regarding NAS setups and the 3-2-1 strategy, but for now I just wanna get a simple hard copy reliable backup going so I can feel comfortable deleting the photos off my camera memory card (which is my current second copy of most photos).</p> <p>Should I buy CCC or ChronoSync, ",
        "id": 2972652,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfgy01/photoshop_backup_external_to_external",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Photoshop Backup External to External",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bufferOverflowCanuck",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T18:02:15.378796+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T17:27:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all,</p> <p>I am moving cross country and packing up the apartment into a moving truck and driving 10.5 hours to the new place.</p> <p>What is the best way to transport 3.5 HDDs in my NAS so they dont get damaged?</p> <p>Moving trucks dont have much support and everything feels the vibrations more than your average car on the road. is there a case, trunk, or box that i can purchase off amazon or something that will help keep my data safe in transport?</p> <p>I&#39;ll try and put the most important stuff on the cloud BUT realistically i dont have close to enough cloud storage for everything</p> <p>tips, tricks, products all welcome!</p> <p>EDIT: for reference I have a QNAP TS-453D , not an enterprise rack. im a baby data horder. 28TB in the NAS , with additional 24TB in my tower. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bufferOverflowCanuck\"> /u/bufferOverflowCanuck </a> <br/> <span><a href=\"https://ww",
        "id": 2972653,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfgo56/moving_hard_drives_in_moving_truck",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Moving hard drives in moving truck",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/scampy008",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T16:57:15.215819+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T16:18:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Guys,</p> <p>Looking for simple cheap 6G SAS raid controller that can load / be seen / configured in UEFI mode.</p> <p>I don&#39;t mind JBOD or Raid (better)</p> <p>It seems hard to tell which cards cab to be recognised in UEFI, I tried a couple of LSI cards, but they only seem to be picked up in Legacy Mode, when I disable Sata drives and enable Legacy Roms in UEFI mode the cards come up with driver health &quot;Failed&quot; and configuration required.</p> <p>Any ideas or help would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/scampy008\"> /u/scampy008 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfexap/looking_for_pcie_sas_raid_controllers_that_will/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfexap/looking_for_pcie_sas_raid_controllers_that_will/\">[comments]</a></span>",
        "id": 2972144,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfexap/looking_for_pcie_sas_raid_controllers_that_will",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for PCIe SAS raid controllers that will work in UEFI mode",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/T0biasCZE",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T16:57:15.378882+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T15:51:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>On my NAS/server, i had a small 128GB NVMe ssd, on which i just had some VMs and docker image... I accidentelly overfilled the ssd, and after server restart, the xfs file system got corrupted and its not being mounted anymore (I am getting kernel error in syslog :|)<br/> Is there some free software that could manually scan the drive and try to recover the files? I found ReclaiMe, and its finding the files, but it costs 120\u20ac for the licence, which is a lot...<br/> Is there some free software that could do this?</p> <p>Alternatively, is there some software that could repair the xfs file table? (xfs_repair command doesnt work)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/T0biasCZE\"> /u/T0biasCZE </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfe8hy/free_xfs_recovery_tool/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfe8hy/free_xfs_recovery_tool/",
        "id": 2972145,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfe8hy/free_xfs_recovery_tool",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "free xfs recovery tool?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Think_Theory_8338",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T15:50:46.947617+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T15:42:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfe0s3/anyway_i_can_get_a_pdf_of_this/\"> <img src=\"https://preview.redd.it/omhvc58emw7f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e5d5aff50946f96a42a25323be0c472b961b7cb\" alt=\"Anyway I can get a pdf of this?\" title=\"Anyway I can get a pdf of this?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Think_Theory_8338\"> /u/Think_Theory_8338 </a> <br/> <span><a href=\"https://i.redd.it/omhvc58emw7f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfe0s3/anyway_i_can_get_a_pdf_of_this/\">[comments]</a></span> </td></tr></table>",
        "id": 2971639,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfe0s3/anyway_i_can_get_a_pdf_of_this",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/omhvc58emw7f1.png?width=640&crop=smart&auto=webp&s=8e5d5aff50946f96a42a25323be0c472b961b7cb",
        "title": "Anyway I can get a pdf of this?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BadWi-Fi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T14:46:52.543945+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T14:00:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought a bunch of Ritek DVDs ( not blu ray). How do I know that this are genuine. Also, can someone tell how to see an MID of a disc?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BadWi-Fi\"> /u/BadWi-Fi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfbj9c/how_do_i_see_if_my_ritek_mdiscs_are_real/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lfbj9c/how_do_i_see_if_my_ritek_mdiscs_are_real/\">[comments]</a></span>",
        "id": 2971024,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfbj9c/how_do_i_see_if_my_ritek_mdiscs_are_real",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I see if my ritek M-Discs are real?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/n3IVI0",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T13:40:36.908938+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T13:30:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have for years been downloading comics from <a href=\"http://GoComics.com\">GoComics.com</a> via wget. Recently, they have made changes to the website that have killed my handy bash script. They seem to be hiding the main comic of the day behind a javascript loader. I&#39;ll use Sherman&#39;s Lagoon as an example.</p> <p>wget -E -H -k -K -p -nd -R html,svg,gif,css,jpg,jpeg,png,js,json,ico -P &lt;directory of choice&gt; -T 5 -t 1 -e robots=off --http-user=USER -U &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0&quot; --referer=&quot;<a href=\"https://gocomics.com\">https://gocomics.com</a>&quot; <a href=\"https://www.gocomics.com/shermanslagoon/$(date\">https://www.gocomics.com/shermanslagoon/$(date</a> +%Y)/$(date +%m)/$(date +%d)</p> <p>This will download the old comics down below, but not the latest comic being displayed by the Viewer up top. Can anybody figure out how to get w",
        "id": 2970147,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfav9u/overcoming_gocomics_obfuscation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Overcoming GoComics Obfuscation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Any_Bandicoot9863",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T15:50:47.261079+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T13:20:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I don\u2019t usually post like this, but I\u2019m honestly at a breaking point. About a week ago, I reported a very serious and time-sensitive issue to the Internet Archive. I\u2019ve done everything I can think of:</p> <ul> <li>Emailed them (<a href=\"mailto:info@archive.org\">info@archive.org</a>)</li> <li>Submitted their official contact form</li> <li>Opened a polite GitHub issue</li> <li>Even tweeted at them</li> </ul> <p>But I haven\u2019t received a single reply. Not even a confirmation.</p> <p>I know they\u2019re a nonprofit. I know they\u2019re overloaded. I understand all of that \u2014 I really do. But this isn\u2019t a casual bug report or a link fix. It\u2019s something that could be deeply harmful if ignored. And for a week I\u2019ve felt everything: scared, stressed, angry, numb. I keep refreshing my inbox hoping someone \u2014 anyone \u2014 will acknowledge it.</p> <p>I\u2019m not posting any sensitive details here, just asking:</p> <p><strong>Has anyone here ever gotten a real resp",
        "id": 2971640,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfanwm/spent_a_week_trying_to_reach_the_internet_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Spent a week trying to reach the Internet Archive \u2014 still no response. What can I do?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Any_Bandicoot9863",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T16:57:15.766654+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T13:14:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019ve been trying to get the Internet Archive\u2019s attention about a <strong>serious and time-sensitive issue</strong> I reported over a week ago. I\u2019ve done everything I can think of:</p> <ul> <li>Emailed them at <code>info@archive.org</code></li> <li>Submitted the official contact form</li> <li>Posted a polite GitHub issue</li> <li>Even tweeted at @internetarchive</li> </ul> <p>Still no reply.</p> <p>I know they\u2019re a nonprofit and probably flooded with requests, but this isn&#39;t just a normal takedown or technical bug \u2014 it&#39;s something that really <strong>needs human eyes ASAP</strong>. The silence has been honestly overwhelming \u2014 I\u2019ve gone from stressed to anxious to just plain frustrated.</p> <p>Has anyone here had luck getting a faster response from them?<br/> Maybe a backchannel, an active team member, or even a time of day they\u2019re more likely to reply?</p> <p>Any advice would mean a lot. \ud83d\ude4f<br/> Thanks for reading \u2014 and for ",
        "id": 2972146,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lfaj90/spent_a_week_trying_to_reach_the_internet_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Spent a week trying to reach the Internet Archive \u2014 still no response. What can I do?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dunnno",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T11:13:36.149327+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T10:39:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, because of DOGE (i think), the twitter account of the Mars robot is going to go down in the coming few weeks.</p> <p>I tried to teach myself and use scripts and stuff i could come across here and on the internet but unfortunately it&#39;s beyond my comprehension. I tried some softs as well that doesn&#39;t work that much and the others are not free.</p> <p>If someone is more intelligent than me and willing to preserve a full archicve of the timeline (preferably like a singlefile.html or something browsable), i&#39;d more very grateful :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dunnno\"> /u/dunnno </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf7ler/nasapersevere_x_account_is_gonna_go_down/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf7ler/nasapersevere_x_account_is_gonna_go_down/\">[comments]</a></span>",
        "id": 2969636,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf7ler/nasapersevere_x_account_is_gonna_go_down",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NASAPersevere X account is gonna go down",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/eymo-1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T11:13:36.721486+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T10:18:35+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf79ay/which_ssd_is_the_best/\"> <img src=\"https://external-preview.redd.it/tHecoW2kURGAHM_Lsnq9pDS9fwUiej9wOtxIWXyU45w.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4299b2e2db7a030186dfddd3d5322b5c42f45d0\" alt=\"which SSD is the best ?\" title=\"which SSD is the best ?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I was wondering which 1tb SSD is better, these are my options due to my budget 3000 EGP (70 USD)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/eymo-1\"> /u/eymo-1 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1lf79ay\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf79ay/which_ssd_is_the_best/\">[comments]</a></span> </td></tr></table>",
        "id": 2969637,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf79ay/which_ssd_is_the_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/tHecoW2kURGAHM_Lsnq9pDS9fwUiej9wOtxIWXyU45w.jpeg?width=640&crop=smart&auto=webp&s=d4299b2e2db7a030186dfddd3d5322b5c42f45d0",
        "title": "which SSD is the best ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Reasonable_Brief578",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T09:04:16.162019+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T08:59:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf62ip/localalbum_a_simple_selfhosted_photo_album_for/\"> <img src=\"https://external-preview.redd.it/B1BzWKjdJTFAequP1scDU_scSCdYVrujbJu8h14Budw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07a012e85cd773107489b7dc09149a48d9d7e5d5\" alt=\"\ud83d\uddbc\ufe0f LocalAlbum \u2013 A Simple, Self-Hosted Photo Album for Browsing Local Media\" title=\"\ud83d\uddbc\ufe0f LocalAlbum \u2013 A Simple, Self-Hosted Photo Album for Browsing Local Media\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone! \ud83d\udc4b</p> <p>I\u2019d like to share a small open-source project I built called <a href=\"https://github.com/Laszlobeer/localalbum\"><strong>LocalAlbum</strong></a> \u2014 a simple desktop app that lets you easily browse local photo and video collections using your default browser.</p> <p><a href=\"https://reddit.com/link/1lf62ip/video/os1sriphmu7f1/player\">https://reddit.com/link/1lf62ip/video/os1sriphmu7f1/player</a></p> <p>preview</p> <h1>\ud83d\udd27 What it does:</h1> <ul> <li>\ud83d\udcc2 S",
        "id": 2968895,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf62ip/localalbum_a_simple_selfhosted_photo_album_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/B1BzWKjdJTFAequP1scDU_scSCdYVrujbJu8h14Budw.png?width=640&crop=smart&auto=webp&s=07a012e85cd773107489b7dc09149a48d9d7e5d5",
        "title": "\ud83d\uddbc\ufe0f LocalAlbum \u2013 A Simple, Self-Hosted Photo Album for Browsing Local Media",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DRONE_SIC",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T09:04:16.444864+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T08:47:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf5wfi/airbnb_chrome_extension_to_1_build_your_own_db_of/\"> <img src=\"https://external-preview.redd.it/OWJvdnpyMDZndTdmMdcNnXimUPb154APUZ_HkDgnsFh1oE8z0_V9Z748y_eO.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af9671f2350357664368912d13b610cfbb9b6f7d\" alt=\"AirBnB Chrome Extension to #1 build your own DB of detailed listing data, and #2 get pricing &amp; occupancy stats from the source itself (replacing external-products like AirDNA, Rabbu, etc.)\" title=\"AirBnB Chrome Extension to #1 build your own DB of detailed listing data, and #2 get pricing &amp; occupancy stats from the source itself (replacing external-products like AirDNA, Rabbu, etc.)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hoard your area&#39;s Airbnb market data with this Chrome extension, directly on Airbnb itself.</p> <p>I made this and think it provides a lot of value to the right people, hopefully this is allowed here since data-hoar",
        "id": 2968896,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf5wfi/airbnb_chrome_extension_to_1_build_your_own_db_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/OWJvdnpyMDZndTdmMdcNnXimUPb154APUZ_HkDgnsFh1oE8z0_V9Z748y_eO.png?width=640&crop=smart&auto=webp&s=af9671f2350357664368912d13b610cfbb9b6f7d",
        "title": "AirBnB Chrome Extension to #1 build your own DB of detailed listing data, and #2 get pricing & occupancy stats from the source itself (replacing external-products like AirDNA, Rabbu, etc.)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TopConnection2030",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T09:04:16.609940+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T08:10:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey,</p> <p>I&#39;m looking into upgrading my personal NAS. Probably 8-16TB. But I&#39;m not sure which of the drives to get. I also thought about getting different series, to avoid manufacturing defects. </p> <p>And can I buy used server drives? There are plenty of EXOS e.g. drives for good prices in &quot;perfect condition&quot; out there.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TopConnection2030\"> /u/TopConnection2030 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf5ccv/ironwolf_pro_vs_exos_vs_red_pro_used_okay_as_well/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf5ccv/ironwolf_pro_vs_exos_vs_red_pro_used_okay_as_well/\">[comments]</a></span>",
        "id": 2968897,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf5ccv/ironwolf_pro_vs_exos_vs_red_pro_used_okay_as_well",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ironwolf Pro vs Exos vs Red Pro (used okay as well?)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/xTHEFLASH0504x",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T07:58:40.011109+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T07:43:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a good amount of JAVs, is there any reliable way to auto download subs for all of them or will I have to manually download ALL subs and embed them myself, idm doing the embedding.</p> <p>Even if 80% of them are correct, thats good, the remaining i can do on my own</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/xTHEFLASH0504x\"> /u/xTHEFLASH0504x </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf4xhy/mass_download_subtitles_for_jav/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf4xhy/mass_download_subtitles_for_jav/\">[comments]</a></span>",
        "id": 2968591,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf4xhy/mass_download_subtitles_for_jav",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mass download subtitles for JAV",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheRealHarrypm",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T07:58:40.367054+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T07:18:03+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf4jym/ltfs_manager_a_human_usable_gui_for_ltfs_on_linux/\"> <img src=\"https://b.thumbs.redditmedia.com/Jr9ouU4Exd8DZ39WEkWwdPTYAgp2JxBgr4ygAccVV_g.jpg\" alt=\"LTFS Manager - A human usable GUI for LTFS on Linux\" title=\"LTFS Manager - A human usable GUI for LTFS on Linux\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheRealHarrypm\"> /u/TheRealHarrypm </a> <br/> <span><a href=\"/r/lto/comments/1lerzh9/ltfs_manager_a_human_usable_gui_for_ltfs_on_linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf4jym/ltfs_manager_a_human_usable_gui_for_ltfs_on_linux/\">[comments]</a></span> </td></tr></table>",
        "id": 2968592,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf4jym/ltfs_manager_a_human_usable_gui_for_ltfs_on_linux",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/Jr9ouU4Exd8DZ39WEkWwdPTYAgp2JxBgr4ygAccVV_g.jpg",
        "title": "LTFS Manager - A human usable GUI for LTFS on Linux",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ezio-Trilogy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T05:49:07.042719+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T05:12:43+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf2kk1/can_a_genericunbranded_power_switch_like_this_be/\"> <img src=\"https://preview.redd.it/ptsdiueeft7f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=011f112c2ef6ecddd0f66a6584fca8543a92de3b\" alt=\"Can a generic/unbranded power switch like this be trusted?\" title=\"Can a generic/unbranded power switch like this be trusted?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>There&#39;s not much of a market for these products so it&#39;s either one of these cheap ones from China for $15-20 or the Kingwin brand for the marked up price of $65-70 in Australia.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ezio-Trilogy\"> /u/Ezio-Trilogy </a> <br/> <span><a href=\"https://i.redd.it/ptsdiueeft7f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf2kk1/can_a_genericunbranded_power_switch_like_this_be/\">[comments]</a></span> </td></tr><",
        "id": 2968103,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf2kk1/can_a_genericunbranded_power_switch_like_this_be",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/ptsdiueeft7f1.png?width=640&crop=smart&auto=webp&s=011f112c2ef6ecddd0f66a6584fca8543a92de3b",
        "title": "Can a generic/unbranded power switch like this be trusted?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Electrical-Reveal-25",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T05:49:07.213105+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T05:00:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Chat GPT recommended Raid 6 and Raid 10.</p> <ul> <li><p>Also, could I use a raid array like this on top of Windows on my PC, or would I need to install a different OS to do that? </p></li> <li><p>What software could I use to accomplish this goal?</p></li> <li><p>Would a setup like this (4 hard drives in an old PC - Dell Optiplex to be specific) be as good as a dedicated synology NAS?</p></li> </ul> <p>Thanks for reading!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electrical-Reveal-25\"> /u/Electrical-Reveal-25 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf2dgj/sorry_if_this_is_a_dumb_question_what_raid_array/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf2dgj/sorry_if_this_is_a_dumb_question_what_raid_array/\">[comments]</a></span>",
        "id": 2968104,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf2dgj/sorry_if_this_is_a_dumb_question_what_raid_array",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sorry if this is a dumb question: what raid array should I use if I want to use 4 HDD and have it set so that if 2 drives fail, the other 2 will still have everything?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thepeussybusta",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T04:43:44.801619+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T04:10:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf1i6x/how_risky_would_using_a_cable_like_this_be/\"> <img src=\"https://preview.redd.it/sr268pu17t7f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfcc90fa6a02f66d79543e32d689c316c19a1ec8\" alt=\"how risky would using a cable like this be?\" title=\"how risky would using a cable like this be?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>i recently stuck a great deal on 10 2.5in 1tb seagate sas drives for $25 free shipping. (they accidentally sent me 11). im trying to get rid of all my super old power hungry drives and replace them with something more power efficient. these drives fit the bill with a operating power draw of 5.9w. the 11 drives theoretically would draw a total of about 65w. i was getting conflicting results on how much a sata power cable can handle so i turned to here to ask. i had planned on using hot swap 5.25in racks but most of them don&#39;t support 15mm thick drives and are super expen",
        "id": 2967880,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf1i6x/how_risky_would_using_a_cable_like_this_be",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/sr268pu17t7f1.png?width=640&crop=smart&auto=webp&s=bfcc90fa6a02f66d79543e32d689c316c19a1ec8",
        "title": "how risky would using a cable like this be?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/blueberry1997",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T04:43:45.080272+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T03:49:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve owned many optical drives over the years with no issues, but they do get tired of course. I had this old Pioneer IDE drive in a factory USB 2.0 enclosure that was my go-to for reading troubled or damaged discs that my little USB (bus powered) Samsung BD-R writer (which is not a great unit) or HL-DT-ST DVD writer (out of a 2012 MacBook and in a slim USB enclosure -- is usually pretty good but also getting tired) wouldn&#39;t play nice with, but it no longer functions properly. </p> <p>I primarily write audio CD-R&#39;s. but more importantly I care about making very precise backups of aging optical media, both audio and data discs, for some of my day-to-day work. I am often working with discs from the late 90s or early 2000s with various degrees of functionality. </p> <p>Do good, heavy duty drives still exist? Are there any &quot;new old stock&quot; options that are worth pursuing? </p> <p>Also, I know that SATA / internal PC tower drives at le",
        "id": 2967881,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf14pn/strong_optical_drives_for_archiving_old_discs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Strong optical drives for archiving old discs (Audio CD, data DVD-R, etc) recommendations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AnAverageASEANguy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T03:38:04.850767+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T03:31:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Yesterday I received a termination notice on my Microsoft dev OneDrive account that has a ton of videos and other files that I have been loading up over the course of 4-5 years. Estimated storage usage is around 6TB. Mulling over either 1) buying a HDD and offloading everything locally and wait to upload it to another host; or 2) finding an online solution that handles moving between two different tenants. What would be your advised course of action for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AnAverageASEANguy\"> /u/AnAverageASEANguy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf0scq/my_microsoft_dev_account_has_expired_need_a_quick/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lf0scq/my_microsoft_dev_account_has_expired_need_a_quick/\">[comments]</a></span>",
        "id": 2967663,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lf0scq/my_microsoft_dev_account_has_expired_need_a_quick",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My Microsoft Dev account has expired. Need a quick way to move files.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/KingSupernova",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T01:28:05.150482+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T01:01:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Mojang is deleting its old bug-tracker, which includes all of my years of reports and comments. I am unhappy about this, and would like to archive everything on the old site before it goes down. Unfortunately there does not appear to be any native way to export my data, and even if there were, this would not include the rest of the threads in which I made those comments, making them difficult to understand.</p> <p>The old site is still up at <a href=\"http://bugs-legacy.mojang.com/\">bugs-legacy.mojang.com</a>. (Requires a login to see, but anyone who had an account on it back when it was active can log in.) But I don&#39;t know how much longer it&#39;s going to exist.</p> <p>Does anyone know of a tool that can go through all the pages and download them such that I can save and view the site locally? Or an API that would give me access?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KingSupernova\"> /u/KingSupernov",
        "id": 2967246,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lexuag/archive_jira_instance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Archive JIRA instance",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/k198420",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-19T01:28:05.318836+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-19T00:41:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>HI. Does anyone know how I could download videos from Xumo Play? They have seasons of The Bachelor that I&#39;d like to save. Here is the link - <a href=\"https://play.xumo.com/tv-shows/the-bachelor/XM0X0UJWXK9ZBZ\">https://play.xumo.com/tv-shows/the-bachelor/XM0X0UJWXK9ZBZ</a></p> <p>I tried Internet Download Manager &amp; it just downloaded a episode with no audio or video. Thank You for your help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/k198420\"> /u/k198420 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lexf8l/how_to_save_videos_from_xumo_play/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lexf8l/how_to_save_videos_from_xumo_play/\">[comments]</a></span>",
        "id": 2967247,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lexf8l/how_to_save_videos_from_xumo_play",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How To Save Videos From Xumo Play?",
        "vote": 0
    }
]