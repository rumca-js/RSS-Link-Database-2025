[
    {
        "age": null,
        "album": "",
        "author": "/u/Snoo14860",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T21:57:44.198384+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T21:26:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have several scripts that either scrape websites or make API calls, and they write the data to a database. These scripts run mostly 24/7. Currently, I run each script inside a separate Docker container. This setup helps me monitor if they\u2019re working properly, view logs, and manage them individually.</p> <p>However, I&#39;m planning to expand the number of scripts I run, and I feel like using containers is starting to become more of a hassle than a benefit. Even with Docker Compose, making small changes like editing a single line of code can be a pain, as updating the container isn&#39;t fast.</p> <p>I&#39;m looking for software that can help me manage multiple always-running scripts, ideally with a GUI where I can see their status and view their logs. Bonus points if it includes an integrated editor or at least makes it easy to edit the code. The software itself should be able to run inside a container since im self hosting on Truenas.</p> <p>does a",
        "id": 2927329,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1larm1k/how_do_you_manage_your_scraping_scripts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you manage your scraping scripts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/albert_in_vine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T20:47:37.653430+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T20:11:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>In my recent projects, I tried to gather data from lowes using various methods, from straightforward web scraping to making API calls. However, I&#39;m quite frustrated by the strict rate limits they enforce. I have used different types of proxies, including datacenter, ISP, and even residential proxies, but they still block me almost immediately. It&#39;s really driving me crazy!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/albert_in_vine\"> /u/albert_in_vine </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1laptyz/has_anyone_tried_to_get_data_from_lowes_recently/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1laptyz/has_anyone_tried_to_get_data_from_lowes_recently/\">[comments]</a></span>",
        "id": 2926899,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1laptyz/has_anyone_tried_to_get_data_from_lowes_recently",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone tried to get data from Lowes recently?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dracariz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T18:37:43.393580+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T18:29:55+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1landye/playwrightbased_browsers_stealth_performance/\"> <img src=\"https://b.thumbs.redditmedia.com/UIX1ttsSJDOQCfCMAkrNuoQ5SCNdniaeK1qQ6G6Ct4U.jpg\" alt=\"Playwright-based browsers stealth &amp; performance benchmark (visual)\" title=\"Playwright-based browsers stealth &amp; performance benchmark (visual)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I built a benchmarking tool for comparing browser automation engines on their ability to bypass bot detection systems and performance metrics. It shows that camoufox is the best.</p> <p>Don&#39;t want to share the code for now (legal reasons), but can share some of the summary:</p> <p><a href=\"https://preview.redd.it/bybdoc79mq6f1.png?width=5964&amp;format=png&amp;auto=webp&amp;s=b39a8e82026de6bf27f37c42a1b380d0ab884fce\">https://preview.redd.it/bybdoc79mq6f1.png?width=5964&amp;format=png&amp;auto=webp&amp;s=b39a8e82026de6bf27f37c42a1b380d0ab884fce</a></p> <p><a hre",
        "id": 2926041,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1landye/playwrightbased_browsers_stealth_performance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/UIX1ttsSJDOQCfCMAkrNuoQ5SCNdniaeK1qQ6G6Ct4U.jpg",
        "title": "Playwright-based browsers stealth & performance benchmark (visual)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/scraping_bye",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T23:02:50.584837+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T18:12:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I used a variety of AI tools to create some python code that will check for valid service addresses from a specific website. It kicks it into a csv file and it works kind of like McBroken to check for validity. I already had a list of every address in a csv file that I was looking to check. The code takes about 1.5 minutes to work through the website, and determine validity by using wait times and clicking all the necessary boxes. This means I can check about 950 addresses in a 24 hour period. </p> <p>I made several copies of my code in seperate folders with seperate address lists and am running them simultaniously. So I can now check about 3,000 in 24 hours. </p> <p>I imagine that this website has ample capacity to handle these requests as it\u2019s a large company, but I\u2019m just not sure if this counts as a DDOS, which I am obviously trying to avoid. With that said, do you think I could run 5 version? 10? 15? At what point would it be a DDOS?</p> </div><!",
        "id": 2927612,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lamygg/new_to_scraping_trying_to_avoid_ddos_guidance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New to scraping - trying to avoid DDOS? Guidance needed.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Juicy-J23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T17:32:38.103037+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T17:29:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to pull the data from the tables on these particular urls above and when I inspected the team hitting/pitching urls it seems to be contained in the class = &quot;stats-body-table team&quot;. When i print stats_table i get &quot;None&quot; as the results.</p> <p>code below, any advice?</p> <pre><code>#mlb web scrape for historical team data from bs4 import BeautifulSoup import selenium from selenium import webdriver from selenium.webdriver.chrome.options import Options import pandas as pd import numpy as np #function to scrape website with URL param #returns parsed html def get_soup(URL): #enable chrome options options = Options() options.add_argument(&#39;--headless=new&#39;) driver = webdriver.Chrome(options=options) driver.get(URL) #get page source html = driver.page_source #close driver for webpage driver.quit soup = BeautifulSoup(html, &#39;html.parser&#39;) return soup def get_stats(soup): stats_table = soup.find(&#39;div&#39;, attr={",
        "id": 2925541,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1laluzm/web_scrape_mlb_data_using_beautiful_soup_question",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "web scrape mlb data using beautiful soup question",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bornlex",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T16:26:45.899927+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T15:54:44+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1lajhmu/new_headless_browser_in_town/\"> <img src=\"https://preview.redd.it/2p42xhr4vp6f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa091b258534de3318cb67095da7700e518da31a\" alt=\"New headless browser in town\" title=\"New headless browser in town\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>Have you heard about that Lightpanda browser: <a href=\"https://www.youtube.com/watch?v=lak-DDjuaQU\">https://www.youtube.com/watch?v=lak-DDjuaQU</a> ?<br/> The browser has apparently been entirely written from scratch, in Zig.</p> <p>Sounds like it works with Puppeteer at least, or as a cli tool as explained on their website: <a href=\"https://lightpanda.io\">https://lightpanda.io</a></p> <p>Has someone any feedback?</p> <p>Cheers!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bornlex\"> /u/bornlex </a> <br/> <span><a href=\"https://i.redd.it/2p42xhr4vp6f1.png\">[li",
        "id": 2924954,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lajhmu/new_headless_browser_in_town",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/2p42xhr4vp6f1.png?width=640&crop=smart&auto=webp&s=aa091b258534de3318cb67095da7700e518da31a",
        "title": "New headless browser in town",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Patient-Twist5",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T17:32:38.314363+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T15:23:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Summary: Hello! I&#39;m really new to webscraping, and I am scraping a grocery store&#39;s product catalogue. Right now, for the sake of speed, I am scraping based on back-end API calls that I reverse-engineered, but I am running into an issue of being unable to scrape the entire catalogue due to pagination not displaying products past a certain internal limit. Would anyone happen to have faced a similar issue or know alternatives I can take to scraping a grocery chain&#39;s entire product catalogue? Thank you.</p> <p>Relevant Technical Details/More Detailed Explanation: I am using Scrapling and camoufox in order to automate some necessary configurations such as zipcode setting. If required, I scrape the website&#39;s HTML to find out things like category names/ids in order to set up a format to spam API calls by category. The API calls that I&#39;m dealing with primarily paginate by start (where in the internal database the API starts collecting data",
        "id": 2925542,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1laiq02/need_help_for_scraping_a_grocery_store",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need Help for Scraping a Grocery Store",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mickspillane",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T15:22:38.146856+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T15:11:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a feeling my target site is doing some machine learning on my request pattern to block my account after I successfully make ~2K requests over a span of a few days. They have the resources to do something like this.</p> <p>Some basic tactics I have tried are:</p> <p>- sleep a random time between requests<br/> - exponential backoff on errors which are rare<br/> - scrape everything i need to during an 8 hr window and be quiet for the rest of the day</p> <p>Some things I plan to try:</p> <p>- instead of directly requesting the page that has my content, work up to it from the homepage like a human would</p> <p>Any other tactics people use to make their request patterns more human like?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mickspillane\"> /u/mickspillane </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1laiew3/strategies_to_make_your_request_pattern_appear/\">[link]</a></span> &#3",
        "id": 2924383,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1laiew3/strategies_to_make_your_request_pattern_appear",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Strategies to make your request pattern appear more human like?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/delusionk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T12:05:25.542709+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T10:58:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>I\u2019m trying to automate ChatGPT via browser flows using Playwright (Python) in CLI mode because I can\u2019t afford an OpenAI API key. But Cloudflare challenges are blocking my script.</strong></p> <p><strong>I\u2019ve tried:</strong></p> <ul> <li>headful vs headless</li> <li> custom User-Agent</li> <li> playwright-stealth</li> <li> random waits</li> <li>cookies</li> </ul> <p><strong>Seeking:</strong></p> <ul> <li>fast, reliable solutions</li> <li>proxies or real-browser workarounds</li> <li>CLI-specific advice</li> <li>seeking bypass solutions</li> </ul> <p><strong>Thanks in advance!</strong></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/delusionk\"> /u/delusionk </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lacz78/cloudflare_blocking_browserautomated_chatgpt_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1lacz78/cloudflare_blocking_browseraut",
        "id": 2922676,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lacz78/cloudflare_blocking_browserautomated_chatgpt_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cloudflare blocking browser-automated ChatGPT with Playwright",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Comfortable-Ant-3250",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T10:58:41.286592+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T09:42:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My Selenium Python script scrapes SofaScore API perfectly on my local machine but throws 403 &quot;challenge&quot; errors on Ubuntu server. Same exact code, different results. Local gets JSON data, server gets <code>{ error: { code: 403, reason: &#39;challenge&#39; } }</code>. Tried headless Chrome, user agents, delays, visiting main site first, installing dependencies. Works fine locally with GUI Chrome but fails in headless server environment. Is this IP blocking, fingerprinting, or headless detection? Need solution for server deployment. Code: standard Selenium with <code>--headless --no-sandbox --disable-dev-shm-usage</code> flags.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Comfortable-Ant-3250\"> /u/Comfortable-Ant-3250 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1labsk6/selenium_works_locally_but_403_on_server/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/",
        "id": 2921992,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1labsk6/selenium_works_locally_but_403_on_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Selenium works locally but 403 on server - SofaScore scraping issue",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GuitarAppropriate489",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-13T10:58:41.451727+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-13T07:40:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building a Discord bot that fetches Reels views and updates a database every 2 hours. The bot needs to process 1000+ Reels, but I&#39;m encountering blocking issues. Would using proxies be an effective solution?</p> <p>Can anyone help me with this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GuitarAppropriate489\"> /u/GuitarAppropriate489 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1laa1jg/reel_scraping_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1laa1jg/reel_scraping_help/\">[comments]</a></span>",
        "id": 2921993,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1laa1jg/reel_scraping_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Reel scraping ! Help",
        "vote": 0
    }
]