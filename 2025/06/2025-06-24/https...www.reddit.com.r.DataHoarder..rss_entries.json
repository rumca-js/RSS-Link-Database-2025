[
    {
        "age": null,
        "album": "",
        "author": "/u/Fuzzy-Zone-5535",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-25T00:07:00.363635+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T23:34:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>YT-dlp doesn&#39;t work, I don&#39;t think it&#39;s DRM-protected so it should be possibly to download those videos somehow, but how? I don&#39;t think torrents of those shows exist as I&#39;ve searched. Is my only solution to screenrecord the videos?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fuzzy-Zone-5535\"> /u/Fuzzy-Zone-5535 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljqk51/how_to_download_a_video_from_a_piracy_streaming/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljqk51/how_to_download_a_video_from_a_piracy_streaming/\">[comments]</a></span>",
        "id": 3011739,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljqk51/how_to_download_a_video_from_a_piracy_streaming",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download a video from a piracy streaming site?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SENSUAL_WATERMELON",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T23:01:48.793159+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T22:18:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There are some old series I used to watch, that have been taken down, and the only choice now is to get them on Amazon Video. They&#39;re not even on Amazon Prime Video. All the torrents are dead since long, and I can&#39;t find it anywhere else.</p> <p>Can I somehow extract them from Amazon Video, or do I have to go on a hunt for someone who has a physical copy (which is rare)?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SENSUAL_WATERMELON\"> /u/SENSUAL_WATERMELON </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljoshq/old_series_only_available_for_purchase_on_amazon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljoshq/old_series_only_available_for_purchase_on_amazon/\">[comments]</a></span>",
        "id": 3011408,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljoshq/old_series_only_available_for_purchase_on_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Old series only available for \"purchase\" on Amazon Video, torrents are dead since ages, and I'd like to get them before they disappear for good",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hellishdelusion",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T23:01:48.996464+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T22:00:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried 5 or 6 downloaders for old twitch vods (about 5 years old) and they didn&#39;t seem to work. Does anyone know a good tool for this?</p> <p>Id like to preserve some vods from a content creator that since passed and I&#39;m not sure how long twitch will keep it up for. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hellishdelusion\"> /u/hellishdelusion </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljocgt/twitch_vod_downloader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljocgt/twitch_vod_downloader/\">[comments]</a></span>",
        "id": 3011409,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljocgt/twitch_vod_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Twitch vod downloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DandadanAsia",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T20:53:08.339430+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T19:50:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I plan to carry my NAS (Synology) and hard drives to another country. Is it safe? Will airport security check the contents of the hard drives? I have a lot of &quot;downloaded content&quot;.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DandadanAsia\"> /u/DandadanAsia </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljl2rr/traveling_abroad_with_a_nas_is_it_safe_and_what/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljl2rr/traveling_abroad_with_a_nas_is_it_safe_and_what/\">[comments]</a></span>",
        "id": 3010465,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljl2rr/traveling_abroad_with_a_nas_is_it_safe_and_what",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Traveling Abroad with a NAS: Is It Safe and What to Expect at Airport Security?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kwajagimp",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T19:47:11.252470+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T19:08:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Folks - so currently I have a 8 drive NAS with 12 TB drives with TrueNAS and a Z2 setup. </p> <p>I would like to upgrade the total storage (who ever has enough?) but I&#39;m not blessed with the kind of money where I can buy 8x 14TB (or larger) drives at one time. I&#39;ve bought cars for less than that!</p> <p>So let&#39;s say I buy one larger drive a month. (That would be doable on my budget.) I know that if you put in a larger drive into an array of smaller drives, the larger one will only use storage up to the capacity of the smaller drives.</p> <p>So what&#39;s a better call here - is there a way to introduce larger size drives one at a time until the only drives are all replaced, and somehow then rebuild the array to access all of the extra storage space on each of the drives?</p> <p>Or is it better to just acquire the drives and put them on a shelf until I have 8, then replace them all together, more or less? (Wouldn&#39;t this have the samw pr",
        "id": 3010084,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljjz7m/best_way_to_upgrade_drives_over_time",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to upgrade drives over time?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Traditional-Cream691",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T19:47:11.520666+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T18:49:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How many domains do you guys own? </p> <p>I started having ideas and buying domains for them like somebody was going to take them. </p> <p>I now have 33 domains. I should probably get rid of a few.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Traditional-Cream691\"> /u/Traditional-Cream691 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljjhkb/i_own_33_domains_and_i_use_10/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljjhkb/i_own_33_domains_and_i_use_10/\">[comments]</a></span>",
        "id": 3010085,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljjhkb/i_own_33_domains_and_i_use_10",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "i own 33 domains and I use 10",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ContributionHead9820",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T18:41:50.346467+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T17:57:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Basically the title. What\u2019s the best way to backup my iCloud Photos to a NAS? It\u2019d be nice If it could do it automatically like once a week. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ContributionHead9820\"> /u/ContributionHead9820 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lji37g/backup_icloud_photos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lji37g/backup_icloud_photos/\">[comments]</a></span>",
        "id": 3009492,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lji37g/backup_icloud_photos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup iCloud Photos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cr3eperboy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T18:41:50.555007+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T17:42:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I got a WD Elements Portable External Hard Drive 5TB to use just as storage for an arbitrary of things to store, that differ in file sizes. I also use Linux as my daily drive OS. ExFAT would be the best option right since it works with Linux, Mac and Windows. Where NTFS only works with Windows.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cr3eperboy\"> /u/Cr3eperboy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljhoxk/whats_the_best_format/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljhoxk/whats_the_best_format/\">[comments]</a></span>",
        "id": 3009493,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljhoxk/whats_the_best_format",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What's the best format?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Bloodrain_souleater",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T18:41:51.139142+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T17:41:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I want to have an anime collection of completed series from 2010 to 2017 but I cannot seem to find anything like that. I have to get everything one By one which is time consuming. So is there any way to get the entire collection from 2010 to 2017. Even if the anime is ongoing its okay as long as the seasons are completed.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Bloodrain_souleater\"> /u/Bloodrain_souleater </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljhnpi/any_way_to_get_entire_anime_collection_from/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljhnpi/any_way_to_get_entire_anime_collection_from/\">[comments]</a></span>",
        "id": 3009494,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljhnpi/any_way_to_get_entire_anime_collection_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any way to get entire anime collection from 2010-2017",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jason2306",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T17:36:50.846178+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T17:25:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there I would love some help since i&#39;m going insane trying to get the dnd map zip attachments I subscribed for from creators on patreon. I can&#39;t download this by hand one by one. But most patreon related extensions either seem broken or bad for other reasons like asking you to pay to even use it per creator or some other nonsense</p> <p>Patreon gui dl doesn&#39;t seem to download attachments in posts, it does download post previews but those are low res. How do you actually get the zip file attachments? Is this just not possible? Doing this manually would take a long time and would be a terrible experience, how do other people do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jason2306\"> /u/jason2306 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljh8u2/how_do_i_download_all_the_patreon_download/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoar",
        "id": 3008878,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljh8u2/how_do_i_download_all_the_patreon_download",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I download all the patreon download attachment zip files from a creator? Struggling with these options",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nbtm_sh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T16:31:51.991997+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T16:20:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Unsure if this is the best place to ask, but I\u2019m trying to organise DJ sets in a way that makes most sense and is searchable (especially for external users on platforms like Soulseek). My current music library is structured like this:</p> <p>Label-Publisher/Album Artist/Album/Track</p> <p>I\u2019m a little bit unsure what fields to put for DJ sets, and how would you handle concerts with many sets. The structure I\u2019ve come up with is this:</p> <p>Label-Publisher = Event Organiser</p> <p>Album Artist = DJ / Producer</p> <p>Album = Event / Concert (Date)</p> <p>Track = DJ Set name + queue file for set tracks</p> <p>But this kinda falls apart if I wanna listen to all DJ sets at a particular concert, as they\u2019re all different albums. If it\u2019s organised like that, and we take an example, it goes like this: <code>HARDCORE TANO*C/DJ Myosuke/DJ Myosuke Live @ TANO*C TOUR 2025 (Tokyo 2025-06-22)/Set.ext</code></p> <p>This fits cleanly into the music organisation, but o",
        "id": 3008330,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljfidl/how_to_organise_dj_sets",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to organise DJ sets?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/immascatman4242",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T16:31:52.196549+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T15:46:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I used a combo VHS/DVD deck about ten years ago to directly transfer some VHS tapes onto DVD-R, but I now no longer have access to that deck. Do the Philips DVP3150V or the Sony SLV-D370P have those recording capabilities? I&#39;ve looked over each respective manual, but I&#39;m new to this and am not fully sure of what to look for. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/immascatman4242\"> /u/immascatman4242 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljem3l/are_there_vhs_to_dvd_recording_capabilities_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljem3l/are_there_vhs_to_dvd_recording_capabilities_on/\">[comments]</a></span>",
        "id": 3008331,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljem3l/are_there_vhs_to_dvd_recording_capabilities_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are there VHS to DVD recording capabilities on the Philips DVP3150V or the Sony SLV-D370P?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Theunknown87",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T15:26:50.425219+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T15:00:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I have a 4Tb western digital NAS that I use data that I don\u2019t really care about but I don\u2019t want it taking up data on my main drives. </p> <p>I\u2019m only using 3.5tb of space. </p> <p>I am trying to reorganize everything and using all dupe, there is a ton of duplicates. </p> <p>Is there a dumbed down way that I could transfer the data from the NAS to my 8tb external drive so that I could get rid of most of the duplicates? </p> <p>Then because for that old NAS, I care more about space so I\u2019d reformat It and change it so it\u2019s not in raid and make it become 8tb. </p> <p>My most important data is already on my new synology and that whole thing is backed up to synology c2. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Theunknown87\"> /u/Theunknown87 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljddxo/transfer_data_to_external_drive_and_duplicates/\">[link]</a></span> &#32; <span><a href=\"h",
        "id": 3007575,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljddxo/transfer_data_to_external_drive_and_duplicates",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Transfer data to external drive and duplicates.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ope_poe",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T15:26:50.140600+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T14:42:52+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ope_poe\"> /u/ope_poe </a> <br/> <span><a href=\"https://nascompares.com/review/aoostar-wtr-max-nas-review-nas-perfection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljcxqv/aoostar_wtr_max_nas_review_nas_perfection/\">[comments]</a></span>",
        "id": 3007574,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljcxqv/aoostar_wtr_max_nas_review_nas_perfection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Aoostar WTR Max NAS Review \u2013 NAS Perfection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mdof2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T15:26:49.789674+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T14:30:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Not sure if this is /damnthatsinteresting or /datahorder, because I fear for those that take it upon themselves to archive this. 20TB every 24 hours, for the next decade. </p> <p>From the article: &quot;Rubin will generate a whopping 20 terabytes of data every 24 hours. The latest iPhone holds up to one terabyte of data.&quot;</p> <p>More here: <a href=\"https://www.wsj.com/science/space-astronomy/worlds-largest-digital-camera-snaps-its-first-photos-of-the-universe-68099904?st=1q5nHA&amp;mod=1440&amp;user_id=66c4c73d600ae15075a4db28\">https://www.wsj.com/science/space-astronomy/worlds-largest-digital-camera-snaps-its-first-photos-of-the-universe-68099904?st=1q5nHA&amp;mod=1440&amp;user_id=66c4c73d600ae15075a4db28</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mdof2\"> /u/mdof2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljcmdd/were_gonna_need_a_bigger_boat_worlds_largest/\">[link]</a",
        "id": 3007573,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljcmdd/were_gonna_need_a_bigger_boat_worlds_largest",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "We're gonna need a bigger boat. World\u2019s Largest Digital Camera Snaps Its First Photos of the Universe. Do we mirror a copy of it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Bewilderedfae",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T14:20:53.803338+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T13:55:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there a searchable yahoo answers yet? I remember seeing some people on here saying they were working on it a while ago. I was looking on the wayback machine but I don&#39;t even remember what category the post was in let alone the url. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Bewilderedfae\"> /u/Bewilderedfae </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljbqyt/searchable_yahoo_answers/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ljbqyt/searchable_yahoo_answers/\">[comments]</a></span>",
        "id": 3006964,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ljbqyt/searchable_yahoo_answers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Searchable Yahoo Answers?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jontarg12",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T11:07:11.982141+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T10:53:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi fellow hoarders,</p> <p>First time buying HDD expansion so need your help. My use case is to: 1. Stream 4K videos to other devices in the house 2. Seed torrents (Bluray REMUX) to private trackers (for about 12h/day for 2 weeks, then delete unless it&#39;s something I really like) 3. Flexibility: in a few years, if I upgrade to NAS or Unraid, the external HDD can be shucked or used as cold storage.</p> <p>My budget is flexible and I&#39;m looking for convenience and value-for-money. A bit adverse to subscription-based services so seedboxes etc. are a no-go. </p> <p>I have 3 questions:</p> <ol> <li><p>I&#39;m eyeing Seagate Expansion 10TB (STKP10000400) (206 eur) below since it seems like a guaranteed CMR drive according to my research. Is there any reason I should NOT get this drive (for my use case)? There&#39;s a 20TB for 355 eur which is better value-for-money but I heard that very big drives are louder and maybe more prone to failures, and I pro",
        "id": 3005510,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lj7yf9/is_seagate_expansion_hdd_10tb_good_for_torrents",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Seagate Expansion HDD 10TB good for torrents and streaming?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/stranger_synchs",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T10:01:50.673768+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T09:42:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>That&#39;s a really important database. I&#39;m trying but I can&#39;t find companies names to scrape as there is no companies names</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/stranger_synchs\"> /u/stranger_synchs </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lj6svz/fakespot_is_going_to_end_on_1_july_any_way_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lj6svz/fakespot_is_going_to_end_on_1_july_any_way_to/\">[comments]</a></span>",
        "id": 3005107,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lj6svz/fakespot_is_going_to_end_on_1_july_any_way_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fakespot is going to end on 1 July. Any way to scrape that whole database?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mr_Worcester",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T10:01:50.912977+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T09:07:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, im not totally sure this sub is the right place to ask this (so in case please direct me to a more suitable one). I have two 1TB NVMe SSDs (one Gen4, one Gen3) that I want to combine into RAID 0 for convenience - just want one drive letter instead of managing two separate drives.</p> <p>Problem: My ASUS ROG B550-F motherboard (using RAIDXpert2) automatically configured each drive as its own single-disk RAID array during setup. Now I can&#39;t combine them into RAID 0 without deleting the existing arrays, which would wipe my data. Windows Storage Spaces won&#39;t work either since the drives appear as single-disk RAID arrays rather than individual drives. Since I don&#39;t have a spare 1TB drive for backup, are there any alternatives to get these drives working as one volume without data loss? My main goal is just having all files in one location without manually moving things between the two.</p> </div><!-- SC_ON --> &#32; submitted by &#",
        "id": 3005108,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lj6939/ssd_raid_configuration_alternatives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SSD Raid Configuration alternatives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/garden-3750",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T08:56:52.146588+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T08:48:47+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lj5z2r/sega_delists_some_mobile_games_mostly_retro_ports/\"> <img src=\"https://external-preview.redd.it/n97Rxp0xRXLEROAPf_JpLbJCahn5PuoHuspudvlkjm0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f3c90757d3547e965c27f2ae52172b5b69daf6f\" alt=\"Sega delists some mobile games (mostly retro ports), makes them free in prior\" title=\"Sega delists some mobile games (mostly retro ports), makes them free in prior\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>The internet permission appears to have been removed at least from the Android games and Sega clarifies that the titles can be played offline. I recommend storing the APK files.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/garden-3750\"> /u/garden-3750 </a> <br/> <span><a href=\"https://www.androidauthority.com/sega-retro-games-android-free-3568153/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoar",
        "id": 3004738,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lj5z2r/sega_delists_some_mobile_games_mostly_retro_ports",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/n97Rxp0xRXLEROAPf_JpLbJCahn5PuoHuspudvlkjm0.jpeg?width=640&crop=smart&auto=webp&s=6f3c90757d3547e965c27f2ae52172b5b69daf6f",
        "title": "Sega delists some mobile games (mostly retro ports), makes them free in prior",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mission_Grapefruit92",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T08:56:52.348543+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T08:36:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.amazon.com/gp/product/B0CB45PRMF/ref=ox_sc_act_title_2?smid=A3GES9IA7UE877&amp;th=1\">Amazon.com: ELUTENG NVME to USB Adapter USB 3.1 Gen 2 to M.2 NVMe SSD Converter Adapter 10Gbps PCIe Based M Key Hard Drive Reader Max 4TB Support UASP for 2280 2260 2242 2230 SSD (Only for M.2 NVME) : Electronics</a></p> <p>OR, do you know a good, cheap option for a USB to NVMe adapter? I dont want to spend a lot because i&#39;m probably only gonna use it once or twice</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mission_Grapefruit92\"> /u/Mission_Grapefruit92 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lj5s95/have_you_heard_of_and_do_you_trust_this/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lj5s95/have_you_heard_of_and_do_you_trust_this/\">[comments]</a></span>",
        "id": 3004739,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lj5s95/have_you_heard_of_and_do_you_trust_this",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Have you heard of, and do you trust, this product/brand?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jhenn08",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T02:25:28.388060+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T01:56:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking to buy a pre-built 24bay 4U server from the server store. Do you guys think this is a good deal?<a href=\"https://www.theserverstore.com/supermicro-superstorage-36x-bay-4u-plex-media-server-sas3\">https://www.theserverstore.com/supermicro-superstorage-36x-bay-4u-plex-media-server-sas3</a></p> <p>I&#39;m selling my old server so won&#39;t have any spare internal parts. Any options or tweaks that you suggest I make on the server listed above? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jhenn08\"> /u/jhenn08 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1liyyp1/supermicro_server_advice/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1liyyp1/supermicro_server_advice/\">[comments]</a></span>",
        "id": 3003196,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1liyyp1/supermicro_server_advice",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Supermicro Server Advice",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JesseJamesTheCowboy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T01:20:29.002743+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T00:45:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for recommendations for a das bay that holds between 4-8 hdds, no raid, no nonsense just want my pc to read 4 drives or whatevers in the bays. Should also have a fan for active cooling too, which I think is probably a given. Willing to spend more for something actually higher quality with more features such as hot swapping or not inappropriately putting the drives to sleep. Mostly just want something reliable that people here would recommend. Thanks. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JesseJamesTheCowboy\"> /u/JesseJamesTheCowboy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lixim7/help_me_pick_a_das_storage_bay/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lixim7/help_me_pick_a_das_storage_bay/\">[comments]</a></span>",
        "id": 3002938,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lixim7/help_me_pick_a_das_storage_bay",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help me pick a das storage bay.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TraitOpenness",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T01:20:28.501070+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T00:43:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1lixhlu/libgen_and_all_of_its_mirrors_are_down_except_one/\"> <img src=\"https://b.thumbs.redditmedia.com/QQRxlUqg1CCwB5lx0COKEuKnKmZKUqCanlIsBqbLCjE.jpg\" alt=\"Libgen and all of its mirrors are down except one URL I found displays a big bold message from the US government:\" title=\"Libgen and all of its mirrors are down except one URL I found displays a big bold message from the US government:\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TraitOpenness\"> /u/TraitOpenness </a> <br/> <span><a href=\"/r/libgen/comments/1lixewn/libgen_and_all_of_its_mirrors_are_down_except_one/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lixhlu/libgen_and_all_of_its_mirrors_are_down_except_one/\">[comments]</a></span> </td></tr></table>",
        "id": 3002936,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lixhlu/libgen_and_all_of_its_mirrors_are_down_except_one",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/QQRxlUqg1CCwB5lx0COKEuKnKmZKUqCanlIsBqbLCjE.jpg",
        "title": "Libgen and all of its mirrors are down except one URL I found displays a big bold message from the US government:",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TalkAmbitious2248",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T01:20:28.734846+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T00:32:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am looking for the best and preferably most economical way to safely archive around 5TB of various data that I have have collected through my life. I\u2019m talking photos, videos, games, software, movies etc. Right now I have an external hard drive where this data is stored, but I\u2019m afraid that it\u2019s going to fail one day. I discovered M discs but after visiting this sub I realized they might be a scam. Hope you can help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TalkAmbitious2248\"> /u/TalkAmbitious2248 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lix8wl/best_way_to_archive_5tb_of_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1lix8wl/best_way_to_archive_5tb_of_data/\">[comments]</a></span>",
        "id": 3002937,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1lix8wl/best_way_to_archive_5tb_of_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to archive 5TB of data",
        "vote": 0
    }
]