[
    {
        "age": null,
        "album": "",
        "author": "/u/Optimalutopic",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T20:09:55.954120+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T19:42:13+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ljkv6y/scrape_qa_summarise_anything_locally_at_scale/\"> <img src=\"https://external-preview.redd.it/uM1qUgFFn7Ki2sS81n3jNPIw2XJS-08FaN4Rn_Bgh_c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2080cf5309441ec3b6a916f8be3ce8cdd70a02e9\" alt=\"Scrape, qa, summarise anything locally at scale with coexistAI\" title=\"Scrape, qa, summarise anything locally at scale with coexistAI\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Have you ever imagined If you can spin a local server, which your whole family can use and this can do everything what perplexity does? I have built something which can do this! And more indian touch going to come soon</p> <p>I\u2019m excited to share a framework I\u2019ve been working on, called coexistAI.</p> <p>It allows you to seamlessly connect with multiple data sources \u2014 including the web, YouTube, Reddit, Maps, and even your own local documents \u2014 and pair them with either local or proprietary LLMs",
        "id": 3010287,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ljkv6y/scrape_qa_summarise_anything_locally_at_scale",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/uM1qUgFFn7Ki2sS81n3jNPIw2XJS-08FaN4Rn_Bgh_c.png?width=640&crop=smart&auto=webp&s=2080cf5309441ec3b6a916f8be3ce8cdd70a02e9",
        "title": "Scrape, qa, summarise anything locally at scale with coexistAI",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lazaruszs",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T20:09:55.683606+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T19:09:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, I&#39;ve been using some automated browsers for scraping and other tasks and I&#39;ve noticed that a lot of blocks will come from canvas fingerprinting and websites seeing that one machine is making all the requests. This is pretty prevalent in the playwright tools, and I wanted to see if anyone knew any browsers that has these features. A few I&#39;ve tried:</p> <p>- <a href=\"https://github.com/daijro/camoufox\">Camoufox</a>: A really great tool that fits exactly what I need, with both fingerprint rotation on each browser and leak fixes. The only issue is that the package hasn&#39;t been updated for a bit (developer has a condition that makes them sick for long periods of time, so it&#39;s understandable) which leads to more detections on sites nowadays. The browser itself is a bit slow to use as well, and is locked to Firefox. </p> <p>- <a href=\"https://github.com/Kaliiiiiiiiii-Vinyzu/patchright-python\">Patchright</a>: Another great tool that ke",
        "id": 3010286,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ljk054/automated_browser_with_fingerprint_rotation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automated browser with fingerprint rotation?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/spiritualquestions",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T19:04:54.957973+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T18:39:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I ran into something very interesting, but was a nice surprise. I created a web scraping script using Python and Selenium and I got everything working locally, but I decided I wanted to make it easier to use, so I decided to put in a GitHub actions workflow, and have parameters that can be added for the scraping. So the script runs now on GitHub actions servers. </p> <p>But here is the strange thing: It runs more than 10x faster using GH actions than when I run the script locally. I was happily surprised by this, but not sure why this would be the case. Any ideas? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/spiritualquestions\"> /u/spiritualquestions </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ljj84j/github_actions_selenium_web_performance_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ljj84j/github_actions_selenium_w",
        "id": 3009740,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ljj84j/github_actions_selenium_web_performance_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "GitHub Actions + Selenium Web Performance Scraping Question",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Late-Driver-7866",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T17:59:55.604239+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T17:32:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t really understand how they can have millions of ads in their database and still validate their ads live status and other things? </p> <p>As far as I know a lot of stats they show are not available via Meta&#39;s API, so how do they do it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Late-Driver-7866\"> /u/Late-Driver-7866 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ljhfs7/how_do_tools_like_dropshipio_get_their_live_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ljhfs7/how_do_tools_like_dropshipio_get_their_live_data/\">[comments]</a></span>",
        "id": 3009244,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ljhfs7/how_do_tools_like_dropshipio_get_their_live_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do tools like dropship.io get their live data?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Late-Driver-7866",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T15:49:56.802382+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T14:56:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im working on a software project by myself and need to scrape data in order for my tool to work.</p> <p>Current plan is this:</p> <p>Get data of a &quot;platform result page&quot; via HTTP request.</p> <p>I then look at the data and use AI to categorize this data. </p> <p>Based on how the data was tagged, it will be left out or passed on to the next stage.</p> <p>Here I am struggling now.<br/> The data I get is not enough. I need further data that I only get from the detail pages.</p> <p>Now I guess there might be ways to make it look natural, as if a user triggers a search, that responds around 50 results, and then has a look at 30 of them.</p> <p>What would be the best way to do that?<br/> I am talking about multiple thousand sets of data per day. </p> <p>Can you recommend me a blueprint to follow?<br/> E.g. what tools, plugins, etc.<br/> What is the best practice around here?</p> <p>To follow up: Ideally I could also check the data I collect once e",
        "id": 3007910,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ljda8t/feedback_on_my_scraping_strategy_developer_first",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Feedback on my scraping strategy (Developer first time doing this)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T13:38:33.667736+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T13:01:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 3006729,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ljaihw/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/integron11",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-24T07:06:12.838786+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-24T04:16:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to collect data on what is the Gross Vehicle Weight Rating, Payload, curb weight, Vehicle Length and Wheel Base for every model and trim of car that is available. I&#39;ve tried using python with the selenium and selenium stealth on Edmunds and cars.com. I&#39;m unable to scrape those sites as they seem to render pages in such a way as to protect against bots and scrapers and the javascript somehow prevents the page from rendering details such as the GVWR until clicked in a browser. I couldn&#39;t overcome this even with selenium stealth. I looked for a way to purchase API access to a site and carqueryAPI denied my purchase request, flagging it as &quot;suspicious&quot;. I looked for other legitimate car data sites I could purchase API data from and couldn&#39;t find any that would sell this service to an end user as opposed to major distributor or dealer. Can anyone advise as to how I can go about this? Thanks!</p> </div><!-- SC_ON --> &#32; s",
        "id": 3004370,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1lj1om9/collecting_automobile_specifications_with_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Collecting Automobile specifications with python web Scraping",
        "vote": 0
    }
]