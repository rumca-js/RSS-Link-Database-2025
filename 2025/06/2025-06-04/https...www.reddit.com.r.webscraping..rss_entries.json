[
    {
        "age": null,
        "album": "",
        "author": "/u/Informal_Energy7405",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T20:44:41.178097+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T19:44:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi hope ur day is going well.<br/> i am working on a project related to perfumes and i need a database of perfumes. i tried scraping fragrantica but i couldn&#39;t so does anyone know if there is a database online i can download?<br/> or if u can help me scrap fragrantica. Link: <a href=\"https://www.fragrantica.com/\">https://www.fragrantica.com/</a><br/> I want to scrape all their perfume related data mainly names ,brands, notes, accords.<br/> as i said i tried but i couldn&#39;t i am still new to scraping, this is my first ever project , and i never tried scraping before.<br/> what i tried was a python code i believe but i couldn&#39;t get it to work, tried to find stuff on github but they didn&#39;t work either.<br/> would love if someone could help</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Informal_Energy7405\"> /u/Informal_Energy7405 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments",
        "id": 2850362,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l3f2lc/perfume_database",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Perfume Database",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Independent-Speech25",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T19:39:42.097348+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T19:05:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Currently working on an internship project that involves compiling a list of Tennessee-based businesses serving the disabled community. I need four data elements (Business name, tradestyle name, email, and url). Rough plan of action would involve:</p> <ol> <li>Finding a reliable source for a bulk download, either of all TN businesses or specifically those serving the disabled community (healthcare providers, educational institutions, advocacy orgs, etc.). Initial idea was to buy the business entity data export from the TNSOS website, but that a) costs $1000, which is not ideal, and b) doesn&#39;t seem to list NAICS codes or website links, which inhibits steps 2 and 3. Second idea is to use the NAICS website itself. You can purchase a record of every TN business that has specific codes, but to get all the necessary data elements costs over $0.50/record for 6600 businesses, which would also be quite expensive and possibly much more than buying from TNSO",
        "id": 2849875,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l3e2x4/seeking_list_of_disabilityserving_tn_businesses",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seeking list of disability-serving TN businesses",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/postytocaster",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T16:22:44.776879+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T16:15:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building a Python project in which I need to create instances of many different HTTP clients with diferent cookies, headers and proxies. For that, I decided to use HTTPX AsyncClient.</p> <p>However, when testing a few things, I noticed that it takes so long for a client to be created (both AsyncClient and Client). I wrote a little code to validate this, and here it is:</p> <pre><code>import httpx import time if __name__ == &#39;__main__&#39;: total_clients = 10 start_time = time.time() clients = [httpx.AsyncClient() for i in range(0, total_clients)] end_time = time.time() print(f&#39;{total_clients} httpx clients were created in {(end_time - start_time):.2f} seconds.&#39;) </code></pre> <p>When running it, I got the following results:</p> <ul> <li>1 httpx clients were created in 0.33 seconds.</li> <li>5 httpx clients were created in 1.35 seconds.</li> <li>10 httpx clients were created in 2.62 seconds.</li> <li>100 httpx clients were created in",
        "id": 2848121,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l39os2/why_are_python_httpx_clients_so_slow_to_be_created",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why are Python HTTPX clients so slow to be created?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sam439",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T09:13:44.967496+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T08:36:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I hit a daily limit and can only upload 14 videos at a time in YouTube. I wanted to maybe select all 4k videos and let it upload one by one but YouTube doesn&#39;t provide that feature. </p> <p>I want to do it with a bot. Can someone share some tips?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sam439\"> /u/sam439 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l30enw/how_to_upload_4000_videos_to_youtube/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l30enw/how_to_upload_4000_videos_to_youtube/\">[comments]</a></span>",
        "id": 2844992,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l30enw/how_to_upload_4000_videos_to_youtube",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to upload 4000 videos to YouTube?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DeepBlueWanderer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T09:13:45.117109+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T08:14:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1l303m2/do_you_know_more_websites_that_do_this_json/\"> <img src=\"https://external-preview.redd.it/hbUMw7RBltYGh1RZ14o1gMgT4gnqxjHYbt63j-KaRmU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9df0d8c837f4d81629467ddd7e3384145111d12c\" alt=\"Do you know more websites that do this? .json extension on reddit\" title=\"Do you know more websites that do this? .json extension on reddit\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So few days ago I found out that if you add /.json in the end of a reddit post link, it shows you the full post, comments and a lot more data available all in text, with json format, do you guys know of more websites that have this kind of system? What are the extensions to be used?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DeepBlueWanderer\"> /u/DeepBlueWanderer </a> <br/> <span><a href=\"https://youtu.be/lxMNOIO01Yw\">[link]</a></span> &#32; <span>",
        "id": 2844993,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l303m2/do_you_know_more_websites_that_do_this_json",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/hbUMw7RBltYGh1RZ14o1gMgT4gnqxjHYbt63j-KaRmU.jpg?width=320&crop=smart&auto=webp&s=9df0d8c837f4d81629467ddd7e3384145111d12c",
        "title": "Do you know more websites that do this? .json extension on reddit",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Adventurous-Mix-830",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T09:13:45.296120+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T07:53:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So Im building a chrome extension that scrapes amazon reviews, it works with DOM API so I dont need to use Puppeteer or similar technology. And as I&#39;m developing the extension I scrape few products a day, and after a week or so my account gets restricted to see /product-reviews page - when I open it I get an error saying webpage not found, and a redirect to Amazon dogs blog. I created a second account which also got blocked after a week - now I&#39;m on a third account. So since I need to be logged in to see the reviews I guess I just need to create a new account each day or so? I also contacted amazon support multiple times and wrote emails, but they give vague explanations of the issue, or say it will resolve itself, but Its clear that my accounts are flagged as bots. Has anyone experienced this issue before? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Adventurous-Mix-830\"> /u/Adventurous-Mix-830 </a> ",
        "id": 2844994,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l2zssz/amazon_account_restricted_to_see_reviews",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Amazon account restricted to see reviews",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/antvas",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T08:06:10.214484+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T07:22:17+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/\"> <img src=\"https://external-preview.redd.it/m5mMB4o9vkwUrawknkXhWGaC-rhijdxuopv3t2U42ZI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3a7fada4e9a558c2e693ac03b4c05ffbe119b8f\" alt=\"What TikTok\u2019s virtual machine tells us about modern bot defenses\" title=\"What TikTok\u2019s virtual machine tells us about modern bot defenses\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><strong>Author here:</strong> There\u2019ve been a lot of Hacker News threads lately about scraping, especially in the context of AI, and with them, a fair amount of confusion about what actually works to stop bots on high-profile websites.</p> <p>In general, I feel like a lot of people, even in tech, don\u2019t fully appreciate what it takes to block modern bots. You\u2019ll often see comments like \u201cjust enforce JavaScript\u201d or \u201cuse a simple proof-of-work,\u201d without acknowledging that attackers won\u2019t stop the",
        "id": 2844666,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/m5mMB4o9vkwUrawknkXhWGaC-rhijdxuopv3t2U42ZI.jpg?width=640&crop=smart&auto=webp&s=f3a7fada4e9a558c2e693ac03b4c05ffbe119b8f",
        "title": "What TikTok\u2019s virtual machine tells us about modern bot defenses",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IveCuriousMind",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T04:49:47.485547+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T03:07:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Before you run to comment that it is impossible, I want to mention that I will not take no for an answer, the objective is clearly to find the solution or invent it.</p> <p>I find myself trying to make a farm of Gmail accounts, so far I have managed to bypass several security filters, to the point that reCaptcha V3 scores me 0.7 out of 1.0 as a human. I have emulated realistic clicks with the Bezier equation. I have evaded CDP detection, webdriver, I have hidden playwright detection... But it is still not enough, the registration continues but finally requests the famous verification for robots with phone numbers.</p> <p>I have managed to create Gmail accounts indefinitely from my phone, without problems, but I still can&#39;t replicate it for my computer.</p> <p>The only thing I have noticed is that while in my non-automated browser I can create accounts in the automated one, even if I only use it to open Google and I manually make the account, it is",
        "id": 2843792,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l2v51g/google_accounts_farm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google Accounts Farm",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Asleep-Patience-3686",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-04T00:25:46.916248+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-04T00:01:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Two weeks ago, I developed a Tampermonkey script for collecting Google Maps search results. Over the past week, I upgraded its features, and now it can:</p> <ol> <li>Automatically scroll to load more results</li> <li>Retrieve email addresses and Plus Codes</li> <li>Export in more formats</li> <li>Support all subdomains of Google Maps sites.</li> </ol> <p><a href=\"https://github.com/webAutomationLover/google-map-scraper\">https://github.com/webAutomationLover/google-map-scraper</a></p> <p>Just enjoy with free and unlimited leads!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Asleep-Patience-3686\"> /u/Asleep-Patience-3686 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l2rgv9/opensource_userscript_for_google_map_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1l2rgv9/opensource_userscript_for_google_map_scraper/\">[comments]</a></span>",
        "id": 2842791,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1l2rgv9/opensource_userscript_for_google_map_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "open-source userscript for google map scraper (upgraded)",
        "vote": 0
    }
]