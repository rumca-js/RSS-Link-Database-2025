[
    {
        "age": null,
        "album": "",
        "author": "/u/Puzzled_Most_9864",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T23:55:00.034182+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T23:27:24+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5y7lr/where_to_store_300gb_data_of_my_old_macbook/\"> <img src=\"https://preview.redd.it/n4bkjypial5f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0b0b04649fd84dd8c0f4ab08bff9ee68375ec6b\" alt=\"Where to store 300GB data of my old macbook?\" title=\"Where to store 300GB data of my old macbook?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I got a new laptop and want to save around 300+ GB data before selling my old one. What would be the best option? Buy a 500GB Hard Drive or buy iCloud data?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Puzzled_Most_9864\"> /u/Puzzled_Most_9864 </a> <br/> <span><a href=\"https://i.redd.it/n4bkjypial5f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5y7lr/where_to_store_300gb_data_of_my_old_macbook/\">[comments]</a></span> </td></tr></table>",
        "id": 2876456,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5y7lr/where_to_store_300gb_data_of_my_old_macbook",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/n4bkjypial5f1.jpeg?width=640&crop=smart&auto=webp&s=e0b0b04649fd84dd8c0f4ab08bff9ee68375ec6b",
        "title": "Where to store 300GB data of my old macbook?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Arcueid-no-Mikoto",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T22:48:51.615278+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T22:07:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Got that error trying to download their manga database:</p> <p><a href=\"https://www.mangaupdates.com/series/\">https://www.mangaupdates.com/series/</a></p> <p>Any way to circumvent the URL limit? It&#39;s annoying it just decides to give up on it&#39;s own and reset the progress.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Arcueid-no-Mikoto\"> /u/Arcueid-no-Mikoto </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5wjmn/httrack_000017_panic_too_many_urls_giving_up100000/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5wjmn/httrack_000017_panic_too_many_urls_giving_up100000/\">[comments]</a></span>",
        "id": 2876217,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5wjmn/httrack_000017_panic_too_many_urls_giving_up100000",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HTTrack: 00:00:17 Panic: Too many URLs, giving up..(>100000)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/moonshot100",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T21:43:54.043130+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T21:26:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all - excuse me if this question seems obvious, I am not that tech savvy.</p> <p>I bought two external hard drives (one back up) to transfer all my photos/videos/files from my iPhones. I connected my phone to my PC and the iPhone storage stores the items in folders by the month. When I drag and drop each folder to my PC, not all the items in the folder are transferring over. I see no errors when importing and it completes fine.</p> <p>I even used the windows Photos app and imported from there and not all the items transferred. It feels like I need to import them in batches per item, not by folder to make sure all of them transfers over.</p> <p>Are there any other methods that work better? I\u2019m in no rush to if I have to be meticulous it\u2019s ok, so long as I don\u2019t lose any files.</p> <p>Thanks in advance for any guidance and tips.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/moonshot100\"> /u/moonshot100 </a> <b",
        "id": 2875933,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5vnov/not_all_items_transferring",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Not all items transferring",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SuperBox4776",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T21:43:53.888899+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T21:23:27+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5vlik/i_neee_help_from_music_hoarders_i_think_im_losing/\"> <img src=\"https://preview.redd.it/tbaugrfeok5f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10a26708bbb29363db932521cb11bded512daad9\" alt=\"I neee help from music hoarders, I think I'm losing data\" title=\"I neee help from music hoarders, I think I'm losing data\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I keep my music library on a 1tb T7 Shield, and have for about a year. I use MusicBee to listen to my music. I notice that every once in a while, data tags will mysteriously disappear from tracks. Genre will just vanish, or the Album-Artist. Another curious effect is that the Title field will be limited in characters so that the end of track names will be cut off. I need help. Is my drive going bad? Its been happening more and more often. Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SuperBox4",
        "id": 2875932,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5vlik/i_neee_help_from_music_hoarders_i_think_im_losing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/tbaugrfeok5f1.jpeg?width=640&crop=smart&auto=webp&s=10a26708bbb29363db932521cb11bded512daad9",
        "title": "I neee help from music hoarders, I think I'm losing data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Intelg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T20:38:54.381071+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T20:38:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Need some help verifying my SAS enclosure is configured properly before I use it. This is my first time dealing with SAS and HBA&#39;s. </p> <p>Context: </p> <ul> <li>I purchased a 12 bay SAS enclosure (backplane Inspur YPCB-00395-1P4) which has 3x SFF-8643 Mini-SAS HD connectors labeled as MiniSAS_0, MiniSAS_1, and MiniSAS_2 - <a href=\"https://imgur.com/a/e3Wci6h\">Photos</a></li> <li>HBA 9500-8i has a single x8 SFF-8654 (SlimSAS) connector.</li> <li>Purchased a cable which converts x8 SFF-8654 (SlimSAS) to 2x SFF-8643</li> <li>AI says that if I connect MiniSAS_0, MiniSAS_1 on the backplane I should be able to get additional bandwidth / throughput... yet I only see a single SAS address for the enclosure.</li> <li>Most of the ./storcli64 commands that the help dialog tells me to try fail with &quot;Un-supported Command&quot;</li> <li>What am I doing wrong? Shouldn&#39;t I be seeing two SAS addresses for the enclosure? My thought is 2 SFF-8643 bandwidth",
        "id": 2875629,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5ulhi/helpanoob_should_my_sas_enclosure_have_two_sas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HelpANoob: Should my SAS enclosure have two SAS hardware addresses reported by StorCLI64?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/March_Embers_13",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T19:33:52.629297+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T19:17:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone know of an updated archive of collection of strategy guides? Specifically the past 10 years? I have older guides. </p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/March_Embers_13\"> /u/March_Embers_13 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5st87/updated_strategy_guide_collection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5st87/updated_strategy_guide_collection/\">[comments]</a></span>",
        "id": 2875367,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5st87/updated_strategy_guide_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Updated Strategy Guide Collection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cheater00",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T18:29:11.032513+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T17:55:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I want to set up a local file server for making files available to my Windows computers. Literally a bunch of disks, no clustering or mirroring or anything special like that. Files would be made available via SMB. As a secondary item, it could also run some long lived processes, like torrent downloads or irc bots. I&#39;d normally just slap Ubuntu on it and call it a day, but I was wondering what everyone else thought was a good idea.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cheater00\"> /u/cheater00 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5qwgj/easy_linux_for_local_file_server/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5qwgj/easy_linux_for_local_file_server/\">[comments]</a></span>",
        "id": 2875029,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5qwgj/easy_linux_for_local_file_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Easy Linux for local file server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SHDrivesOnTrack",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T18:29:11.184817+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T17:39:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Question: how much do you test a new drive before you start trusting it with data. </p> <p>I have a 16T NAS (ubuntu) and I am in the process of upgrading. I bought some drives, one of which is a 28T seagate factory refurbished drive. Normally I would test drives using the linux badblocks command, however I am noticing that larger drives take, well, longer. An 8T drive takes almost 4 days to test. Started testing the 28T drive and estimated that it will take 12 days. </p> <p>Would you test a drive for 12 days before you merge it into a RAID array ?</p> <p>edit to add: running badblocks with defaults: 4 byte pattern tests (AA,55,FF,00), destructive read/write. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SHDrivesOnTrack\"> /u/SHDrivesOnTrack </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5qijy/how_much_do_you_test_a_drive_before_adding_it_to/\">[link]</a></span> &#32; <span><a href=\"htt",
        "id": 2875030,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5qijy/how_much_do_you_test_a_drive_before_adding_it_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How much do you test a drive before adding it to a RAID array ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Tobias_Reaper_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:29.811451+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T17:21:09+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5q3d8/why_are_multiplatform_phone_drives_so_expensive/\"> <img src=\"https://preview.redd.it/6pe2ga3zgj5f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93b71f166d67190dafd6c2d2c2c90e9f9b452cdd\" alt=\"Why Are Multi-Platform Phone Drives So Expensive?\" title=\"Why Are Multi-Platform Phone Drives So Expensive?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tobias_Reaper_\"> /u/Tobias_Reaper_ </a> <br/> <span><a href=\"https://i.redd.it/6pe2ga3zgj5f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5q3d8/why_are_multiplatform_phone_drives_so_expensive/\">[comments]</a></span> </td></tr></table>",
        "id": 2874597,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5q3d8/why_are_multiplatform_phone_drives_so_expensive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/6pe2ga3zgj5f1.png?width=320&crop=smart&auto=webp&s=93b71f166d67190dafd6c2d2c2c90e9f9b452cdd",
        "title": "Why Are Multi-Platform Phone Drives So Expensive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Spektre99",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:30.107786+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T17:01:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Examples of 20TB Seagate Exos drive part numbers.</p> <p>ST20000NM007D</p> <p>ST20000NM004E</p> <p>ST20000NM002C</p> <p>So I can guess.</p> <p>ST = Seagate Technologies</p> <p>2000 = 20TB</p> <p>NM = Perhaps the Exos line?</p> <p>Then what are the 4 digits following?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Spektre99\"> /u/Spektre99 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5pmiw/understanding_decipering_seagate_exos_part_numbers/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5pmiw/understanding_decipering_seagate_exos_part_numbers/\">[comments]</a></span>",
        "id": 2874598,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5pmiw/understanding_decipering_seagate_exos_part_numbers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Understanding (decipering) Seagate Exos part numbers.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/David15M3SGT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:30.255155+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T16:42:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Ok, so I don&#39;t know if I am a date hoarder or not, but I have a lot of files on a NAS that are 100% of my family. Most of the files are JPEG, RAW and either cellphone videos or GoPro footage. My NAS is accessible via my laptop as well as the TV that is in the living room via Plex, but that&#39;s what led me here. My wife is a little less tech savvy than I am and while the files are accessible fairly easily to me, I am concerned that if anything happens to me she won&#39;t know how to retrieve our memories. Does it make sense to dump all of my files onto CDR&#39;s/DVD&#39;s? I have heard that USB flash drives can degrade over time or else I&#39;d just purchase a bunch of those.</p> <p>Thank you for any advice!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/David15M3SGT\"> /u/David15M3SGT </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5p6nr/home_movies_on_nas_now_what/\">[link]</a></sp",
        "id": 2874599,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5p6nr/home_movies_on_nas_now_what",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Home Movies On NAS. Now What?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Foreign_Factor4011",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:30.423519+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T16:39:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. I&#39;ve been trying to save this website: <a href=\"http://musicmap.info\">musicmap.info</a></p> <p>But saving it directly from the browser won&#39;t work, and both HTTrack and Internet Archive can&#39;t save the page properly. Do you have any other way?</p> <p>Thanks in advance to everyone for your time.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Foreign_Factor4011\"> /u/Foreign_Factor4011 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5p49f/help_downloading_this_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5p49f/help_downloading_this_website/\">[comments]</a></span>",
        "id": 2874600,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5p49f/help_downloading_this_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help downloading this website",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/msgenhances",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:30.575874+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T16:26:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hello. My Softraid Raid 5 setup suddenly stopped giving me access to the data and now it&#39;s showing up as it&#39;s missing a Disk. </p> <ol> <li><p>observed degraded performance and couldn&#39;t write on certain folder structure</p></li> <li><p>Reboot </p></li> <li><p>Power cycle of the enclosure </p></li> <li><p>Validation with repair </p></li> <li><p>Windows Disk error check </p></li> <li><p>Reseated the Disk 2</p></li> <li><p>Changed to a new drive for Disk 2</p></li> </ol> <p>Did everything above but all I see is drive letter and error saying its not accessible. </p> <p>Under all drive I see all the drive </p> <p>Is it worth trying to swap to a new enclosure? possible enclosure issue?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/msgenhances\"> /u/msgenhances </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5otty/softraid_8_fortis_5c_5bay_usbc_external_drive/\">[link]</a></span> &#",
        "id": 2874601,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5otty/softraid_8_fortis_5c_5bay_usbc_external_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Softraid 8 Fortis 5C 5-Bay USB-C External Drive Enclosure- Missing Disk on bay #2 and not showing up on windows explorer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kettu92",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T16:18:48.052531+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T16:17:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5omgf/ghetto_cooling_on_cheap_enclosure/\"> <img src=\"https://preview.redd.it/863n331w5j5f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad2265d91bb2ca58fe3e4b05e6b2423cebe99073\" alt=\"Ghetto cooling on cheap enclosure\" title=\"Ghetto cooling on cheap enclosure\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Dropped temps from 53 to 40 during 750gb transfer</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kettu92\"> /u/kettu92 </a> <br/> <span><a href=\"https://i.redd.it/863n331w5j5f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5omgf/ghetto_cooling_on_cheap_enclosure/\">[comments]</a></span> </td></tr></table>",
        "id": 2874267,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5omgf/ghetto_cooling_on_cheap_enclosure",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/863n331w5j5f1.jpeg?width=640&crop=smart&auto=webp&s=ad2265d91bb2ca58fe3e4b05e6b2423cebe99073",
        "title": "Ghetto cooling on cheap enclosure",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thomedes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T16:18:48.207175+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T16:15:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been using SyncThing and love it. </p> <p>Up to now I&#39;ve only used for &quot;small&quot; work. Some dozens of GB and a maximum a 100K files.</p> <p>Now I&#39;m doubting on wether to trust it for keeping replicas of may main disc, a few TB and file count of a million, maybe two. </p> <p>Have you used it for something similar? What is your experience?</p> <p>And the big question: What about security? Would you trust all your files to it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thomedes\"> /u/thomedes </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5ok5o/syncthing_for_a_million_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5ok5o/syncthing_for_a_million_files/\">[comments]</a></span>",
        "id": 2874268,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5ok5o/syncthing_for_a_million_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SyncThing for a million files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cricketpower",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T16:18:48.357192+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T16:05:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As we\u2019re in the EU, the deals for recertifed enterprise HDD\u2019s aren\u2019t as good in North-America. As I\u2019m at the point of buying 6 x 20tb HDD\u2019s I\u2019m unsure if the 10-15% cheaper price for recertified disks is worth it it. If I would be in the US I wouldn\u2019t think twice to with some of the deals on serverpartdeals.</p> <p>Curious what route some EU hoarders do, recertified or new.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cricketpower\"> /u/cricketpower </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5obmc/to_recertifed_or_not/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5obmc/to_recertifed_or_not/\">[comments]</a></span>",
        "id": 2874269,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5obmc/to_recertifed_or_not",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "To recertifed or not..",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BobDaSloth180",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T16:18:48.746021+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T15:53:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was seeing a lot of a brand called UnionSine. Is this brand trusted?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BobDaSloth180\"> /u/BobDaSloth180 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5o1ns/i_am_going_to_buy_a_500_gb_external_hard_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5o1ns/i_am_going_to_buy_a_500_gb_external_hard_drive/\">[comments]</a></span>",
        "id": 2874271,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5o1ns/i_am_going_to_buy_a_500_gb_external_hard_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I am going to buy a 500 gb external hard drive for my xbox one. Any recommendations on what brand?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/voidsyourwarranties",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T16:18:48.506774+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T15:44:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for an inepensive high-capacity nvme, but not sure if non-branded used drives like this are worth the low cost.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/voidsyourwarranties\"> /u/voidsyourwarranties </a> <br/> <span><a href=\"https://ebay.us/m/osluAX\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5nu7c/any_experience_with_drives_like_these/\">[comments]</a></span>",
        "id": 2874270,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5nu7c/any_experience_with_drives_like_these",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any experience with drives like these?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/axebulb_Alex",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T16:18:48.896026+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T15:18:49+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5n8zh/ive_just_bought_a_used_drive_on_ebay_and_upon/\"> <img src=\"https://b.thumbs.redditmedia.com/KKdovizNLifrBmNM5v70bzYTDQ3g_2ktk-mfrRNkdMk.jpg\" alt=\"I've just bought a used drive on ebay and upon checking it is loaded with old Nintendo game files\u2014what should I do with them?\" title=\"I've just bought a used drive on ebay and upon checking it is loaded with old Nintendo game files\u2014what should I do with them?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Last week, I bought a 2TB HDD from ebay so I could back up a load of my old photos. When I plugged it in, I found it had loads of Nintendo games on it. I was going to wipe the drive but what wondering what (legally) can I do with them? Should I message the seller? I don&#39;t own any Nintendo consoles myself.</p> <p><a href=\"https://preview.redd.it/61eampe5vi5f1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=51face2e4f5670bde549ad50d6ba57b7b8c3fd72\">ht",
        "id": 2874272,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5n8zh/ive_just_bought_a_used_drive_on_ebay_and_upon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/KKdovizNLifrBmNM5v70bzYTDQ3g_2ktk-mfrRNkdMk.jpg",
        "title": "I've just bought a used drive on ebay and upon checking it is loaded with old Nintendo game files\u2014what should I do with them?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jugendabest",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T15:15:13.561812+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T14:45:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p> <p>I don&#39;t know if it is the right place for such question, but let&#39;s go.</p> <p>I started to do backups of my important files recently and I currently doing it naively.<br/> What I do is that I copy (using cp command) my <em>home</em> folder and other important personnal folder on a HDD drive on my computer, also on an external drive and twice a year I copy that external drive on a home server. For now it works, but with time, the transfer and the copy will start to take more time.</p> <p>But is it the correct way ? I mean is the &quot;blind&quot; copy/paste a correct way to keep folders/files ? Is there a best and faster way to do it ? </p> <p>For information, I don&#39;t need to do snapshots of my system, just keep my important config files and personal folders safe.</p> <p>Thanks all !</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jugendabest\"> /u/jugendabest </a> <br/> <span><a ",
        "id": 2873915,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5mhze/how_to_do_backups_correctly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to do backups \"correctly\" ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Decent-Parsnip-3644",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:30.839930+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T14:01:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Put together a NAS recently running proxmox in a small pc case with a micro-atx board. Went to expand my storage when I see that horror of horrors it only has 2 (TWO) SATA ports. Its been fine so far but im really looking for something that can support far more drives and services, and im going to skip buying the expansion card step.</p> <p>Hope this is the right place to ask.</p> <p>I want to start again with a proper rack case so I dont paint myself into a corner but have no idea where to start when it comes to buying parts or what I need, or what to avoid, I&#39;m only familiar with PC parts.</p> <p>Ideally im looking for something that can fit 20 drives, which should keep me for a while. Otherwise im using it for video transcoding and general file storage/backup, with a fair few docker apps running and the capacity for remote administration. Might look into hosting a minecraft server for friends so want it to have the capacity for services like th",
        "id": 2874602,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5lja5/looking_for_advice_on_first_rack_build_uk",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for advice on first rack build (UK)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MarinatedPickachu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T14:08:54.052481+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T13:52:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have found myself pondering this topic more than once so I wonder if others have tools that served them well.</p> <p>In the current case I&#39;m using an exFAT formatted external drive. ExFAT because I need to use it between windows and MacOS (and occasionally Linux) for reading and writing so there doesn&#39;t seem to be a good alternative to that.</p> <p>exFAT is certainly not the most resilient filesystem so I wonder if there are things I can use on top to improve</p> <ol> <li><p>the detection of data corruption</p></li> <li><p>the prevention of data corruption</p></li> <li><p>the recovering from data corruption</p></li> </ol> <p>?</p> <p>For 1 actually a local git repository where every file is an LFS file would be quite well suited as it maintains a merkle tree of file and repository hashes (repositories just being long filenames), so the silent corruption or disappearance of some data could be detected, but git can become cumbersome if used fo",
        "id": 2873551,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5lc7m/are_there_aside_from_regular_backups_any",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are there - aside from regular backups - any filesystem-agnostic tools to increase a the resilience of filesystem contents against (and the detection of) data corruption?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Short-Guide4913",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T07:38:28.257015+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T07:16:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Cant use ytdlp or anything like that for the next 10 ish days and i kind of need this now there was another few posts like this but none of the links from there worked</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Short-Guide4913\"> /u/Short-Guide4913 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5exjk/best_webbased_youtube_playlist_video_downloader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5exjk/best_webbased_youtube_playlist_video_downloader/\">[comments]</a></span>",
        "id": 2871885,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5exjk/best_webbased_youtube_playlist_video_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best web-based YouTube playlist video downloader?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/viewless25",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T17:25:31.106280+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T07:10:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried using one of the sites on google but the download stopped before the 3 hour mark. Is there a way to try longer downloads? ideally something i can install locally and run on my machine</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/viewless25\"> /u/viewless25 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5eu9b/how_to_download_longer_youtube_videos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5eu9b/how_to_download_longer_youtube_videos/\">[comments]</a></span>",
        "id": 2874603,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5eu9b/how_to_download_longer_youtube_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download longer youtube videos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/orderdisord",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T07:38:28.048272+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T06:35:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I need some recommendations for some sort of optical printers that could safely print onto my discs. I don&#39;t wanna just write on them with sharpie when I&#39;m trying to record and burn my grandmother&#39;s old VHS tapes onto them before the tapes give out, and I know sharpie could affect the shelf life (Plus i want an excuse to design nice labels for my discs!) any recommendations would be great, especially if they might be easy to get second hand!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/orderdisord\"> /u/orderdisord </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5eb3x/recommendations_for_disc_label_printers/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5eb3x/recommendations_for_disc_label_printers/\">[comments]</a></span>",
        "id": 2871884,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5eb3x/recommendations_for_disc_label_printers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommendations for disc label printers?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Equivalent_Host3709",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T06:33:27.312394+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T06:19:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A have a huge repository of downloaded porn I am looking to clean-up/downsize; specifically, I have a lot of semi-duplicate videos, where I was able to find one source with a really high quality but low bitrate/FPS, others with lower FPS but higher bitrate, low quality but very high bitrate or FPS, etc. etc.</p> <p>Obviously, I want to keep the versions that can give the best viewing experience and pleasure (no motion blur, skin detail, etc.). I am wondering how the three metrics affect video quality, which is the most important to keep high, and which I should prioritize when deleting duplicates (i.e., should I delete the one with lower FPS, or lower bitrate? Always prefer 1080p to higher FPS? 2160p but low FPS or 720p but high FPS?)...</p> <p>Some other dilemmas I&#39;m having: 24FPS vs. 30FPS, if/how high bitrate compensates for lower FPS, data rate vs total bitrate, bitrate vs video quality...</p> <p>I&#39;m a newbie to datahoarding, so try not to",
        "id": 2871638,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5e2mk/question_about_file_metrics_for_pornbitrate_fps",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about file metrics for porn...bitrate, FPS, quality",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jman5150mib",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T06:33:27.569965+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T05:56:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have synology nas and have used shucked drives. Was wonderimg expected lifespans. They are all wd, some are 14tb, 18tb, 20tb, 22 tb and looking into maybe gettimg some 24tb.</p> <p>Are any of these sizes in a nas like ds1520 or ds1522 have different exoected lifespans. I heard 10tb, 18tb , 20 tb and 24tb are likely to last longer than 14tb, 22 tb but was given no evidence. I was told avearge lifesoand was 3-5 but the longer ones are more like 5. Is all if that bull and they are all likely 3-5 or are some really expected to expire sooner?</p> <p>Aboit to buy another 5 drives for a dx517 and cocnerned about longevity.</p> <p>That being said any evidence that some nas or extenders help shorted or lengthen drive life?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jman5150mib\"> /u/Jman5150mib </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l5dptd/drive_lifespans/\">[link]</a>",
        "id": 2871639,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l5dptd/drive_lifespans",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Drive lifespans",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dev_was_here",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-06-07T02:13:28.988010+00:00",
        "date_dead_since": null,
        "date_published": "2025-06-07T02:11:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I heard earlier firmware caused the SSD to prematurely die</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dev_was_here\"> /u/Dev_was_here </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l59vfq/is_the_svt02b6q_firmware_for_the_samsung_evo_870/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l59vfq/is_the_svt02b6q_firmware_for_the_samsung_evo_870/\">[comments]</a></span>",
        "id": 2870879,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l59vfq/is_the_svt02b6q_firmware_for_the_samsung_evo_870",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is the SVT02B6Q firmware for the Samsung Evo 870 safe?",
        "vote": 0
    }
]