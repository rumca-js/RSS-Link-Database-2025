[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T22:32:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jwardv/amazon_product_search_scraping_being_banned/\"> <img src=\"https://b.thumbs.redditmedia.com/iX22bPxwJGrVpCY0eg7zb5o1hsbjElPJHSIhQ1d-qYU.jpg\" alt=\"Amazon product search scraping being banned?\" title=\"Amazon product search scraping being banned?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Well well, my amazon search scraper has stopped working lately. I was working fine just 2 months ago.</p> <p>Amazon product details page still works though.</p> <p>Anybody experiencing the same lately?</p> <p><a href=\"https://preview.redd.it/125bnzvk43ue1.png?width=1424&amp;format=png&amp;auto=webp&amp;s=0615445379c647db5ad6651a5d398af9be1b844c\">https://preview.redd.it/125bnzvk43ue1.png?width=1424&amp;format=png&amp;auto=webp&amp;s=0615445379c647db5ad6651a5d398af9be1b844c</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Infamous_Tomatillo53\"> /u/Infamous_Tomatillo53 </a> <br",
        "id": 2530934,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jwardv/amazon_product_search_scraping_being_banned",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/iX22bPxwJGrVpCY0eg7zb5o1hsbjElPJHSIhQ1d-qYU.jpg",
        "title": "Amazon product search scraping being banned?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T21:27:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am tired of being cheated out of good deals, so I want to create a travel site that gathers available information on flights, hotels, car rentals and bundles to a particular set of airports. </p> <p>Has anybody been able to scrape cheap prices on Flights, Hotels, Car Rentals and/or Bundles??</p> <p>Please help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/icemelts101\"> /u/icemelts101 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jw9ape/travel_deals_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jw9ape/travel_deals_webscraping/\">[comments]</a></span>",
        "id": 2530558,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jw9ape/travel_deals_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Travel Deals Webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T19:40:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a web-scraping bot, made to scrape e-commerce pages gently (not too fast), but I don&#39;t have a proxy rotating service and am worried about being IP banned. </p> <p>Is there an open &quot;bot-testing&quot; webpage that runs a gauntlet of anti-bot tests to see if it can pass all bot tests (hopefully keeping me on the good side of the e-commerce sites for as long as possible). </p> <p>Does such a site exist? Feel free to rip into me, if such a question has been asked before, I may have overlooked a critical post.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cheesecantalk\"> /u/cheesecantalk </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jw6qu9/sites_for_detecting_bots/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jw6qu9/sites_for_detecting_bots/\">[comments]</a></span>",
        "id": 2529702,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jw6qu9/sites_for_detecting_bots",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sites for detecting bots",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T16:02:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, i am trying to upload upto 5 images and submit automatically, but the playwright not waiting until to upload and clicking submit before it finishes uploading, is there way to make it stop or wait until the upload is finished then continue executing the remaining code, thanks!<br/> Here is the code for reference<br/> <code>with sync_playwright() as p:</code></p> <p><code>browser = p.chromium.launch(headless=False)</code></p> <p><code>context = browser.new_context()</code></p> <p><code>page = context.new_page()</code></p> <p><code>&quot;ramining code&quot; to fill the data</code></p> <p><code>page.check(&quot;#privacy&quot;)</code></p> <p><code>log.info(&quot;Form filled with data&quot;)</code></p> <p><code>page.set_input_files(&quot;input[name=&#39;images[]&#39;]&quot;, paths[:5])</code></p> <p><code># page.wait_for_load_state(&quot;networkidle&quot;)</code></p> <p><code># time.sleep(15)</code></p> <p><code>page.click(&quot;button[type=&#39;",
        "id": 2527997,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jw1hqr/wait_for_upload_playwright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Wait for upload? (playwright)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T11:48:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am scraping an e-com store regularly looking at 3500 items. I want to increase the number of items I\u2019m looking at to around 20k. I\u2019m not just checking pricing I\u2019m monitoring the page looking for the item to be available for sale at a particular price so I can then purchase the item. So for this reason I\u2019m wanting to set up multiple servers who each scrape a portion of that 20k list so that it can be cycled through multiple times per hour. The problem I have is in bandwidth usage. </p> <p>A suggestion that I received from ChatGPT was to use a headers only request on each request of the page to check for modification before using selenium to parse the page. It says I would do this using an if-modified-since request.</p> <p>It says if the page has not been changed I would get a 304 not modified status and can avoid pulling anything additional since the page has not been updated.</p> <p>Would this be the best solution for limiting bandwidth costs and a",
        "id": 2525391,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jvvxv9/scraping_efficiency_limit_bandwidth",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping efficiency & limit bandwidth",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T10:44:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m considering scraping Amazon using cookies associated with an Amazon account.</p> <p>The pro is that I can access some things which require me to be logged in.</p> <p>But the con is that Amazon can track my activity at an account level, so changing IPs is basically useless.</p> <p>Does anyone take this approach? If so, have you faced rate limiting issues?</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mickspillane\"> /u/mickspillane </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvuvfr/amazon_rate_limits/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvuvfr/amazon_rate_limits/\">[comments]</a></span>",
        "id": 2525392,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jvuvfr/amazon_rate_limits",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Amazon Rate Limits?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T09:21:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a strange issue that I believe might be related to an EU proxy. For some pages that I&#39;m crawling, my crawler receives data that appears to be changed to ISO-8859-1.</p> <p>For example a jobposting snippet like this</p> <p>{&quot;@type&quot;:&quot;PostalAddress&quot;,&quot;addressCountry&quot;:&quot;DE&quot;,&quot;addressLocality&quot;:&quot;Berlin&quot;,&quot;addressRegion&quot;:null,&quot;streetAddress&quot;:null}</p> <p>I&#39;m occasionally receiving &#39;Berl\u00edn&#39; with an accent on the &#39;i&#39; .</p> <p>Is this something you&#39;ve seen before?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Strijdhagen\"> /u/Strijdhagen </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvtnwt/have_you_ever_had_proxies_in_latin_countries/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvtnwt/have_you_ever_had_proxies_in_latin_countries/\">[comments]<",
        "id": 2525393,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jvtnwt/have_you_ever_had_proxies_in_latin_countries",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Have you ever had proxies in latin countries modifying the encoding?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T07:43:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to make a temporary program that will:</p> <p>- <em>get the classes from a website</em></p> <p>- <em>append any new classes not already found in a list &quot;all_classes&quot; TO all_classes</em></p> <p>for a list of length ~150k words.</p> <p>I do have some code, but it just:</p> <ol> <li>sucks</li> <li>seems to be riddled with annoying bugs and inconsistancies</li> <li>is so slow that it takes a day or more to complete, and even then the results returned are uselessly bug-infested</li> </ol> <p>so it&#39;d be better to just start from the ground up honestly.</p> <p>Here it is anyway though:</p> <pre><code>import time, re import random import aiohttp as aio import asyncio as asnc import logging from diccionario_de_todas_las_palabras_del_espa\u00f1ol import c from diskcache import Cache # Initialize cache = Cache(&#39;scrape_cache&#39;) logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) all_c",
        "id": 2525394,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jvsd7u/i_need_to_speed_the_code_up_for_a_python_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need to speed the code up for a python scraper (aiohttp, asyncio)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T06:20:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Are there any free proxies to scrape sofascore? I am getring 403 errors and it seems my proxies are being banned. Btw is sofascore using cloudflare? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/One_Mechanic_5090\"> /u/One_Mechanic_5090 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvr7t5/scraping_sofascore_using_python/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvr7t5/scraping_sofascore_using_python/\">[comments]</a></span>",
        "id": 2524077,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jvr7t5/scraping_sofascore_using_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping sofascore using python",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-10T06:10:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, what&#39;s the best URL to text tool you&#39;ve encountered that offers an API? </p> <p>I&#39;m looking for something affordable (preferably free). </p> <p>All I need is to provide a URL to an article and retrieve the text through the API, nothing more.</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Elon_tesla_x\"> /u/Elon_tesla_x </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvr2uh/best_url_to_text_and_free_or_cheapest/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jvr2uh/best_url_to_text_and_free_or_cheapest/\">[comments]</a></span>",
        "id": 2524078,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jvr2uh/best_url_to_text_and_free_or_cheapest",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best URL to Text (and free or cheapest)",
        "vote": 0
    }
]