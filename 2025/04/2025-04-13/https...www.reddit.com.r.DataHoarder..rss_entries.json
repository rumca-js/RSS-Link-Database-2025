[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T23:24:03.651315+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T23:14:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyksb2/is_there_a_way_to_see_full_top_100_list_of_these/\"> <img src=\"https://preview.redd.it/a9gw9xr4qoue1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e2ceb52b763a2cc582e0280ff7d856857671e86\" alt=\"Is there a way to see full top 100 list of these lists? (Not only top 25)\" title=\"Is there a way to see full top 100 list of these lists? (Not only top 25)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ali_cicek2\"> /u/Ali_cicek2 </a> <br/> <span><a href=\"https://i.redd.it/a9gw9xr4qoue1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyksb2/is_there_a_way_to_see_full_top_100_list_of_these/\">[comments]</a></span> </td></tr></table>",
        "id": 2550265,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyksb2/is_there_a_way_to_see_full_top_100_list_of_these",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/a9gw9xr4qoue1.png?width=640&crop=smart&auto=webp&s=6e2ceb52b763a2cc582e0280ff7d856857671e86",
        "title": "Is there a way to see full top 100 list of these lists? (Not only top 25)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T22:18:21.479777+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T21:58:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello guys,</p> <p>I have a few TB&#39;s of data I want to store long term (30+ years), but I have a feeling of uncertainty and doubt with keeping it stored anywhere right now. </p> <p>I have been to prison once, and the police took every piece of tech from my house (i got into a major fight in someones house and the police thought it was drug related). I got all my tech back later including my hard drive, but I don&#39;t trust myself anymore with it basically.</p> <p>Also keeping it stored with any company makes it feel a little unsave, because last time I went to prison I could not pay my server bill and all my data I had there got deleted.</p> <p>Probably will never go to prison again, but the experience traumatized me, so wherever I put my data, it feels unsave. It&#39;s a lot of family photo&#39;s I want semi regular access to (weekly/monthly).</p> <p>To be honest I just want to make a few hard drive copies and hand them out to my family members ",
        "id": 2549969,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyj7gr/i_am_afraid_my_data_will_not_endure_traumatized",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I am afraid my data will not endure (traumatized)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T22:18:21.610943+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T21:45:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I&#39;m looking for an NVME based 8-12 bay enclosure that supports both direct connect Thunderbolt 4 and Ethernet, preferably 10Gbe or 2.4Gbe at the very minimum. This will be used for local storage to edit and then upload to our NAS/DAM other the network.</p> <p>Does anyone have recommendations or know of any solid units that fit this? I don&#39;t mind if it has a PCIe 16x card connected to a main editor, but I still need the Thunderbolt in case we need to download footage to a laptop or external NVME drive to edit a project offline.</p> <p>Any ideas or suggestions would be greatly appreciated!!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/likelinus01\"> /u/likelinus01 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyixq8/nvme_raid_enclosure_recommendation_thunderbolt/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyixq8/nvme_raid_enclo",
        "id": 2549970,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyixq8/nvme_raid_enclosure_recommendation_thunderbolt",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NVME RAID Enclosure Recommendation - Thunderbolt and Ethernet",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T22:18:21.322920+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T21:43:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone else use something like Advanced Intrusion Detection Environment (AIDE) to validate file checksums? I have some NTFS-formatted drives for which it&#39;d be handy (so I could use it similar to ZSF/BTRFS bitrot checker)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NatSpaghettiAgency\"> /u/NatSpaghettiAgency </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyiw6j/opinions_on_using_an_intrusion_detection_system/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyiw6j/opinions_on_using_an_intrusion_detection_system/\">[comments]</a></span>",
        "id": 2549968,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyiw6j/opinions_on_using_an_intrusion_detection_system",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Opinions on using an Intrusion Detection System as a bitrot checker?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T21:14:01.264660+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T20:42:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I know approximately nothing about tech so if this is a really stupid question please let me know. I&#39;ve backed up my tumblr blogs using <a href=\"https://github.com/cebtenzzre/tumblr-utils/blob/master/README.md\">tumblr-backup by cebtenzzre</a> to my computer, so now the question is how to actually upload them to internet archive. Tumblr-backup does not save the blog as one singular file, but as multiple file folders holding [in the case of the blogs I&#39;m archiving] many files each. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Here_Be_Drag0ns\"> /u/Here_Be_Drag0ns </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyhj54/how_to_backup_tumblr_blogs_saved_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyhj54/how_to_backup_tumblr_blogs_saved_with/\">[comments]</a></span>",
        "id": 2549749,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyhj54/how_to_backup_tumblr_blogs_saved_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to backup tumblr blogs saved with tumblr-backup to the internet archive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T20:09:22.905874+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T20:03:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m interested in annotating some TV episodes and Movies down to the individual scene (or even frame). For example, I might want to annotating Star Trek: TNG S01E03 or Star Trek: Wrath or Khan to indicate the presence of a character on screen. I could then use those annotations to ask questions like &quot;what percent of the show is this character on screen&quot; or &quot;how many total seconds of the show are these two characters in the same room together in a scene?&quot;, depending on how I structure the annotations. </p> <p>As I see it there are two hard-ish problems I don&#39;t know the best solution to here:</p> <ol> <li><p>How do I ensure that if I annotate &quot;+00:14:21.512 to +00:16:01.001 - Picard is on screen&quot; that those time stamps meaningfully map onto the most common or standardized time stamps so others who might want to use them and map them to a video file would be likely to get the same points in time. I&#39;ve thought abo",
        "id": 2549347,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jygmff/best_practices_for_annotating_tv_and_movies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Practices for Annotating TV and Movies?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T20:09:22.572886+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T20:00:56+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jygkk0/where_are_my_tb5_4_bay_nvme_enclosures/\"> <img src=\"https://preview.redd.it/7c5hi7sjrnue1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e44de8e393a344a62677e612366c1fe1767f784c\" alt=\"Where are my TB5 4 Bay NVMe enclosures?\" title=\"Where are my TB5 4 Bay NVMe enclosures?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Single slot Thunderbolt 5 NVMe enclosures are taking their sweet time to hit the market and have available stock. Most are not even being announced as officially being Thunderbolt 5, only mentioning 80gbps.</p> <p>Does anyone have news on updates to the current Thunderbolt 3 offerings from OWC, StarTech and others to less bottlenecked Thunderbolt 5 versions of their enclosures?</p> <p>Looking to build a 32TB RAID0 DAS but haven&#39;t even been able to find any news on intention from a manufacturer of releasing such a product, let alone an ETA on availability. Am I missing something?</",
        "id": 2549345,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jygkk0/where_are_my_tb5_4_bay_nvme_enclosures",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/7c5hi7sjrnue1.jpeg?width=640&crop=smart&auto=webp&s=e44de8e393a344a62677e612366c1fe1767f784c",
        "title": "Where are my TB5 4 Bay NVMe enclosures?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T20:09:22.719264+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T19:40:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So these high capacity Seagate drives that are cheap on serverpartdeals and in the Best Buy external enclosures that are believed to be binned 30TB HAMR drives...are these safe to put in an enclosure with more than 4 bays?</p> <p>It was my understanding that at least for some Seagate HAMR drives that they should only be put in a Seagate disk shelf so that it controls how many drives adjecent to one another are spinning at the same time because of low vibration tolerance. Does anyone know if that&#39;s the case for these drives?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TootSweetBeatMeat\"> /u/TootSweetBeatMeat </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyg40c/seagate_hamrs_you_know_the_ones_i_mean_safe_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyg40c/seagate_hamrs_you_know_the_ones_i_mean_safe_for/\">[comments]</a></span>",
        "id": 2549346,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyg40c/seagate_hamrs_you_know_the_ones_i_mean_safe_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate HAMRs (you know the ones I mean) safe for multi-bay enclosures?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T20:09:22.249892+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T19:12:01+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyffxe/more_roadblocks_with_reprogramming_lto_tape_drives/\"> <img src=\"https://b.thumbs.redditmedia.com/DO80WHBg4ablheTj9P-kl75edAej6e3JMhf709KXNRY.jpg\" alt=\"More roadblocks with reprogramming LTO tape drives\" title=\"More roadblocks with reprogramming LTO tape drives\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>To begin, I\u2019m posting this a day early before I get home from Spain holiday so I can get plenty of replies with advice so that I can immediately start trying to resolve my roadblock with reprogramming those tape drives so it might be a few hours before I can actually start putting your help to good use and so I can start relying on what worked and what didn\u2019t, those replies will come later unless I have already tried this or to ask a question about it.</p> <p>I have all of the Linux commands ready to go to transmit the HEX data which is shown in a picture and transcribed below (I used a different c",
        "id": 2549343,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyffxe/more_roadblocks_with_reprogramming_lto_tape_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/DO80WHBg4ablheTj9P-kl75edAej6e3JMhf709KXNRY.jpg",
        "title": "More roadblocks with reprogramming LTO tape drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T20:09:22.385991+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T19:05:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Sorry if this is too off topic. If it is feel free to delete.</p> <p>A few months ago I was mailed 11 umatic tapes from an anonymous source that have footage from the canceled Yellow Subarmine sequel- Strawberry Fields. The tapes are moldy and while they have been baked (albeit somewhat poorly) they are in need of a cleaning and above all digitization. The person I mailed them to had his machine break down the same day they arrived and we have been struggling to find someone else who&#39;s willing to do this for free. I do not have steady income and cannot pay the extraordinary fees to have these tapes done by a company.</p> <p>If anyone here has the ability and time to digitize these tapes for us, it would be an incredible help. I am producing a documentary on the studio the film was being produced in as well as building a digital archive of the material that&#39;s been recovered.</p> <p>The tapes are currently in Delaware. Sorry, should&#39;ve said ",
        "id": 2549344,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyfack/need_probono_umatic_digitizing_service_based_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need pro-bono umatic digitizing service - based in Dallas, Texas",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T19:04:28.446320+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T18:31:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi folks,</p> <p>As my storage needs grow, I&#39;ve been considering moving away from my Synology 2419+ (which is used only as NAS, no compute workloads) to a custom build. Ideally, I don&#39;t want to deal with old, large, and noisy rack-mounted units. Right now I&#39;m sitting at ~120TB of usable storage, but due to certain limitations of this specific Synology unit (108TB volume size limit), it creates certain inconveniences that I&#39;d like to avoid in the future. With that being said, here&#39;s the list of my requirements:</p> <ol> <li>300 - 400TB usable capacity in the next 2-3 years.</li> <li>Hot swapping</li> <li>At least 2.5G networking, probably dual NICs, but that&#39;s not a hard requirement</li> <li>No need for redundant PSU, since it won&#39;t be running anything &quot;mission critical&quot; and I&#39;d like to keep things relatively quiet and power efficient.</li> </ol> <p>I&#39;m not 100% sure if my requirements are throwing me into ",
        "id": 2549062,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyehnf/new_nas_build_help_needed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New NAS build help needed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T18:00:50.256528+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T17:47:45+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jydg4p/this_is_what_a_digital_coup_looks_like_carole/\"> <img src=\"https://external-preview.redd.it/Y2M4uZhmBJvIiWQHq83PCKikmfQOsTH6_nlwY_epFao.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=768781fdd95f0e247c2f0ef79803df15cd9d9ac8\" alt=\"This Is What a Digital Coup Looks Like | Carole Cadwalladr | TED - YouTube\" title=\"This Is What a Digital Coup Looks Like | Carole Cadwalladr | TED - YouTube\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sparky1492\"> /u/sparky1492 </a> <br/> <span><a href=\"https://youtu.be/TZOoT8AbkNE?si=0S6FQmWeizz4p9K2\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jydg4p/this_is_what_a_digital_coup_looks_like_carole/\">[comments]</a></span> </td></tr></table>",
        "id": 2548762,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jydg4p/this_is_what_a_digital_coup_looks_like_carole",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Y2M4uZhmBJvIiWQHq83PCKikmfQOsTH6_nlwY_epFao.jpg?width=320&crop=smart&auto=webp&s=768781fdd95f0e247c2f0ef79803df15cd9d9ac8",
        "title": "This Is What a Digital Coup Looks Like | Carole Cadwalladr | TED - YouTube",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T18:00:50.024078+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T17:30:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyd1to/why_do_hard_drives_fail_you_cant_always_blame/\"> <img src=\"https://external-preview.redd.it/xwpGhkbgBPdPgUl9O2ZRZllRhxC7U2hkGM99arlljDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8795746658b2945899c158481bd63912375dbd3\" alt=\"Why Do Hard Drives fail? You can't always blame Seagate, Western Digital or Toshiba.\" title=\"Why Do Hard Drives fail? You can't always blame Seagate, Western Digital or Toshiba.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/threwusall\"> /u/threwusall </a> <br/> <span><a href=\"https://youtu.be/8zoXVSv_9Js\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyd1to/why_do_hard_drives_fail_you_cant_always_blame/\">[comments]</a></span> </td></tr></table>",
        "id": 2548761,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyd1to/why_do_hard_drives_fail_you_cant_always_blame",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/xwpGhkbgBPdPgUl9O2ZRZllRhxC7U2hkGM99arlljDc.jpg?width=320&crop=smart&auto=webp&s=b8795746658b2945899c158481bd63912375dbd3",
        "title": "Why Do Hard Drives fail? You can't always blame Seagate, Western Digital or Toshiba.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T16:54:25.170528+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T16:19:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>While i actually use a 1TB EVO 860 for my OS, my 850 EVO 500GB is starting to be low of space, so i thought of upgrading it to 2TB.... That, and that the actual economy is getting troublesome so before prices spikes the hell out i&#39;d rather get a new SSD!</p> <p>I heard long time ago that SAMSUNG&#39;s EVO 870 SSDs were having a bad batch, but after some years i wanted to ask:</p> <p>-Have they solved the issue right out of the box? (No news from SAMSUNG&#39;s side, that&#39;s why). If so, can i check wherever outside of the box part to see if i&#39;ll get a fixed version?<br/> -Would a firmware update be needed?<br/> -Is the 2TB model safe?. Heard below 2TB it is but 2TB and above could be troublesome</p> <p>-How are the writting speeds compared to EVO 850 and 860?</p> <p>(Can&#39;t use a M.2 due to trying to put one almost incorrectly in my Mobo as an OS and it made the slot smell, so i don&#39;t wanna try putting anything there again... Rest of ",
        "id": 2548443,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jybenq/evo_870_safe_to_buy_now",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "EVO 870 safe to buy now?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T16:54:25.303916+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T15:50:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m aware that this is preaching to the choir and that most of you will already have some automated yt-dlp setup running (or even stocking your Jellyfin library directly with Youtube-content via pinchflat or similar), but if you&#39;re not then I&#39;d like to give you another reason to start sooner rather than later: </p> <p><strong>I think I&#39;m witnessing an increasing trend of channel owners retroactively putting old videos behind a channel-member paywall.</strong><br/> (Maybe it&#39;s just my own subscriptions, I&#39;d rather be crazy than right in this regard) </p> <p>So in addition to content violations, intellectual-property-related takedowns, georestrictions, IP-bans and Youtube constantly doing their best to permanently break download tools I now feel I&#39;m also racing against the channel owners themselves in trying to ensure permanent access to my preferred media selection. </p> <p>If you like it, download it now. At some point in t",
        "id": 2548444,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyaqh3/youtube_videos_get_them_while_you_can",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Youtube videos - get them while you can",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T15:49:23.483718+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T15:47:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I have been testing my web search scraper - it can run 10k+ searches per hour.</p> <p>I need ideas to create demo projects. We could then load the search results into a vector db and build a RAG etc.</p> <p>May be something like: </p> <ul> <li> <code>${city} ${keyword}</code> to build city profiles around a topic.</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nib1nt\"> /u/nib1nt </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyanes/what_would_you_do_with_unlimited_web_searches/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jyanes/what_would_you_do_with_unlimited_web_searches/\">[comments]</a></span>",
        "id": 2548094,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jyanes/what_would_you_do_with_unlimited_web_searches",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What would you do with *unlimited web searches?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T15:49:23.616503+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T15:17:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I&#39;m planning an offline + offsite long-term backup of family photos and would love a sanity check from the community.</p> <p>I own an LG BH16NS40 (2013 model) internal Blu-ray writer with support for writing BDXL and M-DISC. According to the original manual (2013) and LG support (as of 2021), it however officially supports M-DISC DVD+R SL only, not M-DISC BD:</p> <blockquote> </blockquote> <p>I&#39;m considering two M-DISC DVD options:</p> <ul> <li><a href=\"https://m.media-amazon.com/images/I/81H27BbtjdL._AC_SL1500_.jpg\">25-pack for \u20ac130.85 (\u20ac1.11/GB) - US import, Millenniata branding</a></li> <li><a href=\"https://m.media-amazon.com/images/I/61Co4hPPR4L._AC_SL1200_.jpg\">25-pack for \u20ac59.50 (\u20ac0.51/GB) - Ritek, available locally in Germany</a></li> </ul> <p>I&#39;m leaning toward the Ritek discs, since they appear to be officially licensed and are cheaper.</p> <p>With concerns over the long-term reliability of modern Verbatim BD M-DISC",
        "id": 2548095,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy9zzz/ritek_mdisc_dvd_in_2025_the_best_solution_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "RITEK M-DISC DVD in 2025 \u2013 The Best Solution for Offline + Offsite Long-Term Archiving?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T14:44:29.609800+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T14:15:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, I found this on Amazon: <a href=\"https://www.amazon.com/dp/B0DW8ZW47C\">https://www.amazon.com/dp/B0DW8ZW47C</a></p> <p>It is 22tb for 249 which makes it $11.32 per TB which I think is a good deal compare to recent prices increase from SPD and GHD on Ebay.</p> <p>I&#39;d like to buy one of those, shuck it and put it into my NAS.</p> <p>How do I know if this can be shucked. I&#39;ve never done it before.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vinznsk\"> /u/vinznsk </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy8mxu/how_do_i_know_if_i_can_shuck_an_external_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy8mxu/how_do_i_know_if_i_can_shuck_an_external_drive/\">[comments]</a></span>",
        "id": 2547778,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy8mxu/how_do_i_know_if_i_can_shuck_an_external_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I know if I can shuck an external drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T13:39:34.378372+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T12:53:23+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy6yq6/anyone_having_issues_with_opendrive/\"> <img src=\"https://a.thumbs.redditmedia.com/RWTWeIrxzhWd_9laWVYDgEdcsIpV69MmxfanPZo2Qj0.jpg\" alt=\"Anyone having issues with opendrive?\" title=\"Anyone having issues with opendrive?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi all - am a premium/home customer. </p> <p>uploads are way below 10tb, but linked my opendrive to rclone. I did not subscribe to Opendrive to hoard data, but just to keep my more valuable multimedia items, and access them via Rclone when needed. </p> <p>Suddenly my downloads are being throttled to 500kb/s which is causing severe buffering. This is not what I signed up for - the terms and conditions say that &quot;OpenDrive does not throttle download speeds on any of its plans, including the free one&quot; I&#39;ve tested in multiple locations, with/without VPNs, and the speed is the same</p> <p>Can someone please advise.</p> <p>If this is",
        "id": 2547434,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy6yq6/anyone_having_issues_with_opendrive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/RWTWeIrxzhWd_9laWVYDgEdcsIpV69MmxfanPZo2Qj0.jpg",
        "title": "Anyone having issues with opendrive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T13:39:34.027345+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T12:42:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I hooked a drive to a really old laptop I had rebuilt and was missing drivers for a lot of my files. That got me thinking that I need to make sure my files are in the most universal format possible. Documents in pdf and non Adobe pdf reader on all devices and drives, books as epub, sound files as mp3, pictures as jpg. What format would be best for my video files? I am pursuing accessibility instead of lossless storage obviously. I use windows/android devices and vlc media player and have a large codec library but what if I need to connect my drives to a basic device? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/angegowan\"> /u/angegowan </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy6rut/universal_video_format/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy6rut/universal_video_format/\">[comments]</a></span>",
        "id": 2547433,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy6rut/universal_video_format",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Universal video format?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T12:35:19.990607+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T12:27:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://reddit.com/link/1jy6hv8/video/4tak7vufilue1/player\">https://reddit.com/link/1jy6hv8/video/4tak7vufilue1/player</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PercentageMindless95\"> /u/PercentageMindless95 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy6hv8/i_unsubscribed_to_netflix_disney_and_set_up_my_own/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy6hv8/i_unsubscribed_to_netflix_disney_and_set_up_my_own/\">[comments]</a></span>",
        "id": 2546865,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy6hv8/i_unsubscribed_to_netflix_disney_and_set_up_my_own",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I unsubscribed to Netflix & Disney+ and set up my own",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T12:35:20.266228+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T11:42:18+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy5r65/best_hdd_of_wd_pc_use_get_most_tb_or_stick_to/\"> <img src=\"https://b.thumbs.redditmedia.com/s1H8XlKjuN6t5HPU52BHlmkHHoXyamiPzzI7c_F-1sc.jpg\" alt=\"Best HDD of WD, pc use, get most TB or stick to something lower, want max TB personal use but don\u2019t know if it gets worse the higher TB you go. Need 2 drives for storing movies.\" title=\"Best HDD of WD, pc use, get most TB or stick to something lower, want max TB personal use but don\u2019t know if it gets worse the higher TB you go. Need 2 drives for storing movies.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sweatydoodoo\"> /u/sweatydoodoo </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1jy5r65\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy5r65/best_hdd_of_wd_pc_use_get_most_tb_or_stick_to/\">[comments]</a></span> </td></tr></table>",
        "id": 2546867,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy5r65/best_hdd_of_wd_pc_use_get_most_tb_or_stick_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/s1H8XlKjuN6t5HPU52BHlmkHHoXyamiPzzI7c_F-1sc.jpg",
        "title": "Best HDD of WD, pc use, get most TB or stick to something lower, want max TB personal use but don\u2019t know if it gets worse the higher TB you go. Need 2 drives for storing movies.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T12:35:19.852769+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T11:20:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy5fa1/questions_science_is_yet_to_answer_somehow/\"> <img src=\"https://b.thumbs.redditmedia.com/-UGUEU3KSiL5D8VAVPUYb6nOEwHAdOOiAMJsLQw4CIc.jpg\" alt=\"Questions science is yet to answer: Somehow, transferred 12.81TB of data from 4TB drive to a 8TB drive, and it's only 1/3rd done so far.\" title=\"Questions science is yet to answer: Somehow, transferred 12.81TB of data from 4TB drive to a 8TB drive, and it's only 1/3rd done so far.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/c0dpboyg6lue1.png?width=718&amp;format=png&amp;auto=webp&amp;s=e01bce76746f70aa66f90010a70b040199d5167f\">https://preview.redd.it/c0dpboyg6lue1.png?width=718&amp;format=png&amp;auto=webp&amp;s=e01bce76746f70aa66f90010a70b040199d5167f</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CyberpunkLover\"> /u/CyberpunkLover </a> <br/> <span><a href=\"https://www.reddit.com/",
        "id": 2546864,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy5fa1/questions_science_is_yet_to_answer_somehow",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/-UGUEU3KSiL5D8VAVPUYb6nOEwHAdOOiAMJsLQw4CIc.jpg",
        "title": "Questions science is yet to answer: Somehow, transferred 12.81TB of data from 4TB drive to a 8TB drive, and it's only 1/3rd done so far.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T12:35:20.132737+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T11:12:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The Internet Archive&#39;s &#39;Great 78 Project&#39; digitizes historical recordings to preserve musical heritage, but in 2023 the initiative led to major record labels filing a copyright lawsuit. The financial stakes soared last month when the labels proposed to update their claim to $693 million in statutory damages. A recent filing suggests that due to significant progress in settlement discussions, it may not come to that.<br/> +++++++++++++</p> <p>FULL ARTICLE:<br/> <a href=\"https://torrentfreak.com/internet-archive-v-music-labels-500m-copyright-rift-edges-toward-settlement-250409/\">https://torrentfreak.com/internet-archive-v-music-labels-500m-copyright-rift-edges-toward-settlement-250409/</a></p> <p>Where to follow the lawsuit (and get updates):<br/> <a href=\"https://www.courtlistener.com/docket/68101636/umg-recordings-inc-v-internet-archive/?order_by=desc\">https://www.courtlistener.com/docket/68101636/umg-recordings-inc-v-internet-archive/?ord",
        "id": 2546866,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy5at5/internet_archive_vs_music_labels_600m_copyright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Internet Archive vs. Music Labels: $600m+ Copyright Rift Edges Toward Settlement",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T09:43:31.156554+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T09:40:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If your from the UK what price per TB would you generally pay ? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GenericUser104\"> /u/GenericUser104 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy405b/if_your_from_the_uk_what_price_per_tb_would_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jy405b/if_your_from_the_uk_what_price_per_tb_would_you/\">[comments]</a></span>",
        "id": 2546584,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jy405b/if_your_from_the_uk_what_price_per_tb_would_you",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "If your from the UK what price per TB would you generally pay ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T05:23:13.867398+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T04:54:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After reading a lot of very contradictory posts about which drives are loud and which are quiet I&#39;ve come to the conclusion that people mean different things when that complain about noise. </p> <p>I&#39;m only concerned about the sound of the actuator moving not sound the drive spinning.</p> <p>So for those who have experience with more than a handful of drives, please chime in on, which are the best refurbished 16TB drives to get?</p> <p>Use case: plex server 10 feet from by bed (no I can&#39;t put it in another room).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Eco-Libertarian\"> /u/Eco-Libertarian </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxzzh1/what_enterprise_drives_have_the_least_seek_not/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxzzh1/what_enterprise_drives_have_the_least_seek_not/\">[comments]</a></span>",
        "id": 2545769,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxzzh1/what_enterprise_drives_have_the_least_seek_not",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What enterprise drives have the least seek (not spin) noise?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T05:23:14.000063+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T04:46:38+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxzux2/has_anyone_tried_one_of_these_with_2tb_microsd/\"> <img src=\"https://preview.redd.it/yunu6cv08jue1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a6222c4d954bdc04427b29576d242b1567fbed89\" alt=\"Has anyone tried one of these with 2TB microSD cards?\" title=\"Has anyone tried one of these with 2TB microSD cards?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://youtu.be/3frnBoqqI_Q?si=aF01m5oBJqE5JLUx\">https://youtu.be/3frnBoqqI_Q?si=aF01m5oBJqE5JLUx</a> </p> <p>Now that we have 2TB microSD cards, has anyone tried to make a 20TB SATA SSD running 10 microSD cards on one of these RAID0 cards?<br/> Just like when the product came out, this is still a stupid setup, but at least now you can make the argument for storage density.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DiogoAlmeida97\"> /u/DiogoAlmeida97 </a> <br/> <span><a href=\"https://i.redd.it/yu",
        "id": 2545770,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxzux2/has_anyone_tried_one_of_these_with_2tb_microsd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/yunu6cv08jue1.jpeg?width=640&crop=smart&auto=webp&s=a6222c4d954bdc04427b29576d242b1567fbed89",
        "title": "Has anyone tried one of these with 2TB microSD cards?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T05:23:14.181564+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T04:27:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello community. I am trying to set up a RAID to backup some data I need another copy of, so I reached for an (old, loud) 4-bay USB case I had, which had *four* different WesternDigital WD Blue drives at 6 tb. For some reason none of the disks would mount or be recognized by my Mac or PC. They seemed to be spinning (they were vibrating, or seemed to be), but nothing would cause them to be recognized. So I bought a new enclosure but they performed the same, with all six disks refusing to mount (despite the fact hat I&#39;ve used these for RAID backups in the past).</p> <p>I took a spare Seagate and put it in my old enclosure and it worked perfectly. Is there any reason why WE Blue disks could all fail in a way that caused them to not even be seen by the OS? I read something about pin 3 needing to be disabled from certain WesternDigital drives to work in USB and applied tape over that pin (and the two to the left of pin 3), but it made no difference.</p",
        "id": 2545771,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxzk92/four_wd_blue_drives_refuse_to_mount_in_any_usb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Four WD Blue Drives Refuse To Mount In Any USB Enclosure",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T03:12:48.917921+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T02:40:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building my first small NAS from an old PC just to see if I could do it. Four 4TB WD Red with an SSD Boot running OpenMediaVault. Everything going together nicely, and I&#39;m dusting the cobwebs off my limited computer building and Unix/Linux experience from literally decades ago. Enjoying myself quite a bit, actually.</p> <p>I&#39;m fully aware that RAID &quot;is not a backup&quot;, except in my case this RAID system is literally a backup. I don&#39;t plan to work off this NAS; instead it will be a place to back up other things. Phones, pictures, computers, etc. If I get everything working I will immediately start on a better (larger, faster) system with a goal of eliminating all cloud storage. VPN for remote access, media server, etc. But this one will remain as a backup.</p> <p>It was taking forever just to create the RAID 5 on this old computer. I see that OMV wants a restart, so I start researching whether it&#39;s possible/suggested to ",
        "id": 2545444,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxxsyd/raid_5_6_or_10",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "RAID 5, 6, or 10",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T02:07:49.371300+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T01:56:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been googling and searching Reddit and can&#39;t find an answer. For digital magazines or ebooks etc. that are hosted on their own sites, is there a way to download that I&#39;m missing? The highest quality images I can get from the backend scrambles the image, making it worthless unless I want to go into photoshop and do a puzzle. This is something I&#39;ve seen multiple times, so I&#39;m guessing ebook sites use this method commonly, but that also means a good solution should be out there, right? If there is, could anyone let me know? Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/trishbaby\"> /u/trishbaby </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxx0ox/digital_magazine_download_helper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxx0ox/digital_magazine_download_helper/\">[comments]</a></span>",
        "id": 2545322,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxx0ox/digital_magazine_download_helper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Digital Magazine Download Helper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T02:07:49.558687+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T01:18:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I have a 20 TB WD Elements full of TV and movies but when I hook it up to watch video on my TV I get this option. I always choose to just open but curious to what happens if I choose to check and repair </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Rotisseriejedi\"> /u/Rotisseriejedi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxwdf1/what_actually_happens_when_you_choose_check_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxwdf1/what_actually_happens_when_you_choose_check_and/\">[comments]</a></span>",
        "id": 2545323,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxwdf1/what_actually_happens_when_you_choose_check_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What actually happens when you choose \u201ccheck and repair\u201d option on an external HD while hooked up to TV?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-13T01:02:37.065272+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-13T00:38:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been trying singlefile extension in firefox, but it simply doesn&#39;t load some pages. (the archives are from twitter)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Halfblood200\"> /u/Halfblood200 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxvn3a/looking_for_a_reliable_way_to_backup_both_html/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jxvn3a/looking_for_a_reliable_way_to_backup_both_html/\">[comments]</a></span>",
        "id": 2545133,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jxvn3a/looking_for_a_reliable_way_to_backup_both_html",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a reliable way to backup both html and json Wayback Machine archives.",
        "vote": 0
    }
]