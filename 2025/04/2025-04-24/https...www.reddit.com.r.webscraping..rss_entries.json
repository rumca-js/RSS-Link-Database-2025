[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-24T22:19:55.519021+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-24T21:07:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks, I&#39;m working on scraping data from multiple websites, and one of the most time-consuming tasks has been selecting the best CSS selectors. I&#39;ve been doing it manually using F12 in Chrome.</p> <p>Does anyone know of any tools or extensions that could make this process easier or more efficient? I&#39;m using Scrapy for my scraping projects.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/study_english_br\"> /u/study_english_br </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k73ggy/tool_to_speed_up_css_selector_picking_for_scrapy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k73ggy/tool_to_speed_up_css_selector_picking_for_scrapy/\">[comments]</a></span>",
        "id": 9318,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k73ggy/tool_to_speed_up_css_selector_picking_for_scrapy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tool to speed up CSS selector picking for Scrapy?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-24T21:15:53.166209+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-24T06:38:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Cookie farming Proxy</p> <p>I&#39;m trying to create a workflow where I can farm cookies from target</p> <p>Anyone know of a good approach to proxies? This will be in playwright. Currently I have my workflow</p> <ul> <li>loop through X amount of proxies <ul> <li>start browser and set up with proxy</li> <li>go to target account to redirect to login</li> <li>try to login with bogus login details</li> <li>go to a product</li> <li>try to add to product</li> <li>store cookie and organize by proxy</li> <li>close browser</li> </ul></li> </ul> <p>From what I can see in the cookies, it does seem to set them properly. &quot;Properly&quot; as in I do see the anti-bot cookies / headers being set which you wont otherwise get with their redsky endpoints. My issue here is that I feel like farming will get IPs shaped eventually and I&#39;d be wasting money. Or that sometimes using playwright + proxy combo doesnt always work but that&#39;s a different convo for anothe",
        "id": 6124,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k6lm2x/proxy_cookie_farming",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Proxy cookie farming",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-24T21:15:53.415479+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-24T04:14:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve made a bot with selenium to automate a task that I have on my job, and I&#39;ve done with searching for inputs and buttons using xpath like I&#39;ve done in others webscrappers, but this time I wanted to upgrade my skills and decided to automate it using HTTP requests, but I got lost, as soon as I reach the third site that will give me the result I want I simply cant get the response I want from the post, I&#39;ve copy all headers and payload but it still doesn&#39;t return the page I was looking for, can someone analyze where I&#39;m wrong. Steps to reproduce: 1- <a href=\"https://www.sefaz.rs.gov.br/cobranca/arrecadacao/guiaicms\">https://www.sefaz.rs.gov.br/cobranca/arrecadacao/guiaicms</a> - Select ICMS Contribuinte Simples Nacional and then the next select code 379 2- date you can put tomorrow, month and year can put march and 2024, Inscri\u00e7\u00e3o Estadual: 267/0031387 3- this site, the only thing needed is to put Valor, can be any, let&#39;s p",
        "id": 6126,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k6jcfl/need_help_with_http_requests",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help with http requests",
        "vote": 0
    }
]