[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-11T17:40:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have already installed Selenium on my mac but when i am trying to download chrome web driver its not working. I have installed the latest but it doesnt have the webdriver of chrome, it has:<br/> 1) google chrome for testing<br/> 2)resources folder<br/> 3)PrivacySandBoxAttestedFolder<br/> How to handle this please help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dangerous_Ad322\"> /u/Dangerous_Ad322 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jwvrrg/how_to_download_selenium_webdriver/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jwvrrg/how_to_download_selenium_webdriver/\">[comments]</a></span>",
        "id": 2538530,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jwvrrg/how_to_download_selenium_webdriver",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download Selenium Webdriver?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-11T15:30:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I&#39;m building a tool to scrape all articles from a news website. The user provides only the homepage URL, and I want to automatically find all article URLs (no manual config per site).</p> <p>Current stack: Python + Scrapy + Playwright.</p> <p>Right now I use sitemap.xml and sometimes RSS feeds, but they\u2019re often missing or outdated.</p> <p>My goal is to crawl the site and detect article pages automatically.</p> <p>Any advice on best practices, existing tools, or strategies for this?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Empty_Channel7910\"> /u/Empty_Channel7910 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jwsobk/how_to_automatically_extract_all_article_urls/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jwsobk/how_to_automatically_extract_all_article_urls/\">[comments]</a></span>",
        "id": 2538531,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jwsobk/how_to_automatically_extract_all_article_urls",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to automatically extract all article URLs from a news website?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-11T09:51:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Amazon added login request to see more than 10 reviews for a specific ASIN. </p> <p>Is there any API to provide this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/adibalcan\"> /u/adibalcan </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jwm2aw/api_for_getting_more_than_10_reviews_at_amazon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jwm2aw/api_for_getting_more_than_10_reviews_at_amazon/\">[comments]</a></span>",
        "id": 2533628,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jwm2aw/api_for_getting_more_than_10_reviews_at_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "API for getting more than 10 reviews at Amazon",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-11T00:18:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone!</p> <p>I\u2019m looking for a way to check an entire website for grammatical errors and typos. I haven\u2019t been able to find anything that makes sense yet, so I thought I\u2019d ask here.</p> <p>Here\u2019s what I want to do:</p> <p>1) Scrape all the text from the entire website, including all subpages. 2) Put it into ChatGPT (or a similar tool) to check for spelling and grammar mistakes. 3) Fix all the errors.</p> <p>The important part is that I need to keep track of where the text came from \u2013 meaning I want to know which URL on the website the text was taken from in case i find errors in ChatGPT</p> <p>Alternatively, if there are any good, affordable, or free AI tools that can do this directly on the website, I\u2019d love to know!</p> <p>Just to clarify, I\u2019m not a developer, but I\u2019m willing to learn.</p> <p>Thanks in advance for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ProposalAdept\"> /u/ProposalAdep",
        "id": 2531517,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jwcy1y/checking_a_whole_website_for_spellinggrammar",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Checking a whole website for spelling/grammar mistake",
        "vote": 0
    }
]