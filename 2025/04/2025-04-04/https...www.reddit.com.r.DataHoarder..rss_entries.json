[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T20:55:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I know this has been posted before but I tried httracker and cyotek doesn&#39;t support java so I can&#39;t really find any other option at the moment.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Greedy_Drama_5218\"> /u/Greedy_Drama_5218 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrmg25/any_ideas_for_downloading_an_entire_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrmg25/any_ideas_for_downloading_an_entire_website/\">[comments]</a></span>",
        "id": 2487191,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrmg25/any_ideas_for_downloading_an_entire_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any ideas for downloading an entire website?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T20:14:48+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PricePerGig\"> /u/PricePerGig </a> <br/> <span><a href=\"https://pricepergig.com/us?warrantyMonths=0\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrli9x/i_added_warranty_filter_to_pricepergigcom_as/\">[comments]</a></span>",
        "id": 2487190,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrli9x/i_added_warranty_filter_to_pricepergigcom_as",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I added Warranty filter to PricePerGig.com as requested on this sub",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T19:06:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Not at liberty to say more. Please back up</p> <p>Treesearch <a href=\"https://research.fs.usda.gov/treesearch\">https://research.fs.usda.gov/treesearch</a></p> <p>And the Forest Service&#39;s Research Data Archive <a href=\"https://www.fs.usda.gov/rds/archive/\">https://www.fs.usda.gov/rds/archive/</a></p> <p>If we don&#39;t already have it. It&#39;s original data going back a century or more.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/stewie3128\"> /u/stewie3128 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrjvsh/usdausfs_research_and_development_headed_for_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrjvsh/usdausfs_research_and_development_headed_for_the/\">[comments]</a></span>",
        "id": 2486762,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrjvsh/usdausfs_research_and_development_headed_for_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "USDA/USFS Research and Development headed for the same fate as NOAA data in coming days",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T18:55:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just want to hear some perspectives. I\u2019m just a hobbyist and really don\u2019t want to lose my irreplaceable photos.</p> <p>I\u2019m currently running my backup NAS with 1 disk redundancy, but maybe that\u2019s overkill?</p> <p>Wondering what the norm is around here. Grateful for any thoughts/perspectives.</p> <p><strong><em>EDIT: important context!! I ask this question with the assumption that a \u201c3-2-1\u201d backup situation is already in place \u2014 since \u201c3-2-1\u201d doesn\u2019t dictate how many disks of redundancy to use\u2026 because\u2026 of course\u2026 RAID is not a backup. :)</em></strong></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/icysandstone\"> /u/icysandstone </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrjmn0/are_you_backing_up_your_nas_with_another_nas_that/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrjmn0/are_you_backing_up_your_nas_with_another_nas_that/\">[comments",
        "id": 2486763,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrjmn0/are_you_backing_up_your_nas_with_another_nas_that",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are you backing up your NAS with another NAS that has 1 disk redundancy (SHR-1, RAID-5) simply JBOD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T18:08:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Here&#39;s an example:<br/> <a href=\"https://www.linkedin.com/posts/seansemo_takeaction-buildyourdream-entrepreneurmindset-activity-7313832731832934401-Eep_/\">https://www.linkedin.com/posts/seansemo_takeaction-buildyourdream-entrepreneurmindset-activity-7313832731832934401-Eep_/</a></p> <p>I tried:<br/> - .m3u8 search (doesn&#39;t find it)<br/> <a href=\"https://stackoverflow.com/questions/42901942/how-do-we-download-a-blob-url-video\">https://stackoverflow.com/questions/42901942/how-do-we-download-a-blob-url-video</a><br/> - HLS Downloader<br/> - FetchV<br/> - copy/paste link from Console (but it&#39;s only an image in those &quot;blob&quot; cases)</p> <p>- this subreddit thread/post had ideas that didn&#39;t work for me<br/> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ab8812/how_to_download_blob_embedded_video_on_a_website/\">https://www.reddit.com/r/DataHoarder/comments/1ab8812/how_to_download_blob_embedded_video_on_a_website/</a></p> </d",
        "id": 2486321,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrihuu/some_videos_on_linkedin_have_srcblob_and_i_cant",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Some videos on LinkedIn have src=\"blob:(...)\" and I can't find a way to download them",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T18:02:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve had a 4-bay Drobo since like 2012. It has four 6TB WD Red drives in it, and is connected directly to my main Windows 11 Pro desktop for work. I&#39;ve always treated it as a local archive and backup (along with other backups). So I might go a day or two without even accessing it, and when I do I pull the large projects to an internal drive to work from.</p> <p>When I look at replacement options, the most common suggestion is to get something from Synology. Those are all NAS instead of DAS, right? I would need to plug them into my wifi router, correct? Why is that better than just connecting a DAS to my desktop? I don&#39;t need other machines on my network to see the files. If that&#39;s the simplest thing to do, then ok, but that specific feature is not necessary to me.</p> <p>My main concern is ease of setup and use. What is the easiest/simplest way to continue to use those 4 WD drives as my local archive? I&#39;m open to whatever, it does",
        "id": 2486320,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jricvt/finally_leaving_drobounsure_what_to_do_next",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Finally leaving Drobo...Unsure what to do next.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T17:41:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I have an LTO drive which I\u2019ve been using for about 6 months to backup around 6TB at a time (lots of files around 2-10GB) . It\u2019s always taken longer than I was expecting to complete. 15hours+ each time. I didn\u2019t really look into it much until I checked the data sheet. The. transfer rate mentions that it should have been around 300MB/s transfer rate but was getting much less. </p> <p>I came across the term shoe shining and did a bit of experimenting with mbuffer which seems to have solved the problem; reducing the time to around 5hours. </p> <p>The tar command pipes to mbuffer, outputting to the tape drive. </p> <p>tar -cf - . | sudo mbuffer -m 1G -P 100 -s 256k -o /dev/st0</p> <p>Does it matter what the buffer size is, as long as it\u2019s above 300MB (transfer speed) and what would happen if I increased the block size to 512k? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FlashyStatement7887\"> /u/Flash",
        "id": 2485845,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrhvek/lto_tape_shoe_shining_and_block_sizing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LTO tape shoe shining and block sizing",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T17:40:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrhupe/found_my_old_media_after_years/\"> <img src=\"https://a.thumbs.redditmedia.com/Dgg30gW1kbOaL9jyTO_ru11DGOxmZuiB1sITwkXoH98.jpg\" alt=\"Found my old media after years\" title=\"Found my old media after years\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I was cleaning up the garage and discovered that I had not burned all the media in those stacks. I have 50 Memorex mini-CD and probably 60 or 70 DVD+R remaining in those 100-size stacks that I never burned. </p> <p>Sometime around when I bought those, hard drives became so cheap it became easier to archive stuff on a few drives that I kept upgrading over the years and I stopped burning. Even started using Live-USB Linux distros and Windows for booting, so I no longer burned DVD (and they started getting larger than what a DVD could fit).</p> <p>Any advice on whether they will still work? They have been ignored for 10+ years, could be even more. In garage a",
        "id": 2485846,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrhupe/found_my_old_media_after_years",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/Dgg30gW1kbOaL9jyTO_ru11DGOxmZuiB1sITwkXoH98.jpg",
        "title": "Found my old media after years",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T17:22:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently learned about (and invested in) M-disc technology, which appealled to the hoarder in me with the whole &quot;1000 year storage&quot; claim. Capacity is obviously an issue, so I&#39;m only using it to backup my most sensitive and important data, but I&#39;m wondering if anyone here has any experience with them and can attest to (or refute) their claims about longevity and reliability? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/e7615fbf\"> /u/e7615fbf </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrhf90/experience_with_mdiscs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrhf90/experience_with_mdiscs/\">[comments]</a></span>",
        "id": 2485847,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrhf90/experience_with_mdiscs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Experience with M-discs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T17:22:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone! This is my first time posting on Reddit, so I\u2019m sorry if I\u2019m doing anything wrong or if this isn\u2019t the right place.Please feel free to redirect me! Also, English isn\u2019t my first language, so I apologize if anything sounds confusing.</p> <p>I\u2019m looking for help with something that\u2019s been driving me crazy. I need to download all the comments (including replies, if possible) from public Facebook posts, especially from political party pages. The goal is to analyze the comments in an Excel file and classify them as supportive, neutral, or negative toward the post or topic. I\u2019ve spent days searching and trying different things: \u2022 Looked into scraping tools, but I don\u2019t know how to code or where to put code \u2022 Tried exploring the idea of creating an AI app (realized that was way too ambitious!) \u2022 Found GitHub projects, but had no idea what to do with the code \u2022 Checked paid tools, but I\u2019m doing a 3-month unpaid internship, so I can\u2019t afford somet",
        "id": 2486764,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrheu9/need_to_download_and_save_facebook_comments_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need to download and save Facebook comments, help?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T17:18:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,<br/> I recently picked up a ton of hard drives from an acquaintance. </p> <p>8TB, 12TB, and 18TB Hard drives. He said he wiped them all and reformatted. He was using an external hard drive enclosure via USB, and took some photos with CDI (Crystal Disk Info). I received them and wanted to check CDI on them myself. Everything works fine except the 12TB models, no reading at all, theyre not even recognized in bios or CMD. </p> <p>So I asked him to send me the CDI pictures of those 12TB models and they say Interface: UASP (instead of serial ATA like the rest of them). I googled it, and read that it means USB Attached SCSI Protocol, also read a little bit about it. But everything i&#39;m reading basically makes it sound like this interface only applies to external hard drives. So why would this internal SATA hard drive have UASP listed as the interface, and is it possible to convert it to standard interface to use as an internal hard drive with dir",
        "id": 2486765,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrhb2f/possible_to_convert_internal_hard_drive_from_uasp",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Possible to convert internal hard drive from UASP to Serial ATA ? (WD Ultrastar DC HC520 HDD | HUH721212ALE600 12TB)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T16:31:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im currently manually using Treesize Pro for my deduplication needs but its lacking a feature I really want. </p> <p>I would like to set a &quot;source of truth&quot; and then have the tool run over selected locations looking for files that are duplicates from that &quot;Source of Truth&quot;.</p> <p>Is there software out there that would have tha feature</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sunburnedaz\"> /u/sunburnedaz </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrg6zu/deduplication_software/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrg6zu/deduplication_software/\">[comments]</a></span>",
        "id": 2485215,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrg6zu/deduplication_software",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deduplication software",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T16:06:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been using Streamlink and never encountered video/audio sync issues until the streaming service decided to separate the video and audio streams. So I now use this command (see below) but until now there are occasional outputs that aren&#39;t in sync. Also, some files have incorrect timestamps and missing video frames towards the end. I am familiar with python but Streamlink is too complicated to modify. Can somebody help me what should be the correct command? </p> <pre><code>command = [ &#39;streamlink&#39;, &#39;--url&#39;, url, &#39;--default-stream&#39;, &#39;best&#39;, &#39;--output&#39;, output_file, &#39;--stream-segment-threads&#39;, &#39;5&#39;, &#39;--logfile&#39;, log_file.replace(&#39;.txt&#39;, &#39;_hls.txt&#39;), &#39;--loglevel&#39;, &#39;trace&#39;, &#39;--ffmpeg-ffmpeg&#39;, r&#39;C:\\ffmpeg\\bin\\ffmpeg.exe&#39;, &#39;--ffmpeg-verbose-path&#39;, log_file.replace(&#39;.txt&#39;, &#39;_mux.txt&#39;) ] </code></pre> </div><!-- SC_ON --> &",
        "id": 2485216,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrfkse/streamlink_mux_not_in_sync",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Streamlink MUX Not In Sync",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T13:36:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrc2en/the_us_national_oceanic_and_atmospheric_agency_is/\"> <img src=\"https://external-preview.redd.it/Y6jVHVINfnY2umB2_1WG2D5TifDwBivyWoVdYyDhIpk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=53454e341ca7df6da04ffae25cf8e14f978f855c\" alt=\"The US National Oceanic and Atmospheric Agency is poised to eliminate most websites tied to its research division under plans for the cancellation of a cloud web services contract\" title=\"The US National Oceanic and Atmospheric Agency is poised to eliminate most websites tied to its research division under plans for the cancellation of a cloud web services contract\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hollywoodhandshook\"> /u/hollywoodhandshook </a> <br/> <span><a href=\"https://archive.ph/Ug3VR\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jrc2en/the_us_national_oceanic_and_atmospheric_agenc",
        "id": 2484039,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jrc2en/the_us_national_oceanic_and_atmospheric_agency_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Y6jVHVINfnY2umB2_1WG2D5TifDwBivyWoVdYyDhIpk.jpg?width=640&crop=smart&auto=webp&s=53454e341ca7df6da04ffae25cf8e14f978f855c",
        "title": "The US National Oceanic and Atmospheric Agency is poised to eliminate most websites tied to its research division under plans for the cancellation of a cloud web services contract",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T08:15:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Due to the upcoming NOAA (US Climate Agency) shutdown, help in downloading NOAA resources is requested. Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hungry-Wealth-6132\"> /u/Hungry-Wealth-6132 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr6tvg/urgent_noaa_website_will_be_deactivated_soon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr6tvg/urgent_noaa_website_will_be_deactivated_soon/\">[comments]</a></span>",
        "id": 2481793,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr6tvg/urgent_noaa_website_will_be_deactivated_soon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "!URGENT! NOAA website will be deactivated soon",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T07:42:18+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PricePerGig\"> /u/PricePerGig </a> <br/> <span><a href=\"https://pricepergig.com/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr6dmx/i_created_pricepergigcom_to_help_find_the_best/\">[comments]</a></span>",
        "id": 2481512,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr6dmx/i_created_pricepergigcom_to_help_find_the_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I Created PricePerGig.com to help find the best price storage drives - Comment on what feature you'd like next adding.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T07:22:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently (18th March) purchased a 20TB Seagate drive from serverpartdeals, it was $255.84 total (ST20000NM007D).</p> <p>I was thinking of getting another one yesterday and saw that they increased the price to $259.99 (excluding tax).</p> <p>Not sure what to do, I thought I&#39;ll decide tomorrow. I just checked again, and the price is now $304.84 total ($279.99 before tax)</p> <p><a href=\"https://serverpartdeals.com/collections/manufacturer-recertified-drives/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive\">Seagate Exos X20 ST20000NM007D 20TB SATA 3.5&quot; Recertified HDD \u2014 ServerPartDeals.com</a></p> <p>In less than three weeks, the price was hiked almost $50. 16TB drives were $179, now they are $229.</p> <p>Is this happening because of the new tariff?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/manzurfahim\"> /u/manzurfahim </a> <br/> <span><a href=\"https://www",
        "id": 2481513,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr64am/recertified_drive_prices_increasing_rapidly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recertified drive prices increasing rapidly!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T06:58:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For 3 days I&#39;ve been trying to make the decision. Every few hours, I prefer the other one. To clarify, if I went with individual drives, 1 would be in nas, 1 in backup nas, 1 at a friend&#39;s house. I take and replicate frequent snapshots so maximum data loss would be 15 minutes or 1 hour (I adjust the frequency manually based on what I&#39;m currently working on). I would be grateful for some external input on this.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/umataro\"> /u/umataro </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr5s3u/ive_3_new_16tb_ssds_but_only_6_tb_of_non_media/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr5s3u/ive_3_new_16tb_ssds_but_only_6_tb_of_non_media/\">[comments]</a></span>",
        "id": 2481514,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr5s3u/ive_3_new_16tb_ssds_but_only_6_tb_of_non_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've 3 new 16TB SSDs but only 6 TB of (non media) data. I'm inclined to go with 1 for storage, 1 for backup, 1 for offsite backup. All ZFS. What would be the downsides compared to mirror + backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T06:00:40+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr4xct/videoplus_demo_vhsdecode_vs_bmd_intensity_pro_4k/\"> <img src=\"https://external-preview.redd.it/s3EB3YF1J2pxlzFAHbd8fabCpmtTj-pCbRxVcOHsBKQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88cb106ac6806d92b2645a3fa569b86068c642c9\" alt=\"VideoPlus Demo: VHS-Decode vs BMD Intensity Pro 4k\" title=\"VideoPlus Demo: VHS-Decode vs BMD Intensity Pro 4k\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheRealHarrypm\"> /u/TheRealHarrypm </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=B2ldecEC_jA\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr4xct/videoplus_demo_vhsdecode_vs_bmd_intensity_pro_4k/\">[comments]</a></span> </td></tr></table>",
        "id": 2481263,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr4xct/videoplus_demo_vhsdecode_vs_bmd_intensity_pro_4k",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/s3EB3YF1J2pxlzFAHbd8fabCpmtTj-pCbRxVcOHsBKQ.jpg?width=320&crop=smart&auto=webp&s=88cb106ac6806d92b2645a3fa569b86068c642c9",
        "title": "VideoPlus Demo: VHS-Decode vs BMD Intensity Pro 4k",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T05:49:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve tried using anydriib countless times now and it&#39;s never actually worked. I download the file (usually a zip or rar file) and it&#39;s always says the file is corrupt. i have NEVER had any luck using anydebrid or any other debrid site.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Legitimate_Pea_143\"> /u/Legitimate_Pea_143 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr4r0e/does_anydebrid_actually_work_for_anyone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr4r0e/does_anydebrid_actually_work_for_anyone/\">[comments]</a></span>",
        "id": 2481264,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr4r0e/does_anydebrid_actually_work_for_anyone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anydebrid actually work for anyone?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T05:43:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So many years ago I picked up a Nimbie CD robot with the intent of doing my library. After some software frustrations I let it sit. </p> <p>What options are there to make use of the hardware with better software? Bonus points for something that can run in Docker off my Unraid server. </p> <p>If like to be able to set and forget doing proper rips of a large CD collection. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HopeThisIsUnique\"> /u/HopeThisIsUnique </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr4npk/automated_cd_ripping_software/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr4npk/automated_cd_ripping_software/\">[comments]</a></span>",
        "id": 2481004,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr4npk/automated_cd_ripping_software",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automated CD Ripping Software",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T02:34:16+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr1cl4/i_think_its_time_to_save_the_data_and_replace_the/\"> <img src=\"https://preview.redd.it/utwwzbqzbqse1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5edeadafaf5c8f97b4980e31811bb5119f3cf2c1\" alt=\"I think it's time to save the data and replace the drive?\" title=\"I think it's time to save the data and replace the drive?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/duckbutter888\"> /u/duckbutter888 </a> <br/> <span><a href=\"https://i.redd.it/utwwzbqzbqse1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jr1cl4/i_think_its_time_to_save_the_data_and_replace_the/\">[comments]</a></span> </td></tr></table>",
        "id": 2480538,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr1cl4/i_think_its_time_to_save_the_data_and_replace_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/utwwzbqzbqse1.png?width=640&crop=smart&auto=webp&s=5edeadafaf5c8f97b4980e31811bb5119f3cf2c1",
        "title": "I think it's time to save the data and replace the drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T01:42:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been using GoodSync to backup data for a number of years. I use a two-way sync so that the two drives I copy back and forth contain the same data. </p> <p>I&#39;ve noticed that periodically GoodSync&#39;s backup space estimate goes way up in my target drive. When I check what it wants it to sync, I see a list of basically the majority of my files. I&#39;ve noticed this happen with portable hard drives, and today, for the first time in a portable Samsung Shield rugged SSD. </p> <p>I used to believe that it was some kind of break down in the hard drives themselves, but now I&#39;m not sure, since the SSDs have never given me trouble before. </p> <p>Has anyone else experienced this? Is there a setting that maybe I&#39;m not using correctly that is somehow making GoodSync &quot;refresh&quot; the data?</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TeacupTenor\"> /u/TeacupTenor </a> <br/> <spa",
        "id": 2480318,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jr0cac/possible_goodsync_bug",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Possible Goodsync Bug?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T01:10:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;d like to back up my main file server to another machine I built. I have about 40TB of data: 80% is large-ish media files, 20% is documents, photos and smaller files. I&#39;d like a solution that can take that into account when setting up the backup. Currently I&#39;m using, and successfully, Duplicati. It&#39;s free and open source and I like there is a Web UI even if it&#39;s kinda plain. What I don&#39;t like is that it isn&#39;t super fast. It will spike to 3.5Gb/s network thruput for a few seconds, then jump down to 1Gb/s or less for a minute or so. I am using a Threadripper 5955WX for the backup machine with a bcache backed RAID6 array. Based on <code>fio</code> test I should be able to sustain 3.5GB/s random writes and my file server can sustain that based on tests. What I think is happening is it appears that only 1-thread is being used for compression / etc. SO, I want something faster.</p> <p>What I want: Speed - should be able to uti",
        "id": 2480093,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jqzpx7/linux_local_backup_solutions_paid_is_okay",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Linux local backup solutions? Paid is okay",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T01:07:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jqzo17/vob_files_appear_corrupted_when_viewed_in_file/\"> <img src=\"https://b.thumbs.redditmedia.com/3erxWTUxiPfLETvlKYrvT2CCPdJ8YjiGKFxiSe1ngOo.jpg\" alt=\"VOB files appear corrupted when viewed in file explorer but appear fine when played from the DVD\" title=\"VOB files appear corrupted when viewed in file explorer but appear fine when played from the DVD\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Basically as the title says, I&#39;m ripping some movies and this specific movie is the only one that this happens to, all the other movies I&#39;ve ripped so far have been fine.</p> <p>Is this some sort of copy protection?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CantStandIdoits\"> /u/CantStandIdoits </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1jqzo17\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jqzo17/vob_files",
        "id": 2480092,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jqzo17/vob_files_appear_corrupted_when_viewed_in_file",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/3erxWTUxiPfLETvlKYrvT2CCPdJ8YjiGKFxiSe1ngOo.jpg",
        "title": "VOB files appear corrupted when viewed in file explorer but appear fine when played from the DVD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T00:32:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to get a list of all files on a hard drive. For example on E: I have 5 folders and inside those folders are thousands of movies. There is also some sub folders inside the folders. What is the best way to go about getting a list of everything?</p> <p>I tried doing this command i found on Google, but it doesn&#39;t do anything.</p> <blockquote> <p>dir e:*.* /s /on &gt; c:\\filelist.txt</p> </blockquote> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TristinMaysisHot\"> /u/TristinMaysisHot </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jqyyst/best_way_to_list_off_all_files_on_a_hard_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jqyyst/best_way_to_list_off_all_files_on_a_hard_drive/\">[comments]</a></span>",
        "id": 2480094,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jqyyst/best_way_to_list_off_all_files_on_a_hard_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to list off all files on a hard drive?",
        "vote": 0
    }
]