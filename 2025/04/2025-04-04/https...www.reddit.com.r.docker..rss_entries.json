[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T21:45:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a Cloudflare Tunnel setup to access my home NAS/Cloud, with the connector installed through docker, and today, suddenly, the container stopped working randomly. I even removed it and created another one just for the same thing to happen almost immediately after.</p> <p>In Portainer it says it&#39;s running on the container page, but on the dashboard it appears as stopped. Restarting the container does nothing, it runs for a few seconds and fails again. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/InternalConfusion201\"> /u/InternalConfusion201 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1jrnm0i/cloudflare_tunnel_connector_randomly_down/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1jrnm0i/cloudflare_tunnel_connector_randomly_down/\">[comments]</a></span>",
        "id": 2487432,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrnm0i/cloudflare_tunnel_connector_randomly_down",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cloudflare Tunnel connector randomly down",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T21:12:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to run an ENTRYPOINT script that ultimately calls</p> <p><code> httpd -DFOREGROUND </code></p> <p>My Dockerfile originally looked like this:</p> <p>``` FROM fedora:42</p> <p>RUN dnf install -y libcurl wget git;</p> <p>RUN mkdir -p /foo; RUN chmod 777 /foo;</p> <p>COPY index.html /foo/index.html;</p> <p>ADD 000-default.conf /etc/httpd/conf.d/000-default.conf</p> <p>ENTRYPOINT [ &quot;httpd&quot;, &quot;-DFOREGROUND&quot; ] ```</p> <p>I modified it to look like this:</p> <p>``` FROM fedora:42</p> <p>RUN dnf install -y libcurl wget git;</p> <p>RUN mkdir -p /foo; RUN chmod 777 /foo;</p> <p>COPY index.html /foo/index.html;</p> <p>ADD 000-default.conf /etc/httpd/conf.d/000-default.conf</p> <p>COPY test_script /usr/bin/test_script RUN chmod +x /usr/bin/test_script;</p> <p>ENTRYPOINT [ &quot;/usr/bin/test_script&quot; ] ```</p> <p>test_script looks like</p> <p>```</p> <h1>!/bin/bash</h1> <p>echo &quot;hello, world&quot; httpd -DFOREGROUND ```</p>",
        "id": 2487433,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrmul2/container_appears_to_exit_instead_of_launching",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Container appears to exit instead of launching httpd",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T20:16:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am working on a simple server dashboard in Next.js. It&#39;s a learning project where I&#39;m learning Next.js, Docker, and other technologies, and using an npm library called systeminformation.</p> <p>I tried to build the project and run it in a container. It worked! Kind of. Some things were missing, like CPU temperatures, and I cannot see all the disks on the system only an overlay (which AI tells me is Docker) and some other thing which isn&#39;t the physical disk. So I did some research and found the --privileged flag. When I run the container with it, it works. I can see CPU temperatures and all the disks, and I can actually see more disks than I have. I think every partition is returned, and I\u2019m not quite sure how to differentiate which is the real drive.</p> <p>My question is: is it okay to use --privileged?</p> <p>Also, is this kind of project fine to be run in Docker? I plan to open the repository once the core features are done, so if an",
        "id": 2487070,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrljmp/question_about_privileged_tag_and_more",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about privileged tag and more.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T19:56:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Everyone,</p> <p>Looking for a bit of advice (again). Before we can push to prod our images need to pass a sysdig scan.. Its harder than it sounds. I can&#39;t give specifics because I am not at my work PC.</p> <p>Out of the box, using the latest available UBI9 image it has multiple failures on docker components - nested docker - (for example runc) because of a vulnerability in the Go libraries used to build that was highlighted a few weeks ago. However even pulling from the RHEL 9 Docker test branch I still get the same failure because I assume Docker are building with the same go setup.</p> <p>I had the same issue with Terraform and I ended up compiling it from source to get it past the sysdig scan. I am not about to compile Docker from source!</p> <p>I will admit I am not extremely familiar with sysdig but surely we cant be the only people having these issues. The docker vulnerabilities may be legitimate but surely people don&#39;t wait weeks a",
        "id": 2487071,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrl246/docker_is_failing_sysdig_scans",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Docker is failing sysdig scans...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T17:46:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello I have a docker compose stack that has a mergerfs container that mounts a file system required for other containers in the stack. I have been able to implement a custom health check that ensure the file system is mounted and then have a depends_on check for each of the other containers.</p> <pre><code> depends_on: mergerfs: condition: service_healthy </code></pre> <p>This works perfectly when I start the stack from a stopped state or restart the stack but when I reboot the computer it seems like all the containers just start with no regard for the dependencies. Is this expected behavior and if so is there something that can be changed to ensure the mergerfs container is healthy before the rest start?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FanClubof5\"> /u/FanClubof5 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1jrhzg1/help_with_containers_coming_up_before_a_depends/\">[link]</a",
        "id": 2486134,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrhzg1/help_with_containers_coming_up_before_a_depends",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with containers coming up before a depends on service_healthy is true.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T16:40:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I am a bit new with using docker so not sure it is possible.</p> <p>I have a Plex server hosted and working fine withing a network 192.168.x.x/24, but also have a direct connection between the server hosting docker and my file server which works fine for some other things on a 10.0.0.x/24 network, I can create another network using portainer and add the new mounted volume to that network, but the container for plex will only allow me to have one network configured in it so I can have it streaming on 192.168 and pulling the files from 10.0.</p> <p>Is there I way I can get this done, maybe have both interfaces on the same network, but with those different IPs?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Gigamontanha\"> /u/Gigamontanha </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1jrgevi/i_want_to_add_a_volume_on_my_container_that_is/\">[link]</a></span> &#32; <span><a href=\"https",
        "id": 2487072,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrgevi/i_want_to_add_a_volume_on_my_container_that_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I want to add a volume on my container that is hosted on a different LAN",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T14:47:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. First post here. I have a Django and VueJS app that I&#39;ve converted into a containerized docker app which also uses docker compose. I have a digitalocean droplet (remote ubuntu server) stood up and I&#39;m ready to deploy this thing. But how do you guys deploy docker apps? Before this was containerized, the way I deployed this app was via a custom ci/cd shell script via ssh I created that does the following:</p> <ul> <li>Pushes code changes up to git repo for source control</li> <li>Builds app and packages the source code</li> <li>Stops web servers on the remote server (Gunicorn and nginx)</li> <li>Makes a backup of the current site</li> <li>Pushes the new site files to the server</li> <li>Restarts the web servers (Gunicorn and nginx)</li> <li>Done</li> </ul> <p>But what needs to change now that this app is containerized? Can I just simply add a step to restart or rebuild the docker images, if so which one: restart or rebuild and why?",
        "id": 2484431,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrdo8h/deploying_containerized_apps_to_remote_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deploying Containerized Apps to Remote Server Help/Advice (Django, VueJS)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T13:49:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Please don&#39;t flame me -- I&#39;ve spent hours and hours and hours doing self-research on these topics. And used AI extensively to solve my problems. And I&#39;ve learned a lot -- but there always seems to be something else.</p> <p>I have docker backups -- it&#39;s just that they don&#39;t work. Or, I haven&#39;t figured out how to get them to just work.</p> <p>I&#39;ve finally figured out much about docker, docker compose, docker.socket, bind mounts, volumes, container names and more. I have worked with my new friend AI to keep my Linux Ubuntu 24 server updated regularly, develop scripts and cron entries to stop docker and docker socket on a schedule, write and update a log file, and to use scripts to zip up (TAR.GZ) both the docker/volumes directory and a separate directory I use for bind mounts. I use rclone daily after that is done to push the backups to a separate Synology server. I save seven days of backups, locally and remote. I save separ",
        "id": 2487073,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrcc6e/backups_restoring_and_permissions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backups, Restoring and Permissions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T13:12:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>SSDNodes is a budget VPS hosting service, and I&#39;ve got 3 (optionally 4) of these VPS instances to work with. My goal is to host a handful of wordpress sites - the traffic is not expected to be &quot;Enterprise Level,&quot; it&#39;s just a few small business sites that see some use but nothing like &quot;A Big Site.&quot; That being said, I&#39;d like to have some confidence that if one VPS has an issue that there&#39;s still some availability. I do realize I can&#39;t expect &quot;High Availability&quot; from a budget VPS host, but I&#39;d like to use the resources I have available to get me &quot;higher availability&quot; than is I had just had one VPS instance. The other bit of bad news for me, is that SSDNodes does not have inter-VPS networking - all traffic between instances has to go between the public interface of each (I reached out to their tech team and they said they&#39;re considering it as a feature for the future.) Ideally, given 10 ",
        "id": 2483849,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jrbjo3/ssdnodes_docker_lemp_wordpress",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SSDNodes + Docker + LEMP + Wordpress",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T04:58:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just did a full upgrade (probably about 3 months since the last one) of a vm running docker and, when it rebooted, docker would not work. </p> <p>As usual, the error in the internal street less than helpful, but it seemed to screw up so the networking. </p> <p>I ended up having to restore from backup but I do want to get updates installed at some point. </p> <p>Happy to go all the way to 24.04 but I really don&#39;t want to mess docker up again. </p> <p>Had anyone seen anything like this and anything I can do to mitigate the risk?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AndyMarden\"> /u/AndyMarden </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1jr3xtx/ubuntu_2204_full_upgrade/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1jr3xtx/ubuntu_2204_full_upgrade/\">[comments]</a></span>",
        "id": 2480925,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jr3xtx/ubuntu_2204_full_upgrade",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ubuntu 22.04 full upgrade",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-04T03:53:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We are uploading images to an AWS Elastic Container Repository in our AWS account, and never to Dockerhub, etc. If that&#39;s the case, is there any concern with exposing build arguments like so?</p> <p>docker build --build-arg CREDENTIALS=&quot;user:password&quot; -t myimage .</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Slight_Scarcity321\"> /u/Slight_Scarcity321 </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1jr2trl/is_exposing_build_arguments_a_concern_with_aws_ecr/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1jr2trl/is_exposing_build_arguments_a_concern_with_aws_ecr/\">[comments]</a></span>",
        "id": 2480690,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1jr2trl/is_exposing_build_arguments_a_concern_with_aws_ecr",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is exposing build arguments a concern with AWS ECR?",
        "vote": 0
    }
]