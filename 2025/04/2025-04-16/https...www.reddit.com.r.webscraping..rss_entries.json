[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T21:11:09.899293+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T20:09:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been trying to scrape some json data from this old website: <a href=\"https://www.egx.com.eg/WebService.asmx/getIndexChartData?index=EGX30&amp;period=0&amp;gtk=1\">https://www.egx.com.eg/WebService.asmx/getIndexChartData?index=EGX30&amp;period=0&amp;gtk=1</a> for the better part of a week without much success.</p> <p>It&#39;s supposed to be a normal GET request but apparently there are anti measures agaist bots in place.</p> <p>I tried using curl, requests, httpx and selenium but the server either drops the connection or blocks me temporarily</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fun_yard_1\"> /u/fun_yard_1 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0u3ul/point_me_in_the_right_direction/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0u3ul/point_me_in_the_right_direction/\">[comments]</a></span>",
        "id": 2574326,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0u3ul/point_me_in_the_right_direction",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Point me in the right direction",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T19:01:29.024051+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T16:32:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m experimenting with Python and BeautifulSoup to create some basic web scraping programs to pull information, clean it, and then export it into Excel.</p> <p>One thing I&#39;ve done is scrape <a href=\"http://whitehouse.gov\">whitehouse.gov</a> weekly to pull presidential actions and dates into an Excel sheet, but I have other similar ideas.</p> <p>What are the potential risks? I&#39;ve checked the Terms and robots.txt files to be sure I&#39;m not going against website guidelines. The code is not polished, but I&#39;m careful not to make excessive or frequent requests. </p> <p>Am I currently realistically risking getting my IP banned? How long do IP bans last? Are there any simple best practices/guardrails I should be adding to my code?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BuffyBlip\"> /u/BuffyBlip </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0orfx/web_scraping_potential",
        "id": 2573274,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0orfx/web_scraping_potential_risks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Scraping Potential Risks?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T14:42:17.201458+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T14:25:58+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1k0lq1m/how_dare_you_trust_the_user_agent_for_bot/\"> <img src=\"https://external-preview.redd.it/0gj0q1LCbuwx2Mlxr8XdwtbOJInUOM7lbPi_NWs-l08.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b1da057c3a166c2af24f8a658dd55cd10c36129\" alt=\"How dare you trust the user agent for bot detection?\" title=\"How dare you trust the user agent for bot detection?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Disclaimer: I&#39;m on the other side of bot development; my work is to detect bots. I mostly focus on detecting abuse (credential stuffing, fake account creation, spam etc, and not really scraping)</p> <p>I wrote a blog post about the role of the user agent in bot detection. Of course, everyone knows that the user agent is fragile, that it is one of the first signals spoofed by attackers to bypass basic detection. However, it&#39;s still really useful in a bot detection context. Detection engines should treat it a the",
        "id": 2570716,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0lq1m/how_dare_you_trust_the_user_agent_for_bot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/0gj0q1LCbuwx2Mlxr8XdwtbOJInUOM7lbPi_NWs-l08.jpg?width=640&crop=smart&auto=webp&s=5b1da057c3a166c2af24f8a658dd55cd10c36129",
        "title": "How dare you trust the user agent for bot detection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T14:42:17.320791+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T13:57:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Writing network responses to the file system is obviously a slow operation too, right?</p> <p>But loading response data into memory and then writing it to the file system all at once looks a bad idea to me. Basically, I don&#39;t trust my own program - it can be shut down at any time, so I need to be able to save response data/error logs to the file system in real time as soon as possible. But AFAIK JS is a single-threaded language.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Gloomy-Status-9258\"> /u/Gloomy-Status-9258 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0l29k/filenetwork_bottleneck_separated_handling_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0l29k/filenetwork_bottleneck_separated_handling_in/\">[comments]</a></span>",
        "id": 2570717,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0l29k/filenetwork_bottleneck_separated_handling_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "file&network bottleneck separated handling in node.js",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T14:42:17.033580+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T13:35:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using playwright to scrape the jobs from the indeed job portal and store in the database but the main problem is cloudflare protection.............. the total scenario is I will be running this script in the was lambda and schedule daily using eventbridge so can anyone give the solution for this usecase?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jiraiya1729\"> /u/jiraiya1729 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0klj3/how_to_by_pass_cloudflare_protection_while/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0klj3/how_to_by_pass_cloudflare_protection_while/\">[comments]</a></span>",
        "id": 2570715,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0klj3/how_to_by_pass_cloudflare_protection_while",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to by pass Cloudflare protection while scraping indeed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T13:24:10.988833+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T12:10:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am currently using playwright with go. What do you guys use or suggest? Let&#39;s say we wanna use 100 browser context at same time.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Infinite_Bend_6174\"> /u/Infinite_Bend_6174 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0iuh5/share_your_current_automation_setup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0iuh5/share_your_current_automation_setup/\">[comments]</a></span>",
        "id": 2570061,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0iuh5/share_your_current_automation_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Share your current automation setup \u2b50",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-16T11:05:34.594257+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-16T10:36:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been listening to \u201cRebrowser\u201d podcast on Spotify. I also knew about \u201cOxycast\u201d but they stopped doing it. Are there any other podcasts that people can recommend?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/arp1em\"> /u/arp1em </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0h81c/can_anyone_recommend_a_podcast_related_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k0h81c/can_anyone_recommend_a_podcast_related_to/\">[comments]</a></span>",
        "id": 2569077,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k0h81c/can_anyone_recommend_a_podcast_related_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can anyone recommend a podcast related to Webscraping?",
        "vote": 0
    }
]