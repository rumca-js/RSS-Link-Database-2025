[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T20:01:56.197812+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T19:32:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I have a small personal use project where I want to scrape (somewhat regularly) the episode ratings for shows from IMDb. However, on the episodes page of a show, it only loads in the first 50 episodes for that season, and when it comes to something like One Piece, that has over 1000 episodes, it becomes very lengthy to scrape (and among the stuff I could find, the data that it fetches, the data in the HTML, etc all only have the data of the 50 shown episodes). Is there any way to get all the episode data either all at once, or in much fewer steps?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dzsaffar\"> /u/Dzsaffar </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k7tp7m/scraping_imdb_episode_ratings/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k7tp7m/scraping_imdb_episode_ratings/\">[comments]</a></span>",
        "id": 2529487,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k7tp7m/scraping_imdb_episode_ratings",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping IMDB episode ratings",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T18:55:47.028520+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T18:07:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m struggling to bypass bot detection on advanced test sites like:</p> <ul> <li><a href=\"https://bot.sannysoft.com\"><code>https://bot.sannysoft.com</code></a></li> <li><a href=\"https://arh.antoinevastel.com/bots/areyouheadless\"><code>https://arh.antoinevastel.com/bots/areyouheadless</code></a></li> <li><a href=\"https://pixelscan.net\"><code>https://pixelscan.net</code></a></li> <li><a href=\"https://fingerprint-scan.com\"><code>https://fingerprint-scan.com</code></a></li> </ul> <p>I\u2019ve tried tweaking Playwright\u2019s settings (user agents, viewport, headful mode), but these sites still detect automation.</p> <p><strong>My Ask:</strong></p> <ol> <li><strong>Stealth Plugins</strong>: Does anyone use <code>playwright-extra</code> or <code>playwright-stealth</code> successfully on these test URLs? What specific configurations are needed?</li> <li><strong>Fingerprinting</strong>: How do you spoof WebGL, canvas, fonts, and timezone to avoid detection?</li> <li><s",
        "id": 2529059,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k7rn75/what_playwright_configurations_or_another_method",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What Playwright Configurations or another method? fix bot detection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T11:19:16.186702+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T11:06:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am completely new to web scrapping and have zero knowledge of coding or python. I am trying to scrape some data off a website coinmarketcap.com. Specifically, I am interested in the volume % under the markets tab on each coin&#39;s page on the website. The top row is the most useful to me (exchange, pair, volume %). I also want the coin symbol and market cap to be displayed as well if possible. I have tried non-coding methods (web scraper) and achieved partial results (able to scrape off the coin names and market cap and 24 hour trading volume, but not the data under the &quot;markets&quot; table/tab), and that too for only 15 coins/pages (I guess the free versions limit). I would need to scrape the information for at least 500 coins (pages) per week (at max , not more than this). I have tried chrome drivers and selenium (chatGPT privided the script) and gotten no where. Should I go further down this path or call it a day as i don&#39;t know how to ",
        "id": 2525238,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k7i7tg/rnnning_into_issues",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Rnnning into issues",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T10:14:13.426200+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T10:12:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So recently, I was trying to learn and follow scraping tutorials on YouTube. I watched John Watson Rooney&#39;s tutorials that were recommended by many redditors here. Following him on one of his <a href=\"https://www.youtube.com/watch?v=DHvzCVLv_FA&amp;t=61s\">video</a>, I got a 403 forbidden error which prevented me from scraping specific <a href=\"https://www.rei.com/c/camping-tents\">REI</a> product details. The website apparently updated their protection, thus giving me different results than I would have if I followed Watson&#39;s code. Now, as a beginner, how do I get over this 403 error? and does anyone here have recommendation of tutorial on how to solve this specific case? I need help. Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dinotrash_\"> /u/dinotrash_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k7hd83/getting_403_forbidden_error_while_following/\">[link]</a></sp",
        "id": 2524739,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k7hd83/getting_403_forbidden_error_while_following",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting 403 forbidden error while following youtube scraping tutorial",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T10:14:13.302181+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T09:27:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My webapp involves hosting headful browsers on my servers then sending them through websocket to the frontend where the users can use them to login to sites like amazon, myntra, ebay, flipkart etc. I also store the user data dir and associated cookies to persist user context and login to sites.</p> <p>Now, since I can host N number of browsers on a particular server and therefore associated with a particular IP, a lot of users might be signing in from the same IP. The big e-commerce sites must have detections and flagging for this (keep in mind this is not browser automation as the user is doing it themselves)</p> <p>How do I keep my IP from getting blocked? </p> <p>Location based mapping of static residential IPs is probably one way. Even in this case, anybody has recommendations for good IP providers in India?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/definitely_aagen\"> /u/definitely_aagen </a> <br/> <spa",
        "id": 2524738,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k7gpvz/how_to_prevent_ip_bans_by_amazon_etc_if_many",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to prevent IP bans by amazon etc if many users login from same IP",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T05:54:11.943742+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T05:13:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, I\u2019m not a web dev \u2014 I\u2019m an Olympiad math instructor vibe-coding to scrape problems from AoPS.</p> <p>On pages like this one: <a href=\"https://artofproblemsolving.com/community/c6h86541p504698\">https://artofproblemsolving.com/community/c6h86541p504698</a></p> <p>\u2026the full post is <strong>clearly visible</strong> in the browser, but missing from driver.page_source and even driver.execute_script(&quot;return document.body.innerText&quot;).</p> <p>Tried:</p> <ul> <li>Waiting + scrolling</li> <li>Checking for iframe or post ID</li> <li>Searching all divs with math keywords (Let, prove, etc.)</li> <li>Using outerHTML instead of page_source</li> </ul> <p>Does anyone know how AoPS injects posts or how to grab them with Selenium? JS? Shadow DOM? Is there a workaround?</p> <p>Thanks a ton \ud83d\ude4f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Maleficent_Yoghurt85\"> /u/Maleficent_Yoghurt85 </a> <br/> <span><a href=\"https://",
        "id": 11231,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k7d350/selenium_post_visible_on_aops_forum_but_not_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Selenium: post visible on AoPS forum but not in page source.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-25T00:29:48.806884+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-25T00:22:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I&#39;m new to using crawl4ai for web scraping and I&#39;m trying to web scrape details regarding a cyber event, but I&#39;m encountering a decoding error when I run my program how do I fix this? I read that it has something to do with windows and utf-8 but I don&#39;t understand it.</p> <pre><code>import asyncio import json import os from typing import List from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig, LLMConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy from pydantic import BaseModel, Field URL_TO_SCRAPE = &quot;https://www.bleepingcomputer.com/news/security/toyota-confirms-third-party-data-breach-impacting-customers/&quot; INSTRUCTION_TO_LLM = ( &quot;From the source, answer the following with one word and if it can&#39;t be determined answer with Undetermined: &quot; &quot;Threat actor type (Criminal, Hobbyist, Hacktivist, State Sponsored, etc), Industry, Motive &quot; &quot;(Financial",
        "id": 10009,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k77pis/crawl4ai_how_to_fix_decoding_error",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "crawl4ai how to fix decoding error",
        "vote": 0
    }
]