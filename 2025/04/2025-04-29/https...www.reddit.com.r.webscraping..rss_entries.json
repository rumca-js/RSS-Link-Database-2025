[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T18:50:35.169730+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T18:22:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Pypi: <a href=\"https://pypi.org/project/amzpy/\">https://pypi.org/project/amzpy/</a></p> <p>Github: <a href=\"https://github.com/theonlyanil/amzpy\">https://github.com/theonlyanil/amzpy</a></p> <p>Earlier I only added product scrape feature and <a href=\"https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/\">shared it here</a>. Now, I:</p> <p>- migrated to curl_cffi from requests. Because it&#39;s much better.</p> <p>- TLS fingerprint + UA auto rotation using fakeuseragent.</p> <p>- async (from sync earlier).</p> <p>- search thousands of search/category pages till N number of pages. This is a big deal.</p> <p>I added search scraping because I am building a niche category price tracker which scrapes 5k+ products and its prices daily.</p> <p>Apart from reviews what else do you want to scrape from amazon?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/convicted_redditor\"> /u/co",
        "id": 2558997,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kav16p/i_updated_my_amazon_scrapper_to_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I updated my amazon scrapper to to scrape search/category pages",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T16:40:35.483972+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T16:25:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to scrape <a href=\"http://web.archive.org\">web.archive.org</a> (using premium rotating proxies tried both residential and datacenter) and I&#39;m using crawl4ai, used both HTTP based crawler and Playwright-based crawler, it keeps failing once we send bulk requests.</p> <p>Tried random UA rotation, referrer from Google, nothing works, resulting in 403, 503, 443, time out errors. How are they even blocking?</p> <p>Any solution?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vegetable_Entrance_4\"> /u/Vegetable_Entrance_4 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kas4sp/web_scraping_from_webarchiveorg_nothing_works/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kas4sp/web_scraping_from_webarchiveorg_nothing_works/\">[comments]</a></span>",
        "id": 2557661,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kas4sp/web_scraping_from_webarchiveorg_nothing_works",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scraping from web.archive.org (NOTHING WORKS)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T16:40:35.615441+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T16:17:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello <a href=\"/r/webscraping\">r/webscraping</a> community!</p> <p>I&#39;m the sole maintainer of <a href=\"https://github.com/sachin-sankar/swiftshadow\">swiftshadow</a>, a lightweight Python library designed to simplify IP proxy rotation for web scraping and related tasks. I&#39;m thrilled to announce that <strong>GitHub Discussions</strong> is now enabled for the repository! (<a href=\"https://github.com/sachin-sankar/swiftshadow?utm_source=reddit.com\">sachin-sankar/swiftshadow: Free IP Proxy rotator library for python</a>)</p> <hr/> <h3>\ud83d\udd27 About swiftshadow</h3> <p>swiftshadow is a Python package designed to manage and rotate free IP proxies. It provides a simple and efficient way to fetch, cache, and validate proxies from multiple sources. The package supports filtering by country and protocol, automatic proxy rotation, and caching to improve performance. (<a href=\"https://sachin-sankar.github.io/swiftshadow/?utm_source=reddit.com\">Swiftshadow</a>)</",
        "id": 2557662,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1karxnl/swiftshadow_update_github_discussions_enabled",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "swiftshadow Update: GitHub Discussions Enabled & HTTPS Proxy Fetching",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T13:22:09.224020+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T13:01:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 2555720,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kand31/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T11:11:33.514250+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T10:53:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all,</p> <p>Just thought I\u2019d share something I\u2019ve been tinkering with \u2014 figuring out how to scrape competitor Google Ads directly from search results using Python and Crawlbase.</p> <p>If you&#39;re in digital marketing, SEO/SEM, or just like scraping things for fun (guilty), this might be useful. Here&#39;s the general approach I took:</p> <p>\ud83d\udd39 <strong>Send search queries through Crawlbase</strong> to safely grab SERP pages without getting flagged.<br/> \ud83d\udd39 <strong>Parse the ads</strong> \u2014 pull out the headlines, URLs, descriptions, and whatever else they\u2019re showing off.<br/> \ud83d\udd39 <strong>Analyze patterns</strong> \u2014 like who&#39;s bidding on what, what kind of messaging they\u2019re testing, and who\u2019s basically trying to own the SERPs.</p> <p><strong>Why I even bothered:</strong></p> <ul> <li>Spy tools are cool but expensive.</li> <li>I wanted more control over what data I collect.</li> <li>It&#39;s kind of satisfying seeing real-time ad data without touch",
        "id": 2554640,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kal04y/scraping_competitor_google_ads_with_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping competitor Google Ads with Python \u2014 surprisingly useful",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T09:01:33.766815+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T08:28:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! I recently created a small script to automatically get `cf_clearance` cookies using Playwright. You can find it here: <a href=\"https://github.com/proplayer919/Cloudflare-Bypass\">https://github.com/proplayer919/Cloudflare-Bypass</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/infinitearcstudios\"> /u/infinitearcstudios </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kaiz0a/i_created_a_python_script_to_automatically_get_cf/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kaiz0a/i_created_a_python_script_to_automatically_get_cf/\">[comments]</a></span>",
        "id": 2553760,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kaiz0a/i_created_a_python_script_to_automatically_get_cf",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I Created a Python script to automatically get `cf_clearance` cookies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T09:01:33.927581+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T08:15:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, </p> <p>I want to compose a list of URLs of websites that match a certain framework, by city. For example, find all businesses located in Manchester, Leeds and Liverpool that have a &quot;Powered by Wordpress&quot; in the footer or somewhere in the code. Because they are a business, the address is also on the page in the footer, so that makes it easy to check.</p> <p>The steps I need are;</p> <ul> <li>\u2705 1. Get list of target cities</li> <li>\u2753 2. For each city, query Google (or other search engines) and get all sites that have both &quot;Powered by Wordpress&quot; and &quot;[city name]&quot; somewhere on the page</li> <li>\u2705 3. Perform other steps like double check the code, save URL, take screenshots etc.</li> </ul> <p>So I know how to do steps 1 and 3, but I don&#39;t know how to perform step 2. </p> <p>Is there any reliable way to do this? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Exuraz\"> /u/Exuraz <",
        "id": 2553761,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kaitdz/how_do_i_find_certain_website_types_on_google",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I find certain website types on Google?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-29T07:56:33.402639+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-29T07:05:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>(If this isn\u2019t accepted delete) I\u2019m looking to hire a web scrapper for my real estate business. This job isn\u2019t demanding and can honestly be a side hustle if you have other projects you\u2019re working on. I simply need someone skilled and proficient to scrape county databases for foreclosure information and import the data into a csv file. If this someone is you, feel free to shoot me a message. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TypicalLowLife\"> /u/TypicalLowLife </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kahuos/web_scraper_for_hire/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kahuos/web_scraper_for_hire/\">[comments]</a></span>",
        "id": 2553446,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kahuos/web_scraper_for_hire",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Scraper (for hire)",
        "vote": 0
    }
]