[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T23:09:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can someone give me the python code to get the public float of a stock?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jay_Simmon\"> /u/Jay_Simmon </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxtyi0/python_code_for_public_float/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxtyi0/python_code_for_public_float/\">[comments]</a></span>",
        "id": 2544867,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxtyi0/python_code_for_public_float",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Python code for public float?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T21:15:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>On Goodreads&#39; Group Bookshelves, they&#39;ll let users list 100 books per page, but it still only goes to a maximum of 100 pages. So if a bookshelf has 26,000 books (one of my groups has about that many), I can only get the first 10,000 or the last 10,000. Which leaves the middle 6,000 unaccounted for. Any ideas on a solution or workaround?</p> <p>I&#39;ve automated it (off and on) successfully and can set it for 100 books per page and download 100 pages fine. I can set the order to &quot;ascending&quot; or &quot;descending&quot; to get the first 10000 or last 10000. In a loop, after it reaches page 100, it just downloads page 100 over and over until it finishes.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mm_reads\"> /u/mm_reads </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxrmbl/goodreads_100_page_limit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscrap",
        "id": 2544464,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxrmbl/goodreads_100_page_limit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Goodreads 100 page limit",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T20:58:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jxr8ud/scrapling_a_costeffective_alternative_to_ai_for/\"> <img src=\"https://preview.redd.it/nsu7b6elvgue1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=20107239894d820b5b692afe487f84db233693e9\" alt=\"Scrapling: A Cost-Effective Alternative to AI for Robust Web Scraping\" title=\"Scrapling: A Cost-Effective Alternative to AI for Robust Web Scraping\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey there.</p> <p>While everyone is running to AI every shit. I have always debated that most of the time, you don&#39;t need AI for Web Scraping, and that&#39;s why I have created this article, and to show Scrapling&#39;s parsing abilities.</p> <p><a href=\"https://scrapling.readthedocs.io/en/latest/tutorials/replacing_ai/\">https://scrapling.readthedocs.io/en/latest/tutorials/replacing_ai/</a></p> <p>So that&#39;s my take so what do you think? I&#39;m looking forward to your feedback and thanks for all the support so ",
        "id": 2544868,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxr8ud/scrapling_a_costeffective_alternative_to_ai_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/nsu7b6elvgue1.png?width=640&crop=smart&auto=webp&s=20107239894d820b5b692afe487f84db233693e9",
        "title": "Scrapling: A Cost-Effective Alternative to AI for Robust Web Scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T20:31:40+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jxqo03/web_data_science/\"> <img src=\"https://external-preview.redd.it/Vnis31k1jEIbTwJuHV6PoROFQgITFIyiyRf9N9cg4uc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5010d10bd287f9bd07d6170486123bd5615b8701\" alt=\"Web Data Science\" title=\"Web Data Science\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Here\u2019s a GitHub repo with notebooks and some slides for my undergraduate class about web scraping. PRs and issues welcome!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/brianckeegan\"> /u/brianckeegan </a> <br/> <span><a href=\"https://github.com/cuinfoscience/INFO4871-Fall2024\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxqo03/web_data_science/\">[comments]</a></span> </td></tr></table>",
        "id": 2544465,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxqo03/web_data_science",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Vnis31k1jEIbTwJuHV6PoROFQgITFIyiyRf9N9cg4uc.jpg?width=640&crop=smart&auto=webp&s=5010d10bd287f9bd07d6170486123bd5615b8701",
        "title": "Web Data Science",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T19:41:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all!</p> <p>I&#39;m relatively new to web scraping and while using headless browser is quite easy as I used to do end-to-end testing as part of my job, the request replication is not something I have experience in. </p> <p>So for the purpose of getting data from one website I tried to copy the browser request as cURL and it goes through. However, if I import this cURL comment to postman, or replicate it using the JS fetch API, it is blocked. I&#39;ve made sure all the headers are in place and in the correct order. What else could be the reason?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vvivan89\"> /u/vvivan89 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxpl7y/api_request_goes_through_curl_but_not_through/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxpl7y/api_request_goes_through_curl_but_not_through/\">[comments]</a></span>",
        "id": 2544114,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxpl7y/api_request_goes_through_curl_but_not_through",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "API request goes through cURL but not through fetch/postman",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T18:03:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Edit:</p> <p>Example: Sports league (USHL) TOS:</p> <p><a href=\"https://sidearmsports.com/sports/2022/12/7/terms-of-service\">https://sidearmsports.com/sports/2022/12/7/terms-of-service</a></p> <p>And this website: <a href=\"https://www.eliteprospects.com/league/ushl/stats/2018-2019\">https://www.eliteprospects.com/league/ushl/stats/2018-2019</a></p> <p>scraped the USHL stats, would the website that was scraped be able to sue <a href=\"http://eliteprospects.com\">eliteprospects.com</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Top_Bend2772\"> /u/Top_Bend2772 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxndsa/a_business_built_on_webscraping_sport_league/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxndsa/a_business_built_on_webscraping_sport_league/\">[comments]</a></span>",
        "id": 2543819,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxndsa/a_business_built_on_webscraping_sport_league",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A business built on webscraping sport league sites for stats. Legal?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T14:32:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a bit out of my depth as I don&#39;t code, but I&#39;ve spent hours trying to get Crawl4AI working (set up on digitalocean) to scrape websites via n8n workflows. </p> <p>Despite all my attempts at content filtering (I want clean article content from news sites), the output is always raw html and it seems that the fit_markdown field is returning empty content. Any idea how to get it working as expected? My content filtering configuration looks like this:</p> <p>&quot;content_filter&quot;: {<br/> &quot;type&quot;: &quot;llm&quot;,<br/> &quot;provider&quot;: &quot;gemini/gemini-2.0-flash&quot;,<br/> &quot;api_token&quot;: &quot;XXXX&quot;,<br/> &quot;instruction&quot;: &quot;Extract ONLY the main article content. Remove ALL navigation elements, headers, footers, sidebars, ads, comments, related articles, social media buttons, and any other non-article content. Preserve paragraph structure, headings, and important formatting. Return clean text th",
        "id": 2542477,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jximz5/getting_crawl4ai_to_work",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting Crawl4AI to work?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T14:10:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Is there an app or program I can use for a quick and easy way to search social media by keywords people post in their bios? I\u2019m not a coder so looking for the easiest and best one to use.</p> <p>Thanks </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LongjumpingLab8\"> /u/LongjumpingLab8 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxi66g/search_by_keywords_in_bio/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxi66g/search_by_keywords_in_bio/\">[comments]</a></span>",
        "id": 2542478,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxi66g/search_by_keywords_in_bio",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Search by keywords in bio?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T14:00:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As the title suggests, I am a student studying data analytics and web scraping is the part of our assignment (group project). The problem with this assignment is that the dataset <strong>must only be scraped,</strong> <strong>no API and legal to be scraped</strong> </p> <p>So please give me any website that can fill the criteria above or anything that may help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/diamond_mode\"> /u/diamond_mode </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxhyjf/recommending_websites_that_are_scrapeable/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxhyjf/recommending_websites_that_are_scrapeable/\">[comments]</a></span>",
        "id": 2542479,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxhyjf/recommending_websites_that_are_scrapeable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommending websites that are scrape-able",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T12:23:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>Recently, I have been working on a web scraper that has to work with dynamic websites in a generic manner. What I mean by dynamic websites is as follows:</p> <ol> <li>The website may be loading the content via js and updating the dom.</li> <li>There may be some content that is only available after some interactions (e.g., clicking a button to open a popup or to show content that is not in the DOM by default).</li> </ol> <p>I handle the first case by using playwright and waiting till the network has been idle for some time.</p> <p>The problem is in the second case. If I know the website, I would just hardcode the interactions needed (e.g., search for all the buttons with a certain class and click them one by one to open an accordion and scrape the data). But the problem is that I will be working with generic websites and have no common layout.</p> <p>I was thinking that I should click on every element that exists, then track the effect o",
        "id": 2541836,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxg2z0/generic_web_scraping_for_dynamic_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Generic Web Scraping for Dynamic Websites",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T11:26:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jxf4xl/asking_you_input_open_source_true_headless_browser/\"> <img src=\"https://preview.redd.it/tvgb8rpc2eue1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8eac092f396a326140d56a45521d23532f5e58bb\" alt=\"ASKING YOU INPUT! Open source (true) headless browser!\" title=\"ASKING YOU INPUT! Open source (true) headless browser!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey guys!</p> <p>I am the Lead AI Engineer at a startup called <a href=\"https://github.com/lightpanda-io/browser\">Lightpanda (GitHub link)</a>, developing the <strong>first true headless browser</strong>, we do not render at all the page compared to chromium that renders it then hide it, making us:<br/> - 10x faster than Chromium<br/> - 10x more efficient in terms of memory usage</p> <p>The project is OpenSource (3 years old) and I am in charge of developing the AI features for it. The whole browser is developed in Zig and use the v8 Javascri",
        "id": 2541835,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxf4xl/asking_you_input_open_source_true_headless_browser",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/tvgb8rpc2eue1.jpeg?width=640&crop=smart&auto=webp&s=8eac092f396a326140d56a45521d23532f5e58bb",
        "title": "ASKING YOU INPUT! Open source (true) headless browser!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T10:57:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What&#39;s the purpose of it? </p> <p>I get that you get a lot of information, but this information can be outdated by a mile. And what are you to use of this information anyway?</p> <p>Yes you can get Emails, which you then can sell to other who&#39;ll make cold calls, but the rest I find hard to see any purpose with?</p> <p>Sorry if this is a stupid question \u00f8.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mizzen_Twixietrap\"> /u/Mizzen_Twixietrap </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxeouq/purpose_of_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jxeouq/purpose_of_webscraping/\">[comments]</a></span>",
        "id": 2541590,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxeouq/purpose_of_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Purpose of webscraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T08:23:38+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jxcl54/i_almost_built_amazon_scraper_for_google_sheets/\"> <img src=\"https://external-preview.redd.it/MWx6MmNtcHI1ZHVlMfz-UeYkzEOFS5-Jm-UWJxQuY7pYjJ-sTM8BGD5OGZeB.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bee91efb84a9c97540f94dea8c58682214cf13f\" alt=\"I almost built amazon scraper for google sheets and ...\" title=\"I almost built amazon scraper for google sheets and ...\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So I built a fastapi api and deployed it to railway. Wrote a google sheets script with the help of chatgpt and it worked (as seen in the video).</p> <p>Then I explored another extension - scrabby - which does the same. Why should I even continue building it.</p> <p>I am again at where it started - what to build.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/convicted_redditor\"> /u/convicted_redditor </a> <br/> <span><a href=\"https://v.redd.it/axq0ni",
        "id": 2541017,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jxcl54/i_almost_built_amazon_scraper_for_google_sheets",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/MWx6MmNtcHI1ZHVlMfz-UeYkzEOFS5-Jm-UWJxQuY7pYjJ-sTM8BGD5OGZeB.png?width=640&crop=smart&auto=webp&s=6bee91efb84a9c97540f94dea8c58682214cf13f",
        "title": "I almost built amazon scraper for google sheets and ...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-12T01:22:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey! I was tasked with scraping a private FB group and scrape all videos and posts on there. I\u2019m an admin inside of the group, so I can do and add anything inside.</p> <p>Does anyone have experience with his and some advice on how to approach this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Live-Orange-8414\"> /u/Live-Orange-8414 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jx5wwe/scraping_private_fb_groups/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jx5wwe/scraping_private_fb_groups/\">[comments]</a></span>",
        "id": 2539697,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jx5wwe/scraping_private_fb_groups",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping private FB groups",
        "vote": 0
    }
]