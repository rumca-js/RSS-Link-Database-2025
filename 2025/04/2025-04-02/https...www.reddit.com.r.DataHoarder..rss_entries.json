[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T19:32:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone have nice tool to scrap everything off their youtube account? Favorite videos, Subscriptions, uploaded videos etc?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AxlJones\"> /u/AxlJones </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpxjm4/saving_my_youtube_account_on_a_weekly_basis/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpxjm4/saving_my_youtube_account_on_a_weekly_basis/\">[comments]</a></span>",
        "id": 2469217,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpxjm4/saving_my_youtube_account_on_a_weekly_basis",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Saving my youtube account on a weekly basis",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T18:10:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,<br/> I am trying to purchase a pair of Toshiba MG drives for a small NAS. I had an &#39;experience&#39; with a seller on Amazon (Toshiba drive sealed in WD anti-static bag, recovery software R-Studio showed extensive usage, and S.M.A.R.T. data had been reset! \ud83d\ude2f (thankfully refunded)), so I&#39;m now wary of third-party sellers on Amazon/eBay, even if they claim them to be new and with warranty.</p> <p>I&#39;m looking at sizes between 12 and 16Tb and having trouble finding reasonable prices. There seems to be a monopoly in the UK, as Scan seems to be one of the only known companies able to get stock of these sizes.</p> <p>Can anyone recommend a good deal from a reliable outlet, please?</p> <p>Thank you for reading this far. :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/daxliniere\"> /u/daxliniere </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpvi1z/monopoly_on_uk_sourc",
        "id": 2469218,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpvi1z/monopoly_on_uk_sources_for_toshiba_mg_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Monopoly on UK sources for Toshiba MG drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T17:34:09+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpujw3/xreveal_pro_stuck_at_99_but_operation_success_its/\"> <img src=\"https://b.thumbs.redditmedia.com/ghkvpl293xlyTG0WjJ05tvOfULpM04eINEL2TsLvSyk.jpg\" alt=\"Xreveal PRO stuck at 99% but operation success. It's normal?\" title=\"Xreveal PRO stuck at 99% but operation success. It's normal?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/qv6j6cp6jgse1.png?width=866&amp;format=png&amp;auto=webp&amp;s=70fde72ef385583dfb49acc9ad6a2a7ba839372a\">https://preview.redd.it/qv6j6cp6jgse1.png?width=866&amp;format=png&amp;auto=webp&amp;s=70fde72ef385583dfb49acc9ad6a2a7ba839372a</a></p> <p>Xreveal PRO stuck at 99% but operation success. It&#39;s normal?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SuperCiao\"> /u/SuperCiao </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpujw3/xreveal_pro_stuck_at_99_but_operation_success_its/\"",
        "id": 2469219,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpujw3/xreveal_pro_stuck_at_99_but_operation_success_its",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/ghkvpl293xlyTG0WjJ05tvOfULpM04eINEL2TsLvSyk.jpg",
        "title": "Xreveal PRO stuck at 99% but operation success. It's normal?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T17:06:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m photographer and currently have about 6.5TB of data, nothing crazy yet. I have a 4TB SSD which I edit off of that&#39;s formatted exFAT, a Seagate 8TB for archiving data that&#39;s exFAT (which is a backup of my SSD and everything) and another newer Seagate AFPS 8TB that right now is just a second backup of the SSD. I have the exFAT HDD and 4TB SSD backed up on Backblaze as well. I&#39;m wondering if I should backup the first Seagate exFAT HDD to the Seagate AFPS HDD and have Best Buy (I have a plus membership so this would be free) offload the data and reformat the ExFAT Drive to AFPS and reload the data. I do have a Windows Laptop, but seldom use it and figured the SSD could be kept as exFAT for that reason. I wanted to get some suggestions, feedback. Wasn&#39;t sure if having both HDD drives as AFPS would make space on the drives more optimized and the read/writing speed a lot faster or worth the trade off in my situation. </p> </div><!-- ",
        "id": 2467878,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jptuey/should_i_format_part_of_my_storage_seagate_hdd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Should I format part of my storage (Seagate HDD 8TB) to AFPS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T16:15:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So last September I got the new iPhone 16 Pro Max with 1 TB of storage. I\u2019m a content creator so I take a lot of photos and screenshots. My phone has gotten filled up with over 170,000 photos. What I\u2019d do in the past is import all my photos onto external drives using my Mac and the Photos app.</p> <p>However, now whenever I plug my new phone into my Mac laptop and try to import to an external drive, the import doesn\u2019t work. It doesn\u2019t work on Image Capture either. I think it\u2019s because there\u2019s so many photos in the library\u2026 but what am I supposed to do? Is my only option now to buy more iCloud storage? Buy a higher-powered Mac computer that can handle the import?</p> <p>I wouldn\u2019t have gotten this 1TB phone if I knew imports weren\u2019t going to work and I couldn\u2019t empty out my camera roll once or twice a year. \ud83d\ude13</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/roseofucf\"> /u/roseofucf </a> <br/> <span><a href=\"https:",
        "id": 2467883,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpskqe/did_i_get_scammed_into_buying_more_icloud_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Did I get scammed into buying more iCloud storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T16:02:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there anyone one here with a good lead</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ArtLongjumping487\"> /u/ArtLongjumping487 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jps91g/looking_for_best_possible_price_for_wd_20_or_22/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jps91g/looking_for_best_possible_price_for_wd_20_or_22/\">[comments]</a></span>",
        "id": 2467882,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jps91g/looking_for_best_possible_price_for_wd_20_or_22",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for best possible price for wd 20 or 22 tb external hard drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T15:47:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jprvak/as_requested_a_4_bay_version_of_my_8_bay_das/\"> <img src=\"https://b.thumbs.redditmedia.com/rDqRf-1jnbNnrPLju1447F8sVC4wggA_7-Ozsurgaqk.jpg\" alt=\"As requested a 4 bay version of my 8 bay DAS\" title=\"As requested a 4 bay version of my 8 bay DAS\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kschaffner\"> /u/kschaffner </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1jprvak\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jprvak/as_requested_a_4_bay_version_of_my_8_bay_das/\">[comments]</a></span> </td></tr></table>",
        "id": 2467875,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jprvak/as_requested_a_4_bay_version_of_my_8_bay_das",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/rDqRf-1jnbNnrPLju1447F8sVC4wggA_7-Ozsurgaqk.jpg",
        "title": "As requested a 4 bay version of my 8 bay DAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T15:42:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was just curious, and figured this would likely be the best sub of experts to weigh in.</p> <p>I was a little shocked by the take down of the Hubble space channel, and it&#39;s also being reported that many sources of publicly available government related data were also being <em>temporarily</em> affected.</p> <p>I don&#39;t know. So, I come to you guys. I&#39;m not trying to make this political at all. I don&#39;t care about any of that. So, while I understand DOGE and how it&#39;s causing interference in general. It&#39;s not my main concern.</p> <p>My main concern is vast data sets being scrubbed, by anyone, for any reason.</p> <p>I appreciate any thoughts or considerations.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SnooOwls221\"> /u/SnooOwls221 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jprrst/is_data_removal_becoming_more_common/\">[link]</a></span> &#32; <span><a href=\"h",
        "id": 2467879,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jprrst/is_data_removal_becoming_more_common",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Data Removal becoming more common?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T15:04:09+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/chillinewman\"> /u/chillinewman </a> <br/> <span><a href=\"/r/space/comments/1jpm99p/the_hubble_space_telescope_youtube_channel_is_gone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpqtuq/the_hubble_space_telescope_youtube_channel_is_gone/\">[comments]</a></span>",
        "id": 2467876,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpqtuq/the_hubble_space_telescope_youtube_channel_is_gone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The Hubble Space Telescope YouTube channel is gone!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T14:52:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying Archivebox, and it has a lot of nice ideas, and it is completely inadequate. I need something that can fetch in parallel. I have ~25k unique bookmarks dating back almost 15 years and just want to preserve what I still can. Does anyone have any recommendations?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kcombinator\"> /u/kcombinator </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpqirm/selfhosted_web_bookmarks_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpqirm/selfhosted_web_bookmarks_archive/\">[comments]</a></span>",
        "id": 2467880,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpqirm/selfhosted_web_bookmarks_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Self-hosted web bookmarks archive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T14:32:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve seen the &quot;Do I need ECC RAM&quot; question come up from time to time, so I thought I&#39;d share my experience with it.</p> <p>The common wisdom is this: cosmic ray bit flips are rare. And the chances that they happen in a bit of memory you actually care about are rarer still. And from a data hoarder perspective, the chances that they occur in a bit of memory you&#39;re just about to write to disk are <em>vanishingly</em> small. So it&#39;s not really worth the jump in price to enterprise equipment, which is often the only way to get ECC RAM (Even when the RAM itself isn&#39;t much more expensive.)</p> <p>Well, I&#39;ve been data hoarding since the late 90&#39;s, and all but the last 5 on consumer-grade, non-ECC equipment. And I&#39;ve finally gotten around to using a program that will go through my hoard, and compare it with existing Linux ISO torrent files, to see if I&#39;ve got the same version. Then I can re-share stuff that&#39;s ",
        "id": 2467877,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpq1sl/do_i_need_ecc_memory_if_i_use_a_checksumming_file",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do I need ECC Memory if I use a checksumming file system like ZFS, BTRFS, Ceph, etc? A Case Study / story time / rant",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T14:14:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What would you do?</p> <p>I thought of this because I&#39;m currently downloading Professor Leonard&#39;s Calculus playlist because I don&#39;t want it to go anywhere before I have a chance to watch it \ud83e\udd7a. So if they announced YouTube is getting wiped in a year (and they didn&#39;t do anything to try and stop the obviously incoming download frenzy) what would you do?</p> <p>I&#39;m not sure if I&#39;m allowed to make a post like this here, if I&#39;m not, my apologies. I didn&#39;t see anything in the rules that would suggest this kind of post is forbidden.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DryDealer3816\"> /u/DryDealer3816 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jppm1d/a_thought_exercise_youtube_is_shutting_down_in_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jppm1d/a_thought_exercise_youtube_is_shutting_down_in_a/\">[comme",
        "id": 2467874,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jppm1d/a_thought_exercise_youtube_is_shutting_down_in_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A thought exercise, YouTube is shutting down in a year and they announced they'll be wiping all the data.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T13:10:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking at getting 2 new drives, 10-20TB each to keep the exact same data on (everything I have). But I&#39;m struggling to make sense of how the different brands and models (such as WD colors) actually stack up against one another. The marketing lingo is extremely thick everywhere I look.</p> <p>Previously I&#39;ve owned a pair of smaller WD Blacks for the same purpose as they promised good reliability and read/write performance, but they were extremely expensive and seem even more so currently.</p> <p>Right now I&#39;m looking at two Toshiba MG10 Enterprise 20TB drives as they promise higher speeds than my blacks and much larger storage at a lower price. Is there a catch?</p> <p>Any help or suggestions would be greatly appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Super5948\"> /u/Super5948 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpo6nr/recommendations_for_",
        "id": 2465826,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpo6nr/recommendations_for_general_purpose_and_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommendations for general purpose and backup HDDs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T12:51:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all,</p> <p>My Pixel 6 died last month, I managed to get the motherboard repaired to salvage the data, I got the phone back today. In an effort to stop this from happening in the future I want to setup an incremental backup system (I would&#39;ve replaced the phone if I didn&#39;t loose all my data).</p> <p>However, it seems that there is no real one stop shop for a backup solution on android.</p> <p>I&#39;m also not sure what data I shuold be saving (Messages, photos, videos, documents, list of installed apps??)</p> <p>Does anyone know of any backup software for android, preferably suited for NAS?</p> <p>What data does everyone backup?</p> <p>I&#39;m not against rooting the phone, but I would rather not.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Scoot_Scoot96\"> /u/Scoot_Scoot96 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpnsno/android_backup_software/\">[link]</a></span> &",
        "id": 2465827,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpnsno/android_backup_software",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Android Backup Software",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T12:39:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for alternatives to scrape public posts from Instagram profiles. I previously used Instaloader and occasionally Gallery-DL, but these tools are increasingly returning errors. Does anyone know of open-source tools that can effectively scrape public posts from specific profiles and export the metadata from each post in a separate JSON format?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Silent_Performer_123\"> /u/Silent_Performer_123 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpnk3p/scraping_instagram_alternatives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpnk3p/scraping_instagram_alternatives/\">[comments]</a></span>",
        "id": 2467881,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpnk3p/scraping_instagram_alternatives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Instagram alternatives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T11:54:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What is the best way of downloading a whole youtube channel? At a specific (low) resolution?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zanza2023\"> /u/zanza2023 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpmpl1/download_a_whole_youtube_channel/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpmpl1/download_a_whole_youtube_channel/\">[comments]</a></span>",
        "id": 2465828,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpmpl1/download_a_whole_youtube_channel",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Download a whole youtube channel?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T11:13:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I love how quickly I can find the right files on my computer using this app. I&#39;ve had it a long time, but I remember that when I first started using it, I could just bring it up and search for something and it would show it. Nowadays when I start it up and search, it shows the results, but then it clears out everything and a green loading bar in the bottom right corner goes up slowly, then after maybe 30 seconds it works as usual after that until I reboot the computer. I&#39;ve scoured through the settings and asked ChatGPT for help on if there&#39;s any indexing setting I&#39;ve missed, but from what I can tell it all looks correct. So now I&#39;m wondering if this is normal for this app now? Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/acriax\"> /u/acriax </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpm04r/anyone_using_everythingexe_should_version_15a/\">[link]</a></spa",
        "id": 2465829,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpm04r/anyone_using_everythingexe_should_version_15a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone using Everything.exe? Should version 1.5a re-index every startup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T10:55:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>New to the sub, new to the urging feeling to back up my stuff. I&#39;m looking to purchase or build backup / occasional use storage device for media. Would like it to be somewhat portable, not fixed on SSD or HDD and kind of like the idea of building something 1-2 tb so that in 3-5 yrs I can add it to a larger enclosure of some kind. </p> <p>I know I&#39;m overthinking this and could use some ideas and help. Does anyone have suggestions for a 100$, 1-2 tb storage reliable, durable external all in one or diy build? </p> <p>Thank you!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Red_Gyarados88\"> /u/Red_Gyarados88 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jplpqa/data_noob_seeking_advice_for_external/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jplpqa/data_noob_seeking_advice_for_external/\">[comments]</a></span>",
        "id": 2465830,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jplpqa/data_noob_seeking_advice_for_external",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data noob seeking advice for external",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T09:22:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This has happened to me a few times now. I try to rip a CD on my main machine (a Macbook Pro M1 Max) with an external DVD drive and there will be a certain track that it gets stuck on. I usually rip with iTunes/Music for simplicity, but when this happens I also try using XLD and it gets stuck on the same track that iTunes did. But then if I plug the exact same external DVD drive into my old 2015 Macbook Air (Intel i5) the CD rips with no issues at all.</p> <p>Why would this happen? The drive is a random brand (Froibetz) usb drive from amazon.</p> <p>As you can imagine the two machines are on different OS, but I don&#39;t think that&#39;s the issue because the Air is currently on Monterey, and I believe the MBP which is now on Sonoma was having the same issue back when it was running Monterey.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Methbot9000\"> /u/Methbot9000 </a> <br/> <span><a href=\"https://www.reddit",
        "id": 2465831,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpkejx/cd_rips_fail_on_m1_macbook_pro_but_rip_fine_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "CD rips fail on M1 Macbook pro but rip fine on 2015 Macbook Air. Any ideas why?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T09:08:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone know of a way to automatically search rule34 tags and cross reference them with my 6000 image collection in order to efficiently tag my images in HydrusNetwork?</p> <p>The images are stored in Hydrus already, so any file names are gone. Does anyone know of an efficient way to tag these (or preferably an automatic one).</p> <p>I am new to hoarding and am looking to store my collection on an external 2tb SSD. But tagging is a chore that will take me an extensive amount of time.</p> <p>I have tried finding guides but have come up empty handed.</p> <p>Thanks!</p> <p>Edit: Minor Spelling Mistake \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/deZgoDnroH\"> /u/deZgoDnroH </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpk84n/is_autotagging_rule34_images_using_hydrusnetwork/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpk84n/is_autotaggi",
        "id": 2465832,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpk84n/is_autotagging_rule34_images_using_hydrusnetwork",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is AutoTagging Rule34 images using HydrusNetwork possible?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T08:25:08+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpjnwq/dvdr_burned_from_macbook_not_playing_on_dvd_player/\"> <img src=\"https://preview.redd.it/8fcjzk5dtdse1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8f681bc1ade7fc56311164131e5065e9f79e5ca\" alt=\"Dvd-R burned from Macbook, not playing on DVD player.\" title=\"Dvd-R burned from Macbook, not playing on DVD player.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So I burnt a DVD-R on my mac (home movies of parties/births etc nothing pirated?!)... and the disc plays fine on there.</p> <p>I try to play it on 2 of my DVD players (DVD-R compatible), but it won&#39;t play! </p> <p>Disc specifics are; HP, 4.7GB, 16x...</p> <p>I tried burning at 8x speed &amp; then 4x speed as 1 DVD player doesn&#39;t support anything above 6x....</p> <p>Anyone know why it won&#39;t play? Or have any advice?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tashlouise94\"> /u/tashlouise94 <",
        "id": 2465833,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpjnwq/dvdr_burned_from_macbook_not_playing_on_dvd_player",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/8fcjzk5dtdse1.jpeg?width=640&crop=smart&auto=webp&s=b8f681bc1ade7fc56311164131e5065e9f79e5ca",
        "title": "Dvd-R burned from Macbook, not playing on DVD player.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T06:27:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>By that I mean a program to download from webnovel.con preferably mobile friendly</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Technical-Formal-376\"> /u/Technical-Formal-376 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpi3lw/is_there_any_programs_or_sites_to_download_from/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpi3lw/is_there_any_programs_or_sites_to_download_from/\">[comments]</a></span>",
        "id": 2463204,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpi3lw/is_there_any_programs_or_sites_to_download_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there any programs or sites to download from the Webnovel site?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T03:38:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been torrenting for a couple weeks now and finished downloading all seasons of aqua teen hunger force. All the episodes are 1080p HMAX but 2gbs or less per season. I just found another season pack for 20gb each. What exactly is the point of getting those for higher storage when these exist? Is there something I&#39;m missing?</p> <p>Edit: you reddit nerds need to stop bitching when someone wants to learn about your hobbies \ud83d\ude02 yall came out the womb knowing about webrips? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SchizophrenicScreams\"> /u/SchizophrenicScreams </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpfg8e/whats_the_point_of_downloading_4k_encoded_movies/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpfg8e/whats_the_point_of_downloading_4k_encoded_movies/\">[comments]</a></span>",
        "id": 2463214,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpfg8e/whats_the_point_of_downloading_4k_encoded_movies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What's the point of downloading 4k encoded movies and 1080p TV episodes when I can just get a high quality web rip for less space?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T03:28:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking to streamline my htpc setup by moving some drives from an old pc server into an enclosure on my main pc and adding a drive.</p> <p>The enclosure that just arrived this week is a Terramaster D4-320. I also just got a Seagate Expansion Desktop 24TB that (I presume) has a Barracuda drive in because it was manufactured in 2025. It is unopened so I can return it.</p> <p>The main use case is an HTPC storage for myself. My main PC might be on for 12-16 hours a day but I will not be read/writing to the drives 24/7. I love getting stuff but as life gets busier, I don&#39;t have enough time to watch. I am hopeful that the all the reliability specs aren&#39;t as relevant because I&#39;m not running a shared server.</p> <p>My concerns are that my drives aren&#39;t &quot;recommended&quot; by Terramaster for use as DAS. The Barracuda 24TB is not whitelisted by Terramaster. Nor are the 2 drives in my old PC server. In fact, one of my old drives (St4",
        "id": 2463206,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpf9rj/using_drives_that_are_not_recommended_or_not",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using drives that are \"not recommended\" or not whitelisted in a HDD enclosure",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T02:46:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>(Apologies if this has been addressed before, but I tried searching the sub and didn&#39;t see anything)</p> <p>I want all of my various machines to back up to my Synology sitting on my local network. I&#39;m then going to back up the Synology to iDrive. </p> <p>I can use any backup software from macOS/Windows/Ubuntu to the NAS, and then I&#39;ll be using iDrive from the NAS to the cloud. </p> <p>My question is this: How do I handle versioning between these two backup steps? Almost all software (including iDrive) support incremental backups, old versions, etc. </p> <p>It seems weird to have a database of multiple file versions running from each machine to the NAS, and then, what? Multiple versions of the multiple-versions-databases to iDrive? I feel like I&#39;m doing something wrong. </p> <p>Any suggestions are welcome!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ArcFarad\"> /u/ArcFarad </a> <br/> <span><a h",
        "id": 2463208,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpefis/versioning_for_backup_to_nas_and_cloud",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Versioning for backup to NAS and Cloud",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T02:26:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just set up my bad and uploaded all my data, I\u2019m using it as a home media server. What are some good ways I can back up all the data preferably online as buying the synology set me back a little and I\u2019m not itching to buy more external storage devices at the moment </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Big-Consideration337\"> /u/Big-Consideration337 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpdy4p/digital_back_up_for_16tb_synology_has/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpdy4p/digital_back_up_for_16tb_synology_has/\">[comments]</a></span>",
        "id": 2463210,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpdy4p/digital_back_up_for_16tb_synology_has",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Digital back up for 16tb synology has",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T01:39:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello Reddit, </p> <p>Recently an SSD of mine failed, and I lost a lot of media. I was able to take it into a shop, and they got a recovery of all the files, but stored in folders based on filetype, not relevancy. The problem with this is that it includes all of the system images, and icons that are found all around the computer. Is there any way I can do a bulk sorting and filtering of filesize, aspect ratio, or any other property? </p> <p>I was looking for the best sub-reddit to post this in, and this seemed to be a decent option. If none of you guys are able to help, could I get some suggestions on where I could turn next? Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tacitquazer\"> /u/Tacitquazer </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpcwmz/help_sorting_through_recovered_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1j",
        "id": 2463211,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpcwmz/help_sorting_through_recovered_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help sorting through recovered files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T00:56:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For those out of the loop, back in December 2020, three big academic publishers\u2014Elsevier, Wiley, and the American Chemical Society\u2014sued Sci-Hub and LibGen in the Delhi High Court (High courts are only second to the Supreme Court in India, which is the highest judicial authority in the country) by filing a &#39;copyright infringement&#39; lawsuit against them. The court then asked it to pause any new uploads, and the website has been inactive since then. There have been multiple hearings since then, but the case keeps on dragging with hopes dwindling for a verdict anytime soon.</p> <p>Here&#39;s the full timeline of the hearings that keeps getting postponed:</p> <p><a href=\"https://www.reddit.com/r/scihub/s/otkFGlQKqh\">https://www.reddit.com/r/scihub/s/otkFGlQKqh</a></p> <p>Now the question is, was this exactly the kind of outcome that the publishers were trying to achieve? If yes, then why did they selected India? Why not any other country?</p> <p>La",
        "id": 2463203,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpc1fw/can_someone_explain_why_the_lawsuit_against",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can someone explain why the lawsuit against Sci-Hub was filed in India, specifically?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T00:49:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I mean ones that go for \u00a315 on amazon Also are they region locked Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Account_5605\"> /u/No_Account_5605 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpbw29/will_a_cheap_external_dvd_drive_work_for_ripping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jpbw29/will_a_cheap_external_dvd_drive_work_for_ripping/\">[comments]</a></span>",
        "id": 2463213,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpbw29/will_a_cheap_external_dvd_drive_work_for_ripping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Will a cheap external dvd drive work for ripping dvds",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-02T00:17:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Ok, so, I have many shows that I have ripped from Blu-rays and I want to change their titles (not filenames) in mass. I know stuff like mkvpropedit can do this. It can even change them all to the filename in one go. But what about a specific part of the filename? All my shows are in a folder for the show, then subfolders for each series/season. Then each episode is named something like &quot;1 - Pilot&quot;, &quot;2 - The Return&quot;, etc. I want to mass set each title for all the files of my choice to just be the parts after the &quot; - &quot;. So, for those examples, it would change their titles to &quot;Pilot&quot; and &quot;The Return&quot; respectively. I have a program called bulk renamer that can rename from a clipboard, so one that uses this element is okay too, and I can just figure out a way to extract the file names into a list, find and replace the beginning bits away and then paste the new titles.</p> <p>I have searched for this everyw",
        "id": 2463205,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jpb890/programtool_to_mass_change_mkvmp4_titles_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Program/tool to mass change mkv/mp4 titles to specific part/string of file name?",
        "vote": 0
    }
]