[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T23:08:33.733119+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T23:06:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If anyone here doesn&#39;t know, The Homebrew Channel is an unofficial channel for the Wii console made by Nintendo. It allows users to run homebrew programs on the console and other such things.</p> <p>The Homebrew Channel has now ceased development because a developer says that &#39;key figures&#39; actually <em>stole</em> the code from Nintendo themselves to make the homebrewing possible. </p> <p>I predict that Nintendo will use this as an excuse to crack down <em>hard</em> on homebrewing and the community at large. </p> <p>Details: <a href=\"https://bsky.app/profile/oatmealdome.bsky.social/post/3lnsudl3djv2r\">https://bsky.app/profile/oatmealdome.bsky.social/post/3lnsudl3djv2r</a></p> <p>Links:</p> <p><a href=\"https://wiibrew.org/wiki/Homebrew_Channel\">https://wiibrew.org/wiki/Homebrew_Channel</a></p> <p><a href=\"https://hbc.hackmii.com/\">https://hbc.hackmii.com/</a></p> <p>I apologize if any details are wrong, I&#39;m actually not a Wii homebrewer,",
        "id": 2542627,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9h57p/calling_all_hoarders_please_back_up_things_made",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Calling all hoarders! Please back up things made by The Homebrew Channel!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T23:08:33.899222+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T22:08:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1k9fx4y/how_to_best_configure_my_setup/\"> <img src=\"https://b.thumbs.redditmedia.com/OYVH93iI42kfRUE_rS1q5bYfZjqkI8nc7Lx9clEDdKU.jpg\" alt=\"How to best configure my setup\" title=\"How to best configure my setup\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So Im trying to figure out the best way to optimize what I&#39;ve got and consolidate my at home stuff to these 2 devices. This is currently for a plex library and possibly Steam cache down the road </p> <p>I have the following 1 ASUSTOR nas with 2 Sata ports (4 NVME, 3 populated, not really concerned with these ATM, they run the OS and critical backups) 1 Yottamaster 5 bay USB DAS (Will be plugged into NAS)</p> <p>For drives i have the following 1 12 TB MDD drive (just obtained and inspired this) 2 8 TB Seagate barracuda currently setup in ASUSTOR nas 2 4 TB WD drives (currently in another machine but will be moved to this setup todayish)</p> <p>I also hav",
        "id": 2542628,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9fx4y/how_to_best_configure_my_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/OYVH93iI42kfRUE_rS1q5bYfZjqkI8nc7Lx9clEDdKU.jpg",
        "title": "How to best configure my setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T20:58:29.816310+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T20:45:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Couple of years ago, I got GIGABYTE BRIX mini PC with Celeron Processor J4105. The machine details can be found on its <a href=\"https://www.gigabyte.com/Mini-PcBarebone/GB-BLCE-4105-rev-10#ov\">home page here</a>.</p> <p>It basically has following relevant specifications:</p> <ul> <li>Front IO: <ul> <li>1 x USB3.0</li> <li>1 x USB3.0 type C</li> </ul></li> <li>Rear IO: 2 x USB 3.0</li> <li>Storage: Supports 2.5&quot; HDD/SSD, 7.0/9.5 mm thick (1 x 6 Gbps SATA 3)</li> <li>Expansion slot <ul> <li>1 x M.2 slot (2280_storage) PCIe X2/SATA</li> <li>1 x PCIe M.2 NGFF 2230 A-E key slot occupied by the WiFi+BT card</li> </ul></li> </ul> <p>Currently I have following things installed:</p> <ul> <li>Samsung SSD 850 EVO 500GB</li> <li>8 GB DDR4 RAM. CPU-Z says following for the RAM: <ul> <li>Total Size: 8192 MB</li> <li>Type: DDR4-SDRAM</li> <li>Frequency: 1197.4 MHz (DDR4-2394) - Ratio 1:12</li> <li>Slot #1 Module - P/N: CB8GS2400.C8JT</li> </ul></li> </ul> <p>I ",
        "id": 2541841,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9e15p/is_windows_on_gigabyte_brix_a_good_option_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Windows on Gigabyte BRIX a good option for data hoarding?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T20:58:29.982197+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T19:16:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1k9byhu/question_about_storage_spaces_pool_usage/\"> <img src=\"https://preview.redd.it/oo5p9fzpffxe1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=da61a8985bb654cfd35f0a17be2c6ab8521d247b\" alt=\"Question about Storage Spaces pool usage\" title=\"Question about Storage Spaces pool usage\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi Hoarders,</p> <p>So I have a tiered storage pool and I want to replace one of the drives in the SSD tier. It&#39;s a PCIe 3.0 drive and I want to swap it for a PCIe 4.0 one. However, the entirety of the pool is used so I can&#39;t pull it. I can&#39;t mark it as offline to have it flushed to the HDD tier either. I have plenty of space on the VHD as seen in the properties. Do I have any options other than destroying the pool, replacing the drive, and then rebuilding from backup?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kennedmh\"> /u/k",
        "id": 2541842,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9byhu/question_about_storage_spaces_pool_usage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/oo5p9fzpffxe1.png?width=640&crop=smart&auto=webp&s=da61a8985bb654cfd35f0a17be2c6ab8521d247b",
        "title": "Question about Storage Spaces pool usage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T20:58:30.102800+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T18:37:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to switch out my 9480 for a 9600W - It just has a couple of DS4246/IOM12 JBODs connected to it, but I can&#39;t figure out how to get my 9600W to see the drives.</p> <p>Am I doing something stupid that is stopping JBOD from working?</p> <p><code># storcli2 /c0/eall/sall show</code></p> <p><code>CLI Version = 008.0012.0000.0004 Nov 19, 2024</code></p> <p><code>Operating system = Linux6.12.24-Unraid</code></p> <p><code>Controller = 0</code></p> <p><code>Status = Success</code></p> <p><code>Description = The Controller is running in safe mode;only limited operations are supported.To exit safe mode,Correct the problem and reboot your computer.No PD found.</code></p> <p><code>Enclosure Count = 2</code></p> <p><code>Properties :</code></p> <p><code>==========</code></p> <p><code>------------------------------------------------------------------------------------------</code></p> <p><code>EID State DeviceType Slots PD Partner-EID Multipath PS ",
        "id": 2541843,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9b1qs/94808i8e_to_9600w16e_migration_wtf_is_safe_mode",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "9480-8i8e to 9600W-16e migration - wtf is \"safe mode\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T18:32:26.517557+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T18:30:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have some videos that I want to convert from H264 to H265. For example 720P H264 Total bitrate 4600 Kbps.</p> <p>I&#39;m trying to figure out if there is a &quot;common&quot; crosswalk for bit rate or a minimum.</p> <p>For example, take H264 bit rate and cut by 50%?</p> <p>For example, if converting to H265 don&#39;t go lower than X bit rate, etc</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/themayor1975\"> /u/themayor1975 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k9aw24/bit_rate_conversion_when_converting_from_h264_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k9aw24/bit_rate_conversion_when_converting_from_h264_to/\">[comments]</a></span>",
        "id": 2541426,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9aw24/bit_rate_conversion_when_converting_from_h264_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bit rate conversion when converting from H264 to H265",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T18:32:26.650434+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T18:29:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, long time since I&#39;ve been here. </p> <p>I used to have a little home server with a single unused desktop pc but I quit when I got a family onedrive, but now I&#39;m getting &quot;fed up&quot; with it so I&#39;m considering rejoining your legions.</p> <p>All I want, in order of priority:</p> <p>Most important: File storage. Not much for now, some 4-6TB is adequate. With even a single mirror HDD. Basic gigabit is fine or maybe 2,5gig (I have 2,5g capable stuff)</p> <p>SILENT OPERATION (I&#39;ll probably live in the same space as the &quot;nas&quot;, so anything that is louder than a fridge is out)</p> <p>Other &quot;good to haves&quot;:</p> <p>Capability of running some self hosting enviroment, like nextcloud.</p> <p>A VPN to access my files anywhere</p> <p>Maybe in the future some media playback capabilities to the TV, but for now I&#39;ll be ok with connecting my tv to the PC and playing back on it.</p> <p>What I&#39;ll miss: </p> <p>Now I",
        "id": 2541427,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9auua/looking_for_a_cheap_selfhostingmediaservernas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a cheap, selfhosting/mediaserver/nas setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T18:32:26.780637+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T17:47:34+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SnooBunnies9252\"> /u/SnooBunnies9252 </a> <br/> <span><a href=\"/r/UgreenNASync/comments/1k99qkt/how_is_the_security_of_ugreen_nas_in_april_2025/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k99v9k/how_is_the_security_of_ugreen_nas_in_april_2025/\">[comments]</a></span>",
        "id": 2541428,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k99v9k/how_is_the_security_of_ugreen_nas_in_april_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How is the security of UGREEN NAS in april 2025?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T18:32:26.290209+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T17:47:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>here is the link to the model: amazon.com/view-3d/vfa?asin=B0C8V967DT&amp;physicalId=A17ktrRtA-PL</p> <p>some products have 3D models you can extract from a zip file, but when inspecting others (such as this one) I see no zip files. these ones also do not load properly on a PC browser for me, only on mobile browser</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Caterpillar4000\"> /u/Caterpillar4000 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k99v33/how_to_extract_3d_model/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k99v33/how_to_extract_3d_model/\">[comments]</a></span>",
        "id": 2541425,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k99v33/how_to_extract_3d_model",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "how to extract 3D model?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T17:27:32.893504+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T17:22:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have about 10 TB of data I want to keep safe. At the same time my budget is rather limited and I don&#39;t think I can afford a proper 3-2-1 solution. I can sacrifice high availability as I do not need to access these that often. My data is static: once uploaded can remain in that form and do not need any sort of update or modification.</p> <p>Currently I store things on several LUKS-encrypted external HDD drives kept in a drawer. Only connecting when I need something. Not sure if sparse usage can improve their life expectancy. I only keep a local catalog on my system so I know where is everything placed. Once drive is full I just start filling next one and do not attempt any sort of migration. This means sometimes related files are disjointed into several drives and require a bit hassle to collect fully but this is an inconvenience I can live with. As far as backup goes, I buy my external HDD drives in pairs and keep everything in two copies. I kee",
        "id": 2541046,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k999qs/storing_10_tb_on_budget",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Storing 10 TB on budget",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T17:27:33.015393+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T17:06:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1k98vmw/a_rust_cli_to_find_and_verify_emails_from_name/\"> <img src=\"https://external-preview.redd.it/_P1isODkyFKhpgjePEpTsOH9fWbyzuJAlp7ijBrQl0I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1cf2310dae4b29ea10b64400986334460df4b41a\" alt=\"A Rust CLI to find and verify emails from name + domain (SMTP + scraping + JSON output)\" title=\"A Rust CLI to find and verify emails from name + domain (SMTP + scraping + JSON output)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I built a tool that might be of interest if you\u2019re into collecting contact data at scale or want to understand how email discovery really works under the hood \u2014 no APIs, no SaaS, no rate limits. </p> <p>It:</p> <ul> <li>Generates all the usual email permutations (<code>john.smith@</code>, <code>j.smith@</code>, etc.)</li> <li>Scrapes the company website for any public addresses</li> <li>Resolves MX records and connects to the mail server directl",
        "id": 2541047,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k98vmw/a_rust_cli_to_find_and_verify_emails_from_name",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/_P1isODkyFKhpgjePEpTsOH9fWbyzuJAlp7ijBrQl0I.jpg?width=640&crop=smart&auto=webp&s=1cf2310dae4b29ea10b64400986334460df4b41a",
        "title": "A Rust CLI to find and verify emails from name + domain (SMTP + scraping + JSON output)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T17:27:33.154837+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T16:24:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The title happened to me yesterday and I couldn&#39;t understand the instructions to fix it. I won&#39;t be back at it until next week. Will it clear on its own? Otherwise I&#39;ll have more questions. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/notned64\"> /u/notned64 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k97wdd/ytdlp_login_to_prove_im_not_a_bot/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k97wdd/ytdlp_login_to_prove_im_not_a_bot/\">[comments]</a></span>",
        "id": 2541048,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k97wdd/ytdlp_login_to_prove_im_not_a_bot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Yt-dlp Login to prove I'm not a bot",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T16:22:47.370403+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T15:53:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I would like to save offline copies of a few dozen of my favorite channels, size is not a concern I&#39;d like it to download every video at the highest resolution and flac audio if available. I tried using a gui off github called scrawler which uses yt-dlp and I quite liked the ui ease of use for a novice like me, it worked on a few smaller 50 video channels but as soon as I added a larger 1000+ video channel it seems to have been flagged by yt as a bot and stopped downloading cache files. </p> <p>I have a few channels with 3000+ videos I&#39;d like to download, I&#39;m not so rushed on it I&#39;m happy to run a script at a slower pace. I was hoping I could get the scrawler gui working for me as I&#39;m really not great at understanding/reading/deciding between all the command line options. </p> <p>Desired output; 1) highest res available + flac audio if available, otherwise next best option 2) video upload date + channel name in start of file name</",
        "id": 2540696,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k9761t/ytdlp_newbie_best_command_line_suggestions_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "yt-dlp newbie, best command line suggestions for downloading full YouTube channels",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T16:22:47.493430+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T15:32:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Still very new to and not very good at this, need help with two issues using wget so far:</p> <ol> <li>Using <code>wget -m -k</code> (am I crazy for thinking <code>wget -mk</code> would work the same, by the way?) to archive blogs and any files they&#39;re hosting, especially videos and PDFs. I like the feature <code>yt-dlp</code> has with <code>--download-archive archive.txt</code>, and I&#39;m wondering if <code>wget</code> has a feature like that, to make updating the archive with new posts easier. Or maybe it already works like that, and I&#39;m slow. Not sure.</li> <li>Been trying to use <a href=\"https://blog.archive.org/2012/04/26/downloading-in-bulk-using-wget/\">this method</a> to download everything a user has uploaded. Last time I tried this was last year, and it left 100+ files undownloaded. Now, this <em>was</em> a while ago, to the point that my terminal&#39;s history doesn&#39;t have the actual commands I used anymore. Still 99% sure I di",
        "id": 2540697,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k96od6/wget_advice",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "wget advice?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T15:17:25.781882+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T14:48:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I&#39;m trying to backup some <a href=\"http://FanFix.io\">FanFix.io</a> subscriptions but I can&#39;t really find any reliable tools. I tried OF-Scraper and some download extensions but it doesn&#39;t support FanFix. Thanks for your time and help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ModelsDeserveBBC\"> /u/ModelsDeserveBBC </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k95o2p/which_are_some_good_tools_to_backup_fanfix_content/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k95o2p/which_are_some_good_tools_to_backup_fanfix_content/\">[comments]</a></span>",
        "id": 2540343,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k95o2p/which_are_some_good_tools_to_backup_fanfix_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which are some good tools to backup FanFix content?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T15:17:25.902847+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T14:34:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What has everyone used with success for their main OS drive backups? Currently I have both a windows built in backup using the windows 7 backup tool and an ease todo free version backup of the same OS drive 1TB nvme to two identical enterprise 24TB drives. Plus I have created a boootable USB drive to boot off of in the event the OS drive fails.</p> <p>For the two backups it&#39;s totaling 1.1TB I&#39;m weary that this may be a waste of space to have two identical backups using two different solutions, curious what everyones thoughts are on this strategy and what they&#39;ve used successfully or if I should be concerned at all about only having one backup solution in the event the OS drive fails before everything else. Perhaps ease todo drops the free version in the future and my backups are null or perhaps windows 7 backup tool is bunk since microsoft themselves stopped supporting it, thoughts?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href",
        "id": 2540344,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k95cdd/windows_backup_solution",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Windows Backup Solution",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T15:17:25.543858+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T14:29:01+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wagldag\"> /u/wagldag </a> <br/> <span><a href=\"/r/geology/comments/1k8hhr5/noaa_deleting_swaths_of_critical_geological/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k958bo/noaa_deleting_swaths_of_critical_geological/\">[comments]</a></span>",
        "id": 2540342,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k958bo/noaa_deleting_swaths_of_critical_geological",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NOAA deleting swaths of Critical Geological datasets by early May. Download to save.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T15:17:25.421013+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T14:21:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been sitting on a few hundred GB of older twitch VODs (2021-2023) from a bigger streamer (100k+ twitch follows), that haven&#39;t been uploaded or archived anywhere else and is currently considered lost. I thought it would be a good idea to archive and make the content available by putting it on the Internet Archive. I even did contact the creator and got their permission to do it.</p> <p>But to my surprise when talking to IA support, they told me that such content is not allowed to upload to IA. I have been quite surprised because:<br/> 1) This is currently not communicated on any of the internet archive&#39;s articles about what can and what can&#39;t be uploaded, such as:</p> <p><a href=\"https://help.archive.org/help/uploading-tips/\">https://help.archive.org/help/uploading-tips/</a></p> <p><a href=\"https://help.archive.org/help/uploading-what-is-not-ok-or-not-ok-to-upload/\">https://help.archive.org/help/uploading-what-is-not-ok-or-not-ok-to-",
        "id": 2540341,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k952ip/the_internet_archive_and_twitchyoutube_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The Internet Archive and Twitch/Youtube Content Preservation: Not allowed?!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T12:02:22.226953+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T11:48:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Should I buy what range of SSDs to save the game data and play at the same time on PC? Entry-Level, Mid-Range or High-End ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yukinoooo\"> /u/Yukinoooo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k924rp/need_advice_to_buy_a_ssd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k924rp/need_advice_to_buy_a_ssd/\">[comments]</a></span>",
        "id": 2539354,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k924rp/need_advice_to_buy_a_ssd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need advice to buy a SSD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T10:57:43.366937+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T10:56:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks!</p> <p>I\u2019m in the middle of a big digital cleanup project \u2014 sorting through several terabytes of files before moving everything to a proper cloud backup service. I\u2019m on Windows 11 and looking for a good tool that can:</p> <p>Detect duplicate files (based on name, size, and preferably checksums)</p> <p>Organize files by type (images, videos, documents, etc.)</p> <p>Display file creation and modification dates</p> <p>Let me move duplicates to a different folder before deleting them</p> <p>A clean, functional GUI is a must. I\u2019m not much of a command-line person, so while CLI suggestions are welcome, I\u2019d strongly prefer something with a graphical interface.</p> <p>Ideally, it should be open-source or free, but I\u2019m willing to pay up to around $50 USD for something solid and reliable.</p> <p>So far I\u2019ve looked at AllDup and Duplicate Cleaner Free/Pro \u2014 has anyone here tried those, or got better recommendations?</p> <p>Would love to hear what tool",
        "id": 2539029,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k91a7h/looking_for_a_good_preferably_opensource",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a good (preferably open-source) duplicate file finder & organizer for Windows (GUI preferred)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T10:57:43.511922+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T10:29:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 3 drives in Mirror in TrueNas so its 2x backup... and still I am emotionally not OK deleting the files from my PC to clear the space which is the whole reason i built that NAS build. Okay I see the files there but like...what if due black magic the files are not really there.</p> <p>What if, I delete the original files and then the bakcups just vanish. Sure they could vanish from the original place as well but I am so used to seeing them there and knowing where they are and stay put.</p> <p>How do I shake this feeling of &quot;something will go wrong and all will be lost&quot; of having years of personal videos, photos as well as projects just pufff, gone. Its also the reason I never delete stuff even if I have bakcups of them, like when is a backup of a backup enough for me? I know I am being paranooid, and its probably a cause of my father who has always been saying that photos are only thing that is left of anything. I feel like if i lose th",
        "id": 2539030,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k90vsm/new_to_nas_but_how_do_you_live_with_this",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New to NAS but... how do you live with this`?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T08:48:25.504322+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T07:43:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>With several of my favorite vTubers graduating (ending streaming as their characters) recently and soon, I made tool to make it easier to archive content that may become unavailable after graduation. It&#39;s still fairly early and missing a lot of features but with several high profile graduations happening, I decided to release it for anyone interested in backing up any of the recent graduates.</p> <p>By default it grabs the video, comments, live chat, and generated English subtitles if available. Under the hood it uses yt-dlp as most people would recommend for downloading streams but helps manage the process with a interactive UI.</p> <p><a href=\"https://github.com/Brok3nHalo/AmeDoko\">https://github.com/Brok3nHalo/AmeDoko</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Brok3nHalo\"> /u/Brok3nHalo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8yjs8/i_made_a_tool_for_archiving_vtub",
        "id": 2538535,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8yjs8/i_made_a_tool_for_archiving_vtuber_streams",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I made a tool for archiving vTuber streams",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T07:43:51.866413+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T07:14:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>see title</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SpicySalter\"> /u/SpicySalter </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8y4t0/are_there_any_universal_file_naming_conventions_i/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8y4t0/are_there_any_universal_file_naming_conventions_i/\">[comments]</a></span>",
        "id": 2538229,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8y4t0/are_there_any_universal_file_naming_conventions_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are there any universal file naming conventions I can follow for consistent storage? Trying to archive some twitter/x creators content among other things like comics/manga.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T06:39:05.386351+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T05:33:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I swear, recently its been ridiculous, I download some from yt, until i hit the limit, then i move to flickr and queue up a few downloads. then i get 429.</p> <p>Repeat with insta, ig, twitter, discord, weibo, or whatever other site i want to archive from.</p> <p>I do use sleep settings in the various downloading programs, but usually it still fails.</p> <p>Plus youtube making it a real pain to get stuff with yt-dlp, constantly failing, and I need to re-open tabs to check whats missing.</p> <p>Anyone else feel like it&#39;s a bit impossible to get into a rhythm?</p> <p>My current solution has been to keep the links in a note, and dump them, then enter one by one. However the issue with this is, sometimes the account is dead by the time i get to it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RacerKaiser\"> /u/RacerKaiser </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8wnza/with_the_r",
        "id": 2537994,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8wnza/with_the_rate_limiting_everywhere_does_anyone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "With the rate limiting everywhere, does anyone else feel like they can't stay in the flow, and it's like playing musical chairs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T04:24:51.727439+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T04:08:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Reddit Fam,</p> <p>First of all I would like to thank you all. Going through posts here had helped me a lot and motivated me to build my own small Home Lab.</p> <p>I am from India and small problem of doing this in India is enterprise drives are really expensive.</p> <p>So I thought what if I can ask someone to buy few from USA/Hong Kong as I have friends coming and going once or twice a year at both the places.</p> <p>I would give an example 12TB Iron Wolf pro is costing me around USD 420-430 in India and same thing will cost around USD 300 in United States and should cost somewhat similar in Hong Kong.</p> <p>Things I want to know is does Segate gives international warranty?</p> <p>If the warranty don&#39;t works in India then does it makes sense to buy Iron Wolf Pros? I mean AFAIK one of the reason Iron Wolf Pros cost so much extra is the data recovery support etc provided by Segate for 5 years. So if I am buying from USA/ Hong Kong and support ",
        "id": 2537630,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8vc9l/buying_segate_ironwolf_pro_from_india_vs_usahong",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Buying Segate IronWolf Pro from India vs USA/Hong Kong?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T04:24:51.476487+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T04:02:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! So I&#39;m in a predicament on how people who takes lots of videos/photos on trips store years of files. I currently store most of my photos/vids in my pc with 12tb of mixed ssd/hdd. Though that&#39;s basically goin out quickly.</p> <p>My question how do you go about storing all these files? Do you compress the files by album? Leave it on raw and store it? Convert files into smaller file type then compress? Or just keep expanding storage?</p> <p>I&#39;ve been hand picking my files and deleting a lot, but the videos are taking up a lot of space still. I am currently shopping/planning on buying/building my own NAS with my old gaming PC. Though would still like to get an advice on how people store their files and back them up. I&#39;ve read the 3-2-1 guide and planning to implement that soon with the NAS that I&#39;m planning and Azure.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jo_So_Flow\"> /u/Jo_So_Flo",
        "id": 2537629,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8v8t7/how_do_you_store_your_family_photosvideos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you store your family photos/videos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T03:19:48.119659+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T03:10:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i am working on building a punch/ reader to store photos ect. on mylar tape for extreme long term storage my first issue is compression.<br/> i am looking for the best way to compress a large amount of photos into as little space as possible because you can only get about 100 bytes /ft what is the current best way to compress for this case.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bluecraney\"> /u/bluecraney </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8uc66/mylar_tape_for_archival_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8uc66/mylar_tape_for_archival_storage/\">[comments]</a></span>",
        "id": 2537400,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8uc66/mylar_tape_for_archival_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "mylar tape for archival storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T03:19:47.720630+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T02:39:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just had a few bad experiences recently regarding data loss and/or corruption (incl. backups getting corrupted) and I am looking for a new robust backup solution for long term mass storage. When I consider factors like having more than one physical backup and tracking file changes for important projects, I think the amount of data I need to store is large enough to justify looking into tape options. I&#39;m talking 3 digits TB, most of which is static. </p> <p>I don&#39;t want to deal with a massive pile of 150 tiny and slow tapes from 15 years ago, it would have to be a relativelly recent LTO version. When I look at brand new drives, it looks like it&#39;s all priced for enterprise and out of my budget. When I look at used gear, it&#39;s affordable but it&#39;s very hard for me to figure out what is a good option, what brands are good or bad, etc.</p> <p>You guys are the expert on this, I welcome ALL advice.</p> </div><!-- SC_ON --> &#32; submitted",
        "id": 2537399,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8tslr/in_2025_whats_the_best_option_for_lto_tape_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "in 2025, what's the best option for LTO tape backup at home?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T03:19:48.243620+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T02:13:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all. I\u2019m looking for an 8TB HDD to store videos. It won\u2019t be moved around and fast access to my videos is paramount. I don\u2019t want a long delay before they start.</p> <p>I\u2019ve done my research and seen that the best options are the Barracuda in an enclosure and the Sandisk professional G-Drive. Also the G-Drive has an Ultrastar HDD which seems to be superior? </p> <p>I\u2019m biased towards the G-Drive because it looks so slick. Price wise they cost pretty much the same. </p> <p>Which one would you choose?</p> <p>Edit: I don\u2019t mind Ironwolf\u2019s or Exos\u2019 price which seems to be Seagate\u2019s more premium products but I\u2019ve read that NAS HDD are not the best for my situation. I guess that the Ultrastar in the G-Drive is rated for video transfer scenarios so basically the best quality for my need? Is this correct?</p> <p>What would be Ultrastar\u2019s standalone HDD equivalent for movie storage?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.redd",
        "id": 2537401,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8tby2/barracuda_vs_gdrive_hdd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Barracuda vs G-Drive HDD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T02:11:16.484642+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T02:07:21+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8t83r/storing_video_on_digital_audio_tape_dat/\"> <img src=\"https://external-preview.redd.it/MPvK7kj7ESGnPvpSrwWZAr9zPRP0gLoOpLOHa462I6g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=060c3b04b0ff6ac148f6f1a965c6939b98e42bd0\" alt=\"Storing Video on Digital Audio Tape (DAT)\" title=\"Storing Video on Digital Audio Tape (DAT)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Yet another unique way to back up my favorite shows.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/churnopol\"> /u/churnopol </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=mOug4t5r0P4\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1k8t83r/storing_video_on_digital_audio_tape_dat/\">[comments]</a></span> </td></tr></table>",
        "id": 2537218,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8t83r/storing_video_on_digital_audio_tape_dat",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/MPvK7kj7ESGnPvpSrwWZAr9zPRP0gLoOpLOHa462I6g.jpg?width=320&crop=smart&auto=webp&s=060c3b04b0ff6ac148f6f1a965c6939b98e42bd0",
        "title": "Storing Video on Digital Audio Tape (DAT)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T02:11:16.203503+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T01:59:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello there <a href=\"/r/datahoarder\">r/datahoarder</a>. I&#39;m not exactly a hoarder myself but I think it&#39;s really interesting reading about techniques, software and hardware. </p> <p>My footprint for my digital stuff is actually comparatively small, currently about 450 GB. I use a simple 3-2-1 backup method. One of my backup hard drives is a Western Digital external 2.5 inch 3.0 USB 500 GB drive. It&#39;s about 5 years old so I think it&#39;s time to replace, right? Seems to be in good condition but you just never know. </p> <p>Right now I&#39;m thinking to replace it with a m2 1 TB drive in an external enclosure. No moving parts so I guess it&#39;s less prone to failure? I dunno. And m2 1 TB seems to be reasonably priced. </p> <p>Any suggestions? Is this a generally good idea or should I do something else?</p> <p>Thanks. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KoholintCustoms\"> /u/KoholintCustoms",
        "id": 2537217,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8t2l8/rec_drive_for_1_tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Rec Drive for 1 TB?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T02:11:16.606827+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T01:25:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just happened to run into this - <a href=\"https://www.amazon.com/Elements-Desktop-External-external-storage/dp/B09VCXWPQG/ref=mp_s_a_1_3_maf_1?crid=1WVQS5FVC4UWL&amp;dib=eyJ2IjoiMSJ9.o2hENIC6azjrLZgnDj-OaV5CUi17_U5_CTvAHNDhwmN_9-9IOF1TM5TLFBvOEVHs4er5WWHkuqocdOzdXz4p-PC9wQEJIb7XS-LCSqQIbvCrHjA3nZHgnFuzY8UgpF3SGzFNqpUsPzSGYqaSGMkW1RfN0ffx_eqXRHi8qGrY1fGEhxOGgv76GQfUR9VR8Ppu7JYAwnF8omdD0MnAzH4LiQ.MgyDFXBq5fUk8kwllKObP2sWxEgkDK4dxpg5rsoNQ0E&amp;dib_tag=se&amp;keywords=western+digital+20tb&amp;qid=1745717042&amp;sprefix=western+difital+20tb%2Caps%2C111&amp;sr=8-3\">https://www.amazon.com/Elements-Desktop-External-external-storage/dp/B09VCXWPQG/ref=mp_s_a_1_3_maf_1?crid=1WVQS5FVC4UWL&amp;dib=eyJ2IjoiMSJ9.o2hENIC6azjrLZgnDj-OaV5CUi17_U5_CTvAHNDhwmN_9-9IOF1TM5TLFBvOEVHs4er5WWHkuqocdOzdXz4p-PC9wQEJIb7XS-LCSqQIbvCrHjA3nZHgnFuzY8UgpF3SGzFNqpUsPzSGYqaSGMkW1RfN0ffx_eqXRHi8qGrY1fGEhxOGgv76GQfUR9VR8Ppu7JYAwnF8omdD0MnAzH4LiQ.MgyDFXBq5fUk8kwllKObP2sWxEgkDK4dxpg5rsoNQ0",
        "id": 2537219,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1k8sg4b/wd_20tb_drive_279",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WD 20tb drive $279",
        "vote": 0
    }
]