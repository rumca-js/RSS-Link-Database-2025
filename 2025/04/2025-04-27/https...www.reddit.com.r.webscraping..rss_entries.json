[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T13:53:53.663442+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T13:50:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/anonymous_2600\"> /u/anonymous_2600 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k94ess/what_is_your_biggest_achievement_in_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k94ess/what_is_your_biggest_achievement_in_web_scraping/\">[comments]</a></span>",
        "id": 2539793,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k94ess/what_is_your_biggest_achievement_in_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is your biggest achievement in web scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T13:53:53.821593+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T13:39:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m building an adaptive rate limiter that adjusts the request frequency based on how often the server returns HTTP 429. Whenever I get a 200 OK, I increment a shared success counter; once it exceeds a preset threshold, I slightly increase the request rate. If I receive a 429 Too Many Requests, I immediately throttle back. Since I\u2019m sending multiple requests in parallel, that success counter is shared across all of them. So mutex looks needed.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Gloomy-Status-9258\"> /u/Gloomy-Status-9258 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k946h4/do_you_introduce_mutex_mechanism_for_your_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1k946h4/do_you_introduce_mutex_mechanism_for_your_scraper/\">[comments]</a></span>",
        "id": 2539794,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k946h4/do_you_introduce_mutex_mechanism_for_your_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "do you introduce mutex mechanism for your scraper?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T10:39:10.276864+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T09:52:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys I&#39;m building a betting bot to place bets for me on Bet365, have done quite a lot of research (high quality anti detection browser, non rotating residential IP, human like mouse movements and click delays)</p> <p>Whilst ive done a lot of research im still new to this field, and I&#39;m unsure of the best method to actually select an element without being detected. I&#39;m using Selenium as a base, which would use something like</p> <pre><code>vegetable = driver.find_element(By.CLASS_NAME, &quot;tomatoes&quot;) </code></pre> <p>Which injects its own JS functions, which would be visible to any anti bot script running.</p> <p>Please could someone give advice on the best way to get around this? I&#39;m wondering if an OCR extension for chrome would work to get element location?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Flewizzle\"> /u/Flewizzle </a> <br/> <span><a href=\"https://www.reddit.com/r/websc",
        "id": 2538882,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k90cfd/anti_detection_when_interacting_with_bet365",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anti detection when interacting with Bet365",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-27T07:01:22.639859+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-27T06:23:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there,</p> <p>I am building a disruptive open source intelligence platform that provides free, instant access to over 500M Li.nked in profiles, which are up to date, and include additional meta information such as mobile phones, current business email, and their personal email addresses within our identity graph of 5.2B parameters.</p> <p>However, in order to maintain the high quality and freshness of the project while ensuring the data stays free for all, we are accepting volunteers to assist in very minor and transparent usage of your Li.nked in account, which will not result in your profile being locked, and you can view the logs yourself of what&#39;s going on. It is to maintain the real-time data refresh for profiles older than 2-months, with hard limits on the number of basic requests you may contribute per month (very low volume, tens-of-mbs of bandwidth total per month). All client sided in your browser.</p> <p>In return, you will have acce",
        "id": 2538104,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1k8xejm/easy_50month_paying_residential_volunteers_low",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[Easy $50/month] Paying residential volunteers (low bandwidth)",
        "vote": 0
    }
]