[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T23:07:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If this isnt the best place to ask please recommend me where. But I ordered<a href=\"https://www.walmart.com/ip/1TB-USB-Flash-Drive-1000GB-1PCS-Rotatable-Memory-Stick-External-Storage-Drive-for-Laptop-PC-MAOLAI/8789810991\"> this USB</a> and planned to use it to move abunch of video files over but whenever I do now after like 900gb was in it corrupts them seemingly.<br/> So Im asking here if people have any recommendations for ones (preferably not too expensive), can be of similar sizes like I&#39;d accept 800gb.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KingAlex105X\"> /u/KingAlex105X </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtz1xn/any_1_terabyte_usb_or_similar_sizes_suggestions/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtz1xn/any_1_terabyte_usb_or_similar_sizes_suggestions/\">[comments]</a></span>",
        "id": 2507090,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtz1xn/any_1_terabyte_usb_or_similar_sizes_suggestions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any 1 Terabyte USB or similar sizes suggestions?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T22:18:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My partner&#39;s grandmother has passed and has left a collection of hundreds possibly thousands of DVDs. These range from official releases to pirated and bootleg copies.</p> <p>What would be the best way to digitize and archive this collection? Is there an external device out there that will let me burn and convert the DVDs? I&#39;d want to possibly upload on <a href=\"http://archive.org\">archive.org</a> if the copyright expired, store on backblaze or maybe another digital archiving site besides a regular torrent, would appreciate any recs on sites and advice in general. I haven&#39;t gone through these yet but figure the project would be a fun learning experience.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/coetaneity92\"> /u/coetaneity92 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jty024/digitizing_and_archiving_old_dvd_collection/\">[link]</a></span> &#32; <span><a href=\"https:",
        "id": 2506818,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jty024/digitizing_and_archiving_old_dvd_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Digitizing and archiving old dvd collection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T22:09:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;m completely new to stashapp, and I&#39;m trying to figure out how to scrape properly. I installed the community scrapers, and some are working fine right out of the box, but a number of the say &quot;could not unmarshal json from script output: EOF&quot; whenever I try to use them, and I don&#39;t have the first clue as to what that menas, any help would be much appreciated</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VobsandBagene\"> /u/VobsandBagene </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtxswt/stashapp_json_errors/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtxswt/stashapp_json_errors/\">[comments]</a></span>",
        "id": 2506819,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtxswt/stashapp_json_errors",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Stashapp JSON errors",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T22:01:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an Easy Store that is filling up and need something else. At one time I heard the passport was really good about surviving drips, but I was not sure if there still is a real difference.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NCResident5\"> /u/NCResident5 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtxmtk/is_the_western_digital_passport_better_than_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtxmtk/is_the_western_digital_passport_better_than_the/\">[comments]</a></span>",
        "id": 2506820,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtxmtk/is_the_western_digital_passport_better_than_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is the Western Digital Passport better than the Easy Store or Essentials drives. Thanks.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T19:28:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is this a typo at Newegg? The deal ends in 11 hours.</p> <h1><a href=\"https://www.newegg.com/seagate-barracuda-st24000dm001-24tb-for-daily-computing-7200-rpm/p/N82E16822185109\">Seagate BarraCuda ST24000DM001 24TB</a> - $249.99</h1> <p>That&#39;s $10.41 per TB. They show the regular price as $299.99, so something is weird.</p> <p>They also have a 16TB Seagate BarraCuda drive for $329, so over $20/TB.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wells68\"> /u/wells68 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtty8a/typo_1041_per_tb_for_24_tb_seagate_barracuda/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtty8a/typo_1041_per_tb_for_24_tb_seagate_barracuda/\">[comments]</a></span>",
        "id": 2505536,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtty8a/typo_1041_per_tb_for_24_tb_seagate_barracuda",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Typo? $10.41 per TB for 24 TB - Seagate Barracuda",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T16:25:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have an 8 bay QNAP NAS in my wall mounted rack. It has 2x 1TB SSD&#39;s and 6x 8TB spinners. I want to replace the 2x 1TB SSD&#39;s with regular spinners. If I replace both of them them with larger than the current 8TB Iron Wolf Pros (ie 16TB each), will it cause an issue with the RAID setup ? I&#39;m really asking if all the HDDs in the RAID stupid need to be the same side HDD ?</p> <p>Cheers</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/R0b0tWarz\"> /u/R0b0tWarz </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtpf51/nas_confusion_with_hdd_additiond/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtpf51/nas_confusion_with_hdd_additiond/\">[comments]</a></span>",
        "id": 2503941,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtpf51/nas_confusion_with_hdd_additiond",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NAS confusion with HDD additiond",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T16:23:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Buenos d\u00edas! Necesito digitalizar una muestra de casi 1.000 videos, en distintos formatos, siendo estos VHS, Betacam, Betacam SP, Data Cartridge y CD. Por favor alguien que me pueda ayudar a encontrar el mejor software y las cosas que necesitar\u00e9.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sofitapulga\"> /u/sofitapulga </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtpd7d/manera_profesional_de_digitalizar_vhs_betacam/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtpd7d/manera_profesional_de_digitalizar_vhs_betacam/\">[comments]</a></span>",
        "id": 2503942,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtpd7d/manera_profesional_de_digitalizar_vhs_betacam",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Manera profesional de digitalizar VHS, Betacam, Betacam SP, Data Cartridge y CD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T16:15:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Right now my hoard is spread across drives of various sizes, generations, and operating systems \u2014 mostly stored in my closet. Maybe 20-24TB in all at the moment. The thing is, almost none of it is replicated at the moment. </p> <p>So I want to get a single drive enclosure (&amp; drives) where I can store everything with some redundancy, as well as make the media available on my home network. I\u2019d like something that I can build out over time, ie. multiple replaceable drive bays that may not all be filled in the beginning. My questions are:</p> <ul> <li>Is it better to get a networked enclosure, or network it using something like a Pi?</li> <li>Are there enclosures that accept HDD and SSD? Should I be looking for one that also takes NVME?</li> <li>I\u2019m a RAID newbie. Do these enclosures have built in RAID or do they need to be connected to something running software?</li> <li>What kind of enclosure is recommended for this?</li> <li>Where is a good sourc",
        "id": 2503943,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtp5ze/getting_started_with_large_data_storage_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting started with large data storage? Drives & Enclosure & Networking",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T15:58:29+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pythonistar\"> /u/Pythonistar </a> <br/> <span><a href=\"https://www.newegg.com/seagate-barracuda-st24000dm001-24tb-for-daily-computing-7200-rpm/p/N82E16822185109\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtoqqr/seagate_barracuda_24tb_22_tib_for_250/\">[comments]</a></span>",
        "id": 2503944,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtoqqr/seagate_barracuda_24tb_22_tib_for_250",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate Barracuda 24TB (22 TiB) for $250",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T15:58:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all,</p> <p>I have a synology, and trying to juggle storage capacity of my backups. I have backups set to run daily, and settings to keep versions for a certain period of time. I also have snapshots set up on my backup folder, set to run at certain intervals and to keep versions for a certain period of time. This has created a huge storage concern, as my snapshots are filling up my storage capacity. I have gone in and tried to reduce the number or stored snapshots, but my snapshots are still huge...the same size as my backups. </p> <p>I can always buy more storage, but I don&#39;t want to waste money if I am doing something silly with my retention policies. But I also don&#39;t want to leave myself exposed if hackers were to delete my backups and I should have done something more with my snapshots.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/whitenack\"> /u/whitenack </a> <br/> <span><a href=\"https://www.",
        "id": 2503945,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtoqol/snapshot_immutable_storage_of_backups",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Snapshot (immutable storage) of backups?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T15:36:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I really want to add a NVMe SSD to a proxmox mini PC via USB and control the drive health and temperature via S.M.A.R.T values.</p> <p>But like 90% of all articles on the internet are false. Drives with a Realtek RLT9220 chip for example are marketed as S.M.A.R.T-pass-through, but they do only with SATA drives. Then there are from sabrent that to pass-through values via USB but they are unreliable and get hot.</p> <p>Are there any proven USB cases out there that work?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vghgvbh\"> /u/vghgvbh </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jto84h/are_there_any_reliable_usb_to_nvme_ssd_cases_out/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jto84h/are_there_any_reliable_usb_to_nvme_ssd_cases_out/\">[comments]</a></span>",
        "id": 2503946,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jto84h/are_there_any_reliable_usb_to_nvme_ssd_cases_out",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are there any reliable USB to NVMe SSD cases out there that pass-through S.M.A.R.T and TRIM values?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T14:42:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am a bit lost, I need to buy a case for Exos X24 24TB SAS, Model No: ST24000NM002H. What do I need to check? </p> <p>Thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_stracci\"> /u/_stracci </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtmwts/can_anyone_recommend_me_what_enclosure_to_buy_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtmwts/can_anyone_recommend_me_what_enclosure_to_buy_for/\">[comments]</a></span>",
        "id": 2503240,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtmwts/can_anyone_recommend_me_what_enclosure_to_buy_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can anyone recommend me what enclosure to buy for Exos X24 24TB Model No: ST24000NM002H",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T13:33:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I am having a dilemma in how secure I want my footage to be, im leaving for a few months in the summer to Asia and I want to store all of my footage at home. Currently I have an FTP server connected to a flash drive that I move data back and forth from computers, but I purchased a WD_Black 5TB D10 drive to plug in instead, but now wondering if its worth dumping more money into getting a nasync and a couple drives so that nothing for sure gets lost. Would I be okay with the FTP server and WD black or is it silly? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DaggerSwagge\"> /u/DaggerSwagge </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtlchx/how_should_i_store_my_footage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtlchx/how_should_i_store_my_footage/\">[comments]</a></span>",
        "id": 2501584,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtlchx/how_should_i_store_my_footage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How should I store my footage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T12:23:03+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtjxkb/weve_made_our_storage_chassis_open_source/\"> <img src=\"https://b.thumbs.redditmedia.com/HkUoVix0AkI0Jj1soJqeSnLSVn-05PojHLBTBbK15RE.jpg\" alt=\"We've made our storage chassis open source - Hakoforge\" title=\"We've made our storage chassis open source - Hakoforge\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HakoForge\"> /u/HakoForge </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1jtjxkb\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtjxkb/weve_made_our_storage_chassis_open_source/\">[comments]</a></span> </td></tr></table>",
        "id": 2501583,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtjxkb/weve_made_our_storage_chassis_open_source",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/HkUoVix0AkI0Jj1soJqeSnLSVn-05PojHLBTBbK15RE.jpg",
        "title": "We've made our storage chassis open source - Hakoforge",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T11:14:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There was a song called shards of memories by skeldon , but it has been removed from all platformms for some reason. Has anyone downloaded it before it was removed? I would love to have this song back.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/banana_master_420\"> /u/banana_master_420 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtiqs8/does_anyone_have_this_song/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtiqs8/does_anyone_have_this_song/\">[comments]</a></span>",
        "id": 2501585,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtiqs8/does_anyone_have_this_song",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone have this song?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T10:12:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to calm the nerves of a friend. They can&#39;t exactly remember their password after using Samsung Magician on their Windows 10 pc to enable hardware encryption on a Samsung T7 portable SSD. But if they felt confident they would be permitted to make like 20 or 30 guesses, then they could probably figure out their password. So, does the T7 permanently lock out users from making any further attempts after submitting a certain number of incorrect guesses?</p> <p>After searching (including their user guide), I didn&#39;t find any mention of a max limit on attempts, which is a good sign, but I&#39;d rather be sure before saying it&#39;s ok to go the brute force route.</p> <p>(Edit - I am already aware the hardware can be rescued by sacrificing the data inside, but the goal is to rescue the data.)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/extrahertz\"> /u/extrahertz </a> <br/> <span><a href=\"https://www.re",
        "id": 2501586,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jthsds/maximum_password_attempts_on_samsung_t7_portable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "maximum password attempts on Samsung T7 portable SSD ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T10:01:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I would like to archive some YouTube videos to the highest resolution. Is there a service available for this purpose or an OSS tool?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AMGraduate564\"> /u/AMGraduate564 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jthmsq/best_approach_to_permanently_archive_youtube/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jthmsq/best_approach_to_permanently_archive_youtube/\">[comments]</a></span>",
        "id": 2501587,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jthmsq/best_approach_to_permanently_archive_youtube",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best approach to permanently archive YouTube videos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T09:17:02+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jth0va/ripping_cds_without_noise_or_sound_issues/\"> <img src=\"https://external-preview.redd.it/bGV3dTd5ODZyZHRlMTBlq3XKm9-YbwEJXhPFPsovRPYXbTAbkGMZ00xUYqQL.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ad81bb97378f8ba890e1daea884e551db482794\" alt=\"Ripping cds without noise or sound issues?\" title=\"Ripping cds without noise or sound issues?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So i have hundreds of CDs i need to rip.</p> <p>I use Windows Media Player for its Database. I have an old Asus external disc drive from 2017 And an LG slim portable DVD writer.</p> <p>Whenever I try to play and rip cds lately though, even ones with no scratches, I get that ffp ffp noise when ripping, you know that noise that sounds like a bird flapping it&#39;s wings that&#39;s like some kind of surface noise.</p> <p>Does anyone know how to stop this and what&#39;s causing it? at least before having to clean every disc.",
        "id": 2501147,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jth0va/ripping_cds_without_noise_or_sound_issues",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/bGV3dTd5ODZyZHRlMTBlq3XKm9-YbwEJXhPFPsovRPYXbTAbkGMZ00xUYqQL.png?width=640&crop=smart&auto=webp&s=8ad81bb97378f8ba890e1daea884e551db482794",
        "title": "Ripping cds without noise or sound issues?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T04:01:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So imagine you got big mounted drives in linux. 100TB ones. and the rule i read is always 20%.. but this means i got 20TB sitting around doing noting. is that 20% still applicable on bigger mounted RAID5 volumes ? need some help and clarification if anyone has that?</p> <p>tx</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Main_Abrocoma6000\"> /u/Main_Abrocoma6000 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtcjbc/100tb_linux_mounts_how_much_free_space_should_i/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtcjbc/100tb_linux_mounts_how_much_free_space_should_i/\">[comments]</a></span>",
        "id": 2499932,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtcjbc/100tb_linux_mounts_how_much_free_space_should_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "100TB linux mounts - how much free space should i keep?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T03:48:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using winmerge to compare folders to see what is different. </p> <p>Using duplicate cleaner (<a href=\"https://www.digitalvolcano.co.uk/duplicatecleaner.html\">https://www.digitalvolcano.co.uk/duplicatecleaner.html</a>) to find duplicate files in general. Also have some fast powershell scripts. </p> <p>For file copy, planning to use teracopy or fastcopy. </p> <p>--</p> <p>Any preference on these tools, or others that can be used? </p> <p>thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/retrorays\"> /u/retrorays </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtcbfu/going_through_backups_10s_of_tb_of_data_best/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtcbfu/going_through_backups_10s_of_tb_of_data_best/\">[comments]</a></span>",
        "id": 2499931,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtcbfu/going_through_backups_10s_of_tb_of_data_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "going through backups (10s of TB of data) - best tools to use for Windows 11",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T03:02:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello i want to use a 2 tb sdcard in my vita and the 2nd time i wrote the zzblank to it then formatted it wont recognize anymore. im wondering if i should try a new sdcard and fresh zzblank then format or is there a way to make the other card totally free of the zzblank stamp then re do it. another issue could be that 2 tb is too large for some reason . any advice or experience is appreciated thnks </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dougmike770\"> /u/dougmike770 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtbii1/2_tb_sd_card_for_ps_vita/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtbii1/2_tb_sd_card_for_ps_vita/\">[comments]</a></span>",
        "id": 2499730,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtbii1/2_tb_sd_card_for_ps_vita",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "2 tb Sd card for ps vita",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T02:52:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jtbbs1/questions_about_refs/\"> <img src=\"https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7\" alt=\"Questions about ReFS.\" title=\"Questions about ReFS.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I had a few questions about ReFS since documentation is not very good. Directed at anybody with experience using it. </p> <p>Objective - want checksumming of files for alerting of present bitrot. ReFS has file integrity streams that in theory do exactly this. I have backups, so I don&#39;t care for redundancy. I just need to know which files are bad ASAP. </p> <p>Setup - ReFS drive is an external drive connected to windows 11 (pro). (Using another pc with enterprise to format.)</p> <p>A couple questions/concerns </p> <p>#1- ReFS &quot;salvage&quot; feature. It removes files from the namespace if the",
        "id": 2505078,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtbbs1/questions_about_refs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&crop=smart&auto=webp&s=883009d39175a2f03b76275ed0f7c6011d94a3a7",
        "title": "Questions about ReFS.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T02:28:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, </p> <p>I&#39;m starting the process of digitizing my old family photos and want to run some things by the experts here. I&#39;ve purchased a Epson V600 and I&#39;ve downloaded SilverFast 9 SE. </p> <p>After doing some research online it seems the best process would be to scan my photos as <a href=\"https://imgur.com/a/BAlVTvE\">48 Bit HDR Raw</a>. Especially because I know nothing about photo editing and that&#39;s a whole other world to learn about before I get good at it. Is scanning in RAW generally the recommended course of action around here? </p> <p>I was also wondering what ppi folks think I should scan in as I&#39;ve seen wildly varying recommendations. SilverFast seems to stop it&#39;s pre-set ppi options at 600 but a lot of places online have said to go way above that <a href=\"https://imgur.com/a/zFIgu6p\">which I guess I could do with a custom input</a>. </p> <p>Are there any additional settings in SilverFast that I should be using? ",
        "id": 2499731,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jtavpi/scanning_old_family_photos_with_silverfast_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scanning old family photos with SilverFast - best process and settings?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T01:50:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Like the title says, I have two old 1TB HDDs that I want to repurpose but that I <strong>do not trust for data storage</strong> as they are both over 6 years old and both make kinda weird noises when they&#39;re actively being read/written to (not the click noise, just static sounds). </p> <p>This is my modest data server setup right now: i got a beelink SER5 as the server running ubuntu 24.04 LTS. I got an ORICO 4 bay raid drive with 2 14TB drives in RAID1 as my main data storage solution. Then I got a usb 2.5/3.5 dock. Currently my two random 1TB drives are sitting in the dock. </p> <p>I guess what I&#39;m asking for is an idea for a project with these two drives that I am skeptical for using for critical data storage. I&#39;m by no means an expert in data storage, but i like to tinker around with stuff like this. One idea I had was trying to do data forensics on one of the drives since I formatted them. Yeah Idk, just looking for ideas!</p> </div>",
        "id": 2499575,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jta6wd/two_old_1tb_hdds_for_testinglearning_any_ideas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Two old 1TB HDDs for testing/learning - any ideas?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T01:37:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jt9yf6/motherload_of_old_vhs_recorded_tv_and_original/\"> <img src=\"https://b.thumbs.redditmedia.com/bRhJoI1NuU9nb13_lkFrhIppRqc3KCkOhe5fxVb9g5U.jpg\" alt=\"Motherload of old VHS (recorded TV and original tapes) I don't intend to keep. What to do with them?\" title=\"Motherload of old VHS (recorded TV and original tapes) I don't intend to keep. What to do with them?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MadDogFenby\"> /u/MadDogFenby </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1jt9yf6\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jt9yf6/motherload_of_old_vhs_recorded_tv_and_original/\">[comments]</a></span> </td></tr></table>",
        "id": 2499573,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jt9yf6/motherload_of_old_vhs_recorded_tv_and_original",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/bRhJoI1NuU9nb13_lkFrhIppRqc3KCkOhe5fxVb9g5U.jpg",
        "title": "Motherload of old VHS (recorded TV and original tapes) I don't intend to keep. What to do with them?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-04-07T00:25:00+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hamilton950B\"> /u/Hamilton950B </a> <br/> <span><a href=\"https://gerrymcgovern.com/data-centers-contain-90-crap-data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jt8m13/data_centers_contain_90_crap_data/\">[comments]</a></span>",
        "id": 2499371,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jt8m13/data_centers_contain_90_crap_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data centers contain 90% crap data",
        "vote": 0
    }
]