[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T22:12:45.377243+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T20:39:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I\u2019m scraping data daily using python playwright. On my local Windows 10 machine, I had some issues at first, but I got things working using BrowserForge + residential smart proxy (for fingerprints and legit IPs). That setup worked perfectly but only locally.</p> <p>The problem started when I moved my scraping tasks to the cloud. I\u2019m using AWS Batch with Fargate to run the scripts, and that\u2019s where everything breaks.</p> <p>After hitting 403 errors in the cloud, I tried alternatives like Camoufox and Patchright \u2013 they work great locally in headed mode, but as soon as I run them on AWS I am instantly getting blocked and I see 403 and a captcha. The captcha requires you to press and hold a button, and even when I solve it manually, I still get 403s afterward.</p> <p>I also tried xvfb to simulate a display and run in headed mode, but it didn\u2019t help \u2013 same result: 403.</p> <p>I also implemented oxymouse to stimulate mouse movements but ",
        "id": 2569319,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbqrte/need_help_scraping_easyparafr_with_playwright_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help scraping easypara.fr with Playwright on AWS \u2013 getting 403",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T21:07:52.390243+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T20:35:38+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kbqoh8/ai_agent_for_creating_web_scrapers_proof_of/\"> <img src=\"https://external-preview.redd.it/bGV2a2pyeWI4MXllMfSuwtMFsz81_YvBeCujB9YP6NmQY-Q3EZ8LTnVSHDmg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4408dd8ed577c131ddad7228feea9bd3f4e3877\" alt=\"AI Agent for Creating Web Scrapers Proof of Concept\" title=\"AI Agent for Creating Web Scrapers Proof of Concept\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey, threw together a proof of concept of an AI agent for creating web scrapers. Found that most other people in this space are using the LLM directly for web parsing, but this is not cost efficient. Tried out having the agent create the web scraper directly then run it via tools.</p> <p>Under the hood, uses langgraph for the agent, scrapy with scrapyd for the actual web scraper, a custom MCP server for manual web browsing, and a custom MCP server in front of scrapyd.</p> <p>Would anyone find this usef",
        "id": 2568841,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbqoh8/ai_agent_for_creating_web_scrapers_proof_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/bGV2a2pyeWI4MXllMfSuwtMFsz81_YvBeCujB9YP6NmQY-Q3EZ8LTnVSHDmg.png?width=640&crop=smart&auto=webp&s=b4408dd8ed577c131ddad7228feea9bd3f4e3877",
        "title": "AI Agent for Creating Web Scrapers Proof of Concept",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T18:57:44.088272+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T18:10:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.musicmagpie.co.uk/store/products/very-best-of-chris-rea\">https://www.musicmagpie.co.uk/store/products/very-best-of-chris-rea</a></p> <p>I&#39;m trying to scrape this site to check for stock on something I&#39;m looking for. I&#39;m using the right headers yet I still get 403. This is with rotating ISP proxies and making 2 requests per second. Anything I should look into or change? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fuarkistani\"> /u/Fuarkistani </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kbn8dj/403_errors_cloudflare/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kbn8dj/403_errors_cloudflare/\">[comments]</a></span>",
        "id": 2567999,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbn8dj/403_errors_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "403 Errors cloudflare",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T15:42:55.131397+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T14:18:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking to run an ecommerce/seo data site and do the advertising for it.</p> <p>I am a tech sales guy (who hates my job) and could desparately use someone who knows how to get data off the web. DM me if this sounds interesting. Would love to chat and brainstorm ideas at the very least!</p> <p>I have made websites before and am a novice programmer, but I would rather find someone smarter than me to out source the web scraping portion of the project.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/1010Dean1010\"> /u/1010Dean1010 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kbhnoa/anyone_looking_for_a_side_gig/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kbhnoa/anyone_looking_for_a_side_gig/\">[comments]</a></span>",
        "id": 2565608,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbhnoa/anyone_looking_for_a_side_gig",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone looking for a side gig??",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T12:05:58.772039+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T11:04:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, theres this quick-commerce website called Swiggy Instamart (<a href=\"https://swiggy.com/instamart/\">https://swiggy.com/instamart/</a>) for which i want to scrape the keyword-product ranking data (i.e. After entering the keyword, i want to check at which rank certain products appear).</p> <p>But the problem is, i could not see the SKU IDs of the products on the website source page. The keyword search page was only showing the product names, which is not so reliable as product names change often and so. The SKU IDs was only visible if i click the product in the list which opens a new page with product details. </p> <p>To reproduce this - open the above link in india region (through VPN or something if there is geoblocking on the site) and then selecting the location as 560009 (ZIPCODE).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/polaristical\"> /u/polaristical </a> <br/> <span><a href=\"https://www.reddit.co",
        "id": 2564096,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbdp1i/help_with_scraping_instamart",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with scraping Instamart",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T13:10:48.357404+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T09:10:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Wondering if anyone has a method for spoofing/adding noise to canvas &amp; font fingerprints w/ JS injection, as to pass [browserleaks.com](<a href=\"https://browserleaks.com/\">https://browserleaks.com/</a>) with unique signatures.</p> <p>I also understand that it is not ideal for normal web scraping to pass as entirely unique as it can raise red flag. I am wondering a couple things about this assumption:</p> <p>1) If I were to, say, visit the same endpoint 1000 times over the course of a week, I would expect the site to catch on if I have the same fingerprint each time. Is this accurate?</p> <p>2) What is the difference between noise &amp; complete spoofing of fingerprint? Is it to my advantage to spoof my canvas &amp; font signatures entirely or to just add some unique noise on every browser instance</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PossibilityNo2175\"> /u/PossibilityNo2175 </a> <br/> <span><a href",
        "id": 2564800,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbc11k/canvas_font_fingerprints",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Canvas & Font Fingerprints",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-04-30T10:11:00.703375+00:00",
        "date_dead_since": null,
        "date_published": "2025-04-30T09:09:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Some websites are very, <em>very</em> restrictive about opening DevTools. The various things that most people would try first \u2014 I tried them too, and none of them worked.</p> <p>So I turned to <code>mitmproxy</code> to analyze the request headers. But for this particular target, I don&#39;t know why \u2014 it just didn\u2019t capture the kind of requests I wanted. Maybe the site is technically able to detect proxy connections?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Gloomy-Status-9258\"> /u/Gloomy-Status-9258 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kbc0h9/anyone_who_has_used_mitmproxy_or_similar_thing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kbc0h9/anyone_who_has_used_mitmproxy_or_similar_thing/\">[comments]</a></span>",
        "id": 2563507,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbc0h9/anyone_who_has_used_mitmproxy_or_similar_thing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "anyone who has used mitmproxy or similar thing before?",
        "vote": 0
    }
]