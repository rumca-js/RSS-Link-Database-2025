[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-06T03:13:13+00:00",
        "description": "<span class=\"field field--name-title field--type-string field--label-hidden\">How to build Ollama to run LLMs on RISC-V Linux</span>\n\n            <p>RISC-V is the new entrant into the SBC/low-end desktop space, and as I'm in possession of a HiFive Premier P550 motherboard, I am running it through my usual gauntlet of benchmarks\u2014partly to see how fast it is, and partly to gauge how far along RISC-V support is in general across a wide swath of Linux software.</p>\n\n<p>From my first tests on the <a href=\"https://www.jeffgeerling.com/blog/2023/risc-v-business-testing-starfives-visionfive-2-sbc\">VisionFive 2 back in 2023</a> to today, RISC-V has seen quite a bit of growth, fueled by economics, geopolitical wrangling, and developer interest.</p>\n\n<p>The P550 uses the ESWIN EIC7700X SoC, and while it doesn't have a <em>fast</em> CPU, by modern standards, it is fast enough\u2014and the system has enough RAM and IO\u2014to run most modern Linux-y things. Including llama.cpp and Ollama!</p>\n\n<h2>Compiling",
        "id": 2052582,
        "language": "en",
        "link": "https://www.jeffgeerling.com/blog/2025/how-build-ollama-run-llms-on-risc-v-linux",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 162,
        "source_url": "https://www.jeffgeerling.com/blog.xml",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to build Ollama to run LLMs on RISC-V Linux",
        "vote": 0
    }
]