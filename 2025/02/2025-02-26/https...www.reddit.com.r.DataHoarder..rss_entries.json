[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T22:55:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Building a new PC, looking for new drives. Got three M.2 slots, 2 x gen 4, 1 x gen 5</p> <p>Already got a Crucial T500 1TB for the OS as there should be no need for persistent write speed.</p> <p>Looking for a 4TB gen 5 for games and whatever the best price per GB is for plex for the remaining gen 4.</p> <p>Thank you for suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/agoosetime\"> /u/agoosetime </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iz0ud2/looking_for_nvme_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iz0ud2/looking_for_nvme_drives/\">[comments]</a></span>",
        "id": 2204057,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iz0ud2/looking_for_nvme_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for NVMe drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T22:16:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I&#39;m looking for a storage app that basically meets the following criteria:</p> <p>-Large storage space (500GB+)</p> <p>-Easily accessible for editing from multiple ends-Fail</p> <p>-proof / includes mirrored backup</p> <p>Any feedback is appreciated! Thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RepressedTraas\"> /u/RepressedTraas </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyzyfa/best_way_to_store_your_photos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyzyfa/best_way_to_store_your_photos/\">[comments]</a></span>",
        "id": 2204056,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyzyfa/best_way_to_store_your_photos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to store your photos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T19:46:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Any objections to a 10 x 28TB RAIDZ2 using the &quot;recertified&quot; 28TB Seagate HAMR drives? (ST28000NM000C) 90% read 10% write, data stored files will be very large, sequential read/write data.</p> <p>Alternatives? I already have two of these as general storage disks in my desktop but I just built my first TrueNAS scale system that has 12 bays (Silverstone RM61-312) and I need to fill the bays!</p> <p>I have a Asustor lockerstor10 gen 3 with 10 x 24tb Seagate exos drives I purchased at retail and they are in a mdadm btrfs raid 10 which is unnecessary for my use case. It&#39;s all I felt comfortable using given my options with Asustor&#39;s ADM OS. I plan on reconfiguring the lockerstor as my backup unit for the things I wouldn&#39;t want to lose.</p> <p>I thought about getting 24tb drives to compliment the drives I already have so I can mash everything together in my truenas system and leave a few for the Asustor backup unit. But, the recertifie",
        "id": 2202742,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iywdlb/raidz2_10_x_28tb_seagate_exoshamr_st28000nm000c",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "RAIDZ2 10 x 28TB Seagate Exos/HAMR (ST28000NM000C) - Yes/No?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T19:07:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Gonna do a backup. So I have a NAS drive with two raid 1s, with my most important files on it. I\u2019ll just do a snapshot and verify checksums while I\u2019m doing it. And forget the cloud; I\u2019ll leave the last copy in my cousin\u2019s safe.</p> <p>I want to put it in three places. It\u2019s only like 4tb. What makes the most sense? I thought I\u2019d keep working off the NAS, keep one portable HDD in my safe, and leave one portable HDD with my cousin. Does that make sense?</p> <p>Looking at WD my passport ultra for the backups. Any other suggestions? Maybe one of them should be ruggedized, like a Lacie?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hmmqzaz\"> /u/hmmqzaz </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyvfph/best_portable_hdd_for_primary_backup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyvfph/best_portable_hdd_for_primary_backup/\">[comments]</a></s",
        "id": 2202743,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyvfph/best_portable_hdd_for_primary_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best portable HDD for primary backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T18:28:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all! Just sharing my recent experience with the fallen king of hard disks: Western Digital. Due to frequent commutes between home and campus, I felt it necessary to bring my data with me on an external device. I went with WD for a 5TB USB3 device because I had very good experiences with them in the past. This drive never really ended up serving this purpose and within a month was hooked up to my main system as an additional drive instead, but within a year of purchase, the drive started to spit out CRC Check errors and I began to lose data integrity. </p> <p>I immediately contacted Support to get an RMA set up, where the CS Rep I received became insistent that I not only was unaware of how hard drives worked, but that I for some reason had a mac, sending me picture diagrams on how to check on whether or not the device was plugged in and visible in Disk Utility. I never once mentioned Macintosh, nor did I make any sort of claim that the drive wasn",
        "id": 2202129,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyugm7/im_done_with_wd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm done with WD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T17:14:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all! </p> <p>I&#39;m a filmmaker and I&#39;m attempting to grapple with the production side of an upcoming film.</p> <p>Basically, over the course of a few months we will be generating an estimated 64TB of video that we will need to be able to safely store, backup reasonably well, and travel with. Additionally, this is a very tight budget production, so I&#39;m trying to tackle this is the most cost conscious way possible. </p> <p>While it would be nice, the data doesn&#39;t need to be particularly quick to access and can even be partially offline. We would just need access to the most recent 24hrs for cataloging purposes. </p> <p>To keep costs and complexity down, at the moment I&#39;m considering simply utilizing a 2x bay HDD dock (like a StarTech station) paired with 8x 16TB drives (like the WD Red Pros). Each drive would be formatted individually in sequence, and when not actively being transferred to would be stored in a pelican case with foa",
        "id": 2201599,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iysni0/ideas_for_128tb_of_storage_that_needs_to_be_flown",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ideas for 128TB of storage that needs to be flown and accessible on a moving ship",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T17:09:55+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PricePerGig\"> /u/PricePerGig </a> <br/> <span><a href=\"https://pricepergig.com/fr\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iysizg/i_updated_pricepergigcom_to_add_amazonfr_france/\">[comments]</a></span>",
        "id": 2201600,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iysizg/i_updated_pricepergigcom_to_add_amazonfr_france",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I Updated PricePerGig.com to add \ud83c\uddeb\ud83c\uddf7Amazon.fr France\ud83c\uddeb\ud83c\uddf7 as requested in this sub",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T16:59:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I got a Qnap TS-673A and want to populate it with drives to get minim 24 TB usable space. Since the NAs will be in my homeoffice I do not want it to be so loud that it becomes distracting, since besides working I sometimes chill, read and game in this room. From working in datacenters in the past I know that enterprise drives can be really noisy.<br/> Now it is really hard do find good data on reliability and noise. It seems the general consesus is that something like a Toshiba 12TB N300 is &quot;quite&quot; but when I look at the data they claim up to 36db when r/w which is above what some of WDs offers claim 29-30db.<br/> In the past some smaller capicity drives seemed to be quiter so I was thinking more smaller drives but I doubt that the noise reduction is that good when using 6 vs 3 drives if the 6 drives indivually are a bit quiter. Can any of you fine people share some of your experience? It would be much appreciated.</p> </div><!-- SC_ON -",
        "id": 2201601,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iys9fo/looking_for_relatively_quite_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for \"relatively quite\" drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T15:53:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Since CD and DVD Recordables are non erasable, is there a way to make a file on a recordable unusable / unreadable? The purpose being that so devices that can&#39;t read specific formats dont completely discard the disc because of one or two unsupported file format files. Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Main_Shock_4269\"> /u/Main_Shock_4269 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyqoia/any_way_to_disable_make_data_unreadable_on_a_cd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyqoia/any_way_to_disable_make_data_unreadable_on_a_cd/\">[comments]</a></span>",
        "id": 2201006,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyqoia/any_way_to_disable_make_data_unreadable_on_a_cd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any way to disable / make data unreadable on a CD / DVD Recordable?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T15:26:52+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Own_Ad6901\"> /u/Own_Ad6901 </a> <br/> <span><a href=\"/r/medicine/comments/1iyonc7/physicians_for_a_healthy_democracy_facebook_post/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyq228/physicians_for_a_healthy_democracy_facebook/\">[comments]</a></span>",
        "id": 2200416,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyq228/physicians_for_a_healthy_democracy_facebook",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Physicians for a Healthy Democracy Facebook Post\u2026I\u2019m not sure if anyone here can possibly help! Just forwarding where I can.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T15:25:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I want to download the End Of Term Web Archive 2024 to perform text analysis and track changes in textual content. I know that the Internet Archive has a collection where we can download WARC files here <a href=\"https://archive.org/details/EndOfTerm2024WebCrawls\">https://archive.org/details/EndOfTerm2024WebCrawls</a>, but it amounts to hundreds of terabytes, and I can&#39;t download everything. Since I&#39;m only interested in HTML files, and perhaps not all domains but just the most visited ones, I wonder if there is a more optimal solution. I thought of two possibles solutions:</p> <ul> <li>WET files, which contain only the text extracted from the EOT and are much smaller, are available here: <a href=\"https://eotarchive.org/data/\">https://eotarchive.org/data/</a> for previous years, but not for 2024. Does anyone know of links for 2024?</li> <li>I tried to download each HTML file individually using the Wayback Machine API, but there is a rate",
        "id": 2201602,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyq0m3/seeking_efficient_methods_to_download_html_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seeking Efficient Methods to Download HTML Files from EOT Web Archive 2024",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T14:55:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Apologies if this is the wrong sub, and if so please advise the appropriate place.</p> <p>I currently use IDrive for personal use. It&#39;s not pretty, but it does the job and is ideal for my requirements.</p> <p>My other business currently uses Nextcloud. This has more features (which aren&#39;t required) and is significantly more expensive as a file sharing and backup solution.</p> <p>Ideally, I would simply transfer my business to IDrive and the business partner would have access to the account. We would both back up our files to IDrive and we would make use of the cloud drive (shared drive) for business files.</p> <p>However, I can&#39;t simultaneously run my personal account through IDrive, alongside this hypothetical new account. This is quite frustrating - I have had conversations with IDrive and they have advised that this is unfortunately a limitation of their product.</p> <p>Now I am looking for an alternative to IDrive. Practically the sam",
        "id": 2200417,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iypcf4/business_data_and_personal_data_solutions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Business Data and Personal Data solutions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T14:55:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i used to be able to download embeded video using FetchV (on Chome), 1DM+ (Android), but now some of the videos can no longer be dected by them for downloading. Anyone can help me? this link: </p> <p><a href=\"https://v.xlys.ltd.ua/play/25279-5.htm\">https://v.xlys.ltd.ua/play/25279-5.htm</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sea-Paleontologist84\"> /u/Sea-Paleontologist84 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iypc48/cannot_download_embeded_video/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iypc48/cannot_download_embeded_video/\">[comments]</a></span>",
        "id": 2201603,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iypc48/cannot_download_embeded_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cannot download embeded video",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T13:31:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iynkkb/first_hdd_some_help_please/\"> <img src=\"https://preview.redd.it/bv6uf8k5khle1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ecce9c6b8539c719eaba5d25799086894299369f\" alt=\"First HDD - Some help please ?\" title=\"First HDD - Some help please ?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi, Just received my first big HDD. I want to use it first as a place to centralised all my datas to then use it as my first backup.</p> <p>I have few questions that I didn&#39;t managed to respond myself using the wiki : - How to verify that it is completly new and not domaged by the transport ? - Wich filesystem should I use ? There is a list in the wiki but it does not detail any info or comparative spreadsheet about them (I will connect it first into my windows PC) ; - How can I then encrypt it to make it that I need a secured password/passphrase when I connect it ? Like a real strong encryption ?</p> <p>If a",
        "id": 2199863,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iynkkb/first_hdd_some_help_please",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/bv6uf8k5khle1.jpeg?width=640&crop=smart&auto=webp&s=ecce9c6b8539c719eaba5d25799086894299369f",
        "title": "First HDD - Some help please ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T13:16:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So pre season testing and skyf1&#39;s crofty claims each single redbull car sends back 1.5billion terrabytes of data each race. Ehhh ok Crofty give me a chance to catch my breath i can only laugh so hard. It was his confidence in what he was saying that got me laughing so hard.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BarneyBStinson\"> /u/BarneyBStinson </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyn9po/sky_f1s_crofty_15b_tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyn9po/sky_f1s_crofty_15b_tb/\">[comments]</a></span>",
        "id": 2199368,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyn9po/sky_f1s_crofty_15b_tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SKY F1'S CROFTY 1.5B TB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T12:36:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Alright, so here\u2019s the deal. </p> <p>I bought a 45 Drives 60-bay server from some guy on Facebook Marketplace. Absolute monster of a machine. I love it. I want to use it. But there\u2019s a problem: </p> <p>\ud83d\udea8 I use Unraid. </p> <p>Unraid is currently at version 7, which means it runs on Linux Kernel 6.8. And guess what? The HighPoint Rocket 750 HBAs that came with this thing don\u2019t have a driver that works on 6.8. </p> <p>The last official driver was for kernel 5.x. After that? Nothing. </p> <p>So here\u2019s the next problem: </p> <p>\ud83d\udea8 I\u2019m dumb. </p> <p>See, I use consumer-grade CPUs and motherboards because they\u2019re what I have. And because I have two PCIe x8 slots available, I have exactly two choices:<br/> 1. Buy modern HBAs that actually work.<br/> 2. Make these old ones work. </p> <p>But modern HBAs that support 60 drives?<br/> \u2022 I\u2019d need three or four of them.<br/> \u2022 They\u2019re stupid expensive.<br/> \u2022 They use different connectors than the ones I have.<br/>",
        "id": 2199369,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iymj20/patching_the_highpoint_rocket_750_driver_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Patching the HighPoint Rocket 750 Driver for Linux 6.8 (Because I Refuse to Spend More Money)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T12:00:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I joined a company that has two dead desktops that were meant to be the office&#39;s local data server. The RAM and MB on them are dead. The 8 SAS drives are fine and I&#39;m thinking we can use them in either a NAS enclosure that hooks up to a mini pc OR we can pack them away instead and buy new SATA HDDs and use a NAS that hooks up to the mini pc. </p> <p>5 local engineers and one remote engineer need to be able to access the data and backup on it.<br/> I would really appreciate your help, thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Huge_Owl9942\"> /u/Huge_Owl9942 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iylwfo/is_it_worth_getting_new_sata_hdd_or_buy_a_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iylwfo/is_it_worth_getting_new_sata_hdd_or_buy_a_nas/\">[comments]</a></span>",
        "id": 2199862,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iylwfo/is_it_worth_getting_new_sata_hdd_or_buy_a_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it worth getting new SATA HDD or buy a NAS that supports SAS drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T11:05:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just noticed that the items to do on the tracker went from 0 a few days ago, to 1.31B to do...<br/> We have done 1.13B so far...</p> <p>How is it decided what is archived, and wo does it? I am just wondering if &quot;someone&quot; decided to add a totally insane number of files to basically bog the system down enough for &quot;someone&quot; to have a better chance of permanently wiping data before it has been archived?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bjorn1978_2\"> /u/bjorn1978_2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyl241/insane_number_of_items_to_do_for_usgovernment/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyl241/insane_number_of_items_to_do_for_usgovernment/\">[comments]</a></span>",
        "id": 2198406,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyl241/insane_number_of_items_to_do_for_usgovernment",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Insane number of items to do for USGovernment archive project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T08:54:31+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyj97o/i_wrote_my_first_data_to_lto_tape_and_feel_like_a/\"> <img src=\"https://a.thumbs.redditmedia.com/XBnjemEj9RrhKoRzGwPeHHnspXHsrhGt4ficHuCnNV4.jpg\" alt=\"I wrote my first data to LTO tape and feel like a big boy!\" title=\"I wrote my first data to LTO tape and feel like a big boy!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>My data hoarding may be different to many; where my actual storage needs are (relatively) low but I want good quality forms of backup and redundancy at all times. Tape has always been the end-game for reliable long-term storage for my setup, and I&#39;ve finally got it going! I haven&#39;t really anyone that I could explain the setup to and get any response other than &#39;why?&#39;, so I had to quickly post my excitement here...</p> <p>It feels so refreshing; as it did when I first started playing with enterprise grade hardware, to get new hardware setup and automated. I&#39;ve got",
        "id": 2197706,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyj97o/i_wrote_my_first_data_to_lto_tape_and_feel_like_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/XBnjemEj9RrhKoRzGwPeHHnspXHsrhGt4ficHuCnNV4.jpg",
        "title": "I wrote my first data to LTO tape and feel like a big boy!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T08:16:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I have some mega accounts and I don&#39;t want they are wiped after 3months. Last year I used megatools+cron to logging in periodically, but now it&#39;s not working anymore. I can&#39;t log into an account using megatools until I log into an account using a browser. </p> <p>Do you know workarounds, should I use proxies? Other tools?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/P0lpett0n3\"> /u/P0lpett0n3 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyir43/multiple_meganz_accounts_how_to_periodically_login/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyir43/multiple_meganz_accounts_how_to_periodically_login/\">[comments]</a></span>",
        "id": 2197707,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyir43/multiple_meganz_accounts_how_to_periodically_login",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Multiple mega.nz accounts (how to periodically log-in?)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T07:48:50+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyid90/42x8tb_lets_dance/\"> <img src=\"https://b.thumbs.redditmedia.com/4Bx6aoeL3RF8saD0Dh8A1f3w_A6T3gAClKJTLfVpPoE.jpg\" alt=\"42x8TB, Let's Dance!\" title=\"42x8TB, Let's Dance!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/4vhvzgewufle1.png?width=2074&amp;format=png&amp;auto=webp&amp;s=b0b87360cd3dc0ef6fc5027cfc1115c6503bb93b\">https://preview.redd.it/4vhvzgewufle1.png?width=2074&amp;format=png&amp;auto=webp&amp;s=b0b87360cd3dc0ef6fc5027cfc1115c6503bb93b</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iDontRememberCorn\"> /u/iDontRememberCorn </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyid90/42x8tb_lets_dance/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyid90/42x8tb_lets_dance/\">[comments]</a></span> </td></tr></table>",
        "id": 2197440,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyid90/42x8tb_lets_dance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/4Bx6aoeL3RF8saD0Dh8A1f3w_A6T3gAClKJTLfVpPoE.jpg",
        "title": "42x8TB, Let's Dance!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T06:52:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 100+ videos. Some are the same with cuts, others are with bad definition, there are originals \u2026. But difficult to check as many are about different things I did with my camera. Used softwares such as videdup and others but doesn\u2019t provide the duplicates. I need a manual check. It would be easy with 10 files but I have many to compare, anyone who can suggest something please ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/True-Entrepreneur851\"> /u/True-Entrepreneur851 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyhl6q/simple_way_to_compare_video_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyhl6q/simple_way_to_compare_video_files/\">[comments]</a></span>",
        "id": 2197080,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyhl6q/simple_way_to_compare_video_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Simple way to compare video files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T06:45:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I read somewhere that it has issues doing that and just winds up making duplicates of all of your files while copying the new files over. I want it so I can leave it running in the tray and if I download a song, it backs up just that, if I save picture it backs up just that, if my game folder changes it backs up that. I do not want it creating duplicates of everything or overwriting files that may have the same name without asking me. </p> <p>I suppose periodically I could just wipe my folders on iDrive and do another full backup with everything new, but that takes days.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GTRacer1972\"> /u/GTRacer1972 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyhhhv/does_anyone_know_how_to_automatically_back_up_new/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyhhhv/does_anyone_know_how_to_automatically_back_up",
        "id": 2197083,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyhhhv/does_anyone_know_how_to_automatically_back_up_new",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone know how to automatically back up new files to iDrive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T03:44:42+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyefkp/wd_mybook_makes_this_sound_when_plugged_in/\"> <img src=\"https://external-preview.redd.it/MmtpNTdheWZuZWxlMfy_aYECh75q9TB1EbeneuiCe5LYkGaWc4X_QCvOVRPD.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c48cdf47fa02c7796a27395e3061c2a2182d08bd\" alt=\"WD MyBook makes this sound when plugged in.\" title=\"WD MyBook makes this sound when plugged in.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>The computer recognizes the device when it\u2019s plugged in, but the hard drive won\u2019t appear.</p> <p>I also noticed that the disc isn\u2019t spinning.</p> <p>Is my hard drive toasted??</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SunTzy69\"> /u/SunTzy69 </a> <br/> <span><a href=\"https://v.redd.it/0qkkyh2gnele1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyefkp/wd_mybook_makes_this_sound_when_plugged_in/\">[comments]</a></span> </td></tr></table>",
        "id": 2197084,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyefkp/wd_mybook_makes_this_sound_when_plugged_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/MmtpNTdheWZuZWxlMfy_aYECh75q9TB1EbeneuiCe5LYkGaWc4X_QCvOVRPD.png?width=640&crop=smart&auto=webp&s=c48cdf47fa02c7796a27395e3061c2a2182d08bd",
        "title": "WD MyBook makes this sound when plugged in.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T02:59:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a unix grump, I mostly hoard code and distro ISOs and here are my top aliases related to hoarding said things. I use zsh, ymmv with other shells.</p> <p>These mostly came about from doing long shell pipelines and just deciding to slap an alias on them.</p> <pre><code># yes I know I could configure aria2, but I&#39;m lazy # description: download my random shit urls faster alias aria=&#39;aria2c -j16 -s16 -x16 -k1M&#39; # I&#39;ll let you figure this one out alias ghrip=&#39;for i in $(gh repo list --no-archived $(basename $PWD) -L 9999 --json name | jq -r &quot;.[].name&quot;); do gh repo clone $(basename $PWD)/$i -- --recursive -j10; done&#39; # ditto last # alias ghripall=&#39;for i in $(gh repo list $(basename $PWD) -L 9999 --json name | jq -r &quot;.[].name&quot;); do gh repo clone $(basename $PWD)/$i -- --recursive -j10; done&#39; </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/orcus\"> /u/o",
        "id": 2197085,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iydk6s/got_any_handy_shell_aliases_around_data_hoarding",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Got any handy shell aliases around data hoarding?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T02:55:40+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iydhuz/wildly_different_read_speeds_from_2_identical/\"> <img src=\"https://external-preview.redd.it/Rt9Tp6oNXEhtkA1aD5IMVSoknLpe_dZIEkZAJwE2GnA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ee76474b3a84a192ad8e6ffcfeae20b8d7b9783\" alt=\"Wildly different read speeds from 2 identical software raid arrays\" title=\"Wildly different read speeds from 2 identical software raid arrays\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi all, I have been on a (mostly) successful adventure to fix the abysmally slow parity raid speeds in the windows storage spaces tool by following this incredible guide. <a href=\"https://storagespaceswarstories.com/storage-spaces-and-slow-parity-performance/#more-63\">https://storagespaceswarstories.com/storage-spaces-and-slow-parity-performance/#more-63</a></p> <p>I have 6 identical Crucial 2tb MX500 ssds over sata directly on my motherboard </p> <p>These are split into 2 different 3 d",
        "id": 2197086,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iydhuz/wildly_different_read_speeds_from_2_identical",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Rt9Tp6oNXEhtkA1aD5IMVSoknLpe_dZIEkZAJwE2GnA.jpg?width=640&crop=smart&auto=webp&s=8ee76474b3a84a192ad8e6ffcfeae20b8d7b9783",
        "title": "Wildly different read speeds from 2 identical software raid arrays",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T02:10:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I have a couple of questions:</p> <ol> <li>Someone advised me to buy <strong>WD Red Pro</strong> to store my media files because they use <strong>CMR</strong>, whereas the <strong>Plus</strong> version does not. Is that correct? How important do you think having <strong>CMR</strong> is? I noticed that if I buy the <strong>Plus</strong> version, I can afford almost <strong>double the storage capacity</strong> compared to the <strong>Pro</strong> version. For example, with <strong>\u20ac200</strong> (which is my maximum budget), I could get an <strong>8TB WD Red Plus</strong>, whereas with the <strong>Pro</strong> version, I could only get a <strong>4TB drive</strong> at most (which costs <strong>\u20ac175</strong>).</li> <li>I&#39;m <strong>not</strong> building a <strong>NAS</strong> that will run <strong>24/7</strong>\u2014I need the drive as a <strong>tertiary storage disk</strong>, alongside <strong>two SSDs</strong>, for a computer I recently built. This me",
        "id": 2197082,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyclc0/hi_is_a_wd_red_plus_fine_for_media_storage_on_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hi. Is a WD Red Plus fine for media storage on my PC (no NAS)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T01:59:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I had a pretty simple setup , Just 2 external hard drives, both about 2-3 years old one Seagate 2TB drive and another WD 5TB drive, the 2TB drive died last month, not sure why, but just one day Windows would not recognize it, it still spun and everything, but I just could not access the files, it was probably corrupt. But now everything Is stored on my main PCs SSD and that 5TB HDD which I am now baby-ing, that was the impetus to start taking my rampant data hoarding more seriously. </p> <p>but I am a newbie at all of this, so where would I begin? for my purposes I am mostly saving Images, PC Backups and Videos and I do not have the means or funds to set up a NAS, and due to constantly moving around, something that can be portable would be nice (but obviously not a requirement.) I&#39;ve tried Cloud Storage but that is not really for me (I do not feel like paying for any subscriptions at this moment.) so I&#39;ve thought about picking up a portable S",
        "id": 2197079,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iycdde/had_an_hdd_die_and_now_i_am_paranoid_and_want_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Had an HDD Die, and now I am paranoid and want to start taking data hoarding more seriously, where do I begin?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T01:49:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. I have been selfhosting with my Dell Optiplex 7050 SFF for a while now, with the 1 TB SATA SSD that it came with, and an external 20 TB WD Elements HDD. I just bought a Seagate Exos X24 24 TB HDD (I have yet to test it, how should I proceed with this on Debian, by the way?).</p> <p>I opened up the Dell Optiplex and saw the SSD connected with the SATA power cable and SATA data cable, both of which are connected to the motherboard. The SATA power cable also has another connector attached to it marked &quot;ICT&quot; and &quot;slimline SATA&quot; which I am unfamiliar with.</p> <p>There is another SATA data cable connected to the motherboard, not connected to anything else, that I can use with this new hard drive. However, I&#39;m unsure of how I should connect the SATA power cable to the new hard drive. Would I need a SATA power splitter cable? Could this be any generic cable I find on Amazon, or would I need to find a specific one? I also",
        "id": 2197087,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyc63r/advice_with_connecting_x24_hard_drive_to_dell",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advice With Connecting X24 Hard Drive to Dell Optiplex",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T00:49:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi folks, not sure if this is the right sub but figure this is data-related and there are some pretty creative people here. </p> <p>As a self-employed business owner who enjoys doing a year of bookkeeping in one shot, I&#39;m trying to automate that process as much as possible this year. </p> <p>What tools and workflows are available to process hundreds of scanned receipts and generate spreadsheets I can review without manually inputting data?</p> <p>In the past, I would scan receipts and manually create a spreadsheet to compare them with bank statements to validate transactions.</p> <p>I&#39;ve upgraded to OCR this year to scan all the receipts into a searchable PDF binder. And now I&#39;m wondering if there is an AI tool that can comb through the text on each receipt, and to the best of its capability, create a spreadsheet where each receipt gets organized into rows and columns containing key data such as subtotals, totals, tips, category of transa",
        "id": 2197088,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyay6j/automating_accounting_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automating accounting data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T00:47:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, here&#39;s what I think I would need: something that can be accessed easily. something that can be written and updated frequently, for example, even nightly for ongoing drafts. But also needs to be able to be stored long-term.</p> <p>obviously I know that with a novel you can just....print a book on archival paper, but I think it&#39;s good to have digital copies too. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Appropriate_Rent_243\"> /u/Appropriate_Rent_243 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyawap/what_would_be_the_best_long_term_physical_media/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyawap/what_would_be_the_best_long_term_physical_media/\">[comments]</a></span>",
        "id": 2197081,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyawap/what_would_be_the_best_long_term_physical_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What would be the Best Long term physical Media for a novelist?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T00:40:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m not expert in what type of SSD to get for my use case. What I only know is basic stuffs like difference between TLC and QLC.</p> <p>Basically, I want to have an SSD that can endure too much reading without worrying of it failing because of too much reading. It&#39;s basically used for storing(writing) photos once and never gets deleted again. It will also be permanently powered on so worries about bitrot. </p> <p>Anything that I need to consider? or does QLC ssds would suffice for my use case?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WisdomSky\"> /u/WisdomSky </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyarlh/ssd_recommendation_for_heavy_duty_reading/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyarlh/ssd_recommendation_for_heavy_duty_reading/\">[comments]</a></span>",
        "id": 2197089,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyarlh/ssd_recommendation_for_heavy_duty_reading",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SSD Recommendation for heavy duty reading",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-26T00:21:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyacrd/got_my_data_protected/\"> <img src=\"https://preview.redd.it/kxleqvx6ndle1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a1845dc91f4decd8fa1b4c405bbe0b8dac9e4b4\" alt=\"Got my data protected!\" title=\"Got my data protected!\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PrimaryRequirement28\"> /u/PrimaryRequirement28 </a> <br/> <span><a href=\"https://i.redd.it/kxleqvx6ndle1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iyacrd/got_my_data_protected/\">[comments]</a></span> </td></tr></table>",
        "id": 2197078,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iyacrd/got_my_data_protected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/kxleqvx6ndle1.png?width=640&crop=smart&auto=webp&s=4a1845dc91f4decd8fa1b4c405bbe0b8dac9e4b4",
        "title": "Got my data protected!",
        "vote": 0
    }
]