[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-21T19:44:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We previously launched an AI Web Agent Chrome Extension that <strong>autonomously completes tasks</strong> on the web, effortlessly <strong>scrapes data</strong> directly into Google Sheets, and seamlessly <strong>integrate with external services</strong> by calling APIs using AI Function Calling \u2013 all with <strong>simple prompts</strong> and <strong>your own Chrome tabs</strong>!</p> <p>We launched demos of our chrome extension driving actions on the browser to</p> <ul> <li><em>Researching and Trading Stocks</em></li> <li><em>Automating Outbound</em></li> <li><em>Uploading to a CRM Database via AI Function Calling</em></li> <li><em>Bulk Applying to Jobs</em></li> </ul> <p>We can also given a Google Sheet column of urls open the urls as batches of tabs and extract the data from the websites back to Sheets. Now we we are setting up an exchange for both the agentic configs for extraction but also extracted data.</p> <p><strong>After installing the</str",
        "id": 2167921,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1iuzr1i/agentic_workflowautomationscraping_exchange_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Agentic Workflow/Automation/Scraping Exchange is this a viable market",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-21T14:00:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking at scraping products from established retailers. Right now I&#39;m targeting about a dozen sites or so. The typical flow is Category Page w/ Product Card -&gt; Product Details Page. </p> <p>I&#39;ve been using Crawl4AI (No LLM strategy) to get the details from the category page but I&#39;m still kind of new to python and am struggling with extracting and crawling the link as it relates to the product. </p> <p>Also, for those experienced. Do you typically set up a single script per website? Per category? I&#39;m trying to figure out how to structure my app. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/againer\"> /u/againer </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iurjn6/what_scraper_would_you_recommend_for_this_use_case/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iurjn6/what_scraper_would_you_recommend_for_this_use_case",
        "id": 2164844,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1iurjn6/what_scraper_would_you_recommend_for_this_use_case",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What scraper would you recommend for this use case?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-21T13:12:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im trying to scrape a page that contains drop down menus of which produces a secondary drop down when an option is selected in the initial menu.</p> <p>I assume this is JavaScript</p> <p>I need help understanding how I can find all data (all plain text) held in these dropdown menus so I can scrape and store for later reference </p> <p>ChatGPT loves giving solutions but doesn\u2019t explain the nuances of this kind of problem </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Happy_Ghost\"> /u/Happy_Ghost </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iuqk8w/scraping_dynamically_loaded_pages/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iuqk8w/scraping_dynamically_loaded_pages/\">[comments]</a></span>",
        "id": 2164845,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1iuqk8w/scraping_dynamically_loaded_pages",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping dynamically loaded pages",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-21T12:08:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to scrape <strong>Shopify job postings</strong> from freelance platforms like Upwork (<a href=\"https://www.upwork.com/freelance-jobs/shopify/\">https://www.upwork.com/freelance-jobs/shopify/</a>). Specifically, I want to extract:</p> <p>\u2714 <strong>Name of the person who posted the job</strong><br/> \u2714 <strong>Their email (if possible)</strong><br/> \u2714 <strong>Link to the job posting</strong></p> <p>I know Scrapy and BeautifulSoup can handle basic scraping, but Upwork and similar platforms have <strong>JavaScript-heavy pages and anti-bot protections plus i didn&#39;t find email of any person on any platform</strong>. Has anyone successfully scraped <strong>freelancer job listings</strong> before?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TapUseful2870\"> /u/TapUseful2870 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iupdzx/has_anyone_successfully_scraped_freelancer_job/\">[link]<",
        "id": 2164280,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1iupdzx/has_anyone_successfully_scraped_freelancer_job",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone successfully scraped freelancer job listings before?",
        "vote": 0
    }
]