[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-18T19:15:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I would consider myself an intermediate level webscraper, for most websites for my job I can scrape pretty effectively and when I run into a wall I can throw proxies at the problem and that works. </p> <p>I&#39;ve finally met my match. A certain website uses cloudfront and perimeterX and I cant seem to get past it. If I try to scrape using requests + rotating proxies I hit a wall. At a certain point the website inserts into the cookies (__pxid, __px3) and headers and I cant seem to replicate it. I&#39;ve tried hitting a base url with a session so I could get the correct cookies but my cookie jar is always sparse lacking all the auth cookies I need for later runs. I tried using curl_cffi thinking maybe they are TLS fingerprinting but I&#39;ve still gotten no successful runs using it. The website then sends me unencoded garbage and I&#39;m sol.</p> <p>So then I tried to use selenium and do browser automation - im still doomed. i need to rotate proxies ",
        "id": 2142118,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1iskx7b/how_to_scrape_a_website_at_an_advanced_level",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape a website at an advanced level",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-18T13:02:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>As with our <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a>, self-promotions and paid products are welcome here \ud83e\udd1d</p> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1isc5zn/weekly_webscrapers_hiring_faqs_etc/\">[l",
        "id": 2139415,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1isc5zn/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-18T11:53:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need help with a project involving data extraction from tables in PDFs (preferably using python). The PDFs all have different layouts but contain the same type of information\u2014they\u2019re about prices from different companies, with each company having its own pricing structure.</p> <p>I\u2019m allowed to create separate scripts for each layout (the method for extracting data should preferably still be the same tho). I\u2019ve tried several libraries and methods to extract the data, but I haven\u2019t been able to get the code to work properly.</p> <p>I hope I explained the problem well. How can I extract the data?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tjieken77\"> /u/Tjieken77 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1isayvz/how_to_extract_data_from_tables_pdf/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1isayvz/how_to_extract_data_from_tables_pdf/\"",
        "id": 2138987,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1isayvz/how_to_extract_data_from_tables_pdf",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to extract data from tables (pdf)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-18T10:18:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The issue is where i cant see &#39;input type&#39; file in HTML even after the file input window was opened, stuck here for a bit long time, could anyone help? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LocalConversation850\"> /u/LocalConversation850 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1is9k7p/anyone_have_idea_on_how_to_upload_a_picture_using/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1is9k7p/anyone_have_idea_on_how_to_upload_a_picture_using/\">[comments]</a></span>",
        "id": 2138527,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1is9k7p/anyone_have_idea_on_how_to_upload_a_picture_using",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone have idea on how to upload a picture using selenium",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-18T09:15:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I would like to know how to scrape <a href=\"http://archive.org\">archive.org</a> </p> <p>To be more precise, i would like for a 5 year period, inside an annuary (i give the url of the annuary to archive.org) , the extract of all website in a given category (like photgraphy) , and then list all the web URL</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/khaloudkhaloud\"> /u/khaloudkhaloud </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1is8p9s/scraping_web_archiveorg_for_urls/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1is8p9s/scraping_web_archiveorg_for_urls/\">[comments]</a></span>",
        "id": 2137909,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1is8p9s/scraping_web_archiveorg_for_urls",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping web archive.org for URLs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-18T05:28:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I\u2019m searching for any way how to download a pdf from a website that opens a pdf as blob:https\u2026</p> <p>I\u2019ve tried multiple ways with playwright but it seems like I can\u2019t get it to work.</p> <p>Someone has an idea how to do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/goodstonkboi\"> /u/goodstonkboi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1is5dqi/scraping_in_memory_created_pdf/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1is5dqi/scraping_in_memory_created_pdf/\">[comments]</a></span>",
        "id": 2136935,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1is5dqi/scraping_in_memory_created_pdf",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping in memory created pdf",
        "vote": 0
    }
]