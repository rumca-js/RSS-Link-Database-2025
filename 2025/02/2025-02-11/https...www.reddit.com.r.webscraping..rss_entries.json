[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T20:42:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://seekingalpha.com/author/sa-transcripts/analysis\">https://seekingalpha.com/author/sa-transcripts/analysis</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Listen_6389\"> /u/Ok_Listen_6389 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1in8txx/need_help_in_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1in8txx/need_help_in_web_scraping/\">[comments]</a></span>",
        "id": 2094180,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1in8txx/need_help_in_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help in web scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T19:59:02+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1in7rxb/waiting_for_the_data_to_flow_in/\"> <img src=\"https://preview.redd.it/7uzai2e08xq61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1075705394c78313a4f5626c0968236b23907b21\" alt=\"waiting for the data to flow in\" title=\"waiting for the data to flow in\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nickenlunctured\"> /u/nickenlunctured </a> <br/> <span><a href=\"https://i.redd.it/7uzai2e08xq61.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1in7rxb/waiting_for_the_data_to_flow_in/\">[comments]</a></span> </td></tr></table>",
        "id": 2094181,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1in7rxb/waiting_for_the_data_to_flow_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/7uzai2e08xq61.png?width=640&crop=smart&auto=webp&s=1075705394c78313a4f5626c0968236b23907b21",
        "title": "waiting for the data to flow in",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T19:32:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Long story short, but I have a list of thousands of PDFs I can view in browser but I can\u2019t download without a cost.</p> <p>Is there anyway I can automate scraping some of the data from each of these PDFs and exporting to CSV? </p> <p>Can I set something up, like a macro to go to the next PDF as well?</p> <p>Apologies I can\u2019t go into loads of detail, but that\u2019s top level, I\u2019m hoping this is the right place? As I understand PDFs and webpage scraping are 2 different things.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/baconhammock69\"> /u/baconhammock69 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1in74ts/can_i_scrape_pdfs_that_i_cant_download_from_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1in74ts/can_i_scrape_pdfs_that_i_cant_download_from_a/\">[comments]</a></span>",
        "id": 2093288,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1in74ts/can_i_scrape_pdfs_that_i_cant_download_from_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I scrape PDFs that I can\u2019t download from a website?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T14:27:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone! I have a proxy service that provides a new IP on every request, but it only kicks in after I restart my browser or launch a new browser context. I\u2019m wondering if anyone knows a trick or solution to force the proxy to rotate IPs on each page load (or each request) without having to restart the browser every time.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OrchidKido\"> /u/OrchidKido </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imzrye/any_workarounds_to_change_proxy_per_page_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imzrye/any_workarounds_to_change_proxy_per_page_with/\">[comments]</a></span>",
        "id": 2090207,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imzrye/any_workarounds_to_change_proxy_per_page_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any workarounds to change proxy per page with playwright (python)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T14:00:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Any ideas? I want them all so I can search them by word. As is, I could copy and paste the exact title of a youtube video and still fail to find it, so I&#39;m not even sure this is worth it. But, there has to be a better way. Prefferably the names and URLs but names are a solid start.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GoldPlusWater\"> /u/GoldPlusWater </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imz6pz/i_want_the_name_of_every_youtube_video/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imz6pz/i_want_the_name_of_every_youtube_video/\">[comments]</a></span>",
        "id": 2094182,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imz6pz/i_want_the_name_of_every_youtube_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I want the name of every youtube video",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T13:52:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, </p> <p>I&#39;m using Crawl4AI. Nice it works.<br/> But one thing I would like is before it feeds the markdown result to an LLM Extraction Strategy, is it possible to remove the links on the input? </p> <p>The links really add up to the token limit. And I have no need for the links, I just need the body content. </p> <p>Is this possible?</p> <p>P.S. I tried searching for the documentation but i can&#39;t find any. Maybe I&#39;m wrong.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bentraje\"> /u/bentraje </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imz1l4/remove_links_crawl4ai_for_llm_extraction_strategy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imz1l4/remove_links_crawl4ai_for_llm_extraction_strategy/\">[comments]</a></span>",
        "id": 2089746,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imz1l4/remove_links_crawl4ai_for_llm_extraction_strategy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Remove Links Crawl4AI for LLM Extraction Strategy?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T13:01:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>As with our <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a>, self-promotions and paid products are welcome here \ud83e\udd1d</p> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1imy1fd/weekly_webscrapers_hiring_faqs_etc/\">[l",
        "id": 2089745,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imy1fd/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T11:40:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>i&#39;m having trouble findung a searchbar in VBA via Selenium.</p> <p>That is the HTML Code:</p> <p>&lt;input placeholder=&quot;Nummern&quot; size=&quot;1&quot; type=&quot;text&quot; id=&quot;input-4&quot; aria-describedby=&quot;input-4-messages&quot; class=&quot;v-field\\_\\_input&quot; value=&quot;&quot;&gt;</p> <p>My VBA Code:</p> <p>Sub ScrapeGestisDatabase()</p> <p>Set ch = New Selenium.ChromeDriver</p> <p>ch.Start baseUrl:=&quot;<a href=\"https://gestis.dguv.de/search\">https://gestis.dguv.de/search</a>&quot;</p> <p>ch.Get &quot;/&quot; &#39; Returns Gestis Search Site</p> <p>ch.FindElementById(&quot;input-4&quot;).SendKeys &quot;74-82-8&quot;</p> <p>End Sub</p> <p>So essentially what i&#39;m trying to do is finding the search bar &quot;Numbers&quot;on the gestis database (<a href=\"https://gestis.dguv.de/search\">https://gestis.dguv.de/search</a>). But my Code doesn&#39;t find it. Also when i type the FindElementsByClass VBA still ",
        "id": 2088759,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imwn4w/help_webscraping_with_vba_cant_find_search_bar",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help: Webscraping with VBA - Can't Find Search Bar",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T07:25:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working on my own calorie tracker in Google Sheets because I don&#39;t like the interface of Calorie tracking apps.</p> <p>I want to have an inbuilt database in the calorie tracker which draws data from Woolworths/Coles, however I have no clue about Web Scraping. I&#39;ve tried using ChatGPT to come up with code. So far it has not been successful. Partly because I don&#39;t have any background knowledge on coding, and partly (I assume) because I don&#39;t have any APIs to access the data. Again, new to coding so forgive me if my terminology is off.</p> <p>I have a macbook air, so I have to use Terminal, and I have Python3. Is there anyone who can point me in the direction of some existing resources, or help me create some script which will successfully scrape data?</p> <p>I found this<a href=\"https://github.com/wulfftech/Australia_GroceriesScraper\"> GitHub</a> page, but unsure how to proceed with it.</p> </div><!-- SC_ON --> &#32; submitted b",
        "id": 2087530,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imt6rd/scraping_nutritional_information_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Nutritional Information from Woolworths/Coles",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-11T06:54:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>everyone,</p> <p>I\u2019m looking for advice on scraping Airbnb listings with a focus on specific booking days (Tuesday to Thursday of any month). I need to extract both property and host details, and I\u2019m aware that Airbnb employs strong anti-scraping measures.</p> <p>What I\u2019m Trying to Extract:</p> <p>Property Details: \u2022 Property ID (always collect) \u2022 Property name \u2022 Price per night \u2022 Coordinates (latitude and longitude) \u2022 Amenities \u2022 Property rating</p> <p>Host Details: \u2022 Host ID (always collect) \u2022 Host name \u2022 Host profile description (page content) \u2022 Total number of listings the host has \u2022 Host rating</p> <p>I have experience with TypeScript, Axios, Cheerio, and Puppeteer, but I\u2019m open to any suggestions on how to tackle this problem effectively.</p> <p>My Main Questions: 1. What\u2019s the best approach to extract this data? Should I lean towards using Puppeteer/Playwright, or is there a way to leverage any Airbnb API endpoints? 2. How can I handle or bypa",
        "id": 2087531,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1imsqzu/need_suggestions_on_airbnb_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need suggestions on airbnb scraping",
        "vote": 0
    }
]