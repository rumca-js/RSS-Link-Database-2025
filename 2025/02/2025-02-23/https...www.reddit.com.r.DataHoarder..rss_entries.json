[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T23:05:38+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwnaja/patreondownloader_mega_workaround/\"> <img src=\"https://a.thumbs.redditmedia.com/k42ZwWzuUNBe398AUXgBgjBw1fYbsFsbarBTH8Wqgv0.jpg\" alt=\"PatreonDownloader Mega Workaround\" title=\"PatreonDownloader Mega Workaround\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Recently PatreonDownloader has been failing for me and a total of three other people, giving a message starting with &quot;Unhandled exception. System.NotSupportedException: Not logged in&quot; and some stuff about Mega. </p> <p>What worked for me was going to and editing the Mega_Credentials_Example JSON file in the PatreonDownloader folder using notepad, replacing the example email and password with my own email and password for Mega, and then renaming the file to remove the &quot;_example&quot; line. </p> <p><a href=\"https://preview.redd.it/m4psjun3zyke1.png?width=712&amp;format=png&amp;auto=webp&amp;s=841e4866729a51702d958c86faf3367fe495a63d\">",
        "id": 2180623,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwnaja/patreondownloader_mega_workaround",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/k42ZwWzuUNBe398AUXgBgjBw1fYbsFsbarBTH8Wqgv0.jpg",
        "title": "PatreonDownloader Mega Workaround",
        "vote": 0
    },
    {
        "age": null,
        "album": null,
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T22:05:36.716513+00:00",
        "description": null,
        "id": 2180301,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/.rss",
        "manual_status_code": 0,
        "page_rating": 25,
        "page_rating_contents": 80,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": true,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 200,
        "tags": [],
        "thumbnail": null,
        "title": "It's A Digital Disease!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T20:29:07+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwjok3/czkawka_for_photo_duplicates/\"> <img src=\"https://external-preview.redd.it/Q2Q75MHKvaK3vXudj3CoQz3gPnzV234gRHO1fJVJb2E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e7f41c055f1ef0250aa67de982b0969175f2262\" alt=\"czkawka for photo duplicates\" title=\"czkawka for photo duplicates\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for someone to hold my hand please with installing this. I came across this reddit and searched and see many suggest this is the best program to find duplicate photos and it happens to be free too! I have 2TB of photos to go through, some were uploads from the wifes phone, others mine and then sometimes kids uploaded them then I started backing up and deleting lower quality ones and omg.....just so much to go through again since I never finished.</p> <p>I&#39;m not very github tech savvy and I did find the <a href=\"https://github.com/qarmin/czkawka/releases/\">re",
        "id": 2180310,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwjok3/czkawka_for_photo_duplicates",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Q2Q75MHKvaK3vXudj3CoQz3gPnzV234gRHO1fJVJb2E.jpg?width=640&crop=smart&auto=webp&s=7e7f41c055f1ef0250aa67de982b0969175f2262",
        "title": "czkawka for photo duplicates",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T18:14:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just bought a dual hard drive caddy as I need to inventory all my drives, and determine which are the most useful for a new NAS build. It&#39;s a mess down here. I&#39;ve probably got 30 drives laying around from 500g to 18TB.</p> <p>I have a smattering of shucked and data center drives that also need evaluation. I was never a fan of the Kapton tape method, so I made some hardware level changes that were useful, but not for this.</p> <p>So the new dual caddy was intended to replace a single drive Xigmatek USB caddy I&#39;ve had for years. My intention was to permanently modify it to work with datacenter drives.</p> <p>After tearing it apart, I realized that SATA pin 3 was never connected anyway. Sure enough, I put it all back together and drop a data center drive in, and windows found it right away. No modifications needed.</p> <p>TLDR: Xigmatek external USB caddies apparently work just fine with unmodified data center drives. Also. I&#39;ve seen t",
        "id": 2180309,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwghb5/learn_from_my_dumb_mistake_external_drive_caddies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Learn from my dumb mistake - external drive caddies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T16:30:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello!</p> <p>Yeah essentially, I want to upgrade the server I have from like 8 tb to 96tb, but, to simply summarize, the prices of refurbed hdds have blown to effectively become way more expensive.</p> <p>Personally, I wanted to buy 12tb hdds for $99, but that seems impossible atp. I found a model I\u2019m satisfied with for $111, but no where NEAR close to the all time lows we had a few months ago.</p> <p>So here\u2019s the question: do you PERSONALLY think the market will get better or worse? I think it\u2019ll lean towards the latter because of current events in my country (U.S.), AI hype driving every computer related thing up, and known refurb sellers receiving less supply\u2026 unless there\u2019s something I\u2019m completely missing here, then pls inform me.</p> <p>Tl;dr: Will refurb enterprise HDD Prices be more affordable or more expensive in 2025 IYO?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WaningBloomWasTaken\"> /u/Waning",
        "id": 2180307,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwe0q2/refurbed_hdd_prices_in_2025_dilemma_better_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Refurbed HDD Prices in 2025 Dilemma: Better or Worse?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T15:34:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Meet Re-Manga! A three-way CLI tool to download some manga or doujinshi from subreddits like <a href=\"/r/manga\">r/manga</a> and <a href=\"/r/doujinshi\">r/doujinshi</a></p> <p>It&#39;s my very first publicly released project, I hope you guys like it! Criticism is greatly appreciated.</p> <p><a href=\"https://github.com/RafaeloHQ/Re-Manga\">https://github.com/RafaeloHQ/Re-Manga</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Leather_Flan5071\"> /u/Leather_Flan5071 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwcqcg/i_made_a_tool_to_download_mangasdoujinshis_off_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwcqcg/i_made_a_tool_to_download_mangasdoujinshis_off_of/\">[comments]</a></span>",
        "id": 2180306,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwcqcg/i_made_a_tool_to_download_mangasdoujinshis_off_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I made a tool to download Mangas/Doujinshis off of Reddit!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T15:27:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was considering this hypothetical scenario where I would have a self hosted large scale library for books. The purpose of this was to see how many books can I store with &quot;just&quot; $1000. One side of the problem is the text compression of the books, but the other is the storage capacity.</p> <p>It would require external drives of some sort. I assume that HDD are the cheapest? However I&#39;m not sure which brand or which capacity size would be the most economical.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Perseus-Lynx\"> /u/Perseus-Lynx </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwckos/how_many_tb_of_storage_can_you_buy_for_1000/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwckos/how_many_tb_of_storage_can_you_buy_for_1000/\">[comments]</a></span>",
        "id": 2180304,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwckos/how_many_tb_of_storage_can_you_buy_for_1000",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How many TB of storage can you buy for $1000?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T14:00:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, i have a lot of bigger txt, csv, sql (dump) files and wondered what the best way is to organize them and make them better searchable.</p> <p>first i thought about pushing all in a nosql, but then it would be over 1TB which i think would be overkill to ever try to initiate and do queries from.</p> <p>Next thought was, searching for common ids or fields, and create my own tree sctructure with files, where then i create an index like file to each with references to the big files where the detailed data about that id/field is stored, so if i want detailed information another script could go to the specific files and lines and grep/collect it.</p> <p>(i also thought about elasticsearch, apache solr, or sth similar, but i have no knowledge in this are yet)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/m4d40\"> /u/m4d40 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwari5/programscripts_",
        "id": 2180314,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwari5/programscripts_to_searchanalyze_many_big_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Program/script(s) to search/analyze many big files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T13:38:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just bought an Orico DS500-C3 for my home setup, with the purpose of accommodating my backup files and my Steam games. I wonder which HDDs should I buy, and whether it makes any difference. </p> <p>Seagate Barracuda, Exos, Skyhawk, Ironwolf?</p> <p>WD Gold, Red, Purple, Blue?</p> <p>Does it really matter considering the 5 GB/s speed of the DAS system? Should I just get the cheaper ones? Or does it make a difference?</p> <p>Thanks for the help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/skaertus\"> /u/skaertus </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwabpe/recommended_hds_for_das/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iwabpe/recommended_hds_for_das/\">[comments]</a></span>",
        "id": 2180319,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iwabpe/recommended_hds_for_das",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommended HDs for DAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T13:14:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Long time home lab. What I am seeing in the erasure of freely available knowledge greatly disturbs me. As someone who effectively grew up in the public library daily (not a great childhood) reasons. It angers me to see the erosion of access to ideas and thoughts\u2026being cheered on while liberties are being crushed by laws.</p> <p>What are some ways and means to help preserve this information so democracy of thought can be preserved? </p> <p>First time ever I\u2019m having people ask me concerning questions of \u201ccan you help me with x\u201d privacy security item personal etc</p> <p>Torrents? Downloadable wiki? Meshtastic net? What tool is used to copy down sites? To preserve them?</p> <p>I already have a pretty large infra at home I can run anything needed. Proxmox as the VE.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MasterIntegrator\"> /u/MasterIntegrator </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/co",
        "id": 2180316,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw9vgr/new_here_where_should_i_start",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New here - where should I start",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T11:15:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This website contained a lot of interesting materials (e.g. design guidelines for Symbian, MeeGo, Windows Phone). Thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BriefProject\"> /u/BriefProject </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw7xp5/does_anybody_have_a_dump_of_developernokiacom/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw7xp5/does_anybody_have_a_dump_of_developernokiacom/\">[comments]</a></span>",
        "id": 2180312,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw7xp5/does_anybody_have_a_dump_of_developernokiacom",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anybody have a dump of developer.nokia.com?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T10:32:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello I have Debian server without gui and I want download some movies from tezfiles. Unfortunately wget doesn&#39;t work, also lynx. Any suggestions? Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zozurr\"> /u/zozurr </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw7bh3/best_way_to_download_from_tezfiles_to_debian/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw7bh3/best_way_to_download_from_tezfiles_to_debian/\">[comments]</a></span>",
        "id": 2180313,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw7bh3/best_way_to_download_from_tezfiles_to_debian",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to download from tezfiles to debian server without gui",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T07:35:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want towrite a news aggregator using ai to counteract flooding the zone. Any recommendations? The cheaper tge better ;)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/azimuth79b\"> /u/azimuth79b </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw4thq/best_news_api/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw4thq/best_news_api/\">[comments]</a></span>",
        "id": 2180326,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw4thq/best_news_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best news api?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T05:48:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There is a shocking number of strategy guides that clearly exist in abundance but are nowhere to be found on the internet. Like the Bradygames guide for Dead Rising. Can be easily found in physical form but not digitally. The question then arises of how much of this is Brady doing their best to make a guide that they do not even produce anymore unattainable digitally; or possibly it is that no one seems to value the act of archiving data that is still physically attainable ...for now.</p> <p>An example of one in my possession that I couldn&#39;t find for the life of me(but now have) is &quot;silent hill 3&#39;s official strategy guide(Bradygames)&quot; which Id be glad to let anyone use/archive if needed(<a href=\"https://www.silenthillmemories.net/publications/guides/silent_hill_3_official_strategy_guide_brady.pdf\">link</a>)... and If you have the Dead Rising Strategy Guide by BradyGames, Please share!</p> </div><!-- SC_ON --> &#32; submitted by &#32",
        "id": 2180315,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw38kl/lost_strategy_guide_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Lost Strategy Guide Media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T05:29:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We have two desktops we store all our documents and pictures to and have a couple terabytes of accumulated data over many years and are starting to worry about losing it. We don&#39;t have a lot of money and would like a backup option of some sort, I looked tonight for WD My Cloud but it seems like it&#39;s no longer sold and ran across a Buffalo LinkStation 210. Open to suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Goats_vs_Aliens\"> /u/Goats_vs_Aliens </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw2xvv/starting_to_worry_about_our_pics_and_other_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw2xvv/starting_to_worry_about_our_pics_and_other_data/\">[comments]</a></span>",
        "id": 2180311,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw2xvv/starting_to_worry_about_our_pics_and_other_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Starting to worry about our pics and other data, maybe NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T02:43:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw00sj/i_wrote_a_python_script_to_let_you_easily/\"> <img src=\"https://b.thumbs.redditmedia.com/klYTBchgo2fxnXm0B26jPI8ok6majnpiQkc9pyRZdpg.jpg\" alt=\"I wrote a Python script to let you easily download all your Kindle books\" title=\"I wrote a Python script to let you easily download all your Kindle books\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/-wildcat\"> /u/-wildcat </a> <br/> <span><a href=\"/r/Calibre/comments/1ivycmc/i_wrote_a_python_script_to_let_you_easily/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1iw00sj/i_wrote_a_python_script_to_let_you_easily/\">[comments]</a></span> </td></tr></table>",
        "id": 2180308,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1iw00sj/i_wrote_a_python_script_to_let_you_easily",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/klYTBchgo2fxnXm0B26jPI8ok6majnpiQkc9pyRZdpg.jpg",
        "title": "I wrote a Python script to let you easily download all your Kindle books",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T02:36:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there,</p> <p>I got an old desktop I use for Linux Mint with two drives. One for OS, the other for data (8TB).</p> <p>I am thinking I want some kind of way to do an rsync backup and buy a separate 8 TB drive to mirror it.</p> <p>Question is, should I opt for something like an old Raspberry Pi 3 or 4, and use a SATA to USB converter for a regular 3.5 inch 8TB Seagate or Western Digital Drive along with some 3D printed enclosure?</p> <p>Or should I get something like an HP thin client for the same thing since it&#39;s going to be a USB type JBOD setup?</p> <p>EDIT: I forgot I have an old mini PC running Kubuntu on it too so maybe I should just opt for a USB enclosure JBOD setup?</p> <p>Curious for recommendations on workflow for what small PC or device to get as well as the types of drives since I even have debated just getting an external 8TB self powered drive for this too.</p> <p>Before you ask, I just want a pair of two drives in this scenario ",
        "id": 2180328,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ivzvu8/question_about_workflow_for_8tb_drive_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question About Workflow For 8TB Drive Backup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-02-23T01:05:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Edit: And they&#39;re gone, like I said below there were only 6 left when I posted, if you got one grats.</p> <p>Edit2: Today I&#39;m seeing 7 more in stock. Might be worth bookmarking this link!</p> <p><a href=\"https://www.walmart.com/ip/WD-20TB-Elements-Desktop-External-Hard-Drive-WDBWLG0200HBK-NESN/1049105244?classType=VARIANT&amp;athbdg=L1100&amp;from=/search\">https://www.walmart.com/ip/WD-20TB-Elements-Desktop-External-Hard-Drive-WDBWLG0200HBK-NESN/1049105244?classType=VARIANT&amp;athbdg=L1100&amp;from=/search</a></p> <p>This may be fairly normal price or a repost, I don&#39;t know. I had an 8GB drive fail and as I&#39;ve been replacing drives I&#39;ve swapped them out for the 20TB WD shucked drives, I had an extra one saved for a hard drive failure, but now I need one for the shelf. </p> <p>I haven&#39;t seen a sale since Black Friday, so I thought I was screwed or going to end up buying used on ebay or something (I&#39;m anti-exos due to their",
        "id": 2180318,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ivy5eh/hdd_western_digital_20tb_wd_elements_external",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[HDD] Western Digital 20TB WD Elements External Hard Drive ($279 flash sale)",
        "vote": 0
    }
]