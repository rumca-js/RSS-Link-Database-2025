# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Hotel web scraping
 - [https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping](https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping)
 - RSS feed: $source
 - date published: 2025-01-28T18:33:05+00:00

<!-- SC_OFF --><div class="md"><p>Before going on my quest of learning how to use python via the help of ChatGPT... I thought I&#39;d start here to see if something already exists.</p> <p>I need to scrape hotel data from sites like <a href="http://booking.com">booking.com</a> and expedia. The minimum data I need is hotel name and hotel contact email for each location.</p> <p>Any help with this would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/n1247"> /u/n1247 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping/">[comments]</a></span>

## Feedback on Tech Stack for Scraping up to 50k Pages Daily
 - [https://www.reddit.com/r/webscraping/comments/1ic89ab/feedback_on_tech_stack_for_scraping_up_to_50k](https://www.reddit.com/r/webscraping/comments/1ic89ab/feedback_on_tech_stack_for_scraping_up_to_50k)
 - RSS feed: $source
 - date published: 2025-01-28T18:21:34+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone,</p> <p>I‚Äôm working on an internal project where we aim to scrape up to 50,000 pages from around 500 different websites daily, and I‚Äôm putting together an MVP for the scraping setup. I‚Äôd love to hear your feedback on the overall approach.</p> <p>Here‚Äôs the structure I‚Äôm considering:</p> <p>1/ Query-Based Scraper: A tool that lets me query web pages for specific elements in a structured format, simplifying scraping logic and avoiding the need to parse raw HTML manually.</p> <p>2/ JavaScript Rendering Proxy: A service to handle JavaScript-heavy websites and bypass anti-bot mechanisms when necessary.</p> <p>3/ NoSQL Database: A cloud-hosted, scalable NoSQL database to store and organize scraped data efficiently.</p> <p>4/ Workflow Automation Tool: A system to schedule and manage daily scraping workflows, handle retries for failed tasks, and trigger notifications if errors occur.</p> <p>The main priorities for the stack are reliability, scala

## Weekly Webscrapers - Hiring, FAQs, etc
 - [https://www.reddit.com/r/webscraping/comments/1ic10em/weekly_webscrapers_hiring_faqs_etc](https://www.reddit.com/r/webscraping/comments/1ic10em/weekly_webscrapers_hiring_faqs_etc)
 - RSS feed: $source
 - date published: 2025-01-28T13:01:09+00:00

<!-- SC_OFF --><div class="md"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels‚Äîwhether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>As with our <a href="https://reddit.com/r/webscraping/about/sticky?num=1">monthly thread</a>, self-promotions and paid products are welcome here ü§ù</p> <p>If you&#39;re new to web scraping, make sure to check out the <a href="https://webscraping.fyi">Beginners Guide</a> üå±</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AutoModerator"> /u/AutoModerator </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1ic10em/weekly_webscrapers_hiring_faqs_etc/">[l

## help scraping data web chart
 - [https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart](https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart)
 - RSS feed: $source
 - date published: 2025-01-28T09:34:46+00:00

<!-- SC_OFF --><div class="md"><p>Hello everyone, I‚Äôm a newbie at this, and I would like to implement some metrics for a personal app I‚Äôm working on. I need to scrape all the lists from this website: <a href="https://chartmasters.org/">https://chartmasters.org/</a>. The problem I‚Äôm facing is that I can only get the top 25 entries from each list, as those are the ones visible when the page loads. Each list has a dropdown menu where you can select ‚ÄúAll,‚Äù and I believe that would be the way to retrieve the complete results. I‚Äôve tried this with several AI tools, but I always encounter errors. Could you help me with this? Thank you very much!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Lawless_Time"> /u/Lawless_Time </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart/

## Is SingleFile good?
 - [https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good](https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good)
 - RSS feed: $source
 - date published: 2025-01-28T05:31:23+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m using it to collect data from a medical flashcard website. But it seems I was limited, even though I did it organically. Could it be my extension? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Doctor-Anonymus"> /u/Doctor-Anonymus </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good/">[comments]</a></span>

## CloudFlare County Assessor website - any bypass?
 - [https://www.reddit.com/r/webscraping/comments/1ibpbbt/cloudflare_county_assessor_website_any_bypass](https://www.reddit.com/r/webscraping/comments/1ibpbbt/cloudflare_county_assessor_website_any_bypass)
 - RSS feed: $source
 - date published: 2025-01-28T00:51:48+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone! I‚Äôm trying to scrape property square footage data from the a county assessor site (using Python + Selenium) so I can quickly total up footage for multiple condo units. The site uses qPublic and apparently employs Cloudflare security.</p> <p>No matter how much I slow down my requests or manually solve the initial ‚ÄúI‚Äôm not a robot‚Äù challenge, Cloudflare still won‚Äôt let me proceed to the next page programmatically. It‚Äôs basically halting my progress after I click ‚ÄúNext‚Äù for the next record.</p> <p>Has anyone encountered and solved this issue? I‚Äôm aware of captcha-solving services, but it seems messy and might violate terms of service. Official data downloads or aggregator services may be a better route, but I‚Äôd love to know if anyone‚Äôs had success automating qPublic without hitting these roadblocks.</p> <p><strong>Any</strong> advice or experience would be hugely appreciated. I‚Äôm at the point where even manual solving doesn‚Äôt help‚ÄîCloudfla

## Ja3 / akamai / header database
 - [https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database](https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database)
 - RSS feed: $source
 - date published: 2025-01-28T00:25:53+00:00

<!-- SC_OFF --><div class="md"><p>Hello guys, i‚Äôm trying to scrape large scale data from a popular site using WAF.</p> <p>In order to efficiently bypass it I need to create fingerprints of real browsers using ja3/akamai and header.</p> <p>Unless creating a harvester website to get those data, anyone knows a place where I could find up to date data ?</p> <p>Thanks in advance </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Beneficial-Bonus-102"> /u/Beneficial-Bonus-102 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database/">[comments]</a></span>

