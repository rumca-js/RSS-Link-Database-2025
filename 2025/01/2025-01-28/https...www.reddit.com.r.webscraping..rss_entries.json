[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T18:33:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Before going on my quest of learning how to use python via the help of ChatGPT... I thought I&#39;d start here to see if something already exists.</p> <p>I need to scrape hotel data from sites like <a href=\"http://booking.com\">booking.com</a> and expedia. The minimum data I need is hotel name and hotel contact email for each location.</p> <p>Any help with this would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/n1247\"> /u/n1247 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping/\">[comments]</a></span>",
        "id": 1993670,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ic8jiz/hotel_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hotel web scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T18:21:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I\u2019m working on an internal project where we aim to scrape up to 50,000 pages from around 500 different websites daily, and I\u2019m putting together an MVP for the scraping setup. I\u2019d love to hear your feedback on the overall approach.</p> <p>Here\u2019s the structure I\u2019m considering:</p> <p>1/ Query-Based Scraper: A tool that lets me query web pages for specific elements in a structured format, simplifying scraping logic and avoiding the need to parse raw HTML manually.</p> <p>2/ JavaScript Rendering Proxy: A service to handle JavaScript-heavy websites and bypass anti-bot mechanisms when necessary.</p> <p>3/ NoSQL Database: A cloud-hosted, scalable NoSQL database to store and organize scraped data efficiently.</p> <p>4/ Workflow Automation Tool: A system to schedule and manage daily scraping workflows, handle retries for failed tasks, and trigger notifications if errors occur.</p> <p>The main priorities for the stack are reliability, scala",
        "id": 1993031,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ic89ab/feedback_on_tech_stack_for_scraping_up_to_50k",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Feedback on Tech Stack for Scraping up to 50k Pages Daily",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T13:01:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>As with our <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a>, self-promotions and paid products are welcome here \ud83e\udd1d</p> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ic10em/weekly_webscrapers_hiring_faqs_etc/\">[l",
        "id": 1989311,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ic10em/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T09:34:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, I\u2019m a newbie at this, and I would like to implement some metrics for a personal app I\u2019m working on. I need to scrape all the lists from this website: <a href=\"https://chartmasters.org/\">https://chartmasters.org/</a>. The problem I\u2019m facing is that I can only get the top 25 entries from each list, as those are the ones visible when the page loads. Each list has a dropdown menu where you can select \u201cAll,\u201d and I believe that would be the way to retrieve the complete results. I\u2019ve tried this with several AI tools, but I always encounter errors. Could you help me with this? Thank you very much!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lawless_Time\"> /u/Lawless_Time </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart/",
        "id": 1989312,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ibxwr3/help_scraping_data_web_chart",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "help scraping data web chart",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T05:31:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using it to collect data from a medical flashcard website. But it seems I was limited, even though I did it organically. Could it be my extension? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Doctor-Anonymus\"> /u/Doctor-Anonymus </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good/\">[comments]</a></span>",
        "id": 1989314,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ibulns/is_singlefile_good",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is SingleFile good?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T00:51:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! I\u2019m trying to scrape property square footage data from the a county assessor site (using Python + Selenium) so I can quickly total up footage for multiple condo units. The site uses qPublic and apparently employs Cloudflare security.</p> <p>No matter how much I slow down my requests or manually solve the initial \u201cI\u2019m not a robot\u201d challenge, Cloudflare still won\u2019t let me proceed to the next page programmatically. It\u2019s basically halting my progress after I click \u201cNext\u201d for the next record.</p> <p>Has anyone encountered and solved this issue? I\u2019m aware of captcha-solving services, but it seems messy and might violate terms of service. Official data downloads or aggregator services may be a better route, but I\u2019d love to know if anyone\u2019s had success automating qPublic without hitting these roadblocks.</p> <p><strong>Any</strong> advice or experience would be hugely appreciated. I\u2019m at the point where even manual solving doesn\u2019t help\u2014Cloudfla",
        "id": 1989316,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ibpbbt/cloudflare_county_assessor_website_any_bypass",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "CloudFlare County Assessor website - any bypass?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-28T00:25:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello guys, i\u2019m trying to scrape large scale data from a popular site using WAF.</p> <p>In order to efficiently bypass it I need to create fingerprints of real browsers using ja3/akamai and header.</p> <p>Unless creating a harvester website to get those data, anyone knows a place where I could find up to date data ?</p> <p>Thanks in advance </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Beneficial-Bonus-102\"> /u/Beneficial-Bonus-102 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database/\">[comments]</a></span>",
        "id": 1989317,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1iborss/ja3_akamai_header_database",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ja3 / akamai / header database",
        "vote": 0
    }
]