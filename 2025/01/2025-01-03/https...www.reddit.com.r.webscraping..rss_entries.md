# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Just asking about Google
 - [https://www.reddit.com/r/webscraping/comments/1hsuu2x/just_asking_about_google](https://www.reddit.com/r/webscraping/comments/1hsuu2x/just_asking_about_google)
 - RSS feed: $source
 - date published: 2025-01-03T19:32:39+00:00

<!-- SC_OFF --><div class="md"><p>How did Google arised as the web-scraping leader of the internet? How did they managed to build their search engine from the very beginning by gathering content from internet pages around the globe and serving them in their pages?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/St3veR0nix"> /u/St3veR0nix </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1hsuu2x/just_asking_about_google/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1hsuu2x/just_asking_about_google/">[comments]</a></span>

## Scraping lawyer information from state specific directories
 - [https://www.reddit.com/r/webscraping/comments/1hsqgj2/scraping_lawyer_information_from_state_specific](https://www.reddit.com/r/webscraping/comments/1hsqgj2/scraping_lawyer_information_from_state_specific)
 - RSS feed: $source
 - date published: 2025-01-03T16:33:09+00:00

<!-- SC_OFF --><div class="md"><p>Hi, I have been asked to create a united database containing details of lawyers such as their practice areas, education history, contact information who are active in their particular states. The state bar associations are listed in this particular website: <a href="https://generalbar.com/State.aspx">https://generalbar.com/State.aspx</a><br/> An example would be <a href="https://apps.calbar.ca.gov/attorney/LicenseeSearch/QuickSearch?FreeText=aa&amp;SoundsLike=false">https://apps.calbar.ca.gov/attorney/LicenseeSearch/QuickSearch?FreeText=aa&amp;SoundsLike=false</a><br/> Now manually handcrafting specific scrapers for each state is perfectly doable but my hair will start turning grey if I did it with selenium/playwright only. The problem is that I have only got until tomorrow to show my results so I would ideally like to finish scraping at least 10-20 state bar directories. Are there any AI or non-AI tools that can significantly speed up the process so

## Scraping a Cloudflare-Protected Website Long-Term?
 - [https://www.reddit.com/r/webscraping/comments/1hsiq6f/scraping_a_cloudflareprotected_website_longterm](https://www.reddit.com/r/webscraping/comments/1hsiq6f/scraping_a_cloudflareprotected_website_longterm)
 - RSS feed: $source
 - date published: 2025-01-03T09:30:45+00:00

<!-- SC_OFF --><div class="md"><p>Hello,</p> <p>Iâ€™ve created a script that scrapes data from a website protected by Cloudflare, and I want to run constantly (24/24 hours). My current setup makes about <strong>4 requests every 2 minutes</strong> to the website. My concern is that Cloudflare might block my IP or detect my bot due to these repeated requests, especially over a long duration, do you believe so? </p> <p>Would i have to: </p> <ul> <li>Reduce the number of requests (ex: 4 requests every 10 minutes) ?</li> <li>Randomize the intervals between requests (e.g., varying between 2-10 minutes)?</li> <li>Use IP rotation to distribute the requests across different IP addresses?</li> </ul> <p>Thanks for the help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/luxmain22"> /u/luxmain22 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1hsiq6f/scraping_a_cloudflareprotected_website_longterm/">[link]</a></span> &#32; <span><a hr

