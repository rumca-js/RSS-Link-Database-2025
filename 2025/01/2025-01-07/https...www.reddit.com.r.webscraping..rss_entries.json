[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T22:28:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1hw3z1l/help_scraping_pages_jaunes_page_size_and/\"> <img src=\"https://b.thumbs.redditmedia.com/6Tar6VMWmAWzC8ThdbJst89IMvCqTF4QAm_7Hknxmlk.jpg\" alt=\"[HELP] Scraping Pages Jaunes: Page Size and Extracting Emails\" title=\"[HELP] Scraping Pages Jaunes: Page Size and Extracting Emails\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p> <p>I\u2019m currently working on a scraping project targeting <a href=\"https://pagesjaunes.fr\">Pages Jaunes</a>, and I\u2019m facing two specific issues I haven\u2019t been able to solve despite thorough research. A colleague in the field confirmed that these are solvable, but unfortunately, they didn\u2019t explain how. I\u2019m reaching out here hoping someone can guide me!</p> <h1>My Two Issues:</h1> <ol> <li><strong>Increase page size to 30 instead of 20</strong> <ul> <li>By default, Pages Jaunes limits the number of results displayed per page to 20. I\u2019d like to scrape more elements in ",
        "id": 1855386,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hw3z1l/help_scraping_pages_jaunes_page_size_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/6Tar6VMWmAWzC8ThdbJst89IMvCqTF4QAm_7Hknxmlk.jpg",
        "title": "[HELP] Scraping Pages Jaunes: Page Size and Extracting Emails",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T21:29:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am a total layman when talking about python or coding in general, but i need to analyze data from reviews in movies social medias for my final paper in History graduation. </p> <p>As I need to analyze the reviews, I thought about scraping and using a word2vec model to process the data that i want to use, but I dont know if I can do this with just already made models and codes that I found in the internet, or if I would need to make something of my own, what I think would be near impossible considering that I&#39;m a total mess in this subjects and I dont have plenty of time because of my part time job as a teacher.</p> <p>If anyone knows something, has any advice on what should I do or even considers that it&#39;s possible to do what I pretend, please say something, cause I&#39;m feeling a bit lost and I love my research. Drop this theme just because of a technical limitation of mine would be a realy sad thing to happen.</p> <p>Btw, if any of what ",
        "id": 1855387,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hw2l5z/how_to_scrape_reviews_from_imdb_or_letterboxd_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape reviews from IMDB or Letterboxd or Rotten??",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T19:29:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.humansecurity.com/learn/blog/human-and-tollbit-the-dynamic-duo-to-mitigate-control-and-monetize-ai-scraping-agents\">https://www.humansecurity.com/learn/blog/human-and-tollbit-the-dynamic-duo-to-mitigate-control-and-monetize-ai-scraping-agents</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blockety\"> /u/blockety </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvznys/tollbit_and_human_security_and_llm_content/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvznys/tollbit_and_human_security_and_llm_content/\">[comments]</a></span>",
        "id": 1853875,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvznys/tollbit_and_human_security_and_llm_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "TollBit and Human Security and LLM content scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T18:51:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019d like to know if it\u2019s possible to scrape contact details from Google? For example, if a person was searching for a product or services on Google, could you scrape their information (google account possibly, email, phone number?)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jamesrovert30\"> /u/jamesrovert30 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvyqu7/non_technical_founder_question/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvyqu7/non_technical_founder_question/\">[comments]</a></span>",
        "id": 1853876,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvyqu7/non_technical_founder_question",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Non technical founder question",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T17:16:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working on an NLP sentiment analysis project focused on Telegram data and want to combine it with graph analysis of users. I&#39;m new to this field and currently learning techniques, so I need some advice:</p> <ol> <li><p>Do I need Telegram\u2019s API? Is it free or paid?</p></li> <li><p>Feasibility \u2013 Has anyone done a similar project? How challenging is this?</p></li> <li><p>Essential Tools/Software \u2013 What tools or frameworks are required for data extraction, processing, and analysis?</p></li> <li><p>System Requirements \u2013 Any specific system setup needed for smooth execution?</p></li> <li><p>Best Resources \u2013 Can anyone share tutorials, guides, or videos on Telegram data scraping or sentiment analysis?</p></li> </ol> <p>I\u2019m especially looking for inputs from experts or anyone with hands-on experience in this area. Any help or resources would be highly appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/u",
        "id": 1852789,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvwfog/how_to_extract_data_from_telegram_for_sentiment",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to Extract Data from Telegram for Sentiment and Graph Analysis?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T15:05:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Language/library/headless browser.</p> <p>I need to spent lesst resources and make it as fast as possible because i need to take 30k ones </p> <p>I already use puppeteer, but its slow for me</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Admirable-Shower-887\"> /u/Admirable-Shower-887 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvteae/what_the_moust_speedy_solution_to_take_page/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvteae/what_the_moust_speedy_solution_to_take_page/\">[comments]</a></span>",
        "id": 1852063,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvteae/what_the_moust_speedy_solution_to_take_page",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What the moust speedy solution to take page screenshot by url?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T14:18:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>TL;DR</strong><br/> I&#39;m writing a Reddit scraper to collect comments and submissions. The amount of data I need to scrape is approximately 7 billion rows per month (~10 million rows per hour). By &quot;rows,&quot; I mean submission and comment text content. I know that&#39;s a huge scale, but it&#39;s necessary to stay competitive in the task I&#39;m working on. I need help with structuring my project.</p> <p><strong>What have I tried?</strong></p> <p>I developed a test scraper for a single subreddit, and ran into two major problems:</p> <ol> <li>Fetching Submissions with lazy loading: To fetch a subreddit&#39;s submissions, I had to deal with lazy loading. I used Selenium to solve this, but it\u2019s very heavy and it takes several seconds per query to mimic human behavior (e.g., scrolling with delays). This makes Selenium not scalable, because I will need a lot of Selenium instances to run asynchronously.</li> <li>Proxy Requirements for subr",
        "id": 1852790,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvse53/scaling_a_reddit_scraper_handling_50b_rowsmonth",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scaling a Reddit Scraper: Handling 50B Rows/Month",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T13:01:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>As with our <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a>, self-promotions and paid products are welcome here \ud83e\udd1d</p> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvqwxr/weekly_webscrapers_hiring_faqs_etc/\">[l",
        "id": 1851006,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvqwxr/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T07:13:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There&#39;s been a huge increase in the amount of web scraping for LLM training recently, and I&#39;ve heard some people talk about it as if there&#39;s nothing they can do to stop it. This got me thinking, why not implement a super lightweight proof-of-work as a defense against it? If enough people threw up a proof-of-work proxy that took just a few milliseconds per request to solve, for example, large organizations would be financially deterred from repeatedly mass-scraping the internet, but normal users would see basically no difference. (Yes, there would inherently be a slight power draw increase, and yes it would scale massively if widely used and probably affect battery lives, but I think if it&#39;s scaled properly it can avoid negatively impacting users while still penalizing huge scrapers).</p> <p>I was surprised I couldn&#39;t find any existing solutions that implemented this, so I thew together a super basic proof of concept proxy for the ",
        "id": 1849220,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvlzz5/proof_of_work_for_scraping_protection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Proof of Work for Scraping Protection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T07:11:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have some stocks, and the complexity of tracking those from several sites with all different presentations and way too much extra data made me wonder if I could track them myself.</p> <p>Well, I can now, but the amount of advice I had to go through from experts, selling their product in the mean time, or enthusiasts and hobbyists using all sorts of code, languages and modules, was exhausting.</p> <p>And what I wanted was quite simple.. just one page in Excel or Calc, keeping track of my stock values, modestly refreshed every 5 minutes. And I had a fair idea of how to do that too. Scheduling the import of a csv file into a Calc work sheet is easy, as is referencing the imported csv values in another, my presentation sheet. So, creating this csv file with stock values became the goal. This is how I did it, eventually I mean, after first following all of the aforementioned advice, and then ignoring most of it, starting from scratch with this in mind:<",
        "id": 1849663,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvlyw3/treat_web_scraped_html_as_text",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Treat web scraped HTML as text",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T02:18:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t want to pay $50 every month and my usage is pretty low.<br/> Is there any service which provides usage based pricing rather than monthly subscription cost?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mugiltsr\"> /u/mugiltsr </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvgv4p/any_usage_based_scraping_services/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hvgv4p/any_usage_based_scraping_services/\">[comments]</a></span>",
        "id": 1848004,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvgv4p/any_usage_based_scraping_services",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any usage based scraping services?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-07T00:55:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Newer to web dev and especially scraping, but I&#39;m looking to scrape a Fedex page that shows tracking information for a particular tracking number (like <a href=\"https://www.fedex.com/fedextrack/?trknbr=123456789012&amp;trkqual=2460503000%7E123456789012%7EFX\">this one</a>), and in turn, scrape other pages for other tracking numbers. </p> <p>I also want to note that signing up for and using the carrier&#39;s dev API to get this information will not work for my use case.</p> <p>I&#39;ve used Playwright, Puppeteer, and Selenium in a non-headless mode, and every time the browser pops up, I get &quot;Unfortunately we are unable to retrieve your tracking results at this time. Please try again later&quot;. I might be using them wrong, but I do know the tracking number is valid because the page loads if I use my normal browser. I&#39;ve also tried looking for APIs I can use in the dev console, but no luck there.</p> </div><!-- SC_ON --> &#32; submitted by",
        "id": 1848515,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hvf4vx/how_do_i_figure_out_if_a_site_is_scrapable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I figure out if a site is scrapable?",
        "vote": 0
    }
]