[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T21:05:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My web scrapper is running when tested locally but when deployed on Digital Ocean the scrapper stopped working after a few days and now getting timeout exception as it&#39;s unable to find the element. For context I&#39;m using selenium, I tried rotating user agents in request but its still not going past this step.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lone_Survivour_\"> /u/Lone_Survivour_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout/\">[comments]</a></span>",
        "id": 1897177,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Geeting timeout",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T20:13:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What\u2019s the site that\u2019s drained the most resources - time, money, or sheer mental energy - when you\u2019ve tried to scrape it?</p> <p>Maybe it\u2019s packed with anti-bot scripts, aggressive CAPTCHAs, constantly changing structures, or just an insane amount of data to process? Whatever it is, I\u2019m curious to know which site really pushed your setup to its limits (or your patience). Did you manage to scrape it in the end, or did it prove too costly to bother with?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/woodkid80\"> /u/woodkid80 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/\">[comments]</a></span>",
        "id": 1896410,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are your most difficult sites to scrape?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T18:29:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Currently, I make a living from web scraping\u2014it\u2019s my online business. I want to share why I believe a Chrome extension for web scraping is much better than using other programming languages like Python, Java, or Node.js.</p> <h1>Advantages of using a Chrome extension for web scraping:</h1> <ul> <li><strong>Automatic cookie management:</strong> It allows you to load cookies automatically, eliminating the need to log back into systems repeatedly. For example, with Puppeteer or Selenium, you would have to manage cookies manually, which is a hassle.</li> <li><strong>API and cookie interception:</strong> A Chrome extension enables you to easily intercept APIs and cookies from a website. Selenium falls short in this aspect, and Puppeteer can only partially compete.</li> <li><strong>Code protection:</strong> You can sell the scraper as a functional extension, where the client only downloads data but doesn\u2019t receive the web scraping recipe (the source code).",
        "id": 1895973,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0kt8c/best_way_to_scrape_data_is_using_a_chrome",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to scrape data is using a Chrome Extension",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T17:15:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>DA\u00f71\u00acDZ\u00f71\u00acDB\u00f71\u00acDD\u00f71736797500\u00acAW\u00f71\u00acDC\u00f71736797500\u00acDS\u00f70\u00acDI\u00f7-1\u00acDL\u00f71\u00acDM\u00f7\u00acDX\u00f7OD,HH,SCR,LT,TA,TV\u00acDEI\u00f7<a href=\"https://static.flashscore.com/res/image/data/SUTtpvDa-4r9YcdPQ-6XKdgOM6.png%C2%ACDV%C3%B71%C2%ACDT%C3%B7%C2%ACSC%C3%B716%C2%ACSB%C3%B71%C2%ACSD%C3%B7bet365%C2%ACA1%C3%B74803557f3922701ee0790fd2cb880003%C2%AC%5C%7E\">https://static.flashscore.com/res/image/data/SUTtpvDa-4r9YcdPQ-6XKdgOM6.png\u00acDV\u00f71\u00acDT\u00f7\u00acSC\u00f716\u00acSB\u00f71\u00acSD\u00f7bet365\u00acA1\u00f74803557f3922701ee0790fd2cb880003\u00ac\\~</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Thelimegreenishcoder\"> /u/Thelimegreenishcoder </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like/\">[comments]</a></span>",
        "id": 1895044,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there anyway to decode an api response like this one?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T16:40:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I have been doing a project for fun involving lxml, an HTML parsing library. However, now I&#39;m wondering if there is a use case for it. I&#39;m going to write a blog post on Medium about what I&#39;ve done. If there&#39;s a use case, I&#39;m going to organize the blog post into &quot;the problem&quot; and &quot;the solution&quot; sections. If not, I&#39;m going to organize it into &quot;my goals&quot; and &quot;how I got there&quot; sections.</p> <p>The relevant part of the project is to see if I can improve on the information lxml provides when it generates errors parsing HTML. Specifically, I&#39;ve been modifying and building the source code to create my own version of lxml. I&#39;ve added function calls to the Cython source code that call functions in the underlying C library, libxml2. These functions are designed to print information about C data structures used by the parser. This way, I have been able to print information about pars",
        "id": 1897178,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0i53w/use_case_for_lxml_source_code_changing_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Use case for lxml source code changing project?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T16:27:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Currently working with Selenium + Beautiful Soup, but heard about Scrapy and Playwright</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/computersmakeart\"> /u/computersmakeart </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web/\">[comments]</a></span>",
        "id": 1895043,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are the current best Python libs for Web Scraping and why?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T15:46:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1i0gvm6/help_scraping_trendtrack_extension/\"> <img src=\"https://b.thumbs.redditmedia.com/mwFEewQ0m_XD3f5ZkQmUf6bLqo32qIdlEIO2T9SX5Ic.jpg\" alt=\"Help scraping Trendtrack extension\" title=\"Help scraping Trendtrack extension\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I tried to scrape from Trendtrack extension.<br/> I tried with playwright and set --load-extension `args` and I received error message</p> <pre><code> browser = p.chromium.launch( headless=False, args=[ &quot;--disable-extensions-except=./extensions/trendtrack&quot;, &quot;--load-extension=./extensions/trendtrack&quot;, ], ) </code></pre> <p><a href=\"https://preview.redd.it/6un2l3ss7sce1.png?width=411&amp;format=png&amp;auto=webp&amp;s=09139f4373d16023bca2916d37eec253921257e9\">https://preview.redd.it/6un2l3ss7sce1.png?width=411&amp;format=png&amp;auto=webp&amp;s=09139f4373d16023bca2916d37eec253921257e9</a></p> </div><!-- SC_ON --> &#32; submitt",
        "id": 1897179,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i0gvm6/help_scraping_trendtrack_extension",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/mwFEewQ0m_XD3f5ZkQmUf6bLqo32qIdlEIO2T9SX5Ic.jpg",
        "title": "Help scraping Trendtrack extension",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-13T03:10:00+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Aggressive_Tree7114\"> /u/Aggressive_Tree7114 </a> <br/> <span><a href=\"https://news.ycombinator.com/item?id=42672336\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i04ko8/ai_agent_for_generating_web_scraper_parsing/\">[comments]</a></span>",
        "id": 1891118,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i04ko8/ai_agent_for_generating_web_scraper_parsing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI Agent for Generating Web Scraper Parsing",
        "vote": 0
    }
]