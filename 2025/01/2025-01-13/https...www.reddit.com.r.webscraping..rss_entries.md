# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Geeting timeout
 - [https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout](https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout)
 - RSS feed: $source
 - date published: 2025-01-13T21:05:13+00:00

<!-- SC_OFF --><div class="md"><p>My web scrapper is running when tested locally but when deployed on Digital Ocean the scrapper stopped working after a few days and now getting timeout exception as it&#39;s unable to find the element. For context I&#39;m using selenium, I tried rotating user agents in request but its still not going past this step.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Lone_Survivour_"> /u/Lone_Survivour_ </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i0omdy/geeting_timeout/">[comments]</a></span>

## What are your most difficult sites to scrape?
 - [https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape](https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape)
 - RSS feed: $source
 - date published: 2025-01-13T20:13:39+00:00

<!-- SC_OFF --><div class="md"><p>What’s the site that’s drained the most resources - time, money, or sheer mental energy - when you’ve tried to scrape it?</p> <p>Maybe it’s packed with anti-bot scripts, aggressive CAPTCHAs, constantly changing structures, or just an insane amount of data to process? Whatever it is, I’m curious to know which site really pushed your setup to its limits (or your patience). Did you manage to scrape it in the end, or did it prove too costly to bother with?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/woodkid80"> /u/woodkid80 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/">[comments]</a></span>

## Best way to scrape data is using a Chrome Extension
 - [https://www.reddit.com/r/webscraping/comments/1i0kt8c/best_way_to_scrape_data_is_using_a_chrome](https://www.reddit.com/r/webscraping/comments/1i0kt8c/best_way_to_scrape_data_is_using_a_chrome)
 - RSS feed: $source
 - date published: 2025-01-13T18:29:05+00:00

<!-- SC_OFF --><div class="md"><p>Currently, I make a living from web scraping—it’s my online business. I want to share why I believe a Chrome extension for web scraping is much better than using other programming languages like Python, Java, or Node.js.</p> <h1>Advantages of using a Chrome extension for web scraping:</h1> <ul> <li><strong>Automatic cookie management:</strong> It allows you to load cookies automatically, eliminating the need to log back into systems repeatedly. For example, with Puppeteer or Selenium, you would have to manage cookies manually, which is a hassle.</li> <li><strong>API and cookie interception:</strong> A Chrome extension enables you to easily intercept APIs and cookies from a website. Selenium falls short in this aspect, and Puppeteer can only partially compete.</li> <li><strong>Code protection:</strong> You can sell the scraper as a functional extension, where the client only downloads data but doesn’t receive the web scraping recipe (the source code).

## Is there anyway to decode an api response like this one?
 - [https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like](https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like)
 - RSS feed: $source
 - date published: 2025-01-13T17:15:27+00:00

<!-- SC_OFF --><div class="md"><p>DA÷1¬DZ÷1¬DB÷1¬DD÷1736797500¬AW÷1¬DC÷1736797500¬DS÷0¬DI÷-1¬DL÷1¬DM÷¬DX÷OD,HH,SCR,LT,TA,TV¬DEI÷<a href="https://static.flashscore.com/res/image/data/SUTtpvDa-4r9YcdPQ-6XKdgOM6.png%C2%ACDV%C3%B71%C2%ACDT%C3%B7%C2%ACSC%C3%B716%C2%ACSB%C3%B71%C2%ACSD%C3%B7bet365%C2%ACA1%C3%B74803557f3922701ee0790fd2cb880003%C2%AC%5C%7E">https://static.flashscore.com/res/image/data/SUTtpvDa-4r9YcdPQ-6XKdgOM6.png¬DV÷1¬DT÷¬SC÷16¬SB÷1¬SD÷bet365¬A1÷4803557f3922701ee0790fd2cb880003¬\~</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Thelimegreenishcoder"> /u/Thelimegreenishcoder </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i0izxt/is_there_anyway_to_decode_an_api_response_like/">[comments]</a></span>

## Use case for lxml source code changing project?
 - [https://www.reddit.com/r/webscraping/comments/1i0i53w/use_case_for_lxml_source_code_changing_project](https://www.reddit.com/r/webscraping/comments/1i0i53w/use_case_for_lxml_source_code_changing_project)
 - RSS feed: $source
 - date published: 2025-01-13T16:40:38+00:00

<!-- SC_OFF --><div class="md"><p>Hi all, I have been doing a project for fun involving lxml, an HTML parsing library. However, now I&#39;m wondering if there is a use case for it. I&#39;m going to write a blog post on Medium about what I&#39;ve done. If there&#39;s a use case, I&#39;m going to organize the blog post into &quot;the problem&quot; and &quot;the solution&quot; sections. If not, I&#39;m going to organize it into &quot;my goals&quot; and &quot;how I got there&quot; sections.</p> <p>The relevant part of the project is to see if I can improve on the information lxml provides when it generates errors parsing HTML. Specifically, I&#39;ve been modifying and building the source code to create my own version of lxml. I&#39;ve added function calls to the Cython source code that call functions in the underlying C library, libxml2. These functions are designed to print information about C data structures used by the parser. This way, I have been able to print information about pars

## What are the current best Python libs for Web Scraping and why?
 - [https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web](https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web)
 - RSS feed: $source
 - date published: 2025-01-13T16:27:09+00:00

<!-- SC_OFF --><div class="md"><p>Currently working with Selenium + Beautiful Soup, but heard about Scrapy and Playwright</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/computersmakeart"> /u/computersmakeart </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i0hthe/what_are_the_current_best_python_libs_for_web/">[comments]</a></span>

## Help scraping Trendtrack extension
 - [https://www.reddit.com/r/webscraping/comments/1i0gvm6/help_scraping_trendtrack_extension](https://www.reddit.com/r/webscraping/comments/1i0gvm6/help_scraping_trendtrack_extension)
 - RSS feed: $source
 - date published: 2025-01-13T15:46:30+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1i0gvm6/help_scraping_trendtrack_extension/"> <img src="https://b.thumbs.redditmedia.com/mwFEewQ0m_XD3f5ZkQmUf6bLqo32qIdlEIO2T9SX5Ic.jpg" alt="Help scraping Trendtrack extension" title="Help scraping Trendtrack extension" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>I tried to scrape from Trendtrack extension.<br/> I tried with playwright and set --load-extension `args` and I received error message</p> <pre><code> browser = p.chromium.launch( headless=False, args=[ &quot;--disable-extensions-except=./extensions/trendtrack&quot;, &quot;--load-extension=./extensions/trendtrack&quot;, ], ) </code></pre> <p><a href="https://preview.redd.it/6un2l3ss7sce1.png?width=411&amp;format=png&amp;auto=webp&amp;s=09139f4373d16023bca2916d37eec253921257e9">https://preview.redd.it/6un2l3ss7sce1.png?width=411&amp;format=png&amp;auto=webp&amp;s=09139f4373d16023bca2916d37eec253921257e9</a></p> </div><!-- SC_ON --> &#32; submitt

## AI Agent for Generating Web Scraper Parsing
 - [https://www.reddit.com/r/webscraping/comments/1i04ko8/ai_agent_for_generating_web_scraper_parsing](https://www.reddit.com/r/webscraping/comments/1i04ko8/ai_agent_for_generating_web_scraper_parsing)
 - RSS feed: $source
 - date published: 2025-01-13T03:10:00+00:00

&#32; submitted by &#32; <a href="https://www.reddit.com/user/Aggressive_Tree7114"> /u/Aggressive_Tree7114 </a> <br/> <span><a href="https://news.ycombinator.com/item?id=42672336">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i04ko8/ai_agent_for_generating_web_scraper_parsing/">[comments]</a></span>

