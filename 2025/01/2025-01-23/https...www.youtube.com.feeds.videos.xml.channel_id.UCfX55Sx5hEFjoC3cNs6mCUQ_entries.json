[
    {
        "age": null,
        "album": "",
        "author": "The Linux Foundation",
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T19:56:30+00:00",
        "description": "AI is driving fundamental change in global infrastructure and application architecture. New data center racks, cooling options, and networking requirements are being driven by each new generation of NVIDIA and AMD GPUs. Beyond these infrastructure changes, fundamental changes are needed in Cloud storage to unblock AI developers looking to train, tune, and infer new AI models.\n\nIn this session, learn a new Cloud storage architecture for high-performance object storage to support new Ai training and inference workloads on Kubernetes-based GPU clusters, including:\n- Leveraging Ceph as the backbone for Kubernetes-based workloads\n- How to set-up Storage Gateways to connect GPU clusters to high-performance object storage\n- Configuring Cloud storage to support both training and inference needs",
        "id": 1966898,
        "language": null,
        "link": "https://www.youtube.com/watch?v=6zTIejnySyY",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 499,
        "source_url": "https://www.youtube.com/feeds/videos.xml?channel_id=UCfX55Sx5hEFjoC3cNs6mCUQ",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://i3.ytimg.com/vi/6zTIejnySyY/hqdefault.jpg",
        "title": "LF Live Webinar: Building High-Performance Object Storage Architecture for AI Workloads",
        "vote": 0
    }
]