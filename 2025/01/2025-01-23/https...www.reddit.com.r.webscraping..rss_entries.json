[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T21:57:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I am looking on taking on a major site scrap, most sites I have done thus far have been around 200Kish links. I have done text, pictures, videos and scraped entire sites for files. </p> <p>All in all, everything has been simple scripts but there is a major project I have been wanting to do for a while. </p> <p>Here is the snag, organization. </p> <p>I need tips for organzation and normalization. I often WAY overdo it when comes to normalization. </p> <p>For example, when using async methods it goes a lot faster but I am faced with a bottlenecks of comparing some sort of details in some way to reduce redundancy and or conflict.</p> <p>Let&#39;s say I want to check to see if a link has been visited, I then have to check the database. Then once I come across new links I then have to check those, although I have mostly found a half bypass. By using a hashing method I simply just put it in the database and if it rejects it, it moves on, else set the in",
        "id": 1968055,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i8f98k/organization_db_normalization_full_site_scrap",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Organization - db normalization - full site scrap",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T13:00:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m looking to see if a project is possitble:</p> <p>Goal: monitor accounts with 100k + followers, read Tweets, extract data IF it contains a certain amount of characters &amp; numbers, for example (12 characters and numbers: ewrweropiu12), and if this is the first time it has been tweeted by a 100k + account, extract that 12 characters into an excel sheet</p> <p>Is this possible?</p> <p>Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Aggressive-Ear2848\"> /u/Aggressive-Ear2848 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i82skp/x_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i82skp/x_webscraping/\">[comments]</a></span>",
        "id": 1964250,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i82skp/x_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "X Webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T12:40:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! </p> <p>I\u2019ve built a script that gets some data behind a login that\u2019s authenticated with sms pass code. I have a small app that ask me for the credentials and then when the script logged in it prompts me for the sms pass code, it then does its thing and lets me download a CSV of the result once done. </p> <p>Now the app is hosted in Vercel but after fighting that serverless functionality I decided to give up due to limitations and try out a VPS where I have more control. </p> <p>The thing is that I\u2019m not that experienced with this stuff hence I\u2019m asking here. But what would be a good (and cheap) solution?</p> <p>My goal is to be able to run the script wherever when I\u2019m on the fly (phone browser for example). As of now I can only do it locally on my pc. </p> <p>Do I have to also host the app on the VPS or can it stay in Vercel and just call the script? (Or maybe I\u2019ll run in to timeout issues again)</p> <p>And as a bonus question, is there anyway t",
        "id": 1966939,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i82fy9/best_host_for_my_puppeteerjs_scraping_script",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best host for my Puppeteer.js scraping script?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T10:28:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I developed a Python package called AmzPy, which is an Amazon product scraper. I created it for one of my SaaS projects that required Amazon product data. Despite having API credentials, Amazon didn\u2019t grant me access to its API, so I ended up scraping the data I needed and packaged it into a library.</p> <p>See it at <a href=\"https://pypi.org/project/amzpy\">https://pypi.org/project/amzpy</a></p> <p>Currently, AmzPy scrapes product details, but I plan to add features like scraping reviews or search results. Developers can also fork the project and contribute by adding more features.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/convicted_redditor\"> /u/convicted_redditor </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_prod",
        "id": 1963210,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I just created an amazon product scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T08:11:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Redditors,</p> <p>I&#39;m working on a research project focused on exploring every nook and cranny of New York City. My goal is to capture every small event, discover hidden places, visit historic landmarks, scenic viewpoints, movie filming locations, and everything in between.</p> <p>I\u2019m looking for:</p> <ol> <li>Websites or platforms that list NYC events, small historic sites, or unique attractions.</li> <li>APIs or data sources that I can use to fetch information programmatically.</li> <li>Recommendations for resources that are free or low-cost.</li> </ol> <p>If you know of any tools, websites, or databases (public or private) that can help me gather this information, I\u2019d greatly appreciate your input.</p> <p>Thanks in advance for any tips, links, or insights you can share!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AIphobic\"> /u/AIphobic </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/",
        "id": 1962443,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i7yny2/comprehensive_sources_for_nyc_events_small_places",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Comprehensive Sources for NYC Events, Small Places, and APIs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T02:12:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there any API archives for data scraping in job sites like JobsDB to track how many jobs applied to specific role/skill in previous years till present?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jade_Krampus-66\"> /u/Jade_Krampus-66 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i7ss6z/jobsites_data_scrape_for_skill_role_trends_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i7ss6z/jobsites_data_scrape_for_skill_role_trends_in/\">[comments]</a></span>",
        "id": 1961232,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i7ss6z/jobsites_data_scrape_for_skill_role_trends_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Jobsites Data scrape for skill & role trends in past years?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T02:04:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>ABC News Video Source is a B-roll news footage archive curated and cataloged by ABC. Every video is streamable at 360p and using curl you can download them watermark free. Issue is if you try accessing the mp4 file directly you get an access denied error.</p> <p>I don&#39;t quite understand how this happens given that curl does not mimic a browser and simply accesses websites directly. In this case, accessing the file with a browser or even a python requests library results in an error. I&#39;m not sure what curl is doing that gives access compared to using requests or general browser usage.</p> <p><a href=\"https://abcnewsvsource.com/\">https://abcnewsvsource.com/</a></p> <p>Would appreciate some pointers in how to get around this. Wanting to back up the library on my computer for easy access given that I do a great deal of research.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/doodlebuuggg\"> /u/doodlebuuggg <",
        "id": 1960597,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i7smri/how_to_download_videos_from_abc_news_video_source",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download videos from ABC News Video Source using Python?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-23T00:11:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for the right tool for scraping data from one open browser tab and placing it into a different open browser tab. I used to use imacros for this but it&#39;s no longer supported. A tab to tab scrape is ideal, but if the scraped data needs to first be pasted into a google ssheet and then pulled from that sheet into the destination tab, that&#39;s fine as well.</p> <p>Whichever program I use, I can&#39;t provide it a URL--it has to be based on scraping from an open tab. The reason for this is that where I scrape from maintains the same URL no matter where I navigate to on the site. </p> <p>Any suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pop317\"> /u/Pop317 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i7q9t1/recommendations_for_open_tab_to_open_tab/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i7q9t1/recommendations",
        "id": 1960598,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i7q9t1/recommendations_for_open_tab_to_open_tab",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommendations for open tab to open tab webscraping?",
        "vote": 0
    }
]