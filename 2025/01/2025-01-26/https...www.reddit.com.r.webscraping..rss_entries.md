# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## StockX Scraper API with Flask!!
 - [https://www.reddit.com/r/webscraping/comments/1iaty56/stockx_scraper_api_with_flask](https://www.reddit.com/r/webscraping/comments/1iaty56/stockx_scraper_api_with_flask)
 - RSS feed: $source
 - date published: 2025-01-26T23:37:05+00:00

<!-- SC_OFF --><div class="md"><p>I just made a StockX Scraper API with Flask. It runs on a waitress server hosted using render. Y&#39;all can check it out on github: <a href="https://github.com/aidanmcintosh07/StockXScraper">https://github.com/aidanmcintosh07/StockXScraper</a></p> <p>Any advice is greatly appreciated!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/D4ring"> /u/D4ring </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1iaty56/stockx_scraper_api_with_flask/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1iaty56/stockx_scraper_api_with_flask/">[comments]</a></span>

## Downloading Zooming Image
 - [https://www.reddit.com/r/webscraping/comments/1iatbvf/downloading_zooming_image](https://www.reddit.com/r/webscraping/comments/1iatbvf/downloading_zooming_image)
 - RSS feed: $source
 - date published: 2025-01-26T23:08:51+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone,</p> <p>Could someone please help me with scraping these 2 HD images, I&#39;ve tried De-Zoomify with no success and the obvious inspect element doesn&#39;t work either. It&#39;s the kind of photos where it gives a small preview but when clicked on, allows you to zoom into a high resolution image but only in sections. Any ideas?</p> <p><a href="https://www.artsy.net/artwork/peter-hince-freddie-mercury-crown">https://www.artsy.net/artwork/peter-hince-freddie-mercury-crown</a></p> <p><a href="https://www.artsy.net/artwork/peter-hince-freddie-mercury-sound-check-puebla-mexico">https://www.artsy.net/artwork/peter-hince-freddie-mercury-sound-check-puebla-mexico</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Chinwonder2"> /u/Chinwonder2 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1iatbvf/downloading_zooming_image/">[link]</a></span> &#32; <span><a href="https://www.reddit.co

## ChatGPT Shadowban after scrapping it's UI
 - [https://www.reddit.com/r/webscraping/comments/1iapcl7/chatgpt_shadowban_after_scrapping_its_ui](https://www.reddit.com/r/webscraping/comments/1iapcl7/chatgpt_shadowban_after_scrapping_its_ui)
 - RSS feed: $source
 - date published: 2025-01-26T20:27:40+00:00

<!-- SC_OFF --><div class="md"><p>So, today I was attempting to programmatically log-in in ChatGPT and ask about restaurant recommendations in my area. The objective is to set up a schedule that runs this every day in the morning and then extract the cited sources to a csv so I can track how often my own restaurant is recommended.</p> <p>I managed to do it using a headless browser + proxy IPs, and worked fine. The problem is that after a few runs (I was testing so maybe did like 4-5 runs in 30 mins), ChatGPT stopped using browser and would just reply without access to internet.</p> <p>When explicitly asked to browse the internet (Search option was already toggled), it keeps saying it does not have access to internet.</p> <p>Is this something that happened to anyone before? And any way to bypass or alternative other than using the OpenAI API (It does not give you access to internet).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/SeriousMr"> /u/

## I Made My Python Proxy Library 15x Faster ‚Äì Perfect for Web Scraping!
 - [https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect](https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect)
 - RSS feed: $source
 - date published: 2025-01-26T20:26:29+00:00

<!-- SC_OFF --><div class="md"><p>Hey <a href="/r/webscraping">r/webscraping</a>! </p> <p>If you‚Äôre tired of getting IP-banned or waiting ages for proxy validation, I‚Äôve got news for you: I just released <strong>v2.0.0</strong> of my Python library, <strong><a href="https://github.com/sachin-sankar/swiftshadow">swiftshadow</a></strong>, and it‚Äôs now <strong>15x faster</strong> thanks to async magic! üöÄ </p> <h3><strong>What‚Äôs New?</strong></h3> <p>‚ö° <strong>15x Speed Boost</strong>: Rewrote proxy validation with <code>aiohttp</code> ‚Äì dropped from <strong>~160s</strong> to <strong>~10s</strong> for 100 proxies.<br/> üåê <strong>8 New Providers</strong>: Added sources like <em>KangProxy</em>, <em>GoodProxy</em>, and <em>Anonym0usWork1221</em> for more reliable IPs.<br/> üì¶ <strong>Proxy Class</strong>: Use <code>Proxy.as_requests_dict()</code> to plug directly into <code>requests</code> or <code>httpx</code>.<br/> üóÑÔ∏è <strong>Faster Caching</strong>: Switched to <code>pickle</code> ‚Äì no mo

## doctolib.fr scraper, my first scraping project !
 - [https://www.reddit.com/r/webscraping/comments/1iakxm4/doctolibfr_scraper_my_first_scraping_project](https://www.reddit.com/r/webscraping/comments/1iakxm4/doctolibfr_scraper_my_first_scraping_project)
 - RSS feed: $source
 - date published: 2025-01-26T17:45:24+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone,</p> <p>I&#39;m proud to share to you my first scraping project. <a href="http://Doctolib.fr">Doctolib.fr</a> is a french website to book doctors appointments. I wanted to gather the informations of all the doctors in my area to compare them easily.</p> <p>The code is entirely made of python code, and uses playwright to bypass the website&#39;s securities.</p> <p>Come take a look and tell me what you think :)</p> <p><a href="https://github.com/gagota/doctolib-scrapper/tree/main">https://github.com/gagota/doctolib-scrapper/tree/main</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Key_Bluejay4726"> /u/Key_Bluejay4726 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1iakxm4/doctolibfr_scraper_my_first_scraping_project/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1iakxm4/doctolibfr_scraper_my_first_scraping_project/">[comments]</a></spa

## Finally found something worth using on google maps
 - [https://www.reddit.com/r/webscraping/comments/1iag1yw/finally_found_something_worth_using_on_google_maps](https://www.reddit.com/r/webscraping/comments/1iag1yw/finally_found_something_worth_using_on_google_maps)
 - RSS feed: $source
 - date published: 2025-01-26T14:36:17+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1iag1yw/finally_found_something_worth_using_on_google_maps/"> <img src="https://b.thumbs.redditmedia.com/REqifAwgZ9TyVGbcmmSJahG6LKPcas9kEgEKAIaKrnM.jpg" alt="Finally found something worth using on google maps" title="Finally found something worth using on google maps" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p><a href="https://preview.redd.it/v7476u5bncfe1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=49f065b367d51d2966f6fbec704f3367b95a2754">https://preview.redd.it/v7476u5bncfe1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=49f065b367d51d2966f6fbec704f3367b95a2754</a></p> <p>and yes, all you need is location and keyword. gotta say, it&#39;s better than using Google dorks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Emtyspaces"> /u/Emtyspaces </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1iag1yw/finally_found_something_worth_usi

## Need your help finding a Nextdoor Data Scraper (YES YOU!)
 - [https://www.reddit.com/r/webscraping/comments/1iaesmq/need_your_help_finding_a_nextdoor_data_scraper](https://www.reddit.com/r/webscraping/comments/1iaesmq/need_your_help_finding_a_nextdoor_data_scraper)
 - RSS feed: $source
 - date published: 2025-01-26T13:40:40+00:00

<!-- SC_OFF --><div class="md"><p>NextDoor the neighborhood app has so many businesses that are either just starting out and have no online presence or just literally have nothing at all besides NextDoor.</p> <p>The thing about NextDoor tho is you need to be logged in to see businesses AND you are only allowed to search for businesses in a 15 miles radius.</p> <p>That doesnt matter though, so many businesses can be found in my 15 mile radius.</p> <p>Thus I ask, anyone have a data scraper for Nextdoor or know how to make one. Would love your insights!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/CoolYeetKiddo"> /u/CoolYeetKiddo </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1iaesmq/need_your_help_finding_a_nextdoor_data_scraper/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1iaesmq/need_your_help_finding_a_nextdoor_data_scraper/">[comments]</a></span>

## Residential Proxies rotation time best practice
 - [https://www.reddit.com/r/webscraping/comments/1ia97j6/residential_proxies_rotation_time_best_practice](https://www.reddit.com/r/webscraping/comments/1ia97j6/residential_proxies_rotation_time_best_practice)
 - RSS feed: $source
 - date published: 2025-01-26T08:12:16+00:00

<!-- SC_OFF --><div class="md"><p>As the title says what is the recommended amount of time to rotate?<br/> For context the scraping is on social media sites.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Intrepid-Bicycle3438"> /u/Intrepid-Bicycle3438 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1ia97j6/residential_proxies_rotation_time_best_practice/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ia97j6/residential_proxies_rotation_time_best_practice/">[comments]</a></span>

## Cheap web scraping hosting
 - [https://www.reddit.com/r/webscraping/comments/1ia8ht1/cheap_web_scraping_hosting](https://www.reddit.com/r/webscraping/comments/1ia8ht1/cheap_web_scraping_hosting)
 - RSS feed: $source
 - date published: 2025-01-26T07:19:46+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m looking for a cheap hosting solution for web scraping. I will be scraping 10,000 pages every day and store the results. Will use either Python or NodeJS with proxies. What would be the cheapest way to host this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/vroemboem"> /u/vroemboem </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1ia8ht1/cheap_web_scraping_hosting/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ia8ht1/cheap_web_scraping_hosting/">[comments]</a></span>

## App API Scraping - Cursor Limit??
 - [https://www.reddit.com/r/webscraping/comments/1ia5lsz/app_api_scraping_cursor_limit](https://www.reddit.com/r/webscraping/comments/1ia5lsz/app_api_scraping_cursor_limit)
 - RSS feed: $source
 - date published: 2025-01-26T04:13:33+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;ve built a script that replicates calls from the app to different public API endpoints. (TikTok)</p> <p>I&#39;ve been using it for hashtag pages, to scrape profile/video data (public).<br/> So far i&#39;ve been capped at 5000 videos (cursor 5000) and my script returns no more data after that.</p> <p>Here&#39;s a snippet of the raw server json response at that cursor:</p> <p>&quot;cursor&quot;: 5040,</p> <p>&quot;extra&quot;: {</p> <p>&quot;fatal_item_ids&quot;: [],</p> <p>&quot;logid&quot;: &quot;2025012602325350792C1EA84EA3696629&quot;,</p> <p>&quot;now&quot;: 1737858773000</p> <p>},</p> <p>&quot;has_more&quot;: 0,</p> <p>&quot;log_pb&quot;: {</p> <p>&quot;impr_id&quot;: &quot;2025012602325350792C1EA84EA3696629&quot;</p> <p>},</p> <p>&quot;status_code&quot;: 0,</p> <p>&quot;status_msg&quot;: &quot;&quot;</p> <p>As you can see, the has_more value is set to 0. Now I have valid tokens/parems/headers in my script, but not the time sensitive ones. 

## Need help Getting Cookies
 - [https://www.reddit.com/r/webscraping/comments/1ia2ahm/need_help_getting_cookies](https://www.reddit.com/r/webscraping/comments/1ia2ahm/need_help_getting_cookies)
 - RSS feed: $source
 - date published: 2025-01-26T01:13:04+00:00

<!-- SC_OFF --><div class="md"><p>Im trying to parse some data from a polish gov website (link) through requests but I get blocked. The website is buggy so puppeteer/selenium is problematic. Apparently, there are some TSPD requests I need to handle first, but I can&#39;t manage to do it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Amanhaalim0"> /u/Amanhaalim0 </a> <br/> <span><a href="https://przegladarka-ekw.ms.gov.pl/eukw_prz/KsiegiWieczyste/wyszukiwanieKW">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ia2ahm/need_help_getting_cookies/">[comments]</a></span>

