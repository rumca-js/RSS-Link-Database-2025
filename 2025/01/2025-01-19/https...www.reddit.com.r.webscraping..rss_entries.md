# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Need help with webscraping an input field containing text
 - [https://www.reddit.com/r/webscraping/comments/1i5as18/need_help_with_webscraping_an_input_field](https://www.reddit.com/r/webscraping/comments/1i5as18/need_help_with_webscraping_an_input_field)
 - RSS feed: $source
 - date published: 2025-01-19T22:17:47+00:00

<!-- SC_OFF --><div class="md"><p>Okay, so I’m trying to pull 150,000 pages worth of publicly available data that just so happens to keep the good stuff inside of uneditable input fields.</p> <p>When you hover your mouse over the data, the cursor changes to a stop sign, but it allows you to manually copy/paste the text. Essentially I want to turn a manual process into an easy, automatic webscraping process.</p> <p>I tried a certain desktop software but it is interpreting the data field as an “input field”.</p> <p>I considered a screen capturing tool that OCRs what it visually sees on screen, which might be the way I need to go.</p> <p>Any recommendations for webscraping tools without screencapturing?</p> <p>If not, any recommendations for tools with screencapturing?</p> <p>If this post breaks the subreddit rules, can someone suggest an alternative subreddit where I can get my question asked in context without further redaction?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a hre

## insta api - 401 error using proxies
 - [https://www.reddit.com/r/webscraping/comments/1i5ap07/insta_api_401_error_using_proxies](https://www.reddit.com/r/webscraping/comments/1i5ap07/insta_api_401_error_using_proxies)
 - RSS feed: $source
 - date published: 2025-01-19T22:14:08+00:00

<!-- SC_OFF --><div class="md"><p>Using residential proxies and the insta API to get the UserID from a username, I get around 50% of requests that end up as unauthorized.</p> <p>Error fetching user ID: Request failed with status 401: Unauthorized</p> <p>I&#39;m running only a few requests concurrently, each on a different proxy, so I don&#39;t think I&#39;m directly hitting any limits.</p> <p>Any way to improve this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/themasterofbation"> /u/themasterofbation </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i5ap07/insta_api_401_error_using_proxies/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i5ap07/insta_api_401_error_using_proxies/">[comments]</a></span>

## Seeking Help to Build an API for the Supreme Federal Court of Brazil
 - [https://www.reddit.com/r/webscraping/comments/1i57lrq/seeking_help_to_build_an_api_for_the_supreme](https://www.reddit.com/r/webscraping/comments/1i57lrq/seeking_help_to_build_an_api_for_the_supreme)
 - RSS feed: $source
 - date published: 2025-01-19T20:06:11+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone,</p> <p>I’m working on creating APIs for the Brazilian Supreme Federal Court (STF) and its transparency portal, Corte Aberta. The goal is to facilitate access to public data, making it easier to analyze judicial decisions, procedural information, and transparency reports.</p> <p>Here are the links to the platforms: 1. Main STF Portal <a href="https://portal.stf.jus.br/">https://portal.stf.jus.br/</a> 2. Corte Aberta Transparency Portal <a href="https://transparencia.stf.jus.br/extensions/corte_aberta/corte_aberta.html">https://transparencia.stf.jus.br/extensions/corte_aberta/corte_aberta.html</a></p> <p>Both platforms contain valuable data, but accessing and organizing it efficiently for research or analysis purposes can be a challenge.</p> <p>If you’re experienced in web scraping, API development, or have worked with similar legal databases, I would greatly appreciate your advice or collaboration. Any guidance on the best approach to ext

## Scraping +10k domains for emails
 - [https://www.reddit.com/r/webscraping/comments/1i52kbu/scraping_10k_domains_for_emails](https://www.reddit.com/r/webscraping/comments/1i52kbu/scraping_10k_domains_for_emails)
 - RSS feed: $source
 - date published: 2025-01-19T16:38:49+00:00

<!-- SC_OFF --><div class="md"><p>Hello everyone,<br/> I’m relatively new to web scraping and still getting familiar with it, as my background is in game development. Recently, I had the opportunity to start a business, and I need to gather a large number of emails to connect with potential clients.</p> <p>I&#39;ve used a scraper that efficiently collects details of localized businesses from Google Maps, and it’s working great—I’ve managed to gather thousands of phone numbers and websites this way. However, I now need to extract emails from these websites.</p> <p>To do this I coded a crawler in Python, using Scrapy, as it’s highly recommended. While the crawler is, of course, faster than manual browsing, it’s much less accurate and it misses many emails that I can easily find myself when browsing the websites manually.</p> <p>For context, I’m not using any proxies but instead rely on a VPN for my setup. Is this overkill, or should I use a proxy instead? Also, is it better to respect 

## Gather user sentiment in real time
 - [https://www.reddit.com/r/webscraping/comments/1i516vh/gather_user_sentiment_in_real_time](https://www.reddit.com/r/webscraping/comments/1i516vh/gather_user_sentiment_in_real_time)
 - RSS feed: $source
 - date published: 2025-01-19T15:39:32+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone, currently working on a project where I&#39;d like to gather user sentiments in real time to do some processing afterwards, the real time here is an important aspect (or at least hourly or daily but something like that). I thought about using the free plan of the reddit API but the amount of data I can gather is not great, I&#39;ve also written a script with playwright to scrape musk&#39;s website (banned word) and it seems to work for now ? At least I managed to scrape the tweets in my feed but I don&#39;t know if I can keep doing that 24/7 because of bot detection protocols etc...</p> <p>Do you have any idea of a website that can regroup user sentiment in approximately real time ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Dyww"> /u/Dyww </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i516vh/gather_user_sentiment_in_real_time/">[link]</a></span> &#32; <span><a href="ht

## Trouble accessing google search in google colab
 - [https://www.reddit.com/r/webscraping/comments/1i50pql/trouble_accessing_google_search_in_google_colab](https://www.reddit.com/r/webscraping/comments/1i50pql/trouble_accessing_google_search_in_google_colab)
 - RSS feed: $source
 - date published: 2025-01-19T15:18:09+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1i50pql/trouble_accessing_google_search_in_google_colab/"> <img src="https://preview.redd.it/tz2nkrwiwyde1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e22bbfb1bacccc11c09a68dcc89049ecd445795" alt="Trouble accessing google search in google colab" title="Trouble accessing google search in google colab" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>What does <a href="https://www.google.com/httpservice/retry/enablejs?sei=">https://www.google.com/httpservice/retry/enablejs?sei=</a>... mean? I found a thread on hackernews: <a href="https://news.ycombinator.com/item?id=42719865">https://news.ycombinator.com/item?id=42719865</a></p> <p>It seems like a recent phenomena as the rest of my program worked well enough three days ago. Now I am unable to load google with Selenium or requests or googlesearch-python.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ambitiousindian

## Ideas for scraping specific business owners names?
 - [https://www.reddit.com/r/webscraping/comments/1i4yk1d/ideas_for_scraping_specific_business_owners_names](https://www.reddit.com/r/webscraping/comments/1i4yk1d/ideas_for_scraping_specific_business_owners_names)
 - RSS feed: $source
 - date published: 2025-01-19T13:35:18+00:00

<!-- SC_OFF --><div class="md"><p>Hi, I am trying to gather data about Hungarian business owners in the US for a university project. One idea I had was searching for Hungarian last names in business databases and on the web, I still have not found such data, I appreciate any advice you can give or a new idea to gather such data.</p> <p>Thank you once again</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Rayanski1"> /u/Rayanski1 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i4yk1d/ideas_for_scraping_specific_business_owners_names/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i4yk1d/ideas_for_scraping_specific_business_owners_names/">[comments]</a></span>

## Getting product price from a page using AI?
 - [https://www.reddit.com/r/webscraping/comments/1i4y5sv/getting_product_price_from_a_page_using_ai](https://www.reddit.com/r/webscraping/comments/1i4y5sv/getting_product_price_from_a_page_using_ai)
 - RSS feed: $source
 - date published: 2025-01-19T13:14:07+00:00

<!-- SC_OFF --><div class="md"><p>Hi, I&#39;m curious, what&#39;s the current state of getting product price from a scraped page using AI?</p> <p>I tried some LLMs (Llama, Mistral) - only Llama works to some extent, and it takes ~$0.7 for a million tokens, which for my case leads to $0.03 for a page (40k tokens) which is a lot. </p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/bomboleyo"> /u/bomboleyo </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i4y5sv/getting_product_price_from_a_page_using_ai/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i4y5sv/getting_product_price_from_a_page_using_ai/">[comments]</a></span>

## Help Needed: Puppeteer Script Failing on AWS Lambda
 - [https://www.reddit.com/r/webscraping/comments/1i4sbch/help_needed_puppeteer_script_failing_on_aws_lambda](https://www.reddit.com/r/webscraping/comments/1i4sbch/help_needed_puppeteer_script_failing_on_aws_lambda)
 - RSS feed: $source
 - date published: 2025-01-19T06:28:32+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone,</p> <p>I’m currently working on a script that scrapes data from Naukri.com using Puppeteer. The script runs perfectly fine locally, but when I deploy it to AWS Lambda and execute it, I encounter the following error:</p> <p>{ &quot;error&quot;: &quot;Failed to fetch page title.&quot;, &quot;details&quot;: &quot;net::ERR_HTTP2_PROTOCOL_ERROR at <a href="https://www.naukri.com/jobs-in-india?functionAreaIdGid=3&amp;glbl_qcrc=1018&amp;glbl_qcrc=1019&amp;glbl_qcrc=1020&amp;cityTypeGid=9508">https://www.naukri.com/jobs-in-india?functionAreaIdGid=3&amp;glbl_qcrc=1018&amp;glbl_qcrc=1019&amp;glbl_qcrc=1020&amp;cityTypeGid=9508</a>&quot; }</p> <p>What I’ve Tried: 1. Local Testing: • The script works flawlessly on my local machine (MacBook M1 Air) using Node.js 16.x. 2. Lambda Configuration: • Using @sparticuz/chromium with Puppeteer for compatibility with AWS Lambda. • Puppeteer launch settings include:</p> <p>args: [ &#39;--disable-dev-shm-usage&#

## How to chromedriver
 - [https://www.reddit.com/r/webscraping/comments/1i4m4ig/how_to_chromedriver](https://www.reddit.com/r/webscraping/comments/1i4m4ig/how_to_chromedriver)
 - RSS feed: $source
 - date published: 2025-01-19T00:51:52+00:00

<!-- SC_OFF --><div class="md"><p>While using ChromeDriver for scraping, I often encounter a Chrome CAPTCHA. Now, I can&#39;t scrape at all because the CAPTCHA appears every time. Do you think downloading an extension and applying it somehow could solve the issue? Or are there other solutions I could try? Thanks a lot!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ermag04"> /u/ermag04 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1i4m4ig/how_to_chromedriver/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1i4m4ig/how_to_chromedriver/">[comments]</a></span>

