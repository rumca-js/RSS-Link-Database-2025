[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T22:12:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I noticed that Sony has a <a href=\"https://pro.sony/s3/\">semi-open S3 bucket</a> where if you have the direct link for a document, you can access it. The main link gives you the keys for the first 1000 links, but I can&#39;t figure out how to get it to list the next 1000 or all of them since it doesn&#39;t provide a continuation-token. </p> <p>A lot of it is indexed by google, but I&#39;m looking for a full list of what&#39;s available, particularly for service manuals for repairing old professional CRT displays. If there&#39;s an alternate way to go about getting this list, I&#39;d definitely be interested.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Derf_Jagged\"> /u/Derf_Jagged </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i62ozn/1000_entries_on_a_s3_bucket_without_continuation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i62ozn/1000_ent",
        "id": 1945537,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i62ozn/1000_entries_on_a_s3_bucket_without_continuation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": ">1000 entries on a S3 bucket without continuation token?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T22:07:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all,</p> <p>I started researching into scraping yesterday. I am new and am ready many posts. I realize some questions are asked but in the last 12 months there have been new plugins and software that speeds things up.</p> <p>For someone who does not know how to code, what would be the shortest route to scrape from a marketplace (From Chile)?. I just want the name, description, photo and once you click on the product list, get the phone contact. </p> <p>I will continue to research but in the meantime, any hints would be appreciated.</p> <p>cheers</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JYanezez\"> /u/JYanezez </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i62l98/easiest_route_to_scraping_a_marketplace_name/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i62l98/easiest_route_to_scraping_a_marketplace_name/\">[comments]</a></span>",
        "id": 1945538,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i62l98/easiest_route_to_scraping_a_marketplace_name",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Easiest Route to Scraping a Marketplace: Name, Description and Photo",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T20:24:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I have a list of names and addresses. I want to scrape mostly social media to get the name of the high school these people attended. </p> <p>What&#39;s the best and easiest to use tool out there, AI or otherwise? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/insideabookmobile\"> /u/insideabookmobile </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i6023c/best_tool_for_getting_high_school_name_from/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i6023c/best_tool_for_getting_high_school_name_from/\">[comments]</a></span>",
        "id": 1945134,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i6023c/best_tool_for_getting_high_school_name_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best tool for getting high school name from social media sites",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T19:16:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>im trying to find services that bypass this. my main issues come from scraping websites like <a href=\"http://academy.com\">academy.com</a> and <a href=\"http://belk.com\">belk.com</a></p> <p>thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cope4321\"> /u/cope4321 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5yc9t/anyone_know_a_service_to_bypass_perimeterx_press/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5yc9t/anyone_know_a_service_to_bypass_perimeterx_press/\">[comments]</a></span>",
        "id": 1944545,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5yc9t/anyone_know_a_service_to_bypass_perimeterx_press",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "anyone know a service to bypass PerimeterX \"Press and Hold\" ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T17:53:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, Im currently webscraping popular marketplaces and want to get buybox information, I dont know how I can consistently get buybox information currently what my flow is, I enter a keyword which can be the MPN, then just grab the first page, but I dont know anywhere in the html where Im getting buybox information any help or direction will be helpful!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Healthy_Leg3350\"> /u/Healthy_Leg3350 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5w8ms/need_ideas_for_how_to_scrape_buybox_information/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5w8ms/need_ideas_for_how_to_scrape_buybox_information/\">[comments]</a></span>",
        "id": 1944546,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5w8ms/need_ideas_for_how_to_scrape_buybox_information",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need Ideas for how to Scrape buybox information",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T17:41:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there. I am a product manager currently working on a project that requires news and competitor websites to be scraped. I got 2 devs on my team to build the crawlers, bit turns out that\u2019s not enough to cover all the sources quickly. </p> <p>What I want to achieve is have crawlers setup for 250 websites within a week.</p> <p>To save time, I built a python utility that extracts html using a proxy service, sends the html to gpt4o mini asking it to construct the xpaths based on the html. </p> <p>Now this approach works fine for a static pages, but the LLM is simply hallucinating and giving random Xpaths for client side rendered pages. This hallucination is happening even when I keep the JS render flag as True in the proxy service.</p> <p>I suspect the website\u2019s html does not contain the proper data for LLM to process. Is there something I can do to get the proper Html for JS rendered websites.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"",
        "id": 1944547,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5vxlw/tried_using_llm_gor_generating_xpaths",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tried using LLM gor generating Xpaths",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T13:44:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am in the process of buying a new PC that will be used for large scale scraping. It will have to be Windows to avoid having to spoof OS. I am split between picking 5950X and 7950X. 5950X seems to be the cheapest CPU per core at higher end but doesn&#39;t support DDR5. How much is RAM speed even important for running dozen or more parallel browser instances? I originally planned to go with at least 64GB capacity but am undecided on speed. Should I opt for the cheaper and older 5950X and subsequently also cheaper MB and RAM? Any other hardware recommendation and discussion for scaling is highly appreciated. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ZenaMeTepe\"> /u/ZenaMeTepe </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5qfk2/consumer_hardware_recommendation_for_large/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5qfk2/consumer_hardwar",
        "id": 1942118,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5qfk2/consumer_hardware_recommendation_for_large",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Consumer hardware recommendation for large headful scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T11:05:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am currently learning Python. I want to scrap a MCQ questions website, semi-automatically, it has a login, and put the contents in a specific format (Question, options, answer, explanation). It is medium sized, around 100 question papers with 75-80 questions each.</p> <p>What framework/browser automation should i learn. Also what other things should i know about. I can afford time to learn things (3 months) and im learning python anyways, but I have no clue about this</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PSSGAMER\"> /u/PSSGAMER </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5ntcr/scraping_a_mcq_site/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5ntcr/scraping_a_mcq_site/\">[comments]</a></span>",
        "id": 1941351,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5ntcr/scraping_a_mcq_site",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping a MCQ site",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T08:32:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a browser extension (preferably for Chrome, FireFox, or Edge) that can help me extract links from a webpage, but I only want the links that contain a specific keyword in their text.</p> <p>I don&#39;t know whether I could do this with only Python libraries like <code>requests</code> and <code>Beautiful Soup</code>, so I was hoping for a quicker, no-code solution using a browser extension.</p> <p>I&#39;ve looked at some popular scraping extensions like Web Scraper and Data Miner, but I&#39;m not sure if they have built-in features for filtering links by keywords within the link text itself. It seems like I might have to export all links and then filter them in a spreadsheet, which is a bit more manual than I&#39;d like.</p> <p>Does anyone know of a browser extension that can do this kind of keyword-based link extraction directly? Any recommendations or tips would be greatly appreciated!</p> <p>Thanks in advance!</p> </div><!-- SC_O",
        "id": 1939744,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5lqt3/browser_extension_to_extract_links_containing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Browser Extension to Extract Links Containing Specific Keywords",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T08:18:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone! In my current project, I\u2019m scraping a website protected by Akamai. The strange thing is that I\u2019m getting two different results from two different computers. On one, the code works perfectly and retrieves the necessary data. On the other, it regularly encounters errors, which I suspect are due to bot detection. What could be the reason for this? The two computers are not very different, and the program is exactly the same. Does anyone have any ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Standard-Engine5840\"> /u/Standard-Engine5840 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5lkc0/one_code_two_pc_two_different_outcome_possible/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5lkc0/one_code_two_pc_two_different_outcome_possible/\">[comments]</a></span>",
        "id": 1939745,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5lkc0/one_code_two_pc_two_different_outcome_possible",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "One code, two pc, two different outcome. Possible bot detection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T03:41:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My question is - does this affect our Data Scraping efforts? Thinking....no.....b/c if we use Headless Browsers those will have JavaScript? </p> <p>Thanks!</p> <p><a href=\"https://www.searchenginejournal.com/confirmed-google-is-requiring-javascript-to-block-seo-tools/537705/\">https://www.searchenginejournal.com/confirmed-google-is-requiring-javascript-to-block-seo-tools/537705/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/concisehacker\"> /u/concisehacker </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5h9jo/google_forcing_users_to_use_javascript_impact/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5h9jo/google_forcing_users_to_use_javascript_impact/\">[comments]</a></span>",
        "id": 1939192,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5h9jo/google_forcing_users_to_use_javascript_impact",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google forcing users to use JavaScript? Impact scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-01-20T01:48:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello dear scrapers, I am trying to scrape all the words available on this website into a spreadsheet for a language learning project in my school.</p> <p>I have tried many data scraping programs but none succeeded so I ask for your generous help.</p> <p>I am trying to extract each table from each language level respectively. Please provide me with guidance.</p> <p><a href=\"https://kielikoulu.yle.fi/vocab\">https://kielikoulu.yle.fi/vocab</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kuray_akumu\"> /u/kuray_akumu </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5f6cs/help_needed_urgently/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1i5f6cs/help_needed_urgently/\">[comments]</a></span>",
        "id": 1938904,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1i5f6cs/help_needed_urgently",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help needed urgently!",
        "vote": 0
    }
]