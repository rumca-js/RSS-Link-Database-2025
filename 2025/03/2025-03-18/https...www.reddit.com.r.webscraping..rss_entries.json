[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T20:29:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Please note this only works for Microsoft Excel</strong></p> <p>I&#39;ve created a powerful yet simple to use module where it takes the team&#39;s past score, offence and defense and matches up with another team. It&#39;s then put on a simulator that runs 1000 times, which gives out a percentage of win probability. All you do is update the data, select your away team and home team. That&#39;s it.</p> <p>Please note that this is just a tool, injuries, time of day or any other factors does account for the score results.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ryu_2211\"> /u/Ryu_2211 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jef1zl/2025_nba_module/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jef1zl/2025_nba_module/\">[comments]</a></span>",
        "id": 2353189,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jef1zl/2025_nba_module",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "2025 NBA MODULE",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T19:37:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is it legal to scrape publicly accessible pages in a website but authenticated just to bypass bot detections and rate limits? </p> <p>Say for a website, it requires a cloudflare cookie to request a page but not the same case when you using authenticated cookie.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SUPERMETROMAN\"> /u/SUPERMETROMAN </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jedrr1/scrape_publicly_accessible_pages_while_logged_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jedrr1/scrape_publicly_accessible_pages_while_logged_in/\">[comments]</a></span>",
        "id": 2352703,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jedrr1/scrape_publicly_accessible_pages_while_logged_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scrape publicly accessible pages while logged in",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T19:34:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i have a problem with a website im scraping where i need to sign up first and then do my actions, but i need to create more accounts to use threads, is any tool to do it? i tried some public email API services but it says invalid recipient email, what\u2019s the best alternatives? i tried with mail.tm API but it doesn\u2019t works.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Embarrassed_Door3175\"> /u/Embarrassed_Door3175 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jedono/email_otp/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jedono/email_otp/\">[comments]</a></span>",
        "id": 2354416,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jedono/email_otp",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "E-Mail OTP",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T13:01:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscrapin",
        "id": 2349140,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1je4eb6/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T12:39:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There are some data points that I would like to continually scrape from Amazon. Things I cannot get from the api or from other providers that have Amazon data. I\u2019ve done a ton of research on the possibility and from what I understand is this isn\u2019t going to be an easy process.</p> <p>So I\u2019m reaching out to the community to see if anyone is currently scraping Amazon or has recent experience and can share some tips or ideas as I get started trying to do this. </p> <p>Broadly I have about 50k products I\u2019m currently monitoring on Amazon through the API and through data service providers. I\u2019m really wanting few additional items and if I can put something together that\u2019s successful perhaps I can scrape the data I\u2019m currently paying for to offset the cost of the scraping operation. I\u2019d also prefer to not have to be in a position where I\u2019m reliant on the data provider to stay in operation.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www",
        "id": 2349141,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1je3yz9/scraping_amazom",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Amazom",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T11:57:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need help with a web scraping task that involves extracting dynamically loaded discount prices from a food delivery page. The challenge is that the discounted prices only appear after adding items to the cart, requiring handling of AJAX-loaded content and proper waiting mechanisms.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OkFilm3368\"> /u/OkFilm3368 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1je37tm/need_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1je37tm/need_help/\">[comments]</a></span>",
        "id": 2354417,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1je37tm/need_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T11:53:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Note: not a developer and have just built a heap of webscrapers for my own use... but lately there have been some webpages that i scrape for job advertisements , that i just dont understand why selenium cant see the container. </p> <p>One example is <a href=\"http://www.hanwha-defence.com.au/careers\">www.hanwha-defence.com.au/careers</a> , </p> <p>my python script has:</p> <pre><code> job_rows = soup.find_all(&#39;div&#39;, class_=&#39;row default&#39;) print(f&quot;Found {len(job_rows)} job rows&quot;) </code></pre> <p>and the element :<br/> &lt;div class=&quot;row default&quot;&gt;</p> <p>&lt;div class=&quot;col-md-12&quot;&gt;</p> <p>&lt;div&gt;</p> <p>&lt;h2 class=&quot;jobName\\_h2&quot;&gt;Office Coordinator&lt;/h2&gt;</p> <p>&lt;h6 class=&quot;jobCategory&quot;&gt;Administration &amp;amp; Customer Service &lt;/h6&gt;</p> <p>&lt;div class=&quot;jobDescription_p&quot; </p> <p>but i&#39;m lost to why it cant see it , please help a noob with suggest",
        "id": 2348643,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1je35go/looking_to_understand_why_i_cant_see_the_container",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to understand why i cant see the container",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T09:42:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to create an X account that posts interesting polls.</p> <p>E.g.,&quot;If you can only use 1 AI model for the next 3 years, what do you choose?&quot;</p> <p>I want a few thousand (URLs) of X posts to understand what poll questions work/inspiration.<br/> However, the only way I can figure out is to fetch a ton of posts and then filter the ones that contain polls (roughly 0.1%.). </p> <p>Is there not a better approach? </p> <p>If anyone has a more efficient approach that will also identify relatively interesting poll questions, so I&#39;m not reading through a random sample, please send me an estimate on price. </p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Optimeyez007\"> /u/Optimeyez007 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1je15zw/how_to_get_a_list_of_urls_for_x_posts_that/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comme",
        "id": 2347784,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1je15zw/how_to_get_a_list_of_urls_for_x_posts_that",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to get a list of urls for X posts that contain polls?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T07:06:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><h2>rnet</h2> <p>This TLS/HTTP2 fingerprint request library uses BoringSSL to imitate <code>Chrome</code>/<code>Safari</code>/<code>OkHttp</code>/<code>Firefox</code> just like <code>curl-cffi</code>. Before this, I contributed a BoringSSL Firefox imitation patch to <code>curl-cffi</code>. You can also use curl-cffi directly.</p> <h2>What Project Does?</h2> <ul> <li>Supports both synchronous and asynchronous clients</li> <li>Requests library bindings written in Rust, safer and faster.</li> <li>Free-threaded safety</li> <li>Request-level proxy settings and proxy rotation</li> <li>HTTP1/HTTP2 WebSocket</li> <li>Headers order</li> <li>Async DNS resolver\uff0cAbility to specify asynchronous DNS IP query strategy</li> <li>Synchronous and asynchronous streaming upload and download</li> <li>Allows you to simulate the TLS/HTTP2 fingerprints of different browsers, as well as the header templates of different browser systems. Of course, you can customize its headers.<",
        "id": 2354418,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jdz7vo/i_published_a_blazingfast_python_http_client_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I published a blazing-fast Python HTTP Client with TLS fingerprint",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T06:10:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Facing the following errors while using Playwright for automated website navigation, JS injection, element and content extraction. Would appreciate any help in how to fix these things, especially because of the high probability of their occurrence when I am automating my webpage navigation process.</p> <p>playwright._impl._errors.Error: ElementHandle.evaluate: Execution context was destroyed, most likely because of a navigation - from code :::::: (element, await element.evaluate(&quot;el =&gt; el.innerHTML.length&quot;)) for element in elements</p> <p>playwright._impl._errors.Error: Page.query_selector_all: Execution context was destroyed, most likely because of a navigation - from code ::::::: elements = await page.query_selector_all(f&quot;//*[contains(normalize-space(.), \\&quot;{metric_value_escaped}\\&quot;)]&quot;)</p> <p>playwright._impl._errors.Error: Page.content: Unable to retrieve content because the page is navigating and changing the conte",
        "id": 2346989,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jdyhyc/help_facing_context_destroyed_errors_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help: facing context destroyed errors with Playwright upon navigation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-18T03:16:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working with a massive dataset (potentially around 10,000-20,000 transcripts, texts, and images combined ) and I need to determine whether the data is related to a specific topic(like certain keywords) after scraping it.</p> <p>What are some cost-effective methods or tools I can use for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Green_Ordinary_4765\"> /u/Green_Ordinary_4765 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jdvrgy/costeffective_ways_to_analyze_large_scraped_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jdvrgy/costeffective_ways_to_analyze_large_scraped_data/\">[comments]</a></span>",
        "id": 2346349,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jdvrgy/costeffective_ways_to_analyze_large_scraped_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cost-Effective Ways to Analyze Large Scraped Data for Topic Relevance",
        "vote": 0
    }
]