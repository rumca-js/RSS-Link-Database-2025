[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T20:27:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone I&#39;m scraping the flipkart page but getting an error again and again. When i print text, i gets &quot;site is overloaded&quot; in output and when i print response, i gets &quot;response 529&quot;. I have used fake user agent for random user agent and time for sleep function.</p> <p>Here is the code i have used for scraping: import requests import time from bs4 import BeautifulSoup import pandas as pd import numpy as np from fake_useragent import UserAgent ua = UserAgent() random_ua = ua.random headers = {&#39;user-agent&#39; : random_ua } url = &quot;<a href=\"https://flipkart.com/\">https://flipkart.com/</a>&quot; respons = requests.get(url, headers) time.sleep(10) print(respons) Can anyone have faced this problem, plz help me... </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Beach_1187\"> /u/No_Beach_1187 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji8v2j/fixing",
        "id": 2390789,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ji8v2j/fixing_flipkarts_site_is_overloaded_error",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fixing Flipkart's 'Site is Overloaded' Error\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T20:25:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I thought to make a chrome extension that would scrape job postings on button click. </p> <p>Is there a risk of users getting banned from that? let&#39;s say the user does a scrape 1 time/minute, and the amount of data is not that much just job posting data</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok-Administration6\"> /u/Ok-Administration6 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji8sru/will_linkeidn_block_the_user_for_automated/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji8sru/will_linkeidn_block_the_user_for_automated/\">[comments]</a></span>",
        "id": 2390790,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ji8sru/will_linkeidn_block_the_user_for_automated",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Will linkeidn block the user for automated scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T19:44:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A client\u2019s system added bot detection. I use puppeteer to download a CSV at their request once weekly but now it can\u2019t be done. The login page has that white and blue banner that says \u201csite protected by captcha\u201d. </p> <p>Can i get some tips on the simplest and cost efficient way to do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cs_cast_away_boi\"> /u/cs_cast_away_boi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji7uru/need_to_get_past_recaptcha_v3_invisible_a_login/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji7uru/need_to_get_past_recaptcha_v3_invisible_a_login/\">[comments]</a></span>",
        "id": 2390057,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ji7uru/need_to_get_past_recaptcha_v3_invisible_a_login",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "need to get past Recaptcha V3 (invisible) a login page once a week",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T15:36:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, just a random thought... (sorry, I do have weird thoughts sometimes... lol) What if LLMs also include data from popular forums (those only accessible via tor). When they claim they have used most data from the internet, did they include those only accessible via tor?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Firm_Effort_7583\"> /u/Firm_Effort_7583 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji2040/what_if_llm_include_darknet_data_forums_to_train/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji2040/what_if_llm_include_darknet_data_forums_to_train/\">[comments]</a></span>",
        "id": 2390791,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ji2040/what_if_llm_include_darknet_data_forums_to_train",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What if LLM include darknet data (forums) to train?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T14:14:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, I regularly work with German company data from <a href=\"https://www.unternehmensregister.de/ureg/\">https://www.unternehmensregister.de/ureg/</a></p> <p>I download financial reports there. You can try it yourself with Volkswagen for example. Problem is: you get a session Id, every report is behind a captcha and after you got the captcha right you get the possibility to download the PDF with the financial report.</p> <p>This is for each year for each company and it takes a LOT of time.</p> <p>Is it possible to automatize this via webscraping? Where are the hurdles? I have basic knowledge of R but I am open to any other language.</p> <p>Can you help me or give me a hint?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Aromatic-Champion-71\"> /u/Aromatic-Champion-71 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ji078r/webscraping_noob_question_automatization/\">[link]</a></span> &#",
        "id": 2388464,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ji078r/webscraping_noob_question_automatization",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webscraping noob question - automatization",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T04:37:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;m currently working on a project where I scrape the price data over time, then visualize the price history with Python. I ran into the problem where the HTML keeps changing as the websites (sites like Best Buy and Amazon) and it is difficult to scrape. I understand I could just use an API, but I wold like to learn with web scraping tools like Selenium and Beautiful Soup.</p> <p>Is this just something that I can&#39;t do due to companies wanting to keep their price data to be competitive?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EpIcAF\"> /u/EpIcAF </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jhrf2u/ecommerce_websites_to_practice_web_scraping_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jhrf2u/ecommerce_websites_to_practice_web_scraping_on/\">[comments]</a></span>",
        "id": 2386273,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jhrf2u/ecommerce_websites_to_practice_web_scraping_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "E-Commerce websites to practice web scraping on?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T03:22:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried Chrome Driver, and basic CAPTCHA solving and all but I get blocked all the time trying to scrape Yelp. Some reddit browsing and it seems they updated moderation against scrapers.</p> <p>I know that there are APIs and such for this but I want to scrape it without any third-party tools. Has anyone ever succeeded in scraping Yelp recently?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gamedev-exe\"> /u/gamedev-exe </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jhq5ff/scraping_yelp_in_2025/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jhq5ff/scraping_yelp_in_2025/\">[comments]</a></span>",
        "id": 2385994,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jhq5ff/scraping_yelp_in_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Yelp in 2025",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-23T00:21:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Has anyone used this API before?</p> <p>If so, what was your experience and are there any endpoints I can use to download posts/reels from my IG feed? I want to extract what would display on that feed, specifically for me.</p> <p>I&#39;d initially stumbled on Instagrapi but saw they recommend Hiker API instead. I emailed their support email address 10 days ago but have gotten radio silence</p> <p>--</p> <p>link to Instagrapi repo: <a href=\"https://github.com/subzeroid/instagrapi?tab=readme-ov-file\">https://github.com/subzeroid/instagrapi?tab=readme-ov-file</a></p> <p>link to Hiker API: <a href=\"https://hikerapi.com/\">https://hikerapi.com/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/onscreencomb9\"> /u/onscreencomb9 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jhmsth/hiker_api/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jhmsth/hiker_ap",
        "id": 2385710,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jhmsth/hiker_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hiker API",
        "vote": 0
    }
]