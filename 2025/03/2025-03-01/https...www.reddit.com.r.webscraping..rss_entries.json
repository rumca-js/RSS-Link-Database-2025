[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T21:21:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys! Im currently scraping amazon for 10k+ products a day without getting blocked. I\u2019m using user agents and just read out the fronted. </p> <p>I\u2019m fairly new to this so I wonder why so many people use proxies and even pay for it when it is very possible to scrape many websites without them? Are they used for websites with harder anti bot measures? Am I going to jail for scraping this way, lol? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/schnold\"> /u/schnold </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j1a447/why_do_proxies_even_exist/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j1a447/why_do_proxies_even_exist/\">[comments]</a></span>",
        "id": 2226312,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j1a447/why_do_proxies_even_exist",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why do proxies even exist?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T20:58:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hii all,<br/> So at work I have a task of scraping Zillow among others, which is a cloudflare protected website. after researching I found out that curl_impersonate and curl_cffi can be used for scraping cloudflare protected websites. I tried everything which I was able to understand but I am not able to implement in my python project. Please can someone give me some guide or steps?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/V_I_K_I_N_G_B_A_T\"> /u/V_I_K_I_N_G_B_A_T </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j19lf7/how_to_use_curl_impersonate_and_curl_cffi_please/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j19lf7/how_to_use_curl_impersonate_and_curl_cffi_please/\">[comments]</a></span>",
        "id": 2226593,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j19lf7/how_to_use_curl_impersonate_and_curl_cffi_please",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to use curl_impersonate and curl_cffi ? Please help!!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T20:41:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m hoping this is the sub and you are the people who can help me. I want to create an Excel file for future use, contacts to save. Is there a tool or extension you recommend that I can use to capture the contact info from websites I use on a daily basis. I have a lot of great contacts that I on Zoom info or on internal sites and I&#39;d love to create an Excel file of those contacts. I keep thinking there is something that can capture the data from my current view if I&#39;m clicking through contacts in a database I&#39;m using. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Illustrious-Half-562\"> /u/Illustrious-Half-562 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j1986m/queston_about_extracting_names_and_contact_info/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j1986m/queston_about_extracting_names_and_contact_info/\">[comments]</a></",
        "id": 2226053,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j1986m/queston_about_extracting_names_and_contact_info",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Queston about Extracting Names and Contact info",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T15:26:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Everyone,</p> <p>Please I am trying to scrape Reddit posts, likes and comments from a Search result on a subreddit into a CSV or directly to excel.</p> <p>Please help \ud83e\udd7a</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/icemelts101\"> /u/icemelts101 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j11wtz/reddit_scraping_without_python/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j11wtz/reddit_scraping_without_python/\">[comments]</a></span>",
        "id": 2224765,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j11wtz/reddit_scraping_without_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Reddit Scraping without Python",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T15:09:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working on a script that automates actions on a specific website that displays a recapcha challenge in one of the steps.<br/> My script works well, its is prety goodrandomly and lazzy the automated action to looks lyke human action, use audio recognition to solve easly the challenge but after a few attempts its detect automated queries from my connection so i implement a condition to reload the scripts using proxy in <a href=\"https://pptr.dev/\">puppeteer</a> and its work great for a few days but now its getting detecting too even if i wait some days to run the script.<br/> The steps is, i use my real IP and the script run until get detected and after this the proxy is set but its is detected too.<br/> What other methods are used:</p> <ul> <li>Use VPN instead of proxy (got detected);</li> <li>Use VPN or proxy + change to a random valid different viewport (got detected);</li> <li>Use VPN or proxy + change to a random valid different viewport + ",
        "id": 2225782,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j11j1i/how_google_detects_automated_queries_in_recaptcha",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How Google Detects Automated Queries in Recaptcha Challenge",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T14:49:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I&#39;m having trouble running multiple Selenium instances on my server. I keep getting this error:</p> <blockquote> </blockquote> <p>I have a server with 7 CPU threads and 8GB RAM. Even when I limit Selenium to 5 instances, I still get this error about 50% of the time. For example, if I send 10 requests, about 5 of them fail with this exception.</p> <p>My server doesn&#39;t seem overloaded, but I&#39;m not sure anymore. I&#39;ve tried different things like immediate retries and restarting Selenium, but it doesn&#39;t help. If a Selenium instance fails to start, it always throws this error.</p> <p>This error usually happens at the beginning, when the browser tries to open the page for scraping. Sometimes, but rarely, it happens in the middle of a session. Nothing is killing the processes in the background as far as I know.</p> <p>Does anyone else run multiple Selenium instances on one machine? Have you had similar issues? How do y",
        "id": 2224379,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j112za/selenium_invalid_session_id_error_when_running",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Selenium: \"invalid session id\" error when running multiple instances",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T12:30:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I made a basic scraper using node js and puppeter , and a simple frontend. The website that I am scraping is <a href=\"http://Uzum.uz\">Uzum.uz</a> , its a local online shop. The scrapers are working fine but the problem I am currently facing is the large amount of products I have to scrape , and it takes hours to complete. The products have to be updated weekly , each product , because I need the fresh info about the price , pcs sold , and etc. Any suggestions on how to make the proccess faster ? Currently the scrapper is creating 5 instances parallelly , when i increase the amount of instances , the website doesnt load properly.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Reasonable-Wolf-1394\"> /u/Reasonable-Wolf-1394 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j0yfhi/need_an_advice_on_scraping_a_large_amount_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/we",
        "id": 2223751,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j0yfhi/need_an_advice_on_scraping_a_large_amount_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need an advice on scraping a large amount of products",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T11:13:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I published my 3rd pypi lib and it&#39;s open source. It&#39;s called <strong>stealthkit</strong> - requests on steroids. Good for those who want to send http requests to websites that might not allow it through programming - like amazon, yahoo finance, stock exchanges, etc.</p> <p><strong>What My Project Does</strong></p> <ul> <li><strong>User-Agent Rotation</strong>: Automatically rotates user agents from Chrome, Edge, and Safari across different OS platforms (Windows, MacOS, Linux).</li> <li><strong>Random Referer Selection</strong>: Simulates real browsing behavior by sending requests with randomized referers from search engines.</li> <li><strong>Cookie Handling</strong>: Fetches and stores cookies from specified URLs to maintain session persistence.</li> <li><strong>Proxy Support</strong>: Allows requests to be routed through a provided proxy.</li> <li><strong>Retry Logic</strong>: Retries failed requests up to three times b",
        "id": 2223471,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I published my 3rd python lib for stealth web scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-01T03:00:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello and howdy, digital miners of r/webscraping!</p> <p>The moment you&#39;ve all been waiting for has arrived - it&#39;s our once-a-month, no-holds-barred, show-and-tell thread!</p> <ul> <li>Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you&#39;ve just unleashed on the world?</li> <li>Maybe you&#39;ve got a ground-breaking product in need of some intrepid testers?</li> <li>Got a secret discount code burning a hole in your pocket that you&#39;re just itching to share with our talented tribe of data extractors?</li> <li>Looking to make sure your post doesn&#39;t fall foul of the community rules and get ousted by the spam filter?</li> </ul> <p>Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!</p> <p>Just a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let&#39;s get",
        "id": 2221480,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j0pou9/monthly_selfpromotion_march_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Monthly Self-Promotion - March 2025",
        "vote": 0
    }
]