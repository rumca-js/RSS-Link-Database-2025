[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T20:04:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi maybe a noob question here - I\u2019m trying to scrape the Woolworths specials url - <a href=\"https://www.woolworths.com.au/shop/browse/specials\">https://www.woolworths.com.au/shop/browse/specials</a></p> <p>Specifically, the product listing. However, I seem to be only able to get the section before the products and the sections after the products. Between those is a bunch of JavaScript code.</p> <p>Could someone explain what\u2019s happening here and if it\u2019s possible to get the product data? It seems it\u2019s being dynamically rendered from a different source and being hidden by the JS code? </p> <p>I\u2019ve used BS4 and Selenium to get the above results. </p> <p>Thanks </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CrabRemote7530\"> /u/CrabRemote7530 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jc3to8/having_trouble_understanding_what_is_preventing/\">[link]</a></span> &#32; <span><a href=\"https:/",
        "id": 2330865,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jc3to8/having_trouble_understanding_what_is_preventing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Having trouble understanding what is preventing scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T19:55:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently got into scraping and ive made a scraper that scrapes an unlimited amount of tweets, using beautifulsoup and python and nitter and also has additional features like saving a ss of the tweet. Whats the best way to monetize this</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Abugees\"> /u/Abugees </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jc3lvf/how_to_monetize_my_nitter_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jc3lvf/how_to_monetize_my_nitter_scraper/\">[comments]</a></span>",
        "id": 2330866,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jc3lvf/how_to_monetize_my_nitter_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to monetize my nitter scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T15:00:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I wanted a complete framework for testing and stealth, but raw Selenium didn&#39;t come with these features out-of-the-box, so I built a framework around it.</p> <p>GitHub: <a href=\"https://github.com/seleniumbase/SeleniumBase\">https://github.com/seleniumbase/SeleniumBase</a></p> <p>It wasn&#39;t originally designed for stealth, so I added two different stealth modes:</p> <ul> <li><a href=\"https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md\">UC Mode</a> - (which works by modifying Chromedriver) - First released in 2022.</li> <li><a href=\"https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md\">CDP Mode</a> - (which works by using the CDP API) - First released in 2024.</li> </ul> <p>The testing components have been around for much longer than that, as the framework integrates with <code>pytest</code> as a plugin. (Most examples in the <a href=\"https://github.com/seleniumbase/SeleniumBase/tree/mast",
        "id": 2331138,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbwz6v/the_library_i_built_because_i_enjoy_selenium",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The library I built because I enjoy Selenium, testing, and stealth",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T13:37:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to share with you my own experience in parsing. Our company parse about 5 000 websites in more then 1000 cities every day with a team of 10 engineers. This is a rather complex process that requires careful parser setup, use of proxies, and constant quality control.</p> <p>In this article we will tell you in detail how we have built this process, what difficulties we face and what technical solutions we use.</p> <p><strong>Where we store data and how we transmit it to clients</strong></p> <p>The volume of data is huge, so we tried different databases to store and analyse it. Finally we use ClickHouse, this is the solution that write more than 400 Mln records a day. It is not so easy to control this number of data but we have our own quality control system using Grafana monitoring system.</p> <p>We have different options how to transmit data - from simple (xls/csv), to more complex like integration with client APIs and databases. Most of the dat",
        "id": 2331394,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbva9r/how_do_we_parse_5_000_sites_a_day_our_system_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do we parse 5 000 sites a day - our system and experience",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T09:46:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, maybe somebody here can help me. I have a script, that visits a page, moves the mouse with ghost cursor and after some ( random) time , my browser plugin redirects. After redirection, i need to check the url for a string. Sometimes, when the mouse is moving, and the page gets redirected by the plugin, i lose controll over the browser, the code just does nothing. The page is on the target url, but the string will never be found. No exception nothing, i guess i lose controll over the browser instance.</p> <p>Is there any way to fix this setup? i tried to check if browser is navigating and abot movement, but it doesnt fix the problem. I&#39;m realy lost, as i tried the same with humancursor on python and got stuck the same way. There is no alternative to using the extension, so i have to get it working somehow reliably. I would realy appreciate some help here.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pan",
        "id": 2327968,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbrmit/problem_with_ghost_cursor_and_puppeteer_please",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Problem with ghost cursor and Puppeteer. Please Help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T06:56:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am working on a javascript enabled crawler which automatically interacts with menus and cookie banners.</p> <p>I am using crawler-test.com and <a href=\"https://badssl.com/\">https://badssl.com/</a> as reference sites, but I wonder what everyone here is using to test their crawler?</p> <p>Are there any such sites for gdpr purposes? accessibility? seo?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Standard-Parsley153\"> /u/Standard-Parsley153 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbpdqn/crawler_test_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbpdqn/crawler_test_website/\">[comments]</a></span>",
        "id": 2327511,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbpdqn/crawler_test_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Crawler Test website",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T04:30:21+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jbn7n6/trying_to_scrape_titles/\"> <img src=\"https://b.thumbs.redditmedia.com/k5W6Fx5QB596PdtrIHIQ9_bz9-w-7_JO4-myi1Hu1as.jpg\" alt=\"Trying to scrape &quot;titles&quot;\" title=\"Trying to scrape &quot;titles&quot;\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/ym9fh36r6soe1.png?width=1891&amp;format=png&amp;auto=webp&amp;s=e3663718fec15d7b12918005475900b98bfbf198\">https://preview.redd.it/ym9fh36r6soe1.png?width=1891&amp;format=png&amp;auto=webp&amp;s=e3663718fec15d7b12918005475900b98bfbf198</a></p> <p>Screenshot: trying to scrape the &quot;works&quot; title you see highlighted using the =importxml ; but i&#39;m not sure what to put in the &quot;xpath_query&quot; portion... I tried //a href, tried typing in the class,works but that didn&#39;t work, unsure what it would specifically be asking for. thank you for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href",
        "id": 2326971,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbn7n6/trying_to_scrape_titles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/k5W6Fx5QB596PdtrIHIQ9_bz9-w-7_JO4-myi1Hu1as.jpg",
        "title": "Trying to scrape \"titles\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-15T01:27:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working with puppeteer using nodejs, and because I\u2019m using my iP address sometimes it gets blocked, I\u2019m trying to see if theres any cheap alternative to use proxies and I\u2019m not sure if aws has proxies</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Alert-Ad-5918\"> /u/Alert-Ad-5918 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbjy5l/does_aws_have_a_proxy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbjy5l/does_aws_have_a_proxy/\">[comments]</a></span>",
        "id": 2326462,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbjy5l/does_aws_have_a_proxy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does aws have a proxy",
        "vote": 0
    }
]