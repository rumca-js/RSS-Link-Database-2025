[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T22:12:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jbfvdo/my_data_storage_mediums_post_18_37th_week/\"> <img src=\"https://external-preview.redd.it/8V1CmfGZwthB1HC-wRwEBivESE5aq6riR7_Jyp-2hhs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4a5561d3a1b398fac22f244c17fb9d8f9fc9fce\" alt=\"My data storage mediums, post 18 (37th week)\" title=\"My data storage mediums, post 18 (37th week)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Today I was given an IBM 3590 tape cartridge by someone completely else to the person that gave me the 3592 tape cartridge but it still came from the same PGS geographical company as the 3592 cartridge which now I am very curious to see what the data is on there assuming I can decode the .TAR format into files, the person also had a few 3590 tape drives at their job which were unfortunately signed off for recycling and they are to be sent off to another country to be scrapped out which means I can\u2019t have a single one of them :( or go",
        "id": 2325776,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbfvdo/my_data_storage_mediums_post_18_37th_week",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/8V1CmfGZwthB1HC-wRwEBivESE5aq6riR7_Jyp-2hhs.jpg?width=640&crop=smart&auto=webp&s=e4a5561d3a1b398fac22f244c17fb9d8f9fc9fce",
        "title": "My data storage mediums, post 18 (37th week)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T21:43:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently got a pCloud subscription to back up my neurotically tagged and organised music collection.</p> <p>pCloud <a href=\"https://www.pcloud.com/help/drive-help-center/whats-the-difference-between-pcloud-drive-and-pcloud-sync\">says a couple of things</a> about backing up folders from your local drive to their cloud:</p> <blockquote> <p>(pCloud) Sync is a feature in pCloud Drive. It allows you to connect locally-stored folders from your PC with pCloud Drive. <strong>This connection goes both ways,</strong> so if you edit or delete the files you\u2019re syncing from your computer, this means that you&#39;ll also be editing them or deleting them from pCloud Drive.</p> </blockquote> <p>That description and especially the bold part leaves me less than confident that pCloud will never edit files in my original local folder. Which is a guarantee I dearly want to have.</p> <p>As a workaround, I&#39;ve simply copied my music folder (C:\\Users\\&lt;username&gt;\\M",
        "id": 2325346,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbf7d7/good_tools_to_sync_folders_oneway_ie_update_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Good tools to sync folders one-way (i.e. update the contents of folder B to match folder A, but 100% never change anything in folder A)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T21:40:22+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jbf4wp/seagate_shuck_sata_to_usb_adapter_interface/\"> <img src=\"https://preview.redd.it/c2bxhw0y5qoe1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42c59a99ab7a36cafa4aba2d527c7e1d3d84a6a1\" alt=\"Seagate Shuck - SATA to USB Adapter Interface\" title=\"Seagate Shuck - SATA to USB Adapter Interface\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I shucked my Seagate Backup Plus Slim 2TB External HDD hoping that the internal SATA to USB adapter could be used for another SATA drive I have. Picture shows the opened casing, I removed the shielding tape and used the adapter but it has a motherboard which seems to restrict it to work only with the Seagate drive.</p> <p>Unfortunately, when I plugged it into my PNY 2.5\u201d drive, nothing popped up.</p> <p>Hoping that someone knows how to make it work universally? I was trying not to buy a SATA to USB adapter because it would take a few days for delivery a",
        "id": 2325347,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbf4wp/seagate_shuck_sata_to_usb_adapter_interface",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/c2bxhw0y5qoe1.jpeg?width=640&crop=smart&auto=webp&s=42c59a99ab7a36cafa4aba2d527c7e1d3d84a6a1",
        "title": "Seagate Shuck - SATA to USB Adapter Interface",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T21:01:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone!</p> <p>Apologies if this isn\u2019t the right place to ask, but I need a little advice on the easiest way to go about backing up my old computer (which has developed some disk issues in recent months with both the boot drive and an internal HDD). To not bore everyone with the details, there have been error messages/indications that a disk failure is imminent and I would like to back up everything from both drives to avoid data loss since I have some important stuff on there.</p> <p>I was thinking I could maybe back up both drives onto a single 4TB HDD. However, I am unsure how feasible that would be as one of the drives has a Windows installation and the other is additional storage. What do you all think the best solution would be? I have important project files on both drives so I\u2019m at a bit of a loss for how to best go about this. </p> <p>Thanks for reading! :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.c",
        "id": 2325348,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbe8tn/best_method_for_backing_up_my_entire_pc_onto_an",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best method for backing up my entire PC onto an external HDD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T20:35:43+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jbdnl0/longtermstorage_for_a_simpleton/\"> <img src=\"https://preview.redd.it/cfj19r4tspoe1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=de162c6f3dbba444d2c3a0f624424fe94066e92f\" alt=\"Long-term-storage for a simpleton\" title=\"Long-term-storage for a simpleton\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JaschaE\"> /u/JaschaE </a> <br/> <span><a href=\"https://i.redd.it/cfj19r4tspoe1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jbdnl0/longtermstorage_for_a_simpleton/\">[comments]</a></span> </td></tr></table>",
        "id": 2324937,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbdnl0/longtermstorage_for_a_simpleton",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/cfj19r4tspoe1.jpeg?width=640&crop=smart&auto=webp&s=de162c6f3dbba444d2c3a0f624424fe94066e92f",
        "title": "Long-term-storage for a simpleton",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T20:35:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks! First time contributor here looking for some insight into a backup need I have.</p> <p>My current backup situation is a single USB SSD that stores my active projects, which I backup to a Hard Drive. It&#39;s not exactly a <em>full</em> <em>backup</em> at the moment, as non-active jobs are only saved onto the backup drive. I&#39;m hoping to get a second drive to RAID 1 with the main backup once I have a bit more money.</p> <p>Onto my issue- I&#39;m looking for a backup software on MacOS that will only add and replace existing files on the backup, not delete ones that don&#39;t match. That way I can keep moving files from the working SSD onto the backup drive, while still being able to clear off space on the working SSD.</p> <p>I think that makes sense? Let me know if I need to clarify better!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PM_ME_TINY_PIANOS\"> /u/PM_ME_TINY_PIANOS </a> <br/> <span><a hr",
        "id": 2324938,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbdncr/nonduplicating_backup_question",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Non-duplicating backup question",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T20:21:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a box or case for internal hard drives (1TB, 2TB, 4TB, 6TB) when I&#39;m not using them. Which models would you recommend ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yukinoooo\"> /u/Yukinoooo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jbdbyn/looking_for_a_case_to_protect_internal_hard_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jbdbyn/looking_for_a_case_to_protect_internal_hard_drives/\">[comments]</a></span>",
        "id": 2324939,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jbdbyn/looking_for_a_case_to_protect_internal_hard_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a case to protect internal hard drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T17:10:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to pull some videos and haven&#39;t found any add-on or app that can do it from <a href=\"http://Podia.com\">Podia.com</a> (an online course platform). </p> <p>Thanks in advance for any thoughts.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/magicmikela\"> /u/magicmikela </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb903w/any_ideastricksways_to_rip_podia_videos_i_cant/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb903w/any_ideastricksways_to_rip_podia_videos_i_cant/\">[comments]</a></span>",
        "id": 2323475,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb903w/any_ideastricksways_to_rip_podia_videos_i_cant",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any ideas/tricks/ways to rip Podia videos?! I can't crack it.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T15:43:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Forgive me for my ignorance on this, as I&#39;m still pretty inexperienced with this, but is there a group or a project that makes data available from various sources, such as Kiwix for downloading Wikipedia? I figure the last 2 months have been a real wake up call and I have since downloaded the .wix for Wiki, but wonder if there is something similar that crawls .gov sites or .uni/.edu sites for archiving purposes and packaged for easy distribution/downloading? </p> <p>Keep in mind, I have no idea how much effort goes into projects like that, and I can definitely appreciate it now that we have seen what happens when we take something for granted.</p> <p>Just a thought that crossed my mind this morning and I wanted to post it before I forgot.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/canigetahint\"> /u/canigetahint </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb6x25/systems_for_a",
        "id": 2322925,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb6x25/systems_for_aggregating_other_sources_outside_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Systems for aggregating other sources outside of Wikipedia?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T15:20:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all! For anyone feeling super disheartened and scared (same, and) I want to say this: a great act of resistance to the current American f@scist government is to DOWNLOAD DATA from the government websites while it is still up. And not just your personal records - public records. </p> <p>For context, I am outside of the USA and work on understanding international wildfire fatalities. The USA has had an incredible archive of data that is now being gradually erased. For example, I can&#39;t download wildfire fatality records from the NIOSH website earlier than 2016 (or earlier than Tr<sup>mps</sup> first term...). I have been able to find most of these on the CDC archives instead, but for how long?</p> <p>Anyway, we can hope that these records won&#39;t actually be removed or that they may be stored offline somewhere safe until a non-psychopath occupies the white house, but much safer to <em>be</em> the backup. Download the documents, store them mayb",
        "id": 2322299,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb6dtk/download_everything",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DOWNLOAD EVERYTHING",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T14:45:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There was someone trying to dedupe 1 million videos which got me interested in the project again. I made a bunch of improvements to the video part as a result, though there is still a lot left to do. The video search is much faster, has a tunable speed/accuracy parameter (<code>-i.vradix</code>) and now also supports much longer videos which was limited to 65k frames previously.</p> <p>To help index all those videos (not giving up on decoding every single frame yet ;-), hardware decoding is improved and exposes most of the capabilities in ffmpeg (nvdec,vulkan,quicksync,vaapi,d3d11va...) so it should be possible to find something that works for most gpus and not just Nvidia. I&#39;ve only been able to test on nvidia and quicksync however so ymmv.</p> <p>New binary release and info <a href=\"https://github.com/scrubbbbs/cbird/releases\">here</a></p> <p>If you want the best performance I recommend using a Linux system and compiling from source. The codege",
        "id": 2322300,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb5kuo/cbird_v08_is_ready_for_spring_cleaning",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "cbird v0.8 is ready for Spring Cleaning!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T14:10:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for a new solution to backup my raw photos that are currently about 5 TB and have a few questions:</p> <ol> <li>Should I use 2 separate external HDDs and sync them from time to time or is 1 enclosure with 2 mirrored HDDs better? I am leaning towards 2 separate ones as it appears to be more redundant.</li> <li>If I get 2 separate HDDs should I buy 2 different brands or is it safe enough to buy 2 of the same model?</li> <li>Anyone here who could share their experience with the G-Drive Project 12 TB? </li> <li>Any other suggestions?</li> </ol> <p>Thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Rick-Valassi\"> /u/Rick-Valassi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb4scj/12_tb_backup_solution/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb4scj/12_tb_backup_solution/\">[comments]</a></span>",
        "id": 2321676,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb4scj/12_tb_backup_solution",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "12 TB backup solution",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T14:05:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello fellow Data Hoarders!</p> <p>I&#39;ve been eagerly awaiting <a href=\"https://github.com/go-gitea/gitea/pull/20311\">Gitea&#39;s PR 20311</a> for over a year, but since it keeps getting pushed out for every release I figured I&#39;d create something in the meantime.</p> <p>This tool sets up and manages pull mirrors from GitHub repositories to Gitea repositories, including the entire codebase, issues, PRs, releases, and wikis.</p> <p>It includes a nice web UI with scheduling functions, metadata mirroring, safety features to not overwrite or delete existing repos, and much more.</p> <p>Take a look, and let me know what you think!</p> <p><a href=\"https://github.com/jonasrosland/gitmirror\">https://github.com/jonasrosland/gitmirror</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jonasrosland\"> /u/jonasrosland </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb4owy/a_web_ui_to_help_mirr",
        "id": 2321675,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb4owy/a_web_ui_to_help_mirror_github_repos_to_gitea",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A web UI to help mirror GitHub repos to Gitea - including releases, issues, PR, and wikis",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T14:05:24+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb4ooy/read_this_and_thought_of_this_group/\"> <img src=\"https://preview.redd.it/gnu0cajafnoe1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=291921a22a3980b8b5db5e339fb7dc69349520b8\" alt=\"Read this and thought of this group\" title=\"Read this and thought of this group\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Goofcheese0623\"> /u/Goofcheese0623 </a> <br/> <span><a href=\"https://i.redd.it/gnu0cajafnoe1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb4ooy/read_this_and_thought_of_this_group/\">[comments]</a></span> </td></tr></table>",
        "id": 2321674,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb4ooy/read_this_and_thought_of_this_group",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/gnu0cajafnoe1.jpeg?width=640&crop=smart&auto=webp&s=291921a22a3980b8b5db5e339fb7dc69349520b8",
        "title": "Read this and thought of this group",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T13:46:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi All</p> <p>First off, </p> <p>&#x200B;</p> <blockquote> <p>Thank you for all the support while I&#39;ve been building out <a href=\"https://pricepergig.com/\">https://pricepergig.com</a> (it will be the best place to find digital storage on the internet, and is right now for Amazon imo, but I would say that right :) )</p> </blockquote> <p>If you were to sign up for price alerts (e.g. the cheapest HDD, or the cheapest NVMe price per TB for example) or in the future alerts for your saved searches HOW would you like to be alerted?</p> <p>If you could also let me know your country that would help me understand, perhaps it&#39;s different in different locations.</p> <p>Backstory, you don&#39;t need to read this!</p> <p>Many people asked for &#39;alerts&#39;, and I assumed email would be ok/good/great, perhaps I was wrong, not so many people have signed up, it could well be just the form looks scary, perhaps I need to point it out more, I can work on that",
        "id": 2321677,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb4agf/how_do_you_want_to_get_alerts_for_the_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do *you* want to get alerts for the best storage prices from pricepergig.com ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T11:00:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>There are a wide number of sites which offer paid access to film references, including:</p> <ul> <li>Shotdeck</li> <li>Film Grab</li> <li>Eyecandy</li> <li>Filmboard</li> <li>Shot Cafe</li> <li>Frame Set</li> <li>Screenmusings</li> </ul> <p>They are paid archives, rather than being true data hoarding / open access.</p> <p>Is there a centralised resource for this form of data hoarding, does anyone know? A group project?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cartrouble111112\"> /u/cartrouble111112 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb187c/film_commercial_music_video_screen_grabs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb187c/film_commercial_music_video_screen_grabs/\">[comments]</a></span>",
        "id": 2320645,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb187c/film_commercial_music_video_screen_grabs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Film / Commercial / Music Video screen grabs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T10:53:31+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb14bx/the_data_hoarders_resisting_trumps_purge_new/\"> <img src=\"https://external-preview.redd.it/xmIfuP2pSQH6e6vlWW989iGnGK_9evmrCIR4KXkTJCs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4411fc3c5ebdce26a0e7f5d7ff91c0419e93389\" alt=\"\u201cThe Data Hoarders Resisting Trump\u2019s Purge\u201d (New Yorker)\" title=\"\u201cThe Data Hoarders Resisting Trump\u2019s Purge\u201d (New Yorker)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/storytracer\"> /u/storytracer </a> <br/> <span><a href=\"https://www.newyorker.com/news/the-lede/the-data-hoarders-resisting-trumps-purge\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb14bx/the_data_hoarders_resisting_trumps_purge_new/\">[comments]</a></span> </td></tr></table>",
        "id": 2320643,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb14bx/the_data_hoarders_resisting_trumps_purge_new",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/xmIfuP2pSQH6e6vlWW989iGnGK_9evmrCIR4KXkTJCs.jpg?width=640&crop=smart&auto=webp&s=d4411fc3c5ebdce26a0e7f5d7ff91c0419e93389",
        "title": "\u201cThe Data Hoarders Resisting Trump\u2019s Purge\u201d (New Yorker)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T10:37:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/zp9vlha0vmoe1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=25233afd4d8804e65b7d6dff7bab03f33fe6ef53\">https://preview.redd.it/zp9vlha0vmoe1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=25233afd4d8804e65b7d6dff7bab03f33fe6ef53</a></p> <p>I want to start a personal project where I scan, OCR and index markdown for old books. This is a book with ALL of Romania&#39;s roads back in 1974. It has tables and maps and all sorts of other interesting historical data points.</p> <p>I already have some idea of data engineering. I&#39;m a software engineer and I&#39;ve made a project that helps with RAG, search and indexing of markdown files (even very big ones). My problem is the OCR part. Any tips?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/alexlazar98\"> /u/alexlazar98 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb0w4o/help_me_with_ocr_and_indexing_o",
        "id": 2320644,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jb0w4o/help_me_with_ocr_and_indexing_of_old_books_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help me with OCR and indexing of old books with tables, data, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T08:54:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there a way to tell Ripme to download only images from a URL that contains both images and videos? And can I set a minimum resolution for dowloaded images? I am new to all this. There doesn&#39;t seem to be a setting, Can this be done vie a config file?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Famous_Assistant5390\"> /u/Famous_Assistant5390 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jazibe/filter_files_to_download_by_ripme/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jazibe/filter_files_to_download_by_ripme/\">[comments]</a></span>",
        "id": 2319766,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jazibe/filter_files_to_download_by_ripme",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Filter files to download by Ripme?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T07:36:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone have a good grasp or understanding from experience if hiding usb drives (or things in general) in plain sight is more effective than concealing from sight? </p> <p>I have important data id like to keep backed up, but mobile and offline. I don&#39;t care if the data got destroyed over time or corrupted but I want to keep it safe from prying eyes.(i have backups i just need this data offline and portable for my own convenience)</p> <p>I&#39;m also somewhat new to using bitlocker encryption and it&#39;s easy to use but I do find myself wondering how hackable it is if at all (for the common attacker on a common person like myself). is it even worth it to buy a dedicated disguised cheap usb(pen style, throw it in my massive pen collection in office? Or can I just write the data to 1 or 2 of my old usb drives? I guess my concern is if an attacker came though my home they&#39;d check for things that might be valuable like my safe, and obvious da",
        "id": 2319767,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jaygva/hiding_usb_drive_in_plain_sight_vs_concealing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hiding USB drive in plain sight vs concealing from sight?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T06:03:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an Orico 9958C3 with hard drives (WD Red and Iron Wolf drives) formated and showing in Windows Disk Manager (NTFS). However, they do not show in Orico&#39;s proprietary Raid Manager software. I have reformated drives, changed slots, restarted, etc. Any advice on how to setup Raid 5?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Zavad6404\"> /u/Zavad6404 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jax7ha/orico_9958c3_raid_setup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jax7ha/orico_9958c3_raid_setup/\">[comments]</a></span>",
        "id": 2319273,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jax7ha/orico_9958c3_raid_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Orico 9958C3 Raid Setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T05:29:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been thinking about trying various software raids, truenas, unraid, freenas, etc. and I&#39;m not sure which one to try first. Are there other major software options that I&#39;m not listing? Which do you recommend I try first and which would you ultimately implement to be the central backup to about 5-6 pcs/laptops and three Synology 8 bay NAS?</p> <p>I&#39;ve been building my own PCs since I was a kid and I pretty much have most of the pcs I&#39;ve ever built, some 8 cores and a spare 16 core pc. Only about a year ago did I finally dive into the world of NAS and RAID and ended up getting three eight bay Synology NAS boxes. They are doing alright for what I&#39;m using them for. I thought at first I&#39;d not be good at learning about these things but I dedicated about three months of reading and youtubing and feel I have a good understanding of the synology ecosystem and some general raid knowledge. </p> <p>Now I&#39;m ready to take the ne",
        "id": 2319024,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jawqle/which_software_raid_should_i_tinker_with_first",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which software raid should I tinker with first and ultimately implement? Tips? Tricks?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T04:53:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m creating my first Plex server and have not purchased any drive larger than 2 TB before. Right now, Western Digital is having a deal where two 12 TB drives are going for $200 each (i.e., ~$16.7/terabyte).</p> <p>Is $15-17 good enough to buy four and take advantage of the limited-time offer or is that &quot;Just buy a couple&quot; territory?</p> <p>How much do you usually spend new per terabyte? Used?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Metallica93\"> /u/Metallica93 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jaw701/how_much_do_you_typically_spend_per_terabyte_new/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jaw701/how_much_do_you_typically_spend_per_terabyte_new/\">[comments]</a></span>",
        "id": 2318810,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jaw701/how_much_do_you_typically_spend_per_terabyte_new",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How much do you typically spend per terabyte new?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T04:28:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Okay, captured minidv taped with WinDV and set it to split into clips instead of one big file so I can see the time and date each clip was taken, and now I want to join them in virtual dub without re encoding using direct stream copy and append clip. Problem is, I can only figure out how to do one at a time. There&#39;s like a hundred clips per tape, and I have tried highlighting all of them and dragging them into virtualdub while holding control but it puts them out of order. How can I combine all of them at once and keep them in the right order by file name. Or do I need some software besides VD. I do not want to just throw them into an editor and end up re encoding them. Thanks. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unusual_Poem_9864\"> /u/Unusual_Poem_9864 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1javspi/virtualdub_append_help/\">[link]</a></span> &#32; <span><a href=\"",
        "id": 2318811,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1javspi/virtualdub_append_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Virtualdub append help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T04:02:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I used an extension called myfavett on chrome but that only grabbed about a 1000 videos and refuses to download any further. Anyone know any workarounds? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Forsaken_Pea3464\"> /u/Forsaken_Pea3464 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1javco4/how_do_i_download_every_single_video_from_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1javco4/how_do_i_download_every_single_video_from_a/\">[comments]</a></span>",
        "id": 2318814,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1javco4/how_do_i_download_every_single_video_from_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do i download EVERY single video from a tiktok profile? User has more than 3500 videos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T03:52:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Right now my set up is an M4 desktop Mac + 2tb external hard drive (for now). I\u2019ve saved a handful of movies and shows on it and have been watching them through infuse on my Apple tv. Have been very satisfied with how it\u2019s all worked out so now I would like to begin the process of going full hoarder mode and really start loading up on shows and movies. </p> <p>My immediate first use case is that I want to add all my favorite shows - mainly 30 min sitcoms like Seinfeld, trailer park boys, it\u2019s always sunny, etc. to the drive. Using Seinfeld as an example, each episode is roughly between 800mb and 1gb as it stands now.</p> <p>I own Apple compressor and would like to run all these shows through it to save on space. Any recommendations for format/audio/visual settings? HEVC? h264? h265? MP4? Other? Really don\u2019t need <em>super</em> high quality here, certainly not 4k, but was thinking 1080. </p> <p>Also would be curious to hear streaming platform recommen",
        "id": 2318812,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jav62l/adding_favorite_tv_shows_to_external_hard_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Adding favorite TV shows to external hard drives - what would be the optimal setting(s) to run them through on compressor to maximize space and have decent quality?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T03:49:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve essentially archived a website and want to be able to view it in say Kiwix but that takes ZIM files, so I want to know how I can compress all the html files and folder structure into a zim file that I can view offline or maybe a WARC (i&#39;m not sure how this would work).</p> <p>The alternative is that I create an app that has a browser that can open html files by decompressing on the fly into ram for example but I feel like this is what a ZIM is. Can anyone help? Thanks.</p> <p>The reason I&#39;m not using a tool like ZimIT is because I have to edit the html code to eliminate cookie popups, so now it&#39;s nice and clean ready to be archived/zimmed up.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Specific-Judgment410\"> /u/Specific-Judgment410 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jav49f/i_have_a_website_that_i_backed_up_offline_and_its/\">[link]</a></span> &#32; <s",
        "id": 2318813,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jav49f/i_have_a_website_that_i_backed_up_offline_and_its",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I have a website that I backed up offline, and it's working well offline - how can I zip it all up and view it in a compressed state? WARC or ZIM? How would I go about doing something like this?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T01:37:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I learned that backup is essential, especially if you&#39;re a klutz and you drop things like hard drives that have your entire digital ebook and comic collection. I have bought 2 drives, one main, and then an extra for backup. I will be buying more backups as I get more money and I get a plan together to make sure that this type of thing never happens to me again. My biggest problem is the recovered files (epub and mobi) all were stripped of their titles and were given random numerical titles while my collection folders that originally stored my books are gone. This being a problem in that I want to get my collection back in order, so I find out what books I&#39;ve lost in the drive drop. This is over 2 terabytes of books. I don&#39;t want to spend the rest of my life reorganizing my collection. Any advice on a Windows software solution that can help me speed up finding the titles and renaming the files/reorganizing them by author? Any help or wisdo",
        "id": 2318159,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jasn5i/restored_files_from_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Restored files from Drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T01:26:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Back at the Barnyard was a low/medium budget Nickelodeon spinoff TV series following the 2006 film with 2 seasons airing from 2007 to 2011 in 4:3 (fullscreen) 480i standard definition picture format. Reportedly no episodes were aired in widescreen although widescreen clips do exist. It appears to be framed for fullscreen, as virtually all broadcast content in the US was either SD fullscreen or HD widescreen.</p> <p>From the little research I&#39;ve done there were 2 DVD box sets, one for each of the 2 seasons with 6 discs each. Each season had 26 episodes. The discs were burned DVD-Rs manufactured on-demand through Amazon CreateSpace around 2016. CreateSpace no longer exists and the discs are obviously out of print now. And we all know what happens to DVD-Rs after just 10 years.</p> <p>In the land of peg legs and eye patches, I see most (if not all) episodes captured and encoded to x264. I also see both seasons muxed individually as 2 really long vid",
        "id": 2318160,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jasfco/back_at_the_barnyard_tv_show",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Back at the Barnyard TV Show",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T00:39:47+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ddcrx\"> /u/ddcrx </a> <br/> <span><a href=\"https://v.redd.it/o6grew6zlioe1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jarhog/sweaky_clean_hard_drives/\">[comments]</a></span>",
        "id": 2318158,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jarhog/sweaky_clean_hard_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sweaky clean hard drives \ud83e\uddfc\ud83e\udee7",
        "vote": 0
    }
]