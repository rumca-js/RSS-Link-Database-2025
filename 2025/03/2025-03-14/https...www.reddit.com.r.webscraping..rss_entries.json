[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T21:59:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Open source project for scraping instgram data ?</p> <p>I am looking for a maintained open source project for scraping instgram data which uses proxies and easily deployable on server. Any suggestion?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vikas_kumar__\"> /u/vikas_kumar__ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbfkgl/open_source_project_for_scraping_instgram_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbfkgl/open_source_project_for_scraping_instgram_data/\">[comments]</a></span>",
        "id": 2325640,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbfkgl/open_source_project_for_scraping_instgram_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open source project for scraping instgram data ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T20:16:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As the title says, I&#39;ve spent the past few days creating a free proxy pricing comparison tool. You all know how hard it can be to compare prices from different providers, so I tried my best and this is the result: <a href=\"https://proxyprice.thewebscraping.club/\">https://proxyprice.thewebscraping.club/</a></p> <p>I hope you don&#39;t flag it as spam or self-promotion, I just wanted to share something useful.</p> <p>EDIT: it&#39;s still an alpha version, so any feedback is welcome. I&#39;m filling it with more companies in these days.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pigik83\"> /u/Pigik83 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/\">[comments]</a></span>",
        "id": 2325152,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've collected 350+ proxy pricing plans and this is the result",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T19:58:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a spreadsheet of direct links to a website that I want to download files from. Each link points to a separate page on the website with the download button to the file. I have all of these links in a spreadsheet. How could I use python to automate this scraping process? Any help is appreciated. <a href=\"http://hospitalpricingfiles.org/\">hospitalpricingfiles.org/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Teckyz\"> /u/Teckyz </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbcryh/pulling_files_off_of_a_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbcryh/pulling_files_off_of_a_website/\">[comments]</a></span>",
        "id": 2325641,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbcryh/pulling_files_off_of_a_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Pulling files off of a website",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T18:51:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>title</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ascaronhu\"> /u/Ascaronhu </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbb6jc/could_you_recommend_an_up_to_date_book_or_online/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbb6jc/could_you_recommend_an_up_to_date_book_or_online/\">[comments]</a></span>",
        "id": 2324263,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbb6jc/could_you_recommend_an_up_to_date_book_or_online",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Could you recommend an up to date book or online course?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T18:15:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is it possible to scape a specific X account\u2019s following list for specific keywords in their bio and once matched return an email, username, and the entire bio? </p> <p>Is there something out there that does this already? I\u2019ve been looking but I\u2019m not getting results. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CampaignRelative4361\"> /u/CampaignRelative4361 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbadyo/scraping_specific_x_accounts_following/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jbadyo/scraping_specific_x_accounts_following/\">[comments]</a></span>",
        "id": 2325642,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jbadyo/scraping_specific_x_accounts_following",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Specific X Account\u2019s Following",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T17:30:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m making a project for my 3 websites, and AI agent should go in them and search for the most matched product to user needs and return most matchs.</p> <p>The thing is; to save the scraped data from one prouduct as a match, I can use NLP but they need structured data, so I should sent each prouduct data to LLM to make the data structured and compare able, and that would cost toomuch.</p> <p>What else can I do? Is there any AI API for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ordacktaktak\"> /u/ordacktaktak </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jb9hfz/how_to_improve_this_algorithm_for_my_project/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jb9hfz/how_to_improve_this_algorithm_for_my_project/\">[comments]</a></span>",
        "id": 2325643,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jb9hfz/how_to_improve_this_algorithm_for_my_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to improve this algorithm for my project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T17:06:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We&#39;ve acquired 1k static HTML sites and I&#39;ve been tasked to scrape the sites and pull individual location/staff members found on these sites into our CMS. There are no patterns to the HTML, it&#39;s all just content that was at some point entered in a WYSIWYG editor. </p> <p>I scrape the website to a JSON file (array of objects, an object for each page) and my first attempts to have AI attempt to parse it and extract location/team data have been a pretty big failure. It has trouble determining unique location data (for example the location details may be in the footer and on a dedicated &#39;Our Location&#39; page so I end up with two slightly different locations that are actually the same), it doesn&#39;t know when the staff data starts/ends if the bio for a staff member is split into different rows/columns, etc.</p> <p>Am I approaching this task wrong or is it simply not doable?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"htt",
        "id": 2323780,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jb8w8v/scraping_and_extracting_locationspeople_from_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping and extracting locations/people from web sites (no patterns)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T16:14:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been on the housing market for over a year, and I&#39;ve been scraping my realtor&#39;s website to get new home information as it pops up. There&#39;s no protection there, so it&#39;s easy.</p> <p>However, part of my setup is that I then take those new addresses and put them into AT&amp;T&#39;s &quot;fiber lookup&quot; page to see if a property can get fiber installed. It&#39;s super critical for me to know this due to my job, etc.</p> <p>I&#39;ve been doing this for a while, and it was fine up until about a month ago. It seems that AT&amp;T has really juiced up their anti-bot protection recently, and I am looking for some help or advice.</p> <p>So far I&#39;ve been using:</p> <p>* Undetected Chromedriver (which is not maintained anymore) <a href=\"https://github.com/ultrafunkamsterdam/undetected-chromedriver\">https://github.com/ultrafunkamsterdam/undetected-chromedriver</a></p> <p>* nodriver (which is what the previous package got moved t",
        "id": 2323230,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jb7o0k/recaptchav3_and_atts_fiber_availability_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "recaptchav3 and AT&T's Fiber availability website issues. See post.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T15:04:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks! We&#39;ve been building <a href=\"https://github.com/getmaxun/maxun\">Maxun</a>, an open-source, no-code web scraping tool. After receiving tons of requests for a <strong>hosted version</strong>, we&#39;re finally launching <strong>Maxun Cloud</strong> soon!</p> <p>We\u2019re opening up <strong>beta access</strong> to a few testers before the official launch. If you\u2019re interested in trying it out, giving feedback, or even stress-testing it, <strong>DM me!</strong> \ud83d\ude80</p> <p>Thank you \ud83d\ude4c</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/carishmaa\"> /u/carishmaa </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jb60p6/help_us_test_maxun_cloud_opensource_nocode/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jb60p6/help_us_test_maxun_cloud_opensource_nocode/\">[comments]</a></span>",
        "id": 2322646,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jb60p6/help_us_test_maxun_cloud_opensource_nocode",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help Us Test Maxun Cloud (Open-Source No-Code Scraping Tool)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T12:05:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019m working on browser automation and looking for a tool that balances efficiency, stability, and stealth. The goal is to interact with browser extensions (password managers, wallets, etc.) while keeping automation as undetectable as possible.</p> <p>I\u2019m considering: \u2705 Selenium \u2013 widely used, but how detectable is it now? \u2705 Playwright \u2013 supposedly better at stealth automation? \u2705 Puppeteer, or any lesser-known alternatives?</p> <p>\ud83d\udca1 I\u2019ve also been testing anti-detect browsers like Dolphin Anty, which help handle fingerprinting and make automation more stable. Does anyone here have experience using them for web scraping?</p> <p>A few key questions: 1\ufe0f\u20e3 Which tool currently works best for avoiding bot detection in web automation? 2\ufe0f\u20e3 Do modern tools already handle randomization (click positions, mouse movements, delays), or should I implement that manually? 3\ufe0f\u20e3 What\u2019s the best approach to interact with browser extensions in an autom",
        "id": 2321391,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jb2bhc/whats_the_best_tool_for_web_automation_avoiding",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s the best tool for web automation & avoiding detection in 2025?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T11:02:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to scrape keyword-product ranking for about 100 keywords for 5 or 6 different zipcodes daily. But i am getting captcha check after some requests everytime. Could you please look into my code and help me with this problem. Any suggestions are welcome</p> <p>Code Link - <a href=\"https://paste.rs/WuSZu.py\">https://paste.rs/WuSZu.py</a></p> <p>Also any suggestion in code writing is also welcome. I am a newbie in this</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/polaristical\"> /u/polaristical </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jb19d7/help_with_scraping_amzn/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jb19d7/help_with_scraping_amzn/\">[comments]</a></span>",
        "id": 2321392,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jb19d7/help_with_scraping_amzn",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with scraping Amzn",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T09:14:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, I am looking for different approaches to bypass cloudflare protection. </p> <p>Right now I am using puppeteer without residential proxies and it seems it cannot handle it. I have rotating agents but seems they are not helping.</p> <p>Looking for different approaches, I am open to change the stack or technologies if required.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NotDeffect\"> /u/NotDeffect </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jazrlx/bypass_cloudflare_protection_march_2025/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jazrlx/bypass_cloudflare_protection_march_2025/\">[comments]</a></span>",
        "id": 2319491,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jazrlx/bypass_cloudflare_protection_march_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bypass Cloudflare protection March 2025",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T08:57:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Until you get blocked by Cloudflare, then it\u2019s all you can talk about. Suddenly, your browser becomes the villain in a cat-and-mouse game that would make Mission Impossible look like a romantic comedy. If only there were a subreddit for this... wait, there is! Welcome to the club, fellow blockbusters.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/moungupon\"> /u/moungupon </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jazjs6/the_first_rule_of_web_scraping_is_dont_talk_about/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jazjs6/the_first_rule_of_web_scraping_is_dont_talk_about/\">[comments]</a></span>",
        "id": 2321393,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jazjs6/the_first_rule_of_web_scraping_is_dont_talk_about",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The first rule of web scraping is... dont talk about web scraping.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T06:30:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>they are tracking you and going to use your data when you use free proxies. Happy scrapping everyone\ud83d\ude07\ud83e\udd17</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/maraline_11\"> /u/maraline_11 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jaxk95/dont_use_free_proxies/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jaxk95/dont_use_free_proxies/\">[comments]</a></span>",
        "id": 2321394,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jaxk95/dont_use_free_proxies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Don't use free proxies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T06:11:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I\u2019ve been running into an issue while trying to scrape data and I was hoping someone could help me out. I\u2019m trying to get data from a website using aiohttp asynchronous calls, but it seems like the website rejects them no matter what I do. However, my synchronous requests go through without any problem.</p> <p>At first, I thought it might be due to headers or cookies problems, but after adjusting those, I still can\u2019t get past the 403 error. Since I am scraping a lot of links, sync calls make my programming extremely slow, and therefore async calls are a must. Any help would be appreciated!</p> <p>Here is an example code of what I am doing:</p> <pre><code>import aiohttp import asyncio import requests link = &#39;https://www.prnewswire.com/news-releases/urovo-has-unveiled-four-groundbreaking-products-at-eurocis-2025-shaping-the-future-of-retail-and-warehouse-operations-302401730.html&#39; headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windo",
        "id": 2321395,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jaxb95/website_rejects_async_requests_but_not_sync",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Website rejects async requests but not sync requests",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-14T04:38:44+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1javygf/replay_xhr_works_but_resend_doesnt/\"> <img src=\"https://preview.redd.it/dpikyzlh3loe1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aeab845bba7a9290bba3df6dbdba53d77ab7920a\" alt=\"Replay XHR works, but Resend doesnt?\" title=\"Replay XHR works, but Resend doesnt?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mefirst42\"> /u/mefirst42 </a> <br/> <span><a href=\"https://i.redd.it/dpikyzlh3loe1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1javygf/replay_xhr_works_but_resend_doesnt/\">[comments]</a></span> </td></tr></table>",
        "id": 2318727,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1javygf/replay_xhr_works_but_resend_doesnt",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/dpikyzlh3loe1.jpeg?width=320&crop=smart&auto=webp&s=aeab845bba7a9290bba3df6dbdba53d77ab7920a",
        "title": "Replay XHR works, but Resend doesnt?",
        "vote": 0
    }
]