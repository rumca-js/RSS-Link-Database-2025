[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T23:17:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need some help.</p> <p>Every now and then I look into moving my backups off of a HDDs. Carrying a large box of HDDs, and then carefully migrating them to fresher drives as they age has been a chore.</p> <p>Tape makes perfect sense, as the optical media stalled at max 100GB capacity, and SSD is too expensive still.</p> <p>And, we finally have Thunderbolt external drives:</p> <p><a href=\"https://ltoworld.com/products/owc-archive-pro-lto-8-thunderbolt-tape-storage-archiving-solution-0tb-no-software-copy?srsltid=AfmBOopwwRkLc2f07XFv7F_eLJWxeXvi7DyHAo7NOsHHeXnwkKCHnxD8j34&amp;gQT=2\">https://ltoworld.com/products/owc-archive-pro-lto-8-thunderbolt-tape-storage-archiving-solution-0tb-no-software-copy?srsltid=AfmBOopwwRkLc2f07XFv7F_eLJWxeXvi7DyHAo7NOsHHeXnwkKCHnxD8j34&amp;gQT=2</a></p> <p>&quot;OWC Archive Pro LTO-8 Thunderbolt Tape Storage/Archiving Solution, 0TB, No Software&quot;</p> <p>However, I still cannot make the math work.</p> <p>For a $5,000 driv",
        "id": 2445850,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnph8i/the_latest_state_of_lto_tape_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The latest state of LTO tape drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T20:51:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>Just got my photo scanner to digitise the analogue photos from older family. </p> <p>What are the best possible settings for proper scan results? Is vuescan delivering better results than the stock software? Any settings advice here, too?</p> <p>Thanks a lot!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sunrisedown\"> /u/sunrisedown </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnmayj/epson_ff680w_best_results_settings_vuescan/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnmayj/epson_ff680w_best_results_settings_vuescan/\">[comments]</a></span>",
        "id": 2445289,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnmayj/epson_ff680w_best_results_settings_vuescan",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Epson FF-680W - best results settings? Vuescan?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T19:51:35+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnkws5/vfx_artist_reveals_the_true_scale_of_data/\"> <img src=\"https://external-preview.redd.it/6x_bTWB0RwsWKtyk7bJa37-Okez4Mh7Btuja78OcVhU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=87fced68c4dcd7e14c0f22b95350ff2f14274b58\" alt=\"VFX Artist Reveals the TRUE Scale of Data!\" title=\"VFX Artist Reveals the TRUE Scale of Data!\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SiliconCathedral\"> /u/SiliconCathedral </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=J-K2yeQylCk\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnkws5/vfx_artist_reveals_the_true_scale_of_data/\">[comments]</a></span> </td></tr></table>",
        "id": 2444995,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnkws5/vfx_artist_reveals_the_true_scale_of_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/6x_bTWB0RwsWKtyk7bJa37-Okez4Mh7Btuja78OcVhU.jpg?width=320&crop=smart&auto=webp&s=87fced68c4dcd7e14c0f22b95350ff2f14274b58",
        "title": "VFX Artist Reveals the TRUE Scale of Data!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T19:46:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Have a passport drive from back in college a few years ago\u2014I have a lot of good footage on there but it\u2019s not showing up on my Mac or pc when I plug it in. </p> <p>I can hear the drive turn on and feel it vibrating with power but it doesn\u2019t show up anywhere. Is there anything I could do? Doesn\u2019t even show up in disk utility </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bthrew\"> /u/bthrew </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnkskz/western_digital_passport_external/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnkskz/western_digital_passport_external/\">[comments]</a></span>",
        "id": 2444996,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnkskz/western_digital_passport_external",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Western Digital Passport external",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T19:42:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I acuired dedicated device for nas with 4 sata bays LFF. Now Im started looking for hard drives and Im a bit overwhelmed. Multiple generations, multiple speeds 5400 5900 or 7200 rpm. There also refurbished one by manufacture for a discounted price. I want to get 4 of them and setup in RAIDZ1, still debating on capacity but 12-22TB each, depending on the deal which I can get.</p> <p>So I have few questions:<br/> 1. Which hdd are most recommended right now?<br/> 2. Should I go with refurbished instead of new if I want to keep also important data?<br/> 3. Best places to get hard drives in europe? (Mainly Spain Poland Germany Czech)<br/> 4. Anything I need to avoid?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HCLB_\"> /u/HCLB_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnkpfd/current_best_bang_for_money_for_sata_35_hdd_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.c",
        "id": 2444997,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnkpfd/current_best_bang_for_money_for_sata_35_hdd_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Current best bang for money for SATA 3.5\" hdd for NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T19:08:29+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/g-e-walker\"> /u/g-e-walker </a> <br/> <span><a href=\"/r/youtubedl/comments/1jnjws1/version_150_of_my_selfhosted_ytdlp_web_app/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnjxbv/version_150_of_my_selfhosted_ytdlp_web_app/\">[comments]</a></span>",
        "id": 2444728,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnjxbv/version_150_of_my_selfhosted_ytdlp_web_app",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Version 1.5.0 of my self-hosted yt-dlp web app",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T18:50:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using Stable Bit and Storage Spaces together. Stable Bit says I&#39;m using 2.56tb of a pool, Storage Spaces says I&#39;m using 3.30tb of a pool. Any idea why the 700gb difference?</p> <p>READ PAST HERE ONLY IF YOUR COMMENT IS NOT AN ANSWER.</p> <p>ANSWERS TO YOUR QUESTIONS:</p> <p>1) Why are you doing this:</p> <p>--Autism, living a childhood dream to fill a Thor V1 case with 20 drives, all the drives were free and I don&#39;t have more money to buy new drives and new hardware to house a dedicated nas</p> <p>2) A nas is better, just get a nas</p> <p>--Yeah I would, but this stuff was free and the case has more than enough space for it, so why would i get a separate housing that costs $300. Besides, this was very easy to set up and I am linux illiterate.</p> <p>3) Storage Spaces is evil,</p> <p>--It&#39;s easy, and it&#39;s not bad once you know how to set it up. Works pretty good for me, and it takes care of everything for you. It knows what",
        "id": 2444729,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnjilu/stable_bit_and_storage_spaces",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Stable Bit and Storage Spaces",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T18:20:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My girlfriend and I are both content creators, and we live full-time in our van traveling the Pan-American Highway. We have about 25 TB of photos and videos spread across 10 external hard drives, finances are extremely tight for us, so we have essentially just been living life on the edge without any back ups for anything. Most of our drives are HDD, so the constant vibrations from driving on rough roads probably drastically increase their chances of failure. We are looking for any affordable backup solution so we aren&#39;t risking so much. Backfire initially seemed to be a perfect solution, but after doing more research, it seems like having this many external drives will likely lead to problems as they want the drives to be connected regularly or they will delete files. I know that the main recommendation for something like this would be getting a bunch of 8 TB HDD&#39;s and just backing up the drives, but since we travel full-time, we don&#39;t r",
        "id": 2444730,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnit0b/affordable_cloud_backup_for_external_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Affordable Cloud Backup for External Drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T18:03:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone. </p> <p>I have a lot of data( 4-5 TB small files like photos, videos, documents ) across 3 computers, 2 mobile phones, 6+ google drive acc, telegram. I also have a lot of credentials(10+ active email accounts for each of 3 email providers for various things(over 500+ accounts created across various websites), a lot of credentials on paper, text files, KeepassXC, 5+ books etc.</p> <p>This is haunting me as the things are everywhere and messy.</p> <p>How do I manage it all? Please help me :( </p> <p>(PS In college right now, so do not have money to buy additional storage for the timebeing. Thanks)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ex0hs\"> /u/ex0hs </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnif15/organizing_my_lifestoragecredentials/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnif15/organizing_my_lifestoragecred",
        "id": 2444731,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnif15/organizing_my_lifestoragecredentials",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Organizing my life(Storage&Credentials)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T16:46:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jngnn0/able_to_test_cdr_longevity_ripped_two_cdrs_from/\"> <img src=\"https://b.thumbs.redditmedia.com/lswSXyxJtVMf_q6jF_J65sdroxyIEN-n6nIirEUinOw.jpg\" alt=\"Able to test CD-R longevity. Ripped two CD-Rs from 1997-1998\" title=\"Able to test CD-R longevity. Ripped two CD-Rs from 1997-1998\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Many times I\u2019ve seen the debate on this subreddit questioning the longevity of CD-Rs, mostly with a mixed response. </p> <p>Was going through my dad\u2019s CD collection and found two CDs burned 1997 and 1998, over 25 years ago. These were stored in ideal conditions, in cases in very low humidity in a cool dark room.</p> <p>They read onto my iMac and windows machine as expected. Was able to play the songs straight from the CD using a media player. Ripped the CDs as FLACs using XLD, pretty fast and with no issue. </p> <p>I\u2019m fairly happy with this finding as I\u2019d love to keep my music on",
        "id": 2444732,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jngnn0/able_to_test_cdr_longevity_ripped_two_cdrs_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/lswSXyxJtVMf_q6jF_J65sdroxyIEN-n6nIirEUinOw.jpg",
        "title": "Able to test CD-R longevity. Ripped two CD-Rs from 1997-1998",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T16:24:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Do the new 6 and 8tb Blue drives have WDIDLE3?</p> <p>Don&#39;t have either drive, just checking before i buy.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BobInBowie\"> /u/BobInBowie </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jng61d/wdidle3_newer_wd_blue_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jng61d/wdidle3_newer_wd_blue_drives/\">[comments]</a></span>",
        "id": 2444037,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jng61d/wdidle3_newer_wd_blue_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WDIDLE3, Newer WD Blue Drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T15:35:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve got a DVD that only reads the files on a PC, I&#39;ve tried other players but they won&#39;t read. Regardless, whenever I try to rip the contents through various ripping programs, I always get an error. The DVD i own has programs pre-installed on the disc, as the video files are RM files, so I believe the intent with their inclusion was for help in reading the files, for contexts sake this DVD is from 2003. I&#39;d like to dump the entirety of the DVD, but I&#39;m stumped on where to go next.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PieceLongjumping7210\"> /u/PieceLongjumping7210 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnf1i9/question_on_ripping_pcdvd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnf1i9/question_on_ripping_pcdvd/\">[comments]</a></span>",
        "id": 2444733,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnf1i9/question_on_ripping_pcdvd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question on ripping PC-DVD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T15:00:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi!</p> <p>I try to archive any media of my hometown and now I begin to do so with Street Panos. Is there a way or software to get all Panos of an area at once? There is a software that offers this feature, but it costs a lot.</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hungry-Wealth-6132\"> /u/Hungry-Wealth-6132 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jne9rc/mass_download_of_google_street_view_panos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jne9rc/mass_download_of_google_street_view_panos/\">[comments]</a></span>",
        "id": 2444734,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jne9rc/mass_download_of_google_street_view_panos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mass download of Google Street View Panos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T14:52:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jne48y/getting_raw_data_from_complex_graphs/\"> <img src=\"https://b.thumbs.redditmedia.com/KFVfhgCTn2LD83yTtrlLPCv1pUWoOhRw7rjKePEoY2Q.jpg\" alt=\"Getting Raw Data From Complex Graphs\" title=\"Getting Raw Data From Complex Graphs\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have no idea whether this makes sense to post here, so sorry if I&#39;m wrong. </p> <p>I have a huge library of existing Spectral Power Density Graphs (signal graphs), and I have to convert them into their raw data for storage and using with modern tools. </p> <p>Is there anyway to automate this process? Does anyone know any tools or has done something similar before? </p> <p>An example of the graph (This is not we&#39;re actually working with, this is way more complex but just to give people an idea).</p> <p><a href=\"https://preview.redd.it/yo47siwmbure1.png?width=554&amp;format=png&amp;auto=webp&amp;s=1b70e08c514bd849eedd5ce46c1c5091f",
        "id": 2443740,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jne48y/getting_raw_data_from_complex_graphs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/KFVfhgCTn2LD83yTtrlLPCv1pUWoOhRw7rjKePEoY2Q.jpg",
        "title": "Getting Raw Data From Complex Graphs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T14:30:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How do you folks catalog your data and make it searchable and explorable? Im a data engineer currently planning to hoard datasets, llm models and basically a huge variety of random data in different formats- wikipedia dumps, stackoverflow, YouTube videos.</p> <p>Is there an equivalent to something like Apace Atlas for this? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lawanda123\"> /u/lawanda123 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jndnd9/cataloging_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jndnd9/cataloging_data/\">[comments]</a></span>",
        "id": 2443397,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jndnd9/cataloging_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cataloging data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T14:19:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hiya,</p> <p>I&#39;ve sorted through my photos using Duplicates.dupeguru. </p> <p>I want to rename them (year / month / date based on the embedded information in the file), but I don&#39;t want to move them. I was going to use PhotoMove but it looks as though using that it would move them all into individual folders.</p> <p>Does anyone know of any free software that will let me bulk rename the individual photo files?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bingobango2911\"> /u/bingobango2911 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jndf3p/renaming_photos_which_software_to_use/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jndf3p/renaming_photos_which_software_to_use/\">[comments]</a></span>",
        "id": 2443398,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jndf3p/renaming_photos_which_software_to_use",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Renaming Photos - Which software to use?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T13:44:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If I download multiple large games over time\u2014each around &gt;120GB at a slow speed of 5 Mbps, will this cause more wear and tear on my NVMe SSD compared to copying the same amount of data quickly from another drive? Specifically, does the prolonged, small-scale writing from slow downloads impact SSD longevity more than faster, sequential writes?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/areyoua1or0\"> /u/areyoua1or0 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jncpio/impact_of_slow_data_downloads_vs_fast_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jncpio/impact_of_slow_data_downloads_vs_fast_data/\">[comments]</a></span>",
        "id": 2443010,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jncpio/impact_of_slow_data_downloads_vs_fast_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Impact of Slow data Downloads vs. Fast Data Transfers on SSD Longevity",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T12:56:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Slightly off topic post and apologies if this isn&#39;t the right place.</p> <p>My late grand father was a hoarder in the days before computers (must be where I got it from) and has left a massive collection of cassette tapes with recorded radio shows on. I am yet to go through all of them, but they are a mix of recordings of radio shows like classic FM, Gardner&#39;s Question Time and other radio shows / podcasts from radio 4. From the labels of the ones i had a quick look at, some of these date back to the early 90&#39;s.</p> <p>Is there somewhere that I could donate these too that would be interested in digitising them and preserving them? It feels like a massive shame to throw them away</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/spudd01\"> /u/spudd01 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnbtgj/physical_tape_collection_donation/\">[link]</a></span> &#32; <span><a href=\"h",
        "id": 2443009,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnbtgj/physical_tape_collection_donation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Physical Tape Collection Donation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T12:47:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t know if this is in the right place, but I have a question regarding the use of the WayBack Machine.</p> <p>I&#39;m trying to save all subdirectories of a website to that service. For example, if I enter the url <a href=\"https://edition.cnn.com/\">https://edition.cnn.com/</a> it saves that, but not <a href=\"https://edition.cnn.com/politics\">https://edition.cnn.com/politics</a> etc.</p> <p>Is there a way to automatically save the entire page and its subdirectories, including images, pdf files that are on the page etc.? Or some other service than the WayBack Machine.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/True-Competition-191\"> /u/True-Competition-191 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnbnst/how_to_automatically_save_a_web_page_with_its/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jnbnst/how_to_automatically_save_a",
        "id": 2444735,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jnbnst/how_to_automatically_save_a_web_page_with_its",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to automatically save a web page with its subpages?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T10:15:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,<br/> I recently bought the Japanese Blu-ray box set of <em>Dragon Ball Kai</em>, but I noticed that the last episodes (77 to 97) no longer feature Kenji Yamamoto\u2019s score\u2014they use the older <em>Dragon Ball Z</em> soundtrack instead.<br/> I\u2019m interested in creating a remix of these final episodes using Yamamoto\u2019s original music to make them match the earlier part of Kai.</p> <p>Does anyone know if it\u2019s possible to find the <strong>Japanese audio</strong> (<strong>not English</strong>) for episodes 77\u201397 with <strong>Yamamoto\u2019s BGM</strong> still intact? Or maybe an older TV broadcast, recording, or any source where the Japanese audio includes his tracks?</p> <p>Also, any tips or tools for doing a custom audio remux without any loss quality, with the Blu-ray footage would be really helpful.</p> <p>Thanks a lot in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SuperCiao\"> /u/SuperCiao </a> <br",
        "id": 2442148,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn9f6s/readding_lost_soundtrack_to_bluray_episodes_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Re-adding lost soundtrack to Blu-ray episodes of Dragon Ball Kai \u2013 anyone archived Yamamoto\u2019s Japanese audio for episode 77\u201397?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T10:06:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,<br/> I recently bought the Japanese Blu-ray box sets of <em>Dragon Ball Kai</em>, but they don\u2019t include <strong>Italian subtitles</strong>\u2014only the original Japanese audio.</p> <p>I\u2019m looking for a way to get <strong>Italian subtitles</strong> for each episode, ideally in <strong>.srt format</strong>, so I can sync them with the Blu-ray episodes.</p> <p>Does anyone know of a reliable website or source where I can download Italian subs for the Japanese version of <em>Dragon Ball Kai</em>?</p> <p>Also, how can I insert into video files without any loss quality?</p> <p>Any help would be greatly appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SuperCiao\"> /u/SuperCiao </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn9ard/looking_to_archive_db_kai_with_italian_subtitles/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn9ard/lo",
        "id": 2442149,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn9ard/looking_to_archive_db_kai_with_italian_subtitles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to archive DB Kai with Italian subtitles \u2013 any .srt sources for the Japanese Blu-rays?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T09:27:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn8sns/cmr_vs_smr_home_media/\"> <img src=\"https://b.thumbs.redditmedia.com/ib6gYdFvvjUrgUAj4_HWqEWfMPoQxaFEtawa_P2nwqQ.jpg\" alt=\"CMR vs SMR (Home Media)\" title=\"CMR vs SMR (Home Media)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Sorry if this has been asked 100 times, I have read over every post I could find and still not sure.</p> <p>I live in Cape Town South Africa, so options are limited. I have a basic i5 12000 , 16gb setup.</p> <p>C Drive (Games and New Downloads) - 500gb nvme</p> <p>2TB Baracuda - Movies, Music, Personal Photos</p> <p>1TB Baracuda - Game ISOs, Anime, Personal Photo Backup</p> <p>4TB WD Green - Series</p> <p>I have run out of space and looking to get a new drive. Options are : </p> <p><a href=\"https://preview.redd.it/k7e6qkhxosre1.png?width=1128&amp;format=png&amp;auto=webp&amp;s=04d6783bdcd56fdb62789b441b6a2fcaf1fcb5de\">https://preview.redd.it/k7e6qkhxosre1.png?width=1128&amp;form",
        "id": 2442150,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn8sns/cmr_vs_smr_home_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/ib6gYdFvvjUrgUAj4_HWqEWfMPoQxaFEtawa_P2nwqQ.jpg",
        "title": "CMR vs SMR (Home Media)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T04:58:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an 8 bay iocrest das with</p> <p>-2 16TB Seagate Exos X16 -1 6TB Toshiba X300 -3 3TB Hitachi 7K3000</p> <p>I am looking to sell it, just don\u2019t know how to price.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mowowkin\"> /u/Mowowkin </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn57gm/how_to_price_47tb_das/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn57gm/how_to_price_47tb_das/\">[comments]</a></span>",
        "id": 2441216,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn57gm/how_to_price_47tb_das",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to price 47TB DAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T04:13:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I came across a number of 8mm films but have no means to digitise/project them myself. I&#39;d just like to see them scanned and online somewhere for archival purposes, they have no personal meaning to me. This isn&#39;t something I can justify spending a whole bunch of money on digitising but I hate the thought of just dumping them and they potentially get ruined, trashed, etc. never to be seen.</p> <p>Anyone know of who, if anyone, in Australia would take/borrow them to scan so they can be put on Internet Archive? </p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mysticalbuttwizard\"> /u/mysticalbuttwizard </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn4iq9/anyonewhere_in_australia_that_digitises_8mm_film/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn4iq9/anyonewhere_in_australia_that_digitises_8mm_film/\">[comments]</a></span>",
        "id": 2441215,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn4iq9/anyonewhere_in_australia_that_digitises_8mm_film",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone/where in Australia that digitises 8mm film for archival purposes, not personal?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T03:57:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I found a &quot;<strong>USED</strong>&quot; &quot;T7 Shield&quot; external SSD drive on amazon and would like to know if anyone has bought one or have any experience? The seller is &quot;Warehouse Deals&quot;.</p> <p>New is $550 while the used one is 360, quite a difference.</p> <p>I&#39;m just a regular consumer using it for personal data. </p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Abdel403\"> /u/Abdel403 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn492a/t7_shield_4tb_used_what_do_you_think/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn492a/t7_shield_4tb_used_what_do_you_think/\">[comments]</a></span>",
        "id": 2441034,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn492a/t7_shield_4tb_used_what_do_you_think",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "T7 Shield 4TB - \"Used\" - What do you think ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-30T02:27:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>apologies if this isn&#39;t the right place to post this, if not then please direct me somewhere.</p> <p>i am not concerned with disk performance, nor may i use other filesystems. i need to store as much data as possible (including many uncompressed small files) in a way that is compatible with MS-DOS, linux, and android.</p> <p>how may i know what the minimum possible allocation unit size is for large-fat32 volumes of different sizes? is there some table of limits?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DidThisSoICouldPost\"> /u/DidThisSoICouldPost </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn2p3q/math_to_determine_minimum_allocation_unit_size_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jn2p3q/math_to_determine_minimum_allocation_unit_size_on/\">[comments]</a></span>",
        "id": 2440894,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jn2p3q/math_to_determine_minimum_allocation_unit_size_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "math to determine minimum allocation unit size on large-fat32?",
        "vote": 0
    }
]