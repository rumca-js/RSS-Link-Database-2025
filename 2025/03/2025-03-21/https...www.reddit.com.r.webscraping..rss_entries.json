[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T21:10:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jgrnvb/captcha/\"> <img src=\"https://preview.redd.it/97hudovyy3qe1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=beb7d987b167425f9b685ef0a28aeee642687283\" alt=\"captcha\" title=\"captcha\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>does anyone have any idea how to break the captcha ? </p> <p>i have been trying for days to find a solution or how i could do to skip or solve the following captcha</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Current_Record_1762\"> /u/Current_Record_1762 </a> <br/> <span><a href=\"https://i.redd.it/97hudovyy3qe1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgrnvb/captcha/\">[comments]</a></span> </td></tr></table>",
        "id": 2379824,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jgrnvb/captcha",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/97hudovyy3qe1.png?width=216&crop=smart&auto=webp&s=beb7d987b167425f9b685ef0a28aeee642687283",
        "title": "captcha",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T20:20:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I&#39;m working on fine-tuning an LLM for digital forensics, but I&#39;m struggling to find a suitable dataset. Most datasets I come across are related to cybersecurity, but I need something more specific to digital forensics.</p> <p>I found ANY.RUN, which has over 10 million reports on malware analysis, and I tried scraping it, but I ran into issues. Has anyone successfully scraped data from ANY.RUN or a similar platform? Any tips or tools you recommend?</p> <p>Also, I couldn\u2019t find open-source projects on GitHub related to fine-tuning LLMs specifically for digital forensics. If you know of any relevant projects, papers, or datasets, I\u2019d love to check them out!</p> <p>Any suggestions would be greatly appreciated. Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MMLightMM\"> /u/MMLightMM </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgqhib/scraping_issues_with_",
        "id": 2379825,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jgqhib/scraping_issues_with_anyrun",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Issues with ANY.RUN",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T19:27:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When website check your extensions do they check exactly how they work? I&#39;m thinking about scraping by after the page is loaded in the browser, the extension save the data locally or in my server to parse it later. But even if it don&#39;t modify the DOM or HTML. will the extension expose what I&#39;m doing?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ancenxdap\"> /u/Ancenxdap </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgp8am/newbie_question_about_extensions/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgp8am/newbie_question_about_extensions/\">[comments]</a></span>",
        "id": 2378781,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jgp8am/newbie_question_about_extensions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[newbie] Question about extensions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T19:07:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>&quot;I need to fetch 500 pieces of data from <a href=\"http://any.com\">any.com</a>, but CF caught me at 245/500 operations.</p> <p>I&#39;m not attacking the website; I&#39;m just retrieving the necessary information for my hobbies.</p> <p>I used Selenium.&quot;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mr01d\"> /u/Mr01d </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgorly/how_can_i_bypass_cf_detection_while_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgorly/how_can_i_bypass_cf_detection_while_web_scraping/\">[comments]</a></span>",
        "id": 2378353,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jgorly/how_can_i_bypass_cf_detection_while_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\"How can I bypass CF detection while web scraping?\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T16:53:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;ve came across a url that has json formatted data connected to it: <a href=\"https://stockanalysis.com/api/screener/s/i\">https://stockanalysis.com/api/screener/s/i</a></p> <p>When looking up the webpage it saw that they have many more data endpoints on it. For example I want to scrape the NASDAQ stocks data which are in this webpage link: <a href=\"https://stockanalysis.com/list/nasdaq-stocks/\">https://stockanalysis.com/list/nasdaq-stocks/</a> </p> <p>How can I get a json data url for different pages on this website?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/grazieragraziek9\"> /u/grazieragraziek9 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jglj0c/how_to_get_json_url_from_this_webpage_for_stock/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jglj0c/how_to_get_json_url_from_this_webpage_for_stock/\">[comments]</a></span>",
        "id": 2377308,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jglj0c/how_to_get_json_url_from_this_webpage_for_stock",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to get JSON url from this webpage for stock data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T13:55:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Want to scrape data from a mobile app, the problem is I don&#39;t know how to find the endpoint API, tried to use Bluestacks to download the app on the pc and Postman and CharlesProxy to catch the response, but didn&#39;t work. Any recommendations?? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DoublePistons\"> /u/DoublePistons </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jghdwb/mobile_app_scrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jghdwb/mobile_app_scrape/\">[comments]</a></span>",
        "id": 2375309,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jghdwb/mobile_app_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mobile App Scrape",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T12:41:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>p2p nodes advertise browser capacity and price, support for concurrency and region selection, escrow payment after use for nodes, before use for users, we could really benefit from this</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/musaspacecadet\"> /u/musaspacecadet </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgfw73/p2p_headfull_browser_network_passive_income_cheap/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgfw73/p2p_headfull_browser_network_passive_income_cheap/\">[comments]</a></span>",
        "id": 2375310,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jgfw73/p2p_headfull_browser_network_passive_income_cheap",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "p2p headfull browser network = passive income + cheap rates",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T06:50:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an Excel file with a total of 3,100 entries. Each entry represents a city in Germany. I have the city name, street address, and town.</p> <p>What I now need is the HR department&#39;s email address and the city&#39;s domain.</p> <p>I would appreciate any suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Expert_Edge7780\"> /u/Expert_Edge7780 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgasi0/web_scraping_of_3000_city_email_addresses_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jgasi0/web_scraping_of_3000_city_email_addresses_in/\">[comments]</a></span>",
        "id": 2373582,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jgasi0/web_scraping_of_3000_city_email_addresses_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scraping of 3,000 city email addresses in Germany",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T04:54:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Was recently pitched on a real estate data platform that provides quite a large amount of comprehensive data on just about every apartment community in the country (pricing, unit mix, size, concessions + much more) with data refreshing daily. Their primary source for the data is the individual apartment communities websites&#39;, of which there are over 150k. Since these website are structured so differently (some Javascript heavy some not) I was just curious as to how a small team (less then twenty people working at the company including non-development folks) achieves this. How is this possible and what would they be using to do this? Selenium, scrappy, playwright? I work on data scraping as a hobby and do not understand how you could be consistently scraping that many websites - would it not require unique scripts for each property?</p> <p>Personally I am used to scraping pricing information from the typical, highly structured, apartment listing w",
        "id": 2373055,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How does a small team scrape data daily from 150k+ unique websites?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T01:22:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, </p> <p>Does anyone knows how to run headful (headless = false) browsers (puppeteer/playwright) at scale, and without using tools like Xvfb?</p> <p>The Xvfb setup is easily detected by anti bots.</p> <p>I am wondering if there is a better way to do this, maybe with VPS or other infra?</p> <p>Thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ElAlquimisto\"> /u/ElAlquimisto </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jg59to/run_headful_browsers_at_scale/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jg59to/run_headful_browsers_at_scale/\">[comments]</a></span>",
        "id": 2372391,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jg59to/run_headful_browsers_at_scale",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Run Headful Browsers at Scale",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-21T01:11:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m currently using chat gpt to generate a google search scrape. Since I&#39;m new to this I&#39;m wondering if there&#39;s any red flags to look out for. The scrape I&#39;m using is implementing hash tags. I&#39;ve been told by Chat gpt that a mobile scrape is possible by implementing vps, ssh, playwright, beautiful soup, regex, and proxy support. I&#39;m not really sure that I even need proxy support. Suggestions or ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CoatAcrobatic1118\"> /u/CoatAcrobatic1118 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jg52g8/any_chat_gpt_red_flags/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jg52g8/any_chat_gpt_red_flags/\">[comments]</a></span>",
        "id": 2372392,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jg52g8/any_chat_gpt_red_flags",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any Chat gpt red flags?",
        "vote": 0
    }
]