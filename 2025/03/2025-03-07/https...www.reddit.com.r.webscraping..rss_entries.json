[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-07T16:53:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Perhaps I am just being paranoid, but I have been trying to get through this sequence of steps for a particular site, and I&#39;m pretty sure I have switched between two different &quot;id&quot; values for a perticular ul element in the xpath that I am using many many times now. Once I get it working where I can locate the element through selenium in python, it then doesn&#39;t work anymore, at which point I check and in the page source the &quot;id&quot; value for that element is a different value from what I had in my previously-working xpath.</p> <p>Is it a thing for an element to change its &quot;id&quot; attribute based on time (to discourage web scraping or something) or browser or browser instance? Or am I just going crazy/doing something really weird and just not catching it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Photograph_01\"> /u/Ok_Photograph_01 </a> <br/> <span><a href=\"https://www.reddi",
        "id": 2270463,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j5tc59/should_a_sites_html_element_id_attribute_remain",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Should a site's html element id attribute remain the same value?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-07T14:20:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, </p> <p>I have a database in <strong>TXT or JSON</strong> format containing information on <strong>term deposits, credit cards, and bank accounts</strong>, including various market offers. Essentially, these are the databases used by financial comparison platforms.</p> <p>Currently, I update everything <strong>manually</strong>, which is too time-consuming. I tried using <strong>ChatGPT&#39;s Deep Research</strong>, but the results were inconsistent\u2014some entries were updated correctly, while others were not, requiring me to manually verify each one.</p> <p>Building <strong>wrappers for each site is not viable</strong> because there are <strong>hundreds of sources</strong>, and they frequently change how they present the data.</p> <p>I&#39;m looking for an <strong>automatic or semi-automatic</strong> method that allows me to update the database <strong>periodically</strong> without investing too much time.</p> <p>Has anyone faced a sim",
        "id": 2269382,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j5p0x6/automating_the_update_of_financial_product",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automating the Update of Financial Product Databases",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-07T08:36:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>title is self-explanatory, need to find a way to get domains. Starting for one country and then expanding after. Is there a &quot;free&quot; way outside of sales nav and other data providers like that? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Significant_Ad3848\"> /u/Significant_Ad3848 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5iyoe/is_there_a_way_i_can_scrape_all_domains_just/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5iyoe/is_there_a_way_i_can_scrape_all_domains_just/\">[comments]</a></span>",
        "id": 2266831,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j5iyoe/is_there_a_way_i_can_scrape_all_domains_just",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "is there a way i can scrape all domains - just domains",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-07T07:34:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can anyone please help me scrape the map images used for this website? <a href=\"https://gamemaps.gg/game/baldurs-gate-3\">https://gamemaps.gg/game/baldurs-gate-3</a>. I&#39;ve been trying to figure out for about a week now and still stuck. Any help would really be appreciated. Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cerbedoge\"> /u/cerbedoge </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5i4t2/extracting_map_image_from_an_interactive_map/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5i4t2/extracting_map_image_from_an_interactive_map/\">[comments]</a></span>",
        "id": 2266549,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j5i4t2/extracting_map_image_from_an_interactive_map",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Extracting map image from an interactive map",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-07T05:04:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working on automating some tasks on a website, but I want to make sure my actions look as human as possible to avoid triggering CAPTCHA or getting blocked. I\u2019m already using random delays, rotating user agents, and proxies, but I\u2019m still running into issues with CAPTCHA on steam register. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jaded-Supermarket247\"> /u/Jaded-Supermarket247 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5fvkm/help_with_free_bypass_hcaptcha_on_steam/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5fvkm/help_with_free_bypass_hcaptcha_on_steam/\">[comments]</a></span>",
        "id": 2267138,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j5fvkm/help_with_free_bypass_hcaptcha_on_steam",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "help with free bypass hcaptcha on steam",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-07T02:55:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>That last couple of months I\u2019ve been successfully scraping google search results with just http requests to a specific url and parsing the results.</p> <p>I just recently started getting 429 errors no matter the IP I\u2019m on. I\u2019m trying to find an alternative where I can scrape google search results without spinning up a headless browser. Suggestions? </p> <p>Feel free to DM me to chat as well</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Palpitation-6604\"> /u/No-Palpitation-6604 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5dkka/google_search_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j5dkka/google_search_scraping/\">[comments]</a></span>",
        "id": 2265568,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j5dkka/google_search_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google search scraping",
        "vote": 0
    }
]