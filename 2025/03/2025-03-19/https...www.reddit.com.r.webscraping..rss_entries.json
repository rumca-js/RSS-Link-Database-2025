[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T17:07:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working with this external API: <a href=\"https://www.goat.com/web-api/v1/product_variants/buy_bar_data?productTemplateId=1451453&amp;countryCode=HK\">Goat AP</a>I. It works perfectly on my local development server, but after deploying my Next.js app, it doesn&#39;t return any data. Any help would be appreciated!</p> <p>Drop the VERCEL URLS of the working code!!!<br/> The LINK: </p> <p>[<a href=\"https://www.goat.com/web-api/v1/product%5C%5C%5C_variants/buy%5C%5C%5C_bar%5C%5C%5C_data?productTemplateId=1451453&amp;countryCode=HK%5C%5D(https://www.goat.com/web-api/v1/product%5C_variants/buy%5C_bar%5C_data?productTemplateId=1451453&amp;countryCode=HK)\">https://www.goat.com/web-api/v1/product\\\\\\_variants/buy\\\\\\_bar\\\\\\_data?productTemplateId=1451453&amp;countryCode=HK\\](https://www.goat.com/web-api/v1/product\\_variants/buy\\_bar\\_data?productTemplateId=1451453&amp;countryCode=HK)</a></p> <p>THe code: </p> <p>Slug would often look like (gel-1130-black-",
        "id": 2360369,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jf20an/20_for_assistance_with_my_api_issue",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "$20 for assistance with my API issue.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T16:40:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Has anyone managed to bypass the Vercel security checkpoint (also known as Vercel Attack Challenge Mode or Vercel Firewall)</p> <p>There are some old GitHub repos that no longer work: - <a href=\"https://github.com/glizzykingdreko/Vercel-Attack-Mode-Solver\">https://github.com/glizzykingdreko/Vercel-Attack-Mode-Solver</a> - <a href=\"https://github.com/YZYLAB/vercel-firewall-bypass\">https://github.com/YZYLAB/vercel-firewall-bypass</a></p> <p>When you visit such a site (random example: <a href=\"https://early.krain.ai/\">https://early.krain.ai/</a>) a &#39;x-vercel-challenge-token&#39; is send as a response header. Then there&#39;s an obfuscated JS and a WASM file that generate a response that you have to send back.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vroemboem\"> /u/vroemboem </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jf1d5l/vercel_security_checkpoint/\">[link]</a></span> &#32;",
        "id": 2360370,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jf1d5l/vercel_security_checkpoint",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Vercel security checkpoint",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T14:33:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to build a slow crawler to learn the basics of a general crawler, what would be a good initial set of seed urls?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Googles_Janitor\"> /u/Googles_Janitor </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jeydwh/how_to_initialize_a_frontier/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jeydwh/how_to_initialize_a_frontier/\">[comments]</a></span>",
        "id": 2359128,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jeydwh/how_to_initialize_a_frontier",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to initialize a frontier?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T09:12:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, does anyone have a good google shopping scraper service that works with EAN? </p> <p>Don\u2019t want to go with the hastle of using residental proxies etc. </p> <p>Preferable if it\u2019s a legit \u201dcompany\u201d/site, not one of those sites ending with API :-)</p> <p>Thanks all Have a nice day!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CurrentIntrepid3005\"> /u/CurrentIntrepid3005 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jesuan/google_shopping_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jesuan/google_shopping_scraper/\">[comments]</a></span>",
        "id": 2356581,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jesuan/google_shopping_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google Shopping scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T09:02:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am curious how do you use AI in web scraping</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/adibalcan\"> /u/adibalcan </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jespgu/how_do_you_use_ai_in_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jespgu/how_do_you_use_ai_in_web_scraping/\">[comments]</a></span>",
        "id": 2356580,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jespgu/how_do_you_use_ai_in_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you use AI in web scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T08:20:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/webscraping\">r/webscraping</a> ,</p> <p>I love web scraping for data analysis\u2014usually in Python, sometimes JS\u2014but modern websites make it a pain. Many are SPAs, meaning you can&#39;t just fetch raw HTML anymore. You need Playwright or Puppeteer to render pages properly.</p> <h1>The problem? Cost and scalability.</h1> <ul> <li><strong>Paying for idle time</strong> \u2013 Most hosting providers charge you 24/7, even when your scraper isn&#39;t running.</li> <li><strong>Scaling is expensive</strong> \u2013 If you want to scrape multiple platforms in parallel, running multiple machines quickly gets costly.</li> </ul> <p>So I built <strong>Leapcell</strong>\u2014a serverless platform where you can deploy Playwright apps instantly and scale up to <strong>2,000 concurrent instances</strong> when needed. No paying for idle machines.</p> <p>Here\u2019s a <strong>live Playwright example</strong> running on Leapcell that takes screenshots and extracts all <code>&lt",
        "id": 2356293,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jes6ro/launched_a_serverless_hosting_option_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "launched a serverless hosting option for Playwright/Puppeteer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T07:19:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>has anyone dealt with `Vercel Security Checkpoint` <a href=\"https://vercel.com/docs/attack-challenge-mode\">this</a> verifying browser during automation? I am trying to use playwright in headless mode but it keeps getting stuck at the &quot;bot check&quot; before the website loads. Any way around it? I noticed there are Vercel cookies that I can &quot;side-load&quot; but they last 24 hours, and possibly not intuitive for automation. Am I approaching it incorrectly?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TitaniumPangolin\"> /u/TitaniumPangolin </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jerf9b/vercel_security_checkpoint/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jerf9b/vercel_security_checkpoint/\">[comments]</a></span>",
        "id": 2356028,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jerf9b/vercel_security_checkpoint",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Vercel Security Checkpoint",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T05:17:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to crawl Airbnb for the UAE region to retrieve listed properties, but there is a hard limit of 15 pages.<br/> How can I get all the listed properties from Airbnb?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Level_River_468\"> /u/Level_River_468 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jepr2r/airbnb_pagination_issue/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jepr2r/airbnb_pagination_issue/\">[comments]</a></span>",
        "id": 2356029,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jepr2r/airbnb_pagination_issue",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Airbnb Pagination Issue",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T01:14:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am so frustrated with running multiple urls in a loop in a spider. When I yield the urls then I get the socket related error from no driver. I have nodriver in the middleware.</p> <p>Have you guys faced such issues?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EstablishmentOver202\"> /u/EstablishmentOver202 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jelg38/nodriver_scrapy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jelg38/nodriver_scrapy/\">[comments]</a></span>",
        "id": 2355168,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jelg38/nodriver_scrapy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Nodriver + Scrapy",
        "vote": 0
    }
]