[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T22:41:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I recently uploaded a bunch of game screenshots and clips, but I want to change their dates to the original creation date, which is included in the file names. When I try to adjust the timestamp, it asks me to specify a pattern for the file name, but I\u2019m not sure what that pattern should be. For example, one of the files is named \u201cA way out-2021_04_20-22_26_50.png\u201d. How can I define the correct pattern for this?</p> <p>p.s a way out is the game\u2019s name</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GaMeZkIleR20t3\"> /u/GaMeZkIleR20t3 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jfa0zx/advanced_renamer_time_stamp_pattern/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jfa0zx/advanced_renamer_time_stamp_pattern/\">[comments]</a></span>",
        "id": 2362963,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jfa0zx/advanced_renamer_time_stamp_pattern",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advanced renamer, time stamp pattern",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T22:14:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a cheap cloud storage which i can use to store my 100GB folder, and most importantly share it with others. I&#39;ve came across iDrive and it seems to offer 100gb at $3 annually but i&#39;m not sure if sharing folder is allowed.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SolarisAZ\"> /u/SolarisAZ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf9ek8/cheap_100gb_cloud_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf9ek8/cheap_100gb_cloud_storage/\">[comments]</a></span>",
        "id": 2362592,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf9ek8/cheap_100gb_cloud_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cheap 100GB Cloud Storage ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T22:10:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Apologies if this isn\u2019t the correct sub. </p> <p>I requested all my existing data from my Reddit account, and while going over the CSV file, I realized some conversations I had (whether in chat, messages, or comments) were missing. </p> <p>I am fairly sure I had a specific convo but I couldn\u2019t see it from what I got. </p> <p>I do see some comments show up as \u201cdeleted\u201d, but is it possible for messages to not appear at all? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JAragon7\"> /u/JAragon7 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf9b8g/does_reddit_periodically_delete_some_data_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf9b8g/does_reddit_periodically_delete_some_data_of/\">[comments]</a></span>",
        "id": 2362593,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf9b8g/does_reddit_periodically_delete_some_data_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does Reddit periodically delete some data of existing accounts? (Messages, chats, comments)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T22:05:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi. I use chan thread watcher to...um ..um yeah download images from 4chan. its stopped working recently. Are there any alternatives you can recommend currently available? thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/justsignmeupcuz\"> /u/justsignmeupcuz </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf977c/chan_thread_watcher_replacement_or_fix/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf977c/chan_thread_watcher_replacement_or_fix/\">[comments]</a></span>",
        "id": 2362594,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf977c/chan_thread_watcher_replacement_or_fix",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Chan Thread Watcher - replacement (or fix?)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T21:34:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hi hoarder friends - question: i have hundreds of audios that are in mp3 - i want to group some of them in m4b format for specific audio sets - question: i know that in m4b you can organize into chapters for the different tracks BUT can you create sub-chapters? or even sub-sub chapters? (its important bc some of the sets are courses that were originally packaged that way) can i create sub-chapters or sub-sub-chapters w m4b?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rbyk72\"> /u/rbyk72 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf8hm7/m4b_queation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf8hm7/m4b_queation/\">[comments]</a></span>",
        "id": 2362964,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf8hm7/m4b_queation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "m4b queation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T20:58:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hi! I downloaded all my social media accounts data, and looking to organise and self-host them, to be able to access through llms&#39; and never lose them due to some stupid new rule (as I already lost my messages with my gf in 2019, and 2 years of documented blog-style memories in instagram in 2022)</p> <p>now I&#39;m trying to set it up on pure cursor / <a href=\"http://repl.it\">repl.it</a> using matrix bridges, or self-developed access, but there are problems</p> <p>1) not every of these apps has api, and <a href=\"http://beeper.com\">beeper.com</a> doesn&#39;t have an api.<br/> 2) I can&#39;t aggregate feeds, and group messages in Facebook, but I would love to.</p> <p>now:<br/> 1) telegram is 8/10 downloaded (and almost updating automatically)<br/> 2) instagram, gmail, linkedin, messages, WhatsApp, Facebook are 3/10 -- connected via <a href=\"http://beeper.com\">beeper.com</a> but no feeds and no locally proxy.</p> <p>can you recommend anyone to talk ",
        "id": 2362965,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf7mh8/selfhosted_selfsovereign_identity_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "self-hosted self-sovereign identity setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T20:56:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I purchased a used 1TB WD hard drive and concerned about possible malware I decided to securely wipe it. Based on some research I decided to use DBAN for this task loading it via Rufus on a bootable USB stick.</p> <p>I was unsure about the optimal method to wipe the drive and I ended up with Method: PRNG Stream, Verify: Last Pass and Rounds: 1. However, after initiating the wipe, the throughput was around 14MB/s and the estimated time to completion is 70 hours, which seems incredibly long!</p> <p>Could you help me understand if it is normal? Should I be adjusting the parameters or using a different method for a faster wipe?</p> <p>Right now I&#39;m also concerned about interrupting the process as I don&#39;t want to risk damaging the drive. Any guidance or suggestions would be much appreciated! Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dekoalade\"> /u/dekoalade </a> <br/> <span><a href=\"https://ww",
        "id": 2362140,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf7l87/70_hours_remaining_to_wipe_hdd_with_dban_how_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "70 hours remaining to wipe HDD with DBAN!! How is this possible and how can I interrupt it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T20:50:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am looking for links mentioned specifically in this Reddit post here: <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jer801/a_list_of_government_website_scrubbing_in_recent/\">https://www.reddit.com/r/DataHoarder/comments/1jer801/a_list_of_government_website_scrubbing_in_recent/</a> I&#39;m not great with finding information on the Internet Archive and want to start a repository of archived websites that are being deleted by our current administration. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheRubyBerru\"> /u/TheRubyBerru </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf7fw3/does_anybody_have_archived_links_of_the_us/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf7fw3/does_anybody_have_archived_links_of_the_us/\">[comments]</a></span>",
        "id": 2362141,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf7fw3/does_anybody_have_archived_links_of_the_us",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anybody have archived links of the US government's latest scrubbed sites?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T20:16:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all!</p> <p>Thinking about how slow it is to rip/backup CDs/Blu-rays on my single machine with a single disk drive with EAC and MakeMKV, and I was wondering if it was possible or feasible to make a machine that exclusively functions as a ripping machine to then drop into my media server.</p> <p>How would I go about doing this and what would I need to buy to make it work?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ELite_Predator28\"> /u/ELite_Predator28 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf6mkg/is_it_a_good_ideafeasible_to_make_a_machine_that/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf6mkg/is_it_a_good_ideafeasible_to_make_a_machine_that/\">[comments]</a></span>",
        "id": 2362139,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf6mkg/is_it_a_good_ideafeasible_to_make_a_machine_that",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it a good idea/feasible to make a machine that is exclusively functions as a CD/Blu-ray ripper?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T19:56:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can&#39;t tell the difference, but there is a price difference. </p> <p>Serverpartdeals, maybe you can answer?</p> <p>TIA!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RastaMonsta218\"> /u/RastaMonsta218 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf64za/seagate_exos_c_vs_h_part_number/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf64za/seagate_exos_c_vs_h_part_number/\">[comments]</a></span>",
        "id": 2361622,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf64za/seagate_exos_c_vs_h_part_number",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate Exos, \"C\" vs \"H\" Part Number",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T19:20:50+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf59fn/st8000vn002_benchmark_on_dxp2800/\"> <img src=\"https://preview.redd.it/dsqwphtl5ppe1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de1adac1d331e82cccb77448b3654c4677c23039\" alt=\"ST8000VN002 benchmark on DXP2800\" title=\"ST8000VN002 benchmark on DXP2800\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Im building my nas and this is the disk i chose. Thought you might be interested in the benchmark. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jcodes\"> /u/jcodes </a> <br/> <span><a href=\"https://i.redd.it/dsqwphtl5ppe1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf59fn/st8000vn002_benchmark_on_dxp2800/\">[comments]</a></span> </td></tr></table>",
        "id": 2361623,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf59fn/st8000vn002_benchmark_on_dxp2800",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/dsqwphtl5ppe1.png?width=320&crop=smart&auto=webp&s=de1adac1d331e82cccb77448b3654c4677c23039",
        "title": "ST8000VN002 benchmark on DXP2800",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T19:18:23+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf57a8/wd80efpx_benchmark_on_dxp2800/\"> <img src=\"https://preview.redd.it/exv81y365ppe1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=968d36fc419f568d388470d7cf00f6a057ece84f\" alt=\"WD80EFPX benchmark on DXP2800\" title=\"WD80EFPX benchmark on DXP2800\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Im building my first nas and will be using this disk. Thought you might be interested in the benchmark. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jcodes\"> /u/jcodes </a> <br/> <span><a href=\"https://i.redd.it/exv81y365ppe1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf57a8/wd80efpx_benchmark_on_dxp2800/\">[comments]</a></span> </td></tr></table>",
        "id": 2361624,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf57a8/wd80efpx_benchmark_on_dxp2800",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/exv81y365ppe1.png?width=640&crop=smart&auto=webp&s=968d36fc419f568d388470d7cf00f6a057ece84f",
        "title": "WD80EFPX benchmark on DXP2800",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T19:17:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I have recently acquired a bunch of used SSDs, and I&#39;d like to wipe them for safety before connecting them to my system.</p> <p>I have an old XP laptop I could use for wiping, but I want to be on the safe side and make sure no virus spreads to the system before/after the wipe.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/toastronomy\"> /u/toastronomy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf56b8/how_would_you_safely_wipe_a_bunch_of_used_ssds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf56b8/how_would_you_safely_wipe_a_bunch_of_used_ssds/\">[comments]</a></span>",
        "id": 2361625,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf56b8/how_would_you_safely_wipe_a_bunch_of_used_ssds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How would you safely wipe a bunch of used SSDs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T18:31:21+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf421b/us_deletes_evidence_of_russias_kidnap_of/\"> <img src=\"https://external-preview.redd.it/kCrwAFsjneeiZefJ-tuQECenm8HaIYp7E6JO_491rxA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d863907e8f734957385d6f0b2df0f6e671c92c3\" alt=\"US \u2018deletes evidence\u2019 of Russia\u2019s kidnap of thousands of Ukrainian children\" title=\"US \u2018deletes evidence\u2019 of Russia\u2019s kidnap of thousands of Ukrainian children\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jacksharkben\"> /u/Jacksharkben </a> <br/> <span><a href=\"https://www.independent.co.uk/news/world/europe/trump-ukraine-children-russia-war-kidnapping-evidence-b2717730.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf421b/us_deletes_evidence_of_russias_kidnap_of/\">[comments]</a></span> </td></tr></table>",
        "id": 2361142,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf421b/us_deletes_evidence_of_russias_kidnap_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/kCrwAFsjneeiZefJ-tuQECenm8HaIYp7E6JO_491rxA.jpg?width=640&crop=smart&auto=webp&s=1d863907e8f734957385d6f0b2df0f6e671c92c3",
        "title": "US \u2018deletes evidence\u2019 of Russia\u2019s kidnap of thousands of Ukrainian children",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T18:11:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf3l37/boys_our_time_has_come/\"> <img src=\"https://preview.redd.it/0oe8i1o4tope1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=edefe910f1bf48f45782a3d05c37bac9ee4b621e\" alt=\"Boys, our time has come.\" title=\"Boys, our time has come.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CSharpSauce\"> /u/CSharpSauce </a> <br/> <span><a href=\"https://i.redd.it/0oe8i1o4tope1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf3l37/boys_our_time_has_come/\">[comments]</a></span> </td></tr></table>",
        "id": 2361143,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf3l37/boys_our_time_has_come",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/0oe8i1o4tope1.jpeg?width=640&crop=smart&auto=webp&s=edefe910f1bf48f45782a3d05c37bac9ee4b621e",
        "title": "Boys, our time has come.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T17:46:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I&#39;ve checked and and pretty sure this is a rules compliant post, so please forgive me if it isn&#39;t.</p> <p>I need to download and archive <em>parts</em> of a website on a weekly basis. Not the whole site. The site is an adverts listings directory, and the sections I need to download are sometimes spread over several pages, separated by &quot;next&quot; arrows, if there&#39;s more than about 25 ads.</p> <p>The URL construction for the head of each section I&#39;d like to download is <strong>DomainName/SectionTitle/Area</strong></p> <p>and on that page there are links to individual pages which are in this format: <strong>DomainName/SectionTitle/Area/AdvertTitle/AdvertID</strong></p> <p>If there&#39;s another page of adverts in the list, then &quot;next arrow&#39; leads to <strong>DomainName/SectionTitle/Area/t+2</strong> which has a link on the next page to <strong>t+3</strong> etc if there are more ads.</p> <p>I want to download ",
        "id": 2360578,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf2y0x/struggling_with_syntax_for_accurate_wget",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Struggling with syntax for accurate wget / (win)httrack / Site Sucker archiving",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T17:01:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello Data Hoarder community, I want to buy my first NAS. I have looked into the topic and I think I found what I wanted. First of all I want to use the NAS to safe all the old Videos and photos from me and my family, backup my PC and to digitize my old Movies and TV-Shows so I can stream them to 1 or 2 devices in my local network. I think the QNAP ts-433 would be enough for me, but I&#39;m not 100% sure if it is powerful enough to run Jellyfin. Can anybody tell me if it&#39;s a good choice or not? Thanks to anyone who is more knowledgeable than me and can help me.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NoLuck7722\"> /u/NoLuck7722 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf1uty/my_first_nas_pls_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf1uty/my_first_nas_pls_help/\">[comments]</a></span>",
        "id": 2362966,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf1uty/my_first_nas_pls_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My first NAS pls help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T17:00:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a habit of scanning physical invoices and saving them on my computer because it makes bookkeeping easier. However, now I need to find an invoice from June 2024, and it&#39;s quite difficult since I don\u2019t scan and save them daily\u2014I usually accumulate a certain amount before saving them all at once. Any tips to find it quickly without having to preview each one individually?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kitchen-Top-8110\"> /u/Kitchen-Top-8110 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf1u7j/looking_for_a_quick_search_method/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf1u7j/looking_for_a_quick_search_method/\">[comments]</a></span>",
        "id": 2360579,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf1u7j/looking_for_a_quick_search_method",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a quick search method",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T15:52:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf080p/i_think_its_safe_to_say_i_am_on_a_list_somewhere/\"> <img src=\"https://preview.redd.it/qor4qije4ope1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c2251dc9900c6d498b2afd53345781fb0cbc5c2\" alt=\"I think it's safe to say I am on a list somewhere at T-Mobile\" title=\"I think it's safe to say I am on a list somewhere at T-Mobile\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SharpDressedBeard\"> /u/SharpDressedBeard </a> <br/> <span><a href=\"https://i.redd.it/qor4qije4ope1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jf080p/i_think_its_safe_to_say_i_am_on_a_list_somewhere/\">[comments]</a></span> </td></tr></table>",
        "id": 2359977,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jf080p/i_think_its_safe_to_say_i_am_on_a_list_somewhere",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/qor4qije4ope1.png?width=640&crop=smart&auto=webp&s=9c2251dc9900c6d498b2afd53345781fb0cbc5c2",
        "title": "I think it's safe to say I am on a list somewhere at T-Mobile",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T15:39:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i have a feeling that recreating the array from scratch (blowing everything away on the existing drive) and copying the data over to a rebuilt array with zero data would be faster than rebuilding the array? If you had a backup of the data, would this be your approach or would you let it rebuild, potentially taking days to rebuild?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jku2017\"> /u/jku2017 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jezx11/raid1_failed_20tb_drive_i_have_backups_whats_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jezx11/raid1_failed_20tb_drive_i_have_backups_whats_the/\">[comments]</a></span>",
        "id": 2359377,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jezx11/raid1_failed_20tb_drive_i_have_backups_whats_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "raid1, failed 20tb drive. i have backups, whats the quickest way to rebuild the array?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T15:13:18+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GeekIsTheNewSexy\"> /u/GeekIsTheNewSexy </a> <br/> <span><a href=\"/r/selfhosted/comments/1jez8p4/major_update_reddit_saved_posts_fetcher_now_more/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jezagi/major_update_reddit_saved_posts_fetcher_now_more/\">[comments]</a></span>",
        "id": 2359978,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jezagi/major_update_reddit_saved_posts_fetcher_now_more",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\ud83d\udce2 Major Update: Reddit Saved Posts Fetcher \u2013 Now More Powerful, Flexible & Docker-Ready! \ud83d\ude80",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T13:34:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, I&#39;m planning my first NAS build and would appreciate some feedback on my parts list and overall approach. I&#39;m moving from a temporary setup (2x4TB RAID1 on my desktop machine + Jellyfin in an LXC container on Proxmox running on an old ThinkPad).</p> <p><strong>My plans</strong>:</p> <ul> <li>OS: TrueNAS Scale</li> <li>Initial storage: 2x18-20TB drives, expanding over time</li> <li>Primary use: File and media storage</li> <li>May run additional services directly on NAS</li> <li>Total budget (including drives): ~$1000</li> </ul> <p><a href=\"https://pcpartpicker.com/list/fbvGVF\">PCPartPicker Part List</a></p> <table><thead> <tr> <th align=\"left\">Type</th> <th align=\"left\">Item</th> <th align=\"left\">Price</th> </tr> </thead><tbody> <tr> <td align=\"left\"><strong>CPU</strong></td> <td align=\"left\"><a href=\"https://pcpartpicker.com/product/qtBzK8/intel-core-i3-14100-35-ghz-quad-core-processor-bx8071514100\">Intel Core i3-14100 3.5 GHz Quad-",
        "id": 2358745,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jex38a/planning_my_first_nas_build_looking_for_advice",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Planning my first NAS build - Looking for advice",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T12:36:26+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jevycu/fancy_giving_me_feedbackcritiques_on_this_android/\"> <img src=\"https://external-preview.redd.it/azl4MW10ZW4xbnBlMSKO-fRWyrEFWdRSPDmUyIMH5yeTfYJqOUBbmTjbfrNL.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=31f546c57b2dc9eeedbfc9c256ad0437b36a3172\" alt=\"Fancy giving me feedback/critiques on this Android app that allows you to 'Google' single HTML files offline?\" title=\"Fancy giving me feedback/critiques on this Android app that allows you to 'Google' single HTML files offline?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/elettroravioli\"> /u/elettroravioli </a> <br/> <span><a href=\"https://v.redd.it/lyjflten1npe1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jevycu/fancy_giving_me_feedbackcritiques_on_this_android/\">[comments]</a></span> </td></tr></table>",
        "id": 2359376,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jevycu/fancy_giving_me_feedbackcritiques_on_this_android",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/azl4MW10ZW4xbnBlMSKO-fRWyrEFWdRSPDmUyIMH5yeTfYJqOUBbmTjbfrNL.png?width=640&crop=smart&auto=webp&s=31f546c57b2dc9eeedbfc9c256ad0437b36a3172",
        "title": "Fancy giving me feedback/critiques on this Android app that allows you to 'Google' single HTML files offline?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T11:34:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all. Reaching out here because I am at my wit\u2019s end.</p> <p>My boss wants me to look for a scanner that scans from above, but not an overhead scanner. He wants to use it for scanning seeds, so he ideally wants the camera/scanning mechanism to come from the top. The dilemma is he wants a tabletop scanner. No overheads, just a plain commercially available scanner\u2026 that somehow works like that.</p> <p>Any help or leads would be greatly appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ecrivaintriste\"> /u/ecrivaintriste </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeuvfg/question_about_scanners/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeuvfg/question_about_scanners/\">[comments]</a></span>",
        "id": 2357684,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jeuvfg/question_about_scanners",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about Scanners",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T10:57:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What&#39;s a better 16tb external hdd?</p> <p>It seems like my current 14tb WD might fail and I want to back it up before it conks off. I&#39;ve not been in touch with compu-tech hence I come seeking light.</p> <p>Bonus question: Are there external drives which can be connected to the network and can be accessed remotely.</p> <p>I&#39;m curious, maybe trying to hit 2 birds with one stone here but can totally be 2 birds and 2 stones. Light the path for this uneducated padwan.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zoikos\"> /u/zoikos </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeuadw/a_question_worth_16tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeuadw/a_question_worth_16tb/\">[comments]</a></span>",
        "id": 2357204,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jeuadw/a_question_worth_16tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A question worth 16tb",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T07:56:26+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PricePerGig\"> /u/PricePerGig </a> <br/> <span><a href=\"https://pricepergig.com/pl\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jervq9/i_updated_pricepergigcom_to_add_poland_amazonpl/\">[comments]</a></span>",
        "id": 2356114,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jervq9/i_updated_pricepergigcom_to_add_poland_amazonpl",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I Updated PricePerGig.com to add \ud83c\uddf5\ud83c\uddf1Poland Amazon.pl\ud83c\uddf5\ud83c\uddf1 as requested in this sub",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T07:04:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This was compiled with the help of DeepSeek AI. I\u2019m sure there is a lot more that could be added but I don\u2019t have any more time. Anyone else want to run with this and expand it?</p> <h3><strong>Federal Government Website Alterations</strong></h3> <ol> <li><p><strong>Jackie Robinson\u2019s Military History Removed (DOD)</strong> </p> <ul> <li><strong>Details</strong>: The Department of Defense (DOD) deleted a webpage detailing Jackie Robinson\u2019s WWII-era court-martial and fight against racism in the Army. Critics linked the removal to anti-DEI political pressure.<br/></li> <li><strong>Source</strong>: <a href=\"https://www.ksbw.com/article/jackie-robinson-army-history-scrubbed-from-dod-website-dei/64225041\">KSBW (May 2024)</a><br/></li> <li><strong>Verify</strong>: Use the <a href=\"https://archive.org\">Wayback Machine</a> to search for the deleted DOD page.<br/></li> </ul></li> <li><p><strong>Navajo Code Talkers Content Removed (U.S. Army/DOD)</strong> </p> ",
        "id": 2356113,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jer801/a_list_of_government_website_scrubbing_in_recent",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A List of government website scrubbing in recent days",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T06:01:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>ive got an mini dell optiplex laying around doing nothing so i was going to make a unraid, or try my hand at freenas to make a backup server for my existing unraid media server.</p> <p>ive got 6 drives ready to go, need and enclosure to house them. I dont need raid built in as i plan on letting unraid or freenas handle that</p> <p>Looking for 8 ports and since i broke the bank on the drives, hoping to get a good deal.</p> <p>something like this <a href=\"https://www.amazon.com/dp/B07MD2LNYX?th=1\">https://www.amazon.com/dp/B07MD2LNYX?th=1</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/topsmack\"> /u/topsmack </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeqe57/usb_3_multidisk_enclosure_recommendations/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeqe57/usb_3_multidisk_enclosure_recommendations/\">[comments]</a></span>",
        "id": 2355879,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jeqe57/usb_3_multidisk_enclosure_recommendations",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "usb 3 multidisk enclosure recommendations?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T05:04:21+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jepk6c/am_i_screwed/\"> <img src=\"https://preview.redd.it/pzayb9sswkpe1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6fd64cd4c9e32eb610cc2485cbd6bcabdbd6e4f\" alt=\"Am I screwed?\" title=\"Am I screwed?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I bought these last month at Best Buy. Just past the return date. One of the drives doesn\u2019t work, I can hear it spinning but not being detected in disk utility. I\u2019m using a Mac. Do I have any options? I did not get any warranty or insurance for them.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BobbythebreinHeenan\"> /u/BobbythebreinHeenan </a> <br/> <span><a href=\"https://i.redd.it/pzayb9sswkpe1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jepk6c/am_i_screwed/\">[comments]</a></span> </td></tr></table>",
        "id": 2355608,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jepk6c/am_i_screwed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/pzayb9sswkpe1.jpeg?width=640&crop=smart&auto=webp&s=f6fd64cd4c9e32eb610cc2485cbd6bcabdbd6e4f",
        "title": "Am I screwed?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T03:49:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was hoping the price of theses things would be more economical now but prices in Canada seem to be going up rather then down since last year. Even 1TB usb 2.5 inch HDD have gone up in price. Obviously the larger size drive price per TB is cheaper but I need to be able to separate data.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TechnoTO\"> /u/TechnoTO </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeocwx/will_we_see_cheaper_512gb_to_1tb_usb_flash_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jeocwx/will_we_see_cheaper_512gb_to_1tb_usb_flash_drives/\">[comments]</a></span>",
        "id": 2355411,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jeocwx/will_we_see_cheaper_512gb_to_1tb_usb_flash_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Will we see cheaper 512GB to 1TB USB flash drives soon?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T03:00:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Project helps you to import and browse a copy of the <a href=\"http://IMDB.com\">IMDB.com</a> movie and tv show database locally.</p> <p><a href=\"https://github.com/non-npc/IMDB-DB-Tools\">https://github.com/non-npc/IMDB-DB-Tools</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dataguzzler\"> /u/dataguzzler </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jenh84/ingest_and_browse_imdb_tsv_archives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jenh84/ingest_and_browse_imdb_tsv_archives/\">[comments]</a></span>",
        "id": 2360580,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jenh84/ingest_and_browse_imdb_tsv_archives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ingest and browse IMDB TSV archives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T02:12:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can anyone recommend a seller of refurbished / recertified hard disk drives? I am aware of these three sellers: ServerPartDeals, GoHardDrive &amp; Rhino Technology. I would prefer dealing with an established seller like one of the above. Unfortunately, none of the above have what i am searching for. Thank you for your advice. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cymbaline1971\"> /u/Cymbaline1971 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jemktm/refurb_recertified_drive_seller/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jemktm/refurb_recertified_drive_seller/\">[comments]</a></span>",
        "id": 2355080,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jemktm/refurb_recertified_drive_seller",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Refurb / recertified drive seller ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T01:21:42+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/p3apod1987\"> /u/p3apod1987 </a> <br/> <span><a href=\"/r/Genealogy/comments/1je60m3/trumps_314_executive_order_hurts_genealogists/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jell74/trumps_314_executive_order_hurts_genealogists/\">[comments]</a></span>",
        "id": 2354811,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jell74/trumps_314_executive_order_hurts_genealogists",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trump's 3/14 executive order hurts genealogists",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T01:05:28+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/westoncox\"> /u/westoncox </a> <br/> <span><a href=\"/r/Genealogy/comments/1je60m3/trumps_314_executive_order_hurts_genealogists/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jel9mm/trumps_314_executive_order_hurts_genealogists/\">[comments]</a></span>",
        "id": 2354812,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jel9mm/trumps_314_executive_order_hurts_genealogists",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trump's 3/14 executive order hurts genealogists",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T00:44:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>2025 version released today</p> <p><a href=\"https://www.archives.gov/research/jfk\">https://www.archives.gov/research/jfk</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blaidd31204\"> /u/blaidd31204 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jekud1/jfk_assassination_records_released/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jekud1/jfk_assassination_records_released/\">[comments]</a></span>",
        "id": 2354810,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jekud1/jfk_assassination_records_released",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "JFK Assassination Records Released",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-19T00:41:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>considering buying some of these iron wolf drives for 300 a piece i have an empty 18 bay server planing on running unraid. </p> <p>how are these drives should i consider exos x18 for 50 dolors more?</p> <p>is the scandal with segate putting used drives as new over?</p> <p>i\u2019m planing on buying 4 now and keeping 2 as parity (simpler to r6) then expanding as i need keeping 2 parity</p> <p>any advice before i drop $1200??</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/foodman5555\"> /u/foodman5555 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jekrvj/iron_wolf_pro_18tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jekrvj/iron_wolf_pro_18tb/\">[comments]</a></span>",
        "id": 2354813,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jekrvj/iron_wolf_pro_18tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "iron wolf pro 18tb",
        "vote": 0
    }
]