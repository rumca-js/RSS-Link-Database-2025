[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T16:05:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need help scraping ONE of the following sites: Target, Walmart, or Amazon Fresh. I need to review data for a data science project, but I was told I must use web scraping. I have no experience, nor does the professor I am working with. I have tried using ChatGPT and other LLMs and have had nothing go anywhere. I need at least 1,000 reviews on 2 specific-ish products, and only once. They do not need to be updated. The closest I have gotten is 8 reviews from Amazon. I would prefer to use Python, and output a CSV, but could figure out another language as I have quite a bit of experience with numerous languages, but mainly use Python. My end goal is to use Python to do some data analysis on the results. If there are any helpful videos, websites, or other items that can help I would be glad to dig in more on my own, or if someone has similar code, I would appreciate bits and pieces of it to get to the more important part of my project.</p> </div><!-- SC_",
        "id": 2368565,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfsdzr/web_scraping_for_an_undergraduate_research_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Scraping for an Undergraduate Research Project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T12:56:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jfo9y1/looking_for_code_review_feedback_on_my_web/\"> <img src=\"https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf\" alt=\"Looking for code review &amp; feedback on my web scraping project\" title=\"Looking for code review &amp; feedback on my web scraping project\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m beginner programmer with python. I&#39;ve been working on a web scraping project and would love some constructive criticism on my code.</p> <p>I&#39;m looking for feedback on efficiency, best practices, and any improvements I could make.</p> <p>Any insights would be greatly appreciated! Thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shy357\"> /u/shy357 </a> <br/> <span><a href=\"https://pastebin.com/3mvhgWg",
        "id": 2366786,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfo9y1/looking_for_code_review_feedback_on_my_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&crop=smart&auto=webp&s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf",
        "title": "Looking for code review & feedback on my web scraping project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T11:40:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Dear Reddit</p> <p>Is there a way to scrape the data of a filled in Lettuce meet? All the methods I found only find a &quot;available between [time_a] and [time_b]&quot;, but this breaks when say someone is available during 10:00-11:00 and then also during 12:00-13:00. I think the easiest way to export this is to get a list of all the intervals (usually 30 min long) and then a list of all recipients who were available during that interval. Can someone help me?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/not_funny_after_all\"> /u/not_funny_after_all </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfmwd1/question_about_scraping_lettucemeet/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfmwd1/question_about_scraping_lettucemeet/\">[comments]</a></span>",
        "id": 2366303,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfmwd1/question_about_scraping_lettucemeet",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about scraping lettucemeet",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T11:23:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, I\u2019m with a background verification company trying to figure out how firms like AuthBridge fetch EPFO data using my UAN number.EPFO isn\u2019t responding\u2014any devs know if it\u2019s APIs, partnerships, or something else?\u201d</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Express_Power_7161\"> /u/Express_Power_7161 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfmm93/employee_provident_fund_organisation_epfo_api_or/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfmm93/employee_provident_fund_organisation_epfo_api_or/\">[comments]</a></span>",
        "id": 2366787,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfmm93/employee_provident_fund_organisation_epfo_api_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Employee Provident Fund Organisation EPFO API OR UAN VERIFICATION API",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T11:03:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I made a web scraper using beautifulsoup and selenium to extract download links for different books from <a href=\"https://www.pdfdrive.com/\">PDF drive</a>. This gives you exact match for the books you are looking for. Follow the guidelines mentioned in the README for more details.</p> <p>Check it out here: <a href=\"https://github.com/CoderFek/PDF-Drive-Scrapper\">https://github.com/CoderFek/PDF-Drive-Scrapper</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WeekendHefty4784\"> /u/WeekendHefty4784 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfmavu/script_to_scrape_books_from_pdf_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfmavu/script_to_scrape_books_from_pdf_drive/\">[comments]</a></span>",
        "id": 2366788,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfmavu/script_to_scrape_books_from_pdf_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Script to scrape books from PDF drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T10:40:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m still a beginner Python coder, however have a very usable webscraper script that is more or less delivering what I need. The only problem is when it finds one single result and then cant scroll, so it falls over.</p> <p>Code Block:</p> <p>while True: results = driver.find_elements(By.CLASS_NAME, &#39;hfpxzc&#39;) driver.execute_script(&quot;return arguments[0].scrollIntoView();&quot;, results[-1]) page_text = driver.find_element(by=By.TAG_NAME, value=&#39;body&#39;).text endliststring=&quot;You&#39;ve reached the end of the list.&quot; if endliststring not in page_text: driver.execute_script(&quot;return arguments[0].scrollIntoView();&quot;, results[-1]) time.sleep(5) else: break driver.execute_script(&quot;return arguments[0].scrollIntoView();&quot;, results[-1])</p> <p>Error :</p> <p>Scrape Google Maps Scrap Yards 1.1 Dev.py&quot;, line 50, in search_scrap_yards driver.execute_script(&quot;return arguments[0].scrollIntoView();&quot;, result",
        "id": 2365877,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jflywo/error_handling",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Error Handling",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T10:39:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p> <p>I&#39;m new into webscraping, is it possible to scrape all Google Ads pages for certain keywords directed at a specific geolocation?</p> <p><strong>For example:</strong></p> <p><strong>Keyword</strong> &quot;smartphone model 12345&quot;</p> <p><strong>Geolocation</strong>: &quot;city/state&quot;</p> <p>My end goal is to optimize Ads campaigns by knowing for a fact which Ads are running and scrape information such as <em>price, title, url,</em> <em>pagespeed,</em> and if possible the content inside the page too.</p> <p>Therefore I can direct campaigns at cities that might give the best return.</p> <p>Thank you all in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sorry-Praline3318\"> /u/Sorry-Praline3318 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfly7g/webscraping_as_means_to_optimize_google_ads/\">[link]</a></span> &#32; <span><a href=\"https://www.red",
        "id": 2366789,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfly7g/webscraping_as_means_to_optimize_google_ads",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webscraping as means to optimize Google Ads campaign?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T07:49:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a website that I have tried all possible methods to access using bot but no method ever worked. </p> <p>Can I share the website here or just ask questions without revealing the website.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/icodeAi\"> /u/icodeAi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfjp66/a_website_that_seems_impossible_to_access_using/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfjp66/a_website_that_seems_impossible_to_access_using/\">[comments]</a></span>",
        "id": 2365432,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfjp66/a_website_that_seems_impossible_to_access_using",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A website that seems impossible to access using bot",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T01:48:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>You know, I feel like not many people know this, but; </p> <p>Chrome dev console has AI assistance that can literally give you all the right tags and such instead of cracking your brain to inspect every html. To help make your web scraping life easier:</p> <p>You could ask to write a snippet to scrape all &lt;titles&gt; etc and it points out the tags for it. Though I haven\u2019t tried complex things yet. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NataPudding\"> /u/NataPudding </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfdzf7/chrome_ai_assistance/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jfdzf7/chrome_ai_assistance/\">[comments]</a></span>",
        "id": 2363675,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfdzf7/chrome_ai_assistance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Chrome AI Assistance",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-20T01:11:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve built a browser automation intensive application for a customer against that customer&#39;s testing ADP deployment.</p> <p>I&#39;m using Next.js with playwright and chromium. All of the browser automations work great, tested many times on the test instance.</p> <p>Unfortunately, in the production instance, there seems to be some type of challenge occurring at login that rejects my log-in attempt with a `400 Bad Request`.</p> <p>I&#39;ve tried switching to <a href=\"https://github.com/rebrowser/rebrowser-patches\">rebrowser-playwright</a>, running headful/headless, checked a bunch of bot detection sites on my browser instance to confirm nothing is obviously incorrect, and even tried running the automation on a hosted service where it also failed the log-in.</p> <p>I&#39;m curious where this community would advise me to go from here - I&#39;d be happy to pay for a service to help us accomplish this, but given even if the hosted service I tried f",
        "id": 2363503,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jfd8i2/automating_browser_actions_on_adp_enterprise_hr",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automating browser actions on ADP enterprise HR software?",
        "vote": 0
    }
]