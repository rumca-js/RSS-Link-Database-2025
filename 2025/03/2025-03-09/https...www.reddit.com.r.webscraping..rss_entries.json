[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T21:31:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! This is an update from anyone interested in this post: <a href=\"https://www.reddit.com/r/webscraping/comments/1iznqaz/comment/mf8nesm/?context=3\">https://www.reddit.com/r/webscraping/comments/1iznqaz/comment/mf8nesm/?context=3</a></p> <p>I wanted to share some recent <strong>fixes</strong> to my web scraping tool, <strong>Scrapeenator</strong>. After a lot of testing and feedback, I\u2019ve made several improvements and bug fixes to make it even better!</p> <h1>What\u2019s New?</h1> <ul> <li><strong>Dependency Management:</strong> Now, running <code>pip install -r requirements.txt</code> installs all dependencies seamlessly.</li> <li><strong>Flask Backend Setup:</strong> The backend now starts with a <code>run_flask.bat</code> file for easier setup.</li> <li><strong>Script Execution:</strong> Fixed issues related to PowerShell&#39;s execution policy by adding proper instructions for enabling it.</li> <li><strong>General Bug Fixes:</strong> A lot ",
        "id": 2283437,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j7i8wo/fixed_white_screen_for_scrapeenator_app",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fixed White screen For scrapeenator app",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T17:41:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I\u2019ve spent a lot of time learning web scraping and feel pretty confident with it now. I\u2019ve worked with different libraries, tried various techniques, and scraped a bunch of sites just for practice.</p> <p>The problem is, I don\u2019t know what to build next. I want to work on a project that\u2019s actually useful or at least a fun challenge, but I\u2019m kinda stuck on ideas.</p> <p>If you\u2019ve done any interesting web scraping projects or have any cool suggestions, I\u2019d love to hear them!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GriddyGriff\"> /u/GriddyGriff </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j7cz9k/need_some_cool_web_scraping_project_ideas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j7cz9k/need_some_cool_web_scraping_project_ideas/\">[comments]</a></span>",
        "id": 2282587,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j7cz9k/need_some_cool_web_scraping_project_ideas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need some cool web scraping project ideas!.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T17:10:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I come from gamedev. I want to try and build my first &quot;real&quot; site that doesn&#39;t use wordpress and uses some coding.</p> <p>I want to make a product guessing site where a random item is picked from amazon, temu or another similar site. The user would then have to guess the price and would be awarded points based on how close he or she was to the guess.</p> <p>You could pick from 1-4 players; all locally though.</p> <p>So, afaik, none of these sites give you an api for their products; instead I&#39;d have to scrape the data. Something like open random category, select random page from the category, then select random item from the listed results. I would then fetch the name, image and price.</p> <p>Question is, do I need a backend for this scraping? I was going to build a frontend only site, but if it&#39;s not very complicated to get into it, I&#39;d be open to making a backend. But I assume the scraper needs to run on some kind of server",
        "id": 2282268,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j7c9al/question_about_my_first_real_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about my first \"real\" website",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T15:05:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Let me share a bit about our website scraping experience. We scrape around 2,000 websites a day with a team of 7 programmers. We upload the data for our clients to our private NextCloud cloud \u2013 it&#39;s seriously one of the best things we&#39;ve found in years. Usually, we put the data in json/xml formats, and clients just grab the files via API from the cloud.</p> <p>We write our scrapers in .NET Core \u2013 it&#39;s just how it ended up, although Python would probably be a better choice. We have to scrape 90% of websites using undetected browsers and mobile proxies because they are heavily protected against scraping. We&#39;re running on about 10 servers (bare metal) since browser-based scraping eats up server resources like crazy :). I often think about turning this into a product, but haven&#39;t come up with anything concrete yet. So, we just do custom scraping of any public data (except personal info, even though people ask for that a lot).</p> <p>W",
        "id": 2281616,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j79i1r/our_website_scraping_experience_2k_websites_daily",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Our website scraping experience - 2k websites daily.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T12:48:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working on a web scraper on a large scale for screenshotting and i want to improve its ability to handle fingerprinting, im using </p> <ul> <li>puppeteer + puppeteer extra</li> <li>multiple instances</li> <li>proxies</li> <li>Dynamic generation of user agent and resolutions</li> </ul> <p>Are there other methods i can use?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Salazar_Ramondo\"> /u/Salazar_Ramondo </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j76xdj/web_scraping_guideline/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j76xdj/web_scraping_guideline/\">[comments]</a></span>",
        "id": 2282269,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j76xdj/web_scraping_guideline",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scraping guideline",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T10:24:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Ciao everyone! Noob here :)</p> <p>I&#39;m looking for suggestions about how to properly scrape hundreds of domains of crowdfunding platforms. My goal is to get the URL of each campaign listed there, starting from that platform domain list - then scrape all details for every campaign (such as capital raised, number of investors, and so on). </p> <p>The thing is: each platform has its own URL scheme (like <a href=\"http://www.platformdomain.com/project/campaign-name\">www.platformdomain.com/project/campaign-name</a>), and I dunno where to start correctly. I want to avoid initial mistakes. </p> <p>My first idea is to somehow get the sitemap for each one and/or scrape the homepage and get the &quot;projects&quot; page, where to start digging.</p> <p>Does someone have suggestions about this? I&#39;d appreciate it! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SpookyLibra45817\"> /u/SpookyLibra45817 </a> <br/> <span>",
        "id": 2280381,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j74t4q/crowdfunding_platforms_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Crowdfunding platforms scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T08:40:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My idea for this project was to web scrape Brazilian Jiu Jitsu gym websites and see whether or not they have a \u201copen mat\u201d class sessions available at their school. I\u2019ve tried to explore where it could be located on different websites and so far there have been 3 ways that I find most commonly:</p> <ol> <li>An image containing the class schedule </li> <li>A separate scheduling app (ex. Zenplanner)</li> <li>HTML tags that contain the text \u201copen mat\u201d</li> </ol> <p>I\u2019m really new to webscraping and don\u2019t really know the best direction to approach from here. Does anyone have tips on how I can proceed? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PsychologicalCry7840\"> /u/PsychologicalCry7840 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j73eyx/best_approach_to_project/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j73eyx/best_approach_to_project/",
        "id": 2279918,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j73eyx/best_approach_to_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Approach to Project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T06:03:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019ll be honest\u2014I don\u2019t know much about web scraping or coding. I had AI (ChatGPT and Claude) generate this script for me, and I\u2019ve put about 6-8 hours into it so far. Right now, it only scrapes a specific <a href=\"/r/horror\">r/horror</a> list on Letterboxd, but I want to expand it to scrape all lists from this source: <a href=\"https://letterboxd.com/dreadit/lists/\">Letterboxd Dreadit Lists</a>.</p> <p>I love horror movies and wanted a way to neatly organize <a href=\"/r/horror\">r/horror</a> recommendations, along with details like release date, trailer link, and runtime, in an Excel file.</p> <p>If anyone with web scraping experience could take a look at my code, I\u2019d love to know:</p> <ol> <li><p>Does it seem solid as-is?</p></li> <li><p>Are there any red flags I should watch out for?</p></li> </ol> <p>Also\u2014was there an easier way? Are there free or open-source tools I could have used instead? And honestly, was 6-8 hours too long ",
        "id": 2279485,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j716qo/new_to_web_scrapingdid_i_overcomplicate_this",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New to Web Scraping\u2014Did I Overcomplicate This?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-09T04:27:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello I have recently started a new job that is behind the times to say the least. It is a sales position serving trucking companies, asphalt companies, and dirt moving companies. Any company that requires a tarp to cover their load. With that being said a have purchased sales rabbit to help manage the mapping. However I need the data (business name, address, and phone number) from my research I think this can be done through scraping and the data can be put into a spreadsheet then uploaded to sales rabbit. Is this something anyone can help with? I would need Alabama Florida Georgia South Carolina North Carolina and Tennessee </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ConfidentEducator560\"> /u/ConfidentEducator560 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1j6zn1i/need_help_looking_to_build_a_spreadsheet_need_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/",
        "id": 2279341,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1j6zn1i/need_help_looking_to_build_a_spreadsheet_need_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help looking to build a spreadsheet need data",
        "vote": 0
    }
]