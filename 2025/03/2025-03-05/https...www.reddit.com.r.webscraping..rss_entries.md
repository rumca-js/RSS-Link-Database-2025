# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## What am I legally and not legally allowed to scrap?
 - [https://www.reddit.com/r/webscraping/comments/1j4h6rd/what_am_i_legally_and_not_legally_allowed_to_scrap](https://www.reddit.com/r/webscraping/comments/1j4h6rd/what_am_i_legally_and_not_legally_allowed_to_scrap)
 - RSS feed: $source
 - date published: 2025-03-05T23:16:46+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;ve dabbled with beautifulsoup and can throw together a very basic webscrapper when I need to. I was contacted to essentally automate a task an employee was doing. They we&#39;re going to a metal market website and gabbing 10 excel files everyday and compiling them. This is easy enough to automate however my concern is that the data is not static and is updated everyday so when you download a file an api request is sent out to a database.</p> <p>While I can still just automate the process of grabbing the data day by day to build a larger dataset would it be illegal to do so? Their api is paid for so I can&#39;t make calls to it but I can just simulate the download process using some automation. Would this technically be illegal since I&#39;m going around the API? All the data I&#39;m gathering is basically public as all you need to do is create an account and you can start downloading files I&#39;m just automating the download. Thanks!</p> </div

## Need suggestion on scraping retail stores product prices and details
 - [https://www.reddit.com/r/webscraping/comments/1j4gdek/need_suggestion_on_scraping_retail_stores_product](https://www.reddit.com/r/webscraping/comments/1j4gdek/need_suggestion_on_scraping_retail_stores_product)
 - RSS feed: $source
 - date published: 2025-03-05T22:42:31+00:00

<!-- SC_OFF --><div class="md"><p>So basically I am looking to scrape multiple websites product prices for the same product (e.g iPhone 16) so that at the end I have list of products with prices from all different stores.</p> <p>The biggest pain point is having unique identifier for each product. I created some very complicated fuzzy search scoring solution but apparently it doesnâ€™t work for most of the cases and it is very tied to certain group - mobile phones.</p> <p>Also I am only going through product catalogs but not product details. Furthermore, for each different website I have different selectors and price extracting. Since I am using Claude to help itâ€™s quite fast.</p> <p>Can somebody suggest alternative solution or should I just create different implementations for each website. I will likely have 10 websites which I need to scrap once per day, gather product prices and store them in my own database but still uniquely identifying a product will be a pain point. I am current

## FBREF scraping
 - [https://www.reddit.com/r/webscraping/comments/1j4fmxj/fbref_scraping](https://www.reddit.com/r/webscraping/comments/1j4fmxj/fbref_scraping)
 - RSS feed: $source
 - date published: 2025-03-05T22:12:49+00:00

<!-- SC_OFF --><div class="md"><p>Has anyone recently been able to scrape the data from FBRef? I had some code that was doing its job until 2024 - but right now it is not working</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/GoldStandard5432"> /u/GoldStandard5432 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j4fmxj/fbref_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j4fmxj/fbref_scraping/">[comments]</a></span>

## Robust Approach for Capturing M3U8 Links with Selenium C#
 - [https://www.reddit.com/r/webscraping/comments/1j4deb2/robust_approach_for_capturing_m3u8_links_with](https://www.reddit.com/r/webscraping/comments/1j4deb2/robust_approach_for_capturing_m3u8_links_with)
 - RSS feed: $source
 - date published: 2025-03-05T20:43:08+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone,</p> <p>Iâ€™m building a desktop app that scrapes app metadata and visual assets (images and videos).<br/> Iâ€™m using Selenium C# to automate the process. </p> <p>So far, everything is going well, but Iâ€™ve run into a challenge with Appleâ€™s App Store. Since they use adaptive streaming for video trailers, the videos arenâ€™t directly accessible as standard files. I know of two ways to retrieve them:</p> <ul> <li>Using network monitor to find the M3U8 file url.</li> <li>Waiting for the page to load and extracting the M3U8 file url from the page source.</li> </ul> <p>I wanted to ask if thereâ€™s a better, simpler, and more robust method than these.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/manofspirit"> /u/manofspirit </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j4deb2/robust_approach_for_capturing_m3u8_links_with/">[link]</a></span> &#32; <span><a href="https://

## Scraping AP Photos
 - [https://www.reddit.com/r/webscraping/comments/1j4a3vf/scraping_ap_photos](https://www.reddit.com/r/webscraping/comments/1j4a3vf/scraping_ap_photos)
 - RSS feed: $source
 - date published: 2025-03-05T18:31:15+00:00

<!-- SC_OFF --><div class="md"><p>Is it possible to scrape the <a href="https://newsroom.ap.org/home/search?query=aptopix&amp;mediaType=photo&amp;st=keyword">AP Newsroom Photos</a> page? My company pays for it, so I have a login. The UI is a huge pain to deal with, though, when downloading multiple images. My problem is the HTML seems to be called up by Javascript, so I don&#39;t know how to get through that while also logging in with my credentials. Should I just give up and use their clunky UI?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Ok-Barber380"> /u/Ok-Barber380 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j4a3vf/scraping_ap_photos/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j4a3vf/scraping_ap_photos/">[comments]</a></span>

## Anti-Detect Browser Analysis: How To Detect The Undetectable Browser?
 - [https://www.reddit.com/r/webscraping/comments/1j499n5/antidetect_browser_analysis_how_to_detect_the](https://www.reddit.com/r/webscraping/comments/1j499n5/antidetect_browser_analysis_how_to_detect_the)
 - RSS feed: $source
 - date published: 2025-03-05T17:58:08+00:00

<!-- SC_OFF --><div class="md"><p>Disclaimer: I&#39;m on the other side of bot development; my work is to detect bots.<br/> I wrote a long blog post about detecting the Undetectable anti-detect browser. I analyze JS scripts they inject to lie about the fingerprint, and I also analyze the browser binary to have a look at potential lower-level bypass techniques. I also explain how to craft a simple JS detection challenge to identify/detect Undectable.</p> <p><a href="https://blog.castle.io/anti-detect-browser-analysis-how-to-detect-the-undetectable-browser/">https://blog.castle.io/anti-detect-browser-analysis-how-to-detect-the-undetectable-browser/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/antvas"> /u/antvas </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j499n5/antidetect_browser_analysis_how_to_detect_the/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j499n5/antidetect_b

## Scraping a Pesky Apex Line Plot
 - [https://www.reddit.com/r/webscraping/comments/1j3tw0h/scraping_a_pesky_apex_line_plot](https://www.reddit.com/r/webscraping/comments/1j3tw0h/scraping_a_pesky_apex_line_plot)
 - RSS feed: $source
 - date published: 2025-03-05T03:31:52+00:00

<!-- SC_OFF --><div class="md"><p>I wish to scrape the <a href="https://www.congestion-pricing-tracker.com/">second line plot</a>, the plot of NYC and Boston/Chicago into a Python df. The issue is that the datapoints are generated dynamically, so Python&#39;s requests can&#39;t get to it.. and I don&#39;t know how to find any of the time series data points when I inspect them. I also already tried to look for any latent APIs in the network tab... and unless I&#39;m missing something, there doesn&#39;t appear to be one. Anybody know where I might begin here? Even if I could get python to return the values (say, 13 for NY Congestion zone and 17 for Boston/Chicago on December 19), I could handle the rest. Any ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/turingincarnate"> /u/turingincarnate </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3tw0h/scraping_a_pesky_apex_line_plot/">[link]</a></span> &#32; <span><a href

## I need help to scrape this web
 - [https://www.reddit.com/r/webscraping/comments/1j3r9zm/i_need_help_to_scrape_this_web](https://www.reddit.com/r/webscraping/comments/1j3r9zm/i_need_help_to_scrape_this_web)
 - RSS feed: $source
 - date published: 2025-03-05T01:19:43+00:00

<!-- SC_OFF --><div class="md"><p>I have been at it for a week, now I need help, I want to scrape data from Chrono24.com for my machine learning project , I have tried Selenium and undetected Chromedriver, yet Iâ€™m unable. Turned off my VPN and everything I know. Can someone, anyone help. ðŸ¥¹ Thank you </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/kofikwakye"> /u/kofikwakye </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3r9zm/i_need_help_to_scrape_this_web/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3r9zm/i_need_help_to_scrape_this_web/">[comments]</a></span>

## I need a puppeteer scrip to download rendered CSS on a page
 - [https://www.reddit.com/r/webscraping/comments/1j3qcjl/i_need_a_puppeteer_scrip_to_download_rendered_css](https://www.reddit.com/r/webscraping/comments/1j3qcjl/i_need_a_puppeteer_scrip_to_download_rendered_css)
 - RSS feed: $source
 - date published: 2025-03-05T00:35:11+00:00

<!-- SC_OFF --><div class="md"><p>I have limited coding skills but with the help of ChatGPT I have installed Python and Puppetteer and used basic test scripts and some poorly written scripts that fail consistently (error in writing by ChatGPT.</p> <p>Not sure if a general js script that someone else has written will do what I need.</p> <p>Site uses 2 css files. One is a generic CSS file added by a website builder. It has lots of css not required for render</p> <p>PurgeCSS tells me 25% is not used</p> <p>Chrome Coverage tells me 90% is not used. I suspect this is more accurate. However the file is so large I cannot scroll and remove the rendered css.</p> <p>so if anyone can tell me where I can get a suitable JS scripts i would appreciate it. Preferably a script that would target the specific generic css file (though not critical)</p> <p>script typo in title noted. cannot edit.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Colink2"> /u/Colink2 <

