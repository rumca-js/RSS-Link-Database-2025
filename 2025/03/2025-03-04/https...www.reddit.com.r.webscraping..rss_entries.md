# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Fastest way to scrape millions of images?
 - [https://www.reddit.com/r/webscraping/comments/1j3o05o/fastest_way_to_scrape_millions_of_images](https://www.reddit.com/r/webscraping/comments/1j3o05o/fastest_way_to_scrape_millions_of_images)
 - RSS feed: $source
 - date published: 2025-03-04T22:48:36+00:00

<!-- SC_OFF --><div class="md"><p>Hello, I&#39;m trying to create a database of image URLs across the web for a sideproject, and would need some help. Right now I am using scrapy with rotating proxies &amp; user agents, along with random 100 domains as starting points. I am getting about 2000 images per day.</p> <p>Is there a way to make the scraping process faster &amp; more efficient? Also, I would like to scrape as much of the internet as possible, how could I programm it like so instead of just 100 domains I manually typed?</p> <p>Machine #1: Windows 11, 32GB DDR4 RAM, 10TB Storage, i7 CPU, GTX 1650 GPU, 5Gbps Internet, Machine #2: Windows 11, 32 GB DDR3 RAM, 7TB Storage, i7 CPU, No GPU, 1Gbps Internet, Machine #3 (VPS): Ubuntu Server 24, 1GB RAM, 100Mbps Internet, Unknown CPU.</p> <p>I just want to store the image URLs, not imagesüòÉ.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/makedonc"> /u/makedonc </a> <br/> <span><a hre

## Need Help with request package
 - [https://www.reddit.com/r/webscraping/comments/1j3ntno/need_help_with_request_package](https://www.reddit.com/r/webscraping/comments/1j3ntno/need_help_with_request_package)
 - RSS feed: $source
 - date published: 2025-03-04T22:40:59+00:00

<!-- SC_OFF --><div class="md"><p>How to register on a website using python request package if it has a captcha validation. Actually I am sending a payload to a website server using appropriate headers and all necessary details. but the website has a captcha validation which needs to validate before registering and I shall put the captcha answer in the payload in order to get successfully registered.... Please help!!!! I am newbie.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LICIOUS_INSAAN"> /u/LICIOUS_INSAAN </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3ntno/need_help_with_request_package/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3ntno/need_help_with_request_package/">[comments]</a></span>

## scraping local service ads?
 - [https://www.reddit.com/r/webscraping/comments/1j3nkf8/scraping_local_service_ads](https://www.reddit.com/r/webscraping/comments/1j3nkf8/scraping_local_service_ads)
 - RSS feed: $source
 - date published: 2025-03-04T22:29:55+00:00

<!-- SC_OFF --><div class="md"><p>I have someone that wants to scrape local service ads and doesn&#39;t seem like a normal scrapers picks up on them. </p> <p>But found <a href="https://www.google.com/localservices/prolist?g2lbs=AAEPWCvbS5ZEOkTZcPiWMlINMNpcrAaICaw3LjcBvfK_MK4rU0d0z7RFSw783r18_mbhjOncOT8e&amp;hl=en-US&amp;gl=us&amp;cs=1&amp;ssta=1&amp;src=1&amp;gsas=1&amp;slp=IhFyb29mX2luc3RhbGxhdGlvbjIXChUaEwoRcm9vZl9pbnN0YWxsYXRpb246nAVDaE1JOC1qM3A1Nlhpd01WMXJaYUJSMzdld2VMRWlZSUJCQUJHZ3dJMjVfVGt3UVEzcU92N2drZ3RPUG1TakNBMEtWTU9OcXhzeVZJZ09pU0poSW1DQVFRQWhvTUNLZjkyb1VHRUtDUTJKVWtJTWEzN0Vnd2dOQ2xURGpqbTdZa1NJRG9raVlTSmdnRUVBTWFEQWk1cnR1SEdSQzk5Zkt2SVNDMzdKbEhNSURRcFV3NG1fYk1JMGlBNkpJbUVpWUlCQkFFR2d3SWhjQ0Iwd01RMi1YSjRRa2d4dFBRU1RDQTBLVk1PT09wNkNSSWdPaVNKaEltQ0FRUUJSb01DSS1VMVlZWUVLX3AxXzBZSUpuUV9ra3dnTkNsVERpTXFQOGtTSURva2lZU0pnZ0VFQVlhREFqTy1fQ0dBUkRzdktLdUNTQ2pvT3hITUlEUXBVdzRrWkQySTBpQTZKSW1FaVVJQkJBSEdnc0k3cmJMWGhDMXJJYXpDU0NJMW9sSU1JRFFwVXc0aE91RUpFaUE2SkltRWlZSUJCQUlHZ3dJcnMzLXB4a

## Storing images
 - [https://www.reddit.com/r/webscraping/comments/1j3naa3/storing_images](https://www.reddit.com/r/webscraping/comments/1j3naa3/storing_images)
 - RSS feed: $source
 - date published: 2025-03-04T22:17:47+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m scraping around 20000 images each night, convert them to wepb and also generate a thumbnail for each of them. This stresses my CPU for several hours. So I&#39;m looking for something more efficient. I started using an old GPU (with openCL), wich works great for resizing, but encoding as webp can only be done with a CPU it seems. I&#39;m using C# to scrape and resize. Any ideas or tools to speed it up without buying extra hardware? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/berghtn"> /u/berghtn </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3naa3/storing_images/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3naa3/storing_images/">[comments]</a></span>

## How to handle proxies and user agents
 - [https://www.reddit.com/r/webscraping/comments/1j3ja58/how_to_handle_proxies_and_user_agents](https://www.reddit.com/r/webscraping/comments/1j3ja58/how_to_handle_proxies_and_user_agents)
 - RSS feed: $source
 - date published: 2025-03-04T19:31:29+00:00

<!-- SC_OFF --><div class="md"><p>Scraping websites have become a headache because of this.so I need a solution(free) for this .I saw a bunch of websites which gives them for a monthly fee but I wanna ask if there is something I can use for free and works</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Perry_2013"> /u/Perry_2013 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3ja58/how_to_handle_proxies_and_user_agents/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3ja58/how_to_handle_proxies_and_user_agents/">[comments]</a></span>

## Best Practices and Improvements
 - [https://www.reddit.com/r/webscraping/comments/1j3hi6r/best_practices_and_improvements](https://www.reddit.com/r/webscraping/comments/1j3hi6r/best_practices_and_improvements)
 - RSS feed: $source
 - date published: 2025-03-04T18:19:44+00:00

<!-- SC_OFF --><div class="md"><p>Hi guys, I have a list of names and I need to build profiles for these People (e.g. bring the education history). It is hundreds of thousands of names. I am trying to google the names and bring the urls in the first page and then extract the content. I am already using a proxy, but I don&#39;t know if I am doing it right, I am using scrapy and at some point the requests start failing. I already tried:</p> <p>1 - tune concurrent requests limit 2 - tune retry mechanism 3 - run multiple instances using GNU parallel and spliting my input data</p> <p>I just one proxy, I don&#39;t know if it is enough and I am relying too much on it, so I&#39;d like to hear best practices and advices for this situation. Thanks in advance</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/NumerousRush7001"> /u/NumerousRush7001 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3hi6r/best_practices_and_improvements/"

## Scraping Unstructured HTML
 - [https://www.reddit.com/r/webscraping/comments/1j3g6a3/scraping_unstructured_html](https://www.reddit.com/r/webscraping/comments/1j3g6a3/scraping_unstructured_html)
 - RSS feed: $source
 - date published: 2025-03-04T17:26:28+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m working on a web scraping project that should extract data even from unstructured HTML. </p> <p>I&#39;m looking at some basic structure like</p> <pre><code>&lt;div&gt;...&lt;.div&gt; &lt;span&gt;email&lt;/span&gt; email@address.com &lt;div&gt;...&lt;/div&gt; </code></pre> <p>note that the [<code>email@address.com</code>](mailto:<a href="mailto:email@address.com">email@address.com</a>) is not wrapped in any HTML element.</p> <p>I&#39;m using <a href="https://github.com/cheeriojs/cheerio">cheeriojs</a> and any suggestions would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/another_devops_guy"> /u/another_devops_guy </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3g6a3/scraping_unstructured_html/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3g6a3/scraping_unstructured_html/">[comments]</a></span>

## Comparing .cvs files
 - [https://www.reddit.com/r/webscraping/comments/1j3fexl/comparing_cvs_files](https://www.reddit.com/r/webscraping/comments/1j3fexl/comparing_cvs_files)
 - RSS feed: $source
 - date published: 2025-03-04T16:56:15+00:00

<!-- SC_OFF --><div class="md"><p>I scraped followers of an insta account on two different occasions and have cvs files, i want to know how i can ‚Äúcompare‚Äù the two files to see which followers the user gained in the time between the files. An easy way preferably </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/lumpybucket"> /u/lumpybucket </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3fexl/comparing_cvs_files/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3fexl/comparing_cvs_files/">[comments]</a></span>

## Ai powered scraper
 - [https://www.reddit.com/r/webscraping/comments/1j3evpx/ai_powered_scraper](https://www.reddit.com/r/webscraping/comments/1j3evpx/ai_powered_scraper)
 - RSS feed: $source
 - date published: 2025-03-04T16:34:39+00:00

<!-- SC_OFF --><div class="md"><p>i want to build a tool where i give the data to an llm and extract the data using it is the best way is to send the html filtered (how to filtrate it the best way) or by sending a screenshot of the website or what is the optimal way and best llm model for that</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Mouradis"> /u/Mouradis </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3evpx/ai_powered_scraper/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3evpx/ai_powered_scraper/">[comments]</a></span>

## Weekly Webscrapers - Hiring, FAQs, etc
 - [https://www.reddit.com/r/webscraping/comments/1j3a8sj/weekly_webscrapers_hiring_faqs_etc](https://www.reddit.com/r/webscraping/comments/1j3a8sj/weekly_webscrapers_hiring_faqs_etc)
 - RSS feed: $source
 - date published: 2025-03-04T13:01:24+00:00

<!-- SC_OFF --><div class="md"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels‚Äîwhether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href="https://webscraping.fyi">Beginners Guide</a> üå±</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href="https://reddit.com/r/webscraping/about/sticky?num=1">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AutoModerator"> /u/AutoModerator </a> <br/> <span><a href="https://www.reddit.com/r/webscrapin

## Detecting proxies server-side using TCP handshake latency?
 - [https://www.reddit.com/r/webscraping/comments/1j395h5/detecting_proxies_serverside_using_tcp_handshake](https://www.reddit.com/r/webscraping/comments/1j395h5/detecting_proxies_serverside_using_tcp_handshake)
 - RSS feed: $source
 - date published: 2025-03-04T11:58:33+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;ve recently came across this concept that detects proxies and VPNs by comparing the TCP handshake time and RTT using Websocket. If these two times do not match up, it could mean that a proxy is being used. Here&#39;s the concept: <a href="https://incolumitas.com/2021/06/07/detecting-proxies-and-vpn-with-latencies/">https://incolumitas.com/2021/06/07/detecting-proxies-and-vpn-with-latencies/</a> </p> <p>Most VPN and proxy detection APIs rely on IP databases, but here&#39;s the two real-world implementations of the concept that I found: </p> <ul> <li> <a href="https://proxy.incolumitas.com/proxy_detect.html">https://proxy.incolumitas.com/proxy_detect.html</a> (original concept - check the &quot;Latency Test&quot;) </li> <li><a href="https://obfusgated.com/en/tools/vpn-detection-test">https://obfusgated.com/en/tools/vpn-detection-test</a> (seems to use the very same detection idea)</li> </ul> <p>From my tests, both tests are pretty accurate when i

## Can a website behave differently when dev tools are opened?
 - [https://www.reddit.com/r/webscraping/comments/1j3653k/can_a_website_behave_differently_when_dev_tools](https://www.reddit.com/r/webscraping/comments/1j3653k/can_a_website_behave_differently_when_dev_tools)
 - RSS feed: $source
 - date published: 2025-03-04T08:19:33+00:00

<!-- SC_OFF --><div class="md"><p>Or at least stop responding to requests? Only if I tweak something in js console, right? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/yyavuz"> /u/yyavuz </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j3653k/can_a_website_behave_differently_when_dev_tools/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j3653k/can_a_website_behave_differently_when_dev_tools/">[comments]</a></span>

## Free proxy list for my wrb scrapping project
 - [https://www.reddit.com/r/webscraping/comments/1j319wc/free_proxy_list_for_my_wrb_scrapping_project](https://www.reddit.com/r/webscraping/comments/1j319wc/free_proxy_list_for_my_wrb_scrapping_project)
 - RSS feed: $source
 - date published: 2025-03-04T03:15:23+00:00

<!-- SC_OFF --><div class="md"><p>Hi, i need a free proxy list for pass a captcha , if somebody knows a free proxy comment below please, thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/idk5454y66"> /u/idk5454y66 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j319wc/free_proxy_list_for_my_wrb_scrapping_project/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j319wc/free_proxy_list_for_my_wrb_scrapping_project/">[comments]</a></span>

## Scraping older documents or new requirements
 - [https://www.reddit.com/r/webscraping/comments/1j2yggi/scraping_older_documents_or_new_requirements](https://www.reddit.com/r/webscraping/comments/1j2yggi/scraping_older_documents_or_new_requirements)
 - RSS feed: $source
 - date published: 2025-03-04T00:49:55+00:00

<!-- SC_OFF --><div class="md"><p>Wondering how others have approached the scenario where websites changing over time so you have updated your parsing logic over time to reflect the new state but then have a need to reparse html from the past. </p> <p>A similar situation is being requested to get a new data point on a site and needing to go back through archived html to get the new data point through history. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/KBaggins900"> /u/KBaggins900 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2yggi/scraping_older_documents_or_new_requirements/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2yggi/scraping_older_documents_or_new_requirements/">[comments]</a></span>

