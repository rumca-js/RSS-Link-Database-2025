[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T23:32:32+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmzbzv/is_my_drive_dying_how_can_i_move_data_from_it/\"> <img src=\"https://preview.redd.it/kavnibjarpre1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed7cf04913d199c39203930cd8a7d2809a5ffeb3\" alt=\"Is my drive dying? How can I move data from it when it's only doing a few kb/s?\" title=\"Is my drive dying? How can I move data from it when it's only doing a few kb/s?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/drweavil\"> /u/drweavil </a> <br/> <span><a href=\"https://i.redd.it/kavnibjarpre1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmzbzv/is_my_drive_dying_how_can_i_move_data_from_it/\">[comments]</a></span> </td></tr></table>",
        "id": 2440219,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmzbzv/is_my_drive_dying_how_can_i_move_data_from_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/kavnibjarpre1.png?width=640&crop=smart&auto=webp&s=ed7cf04913d199c39203930cd8a7d2809a5ffeb3",
        "title": "Is my drive dying? How can I move data from it when it's only doing a few kb/s?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T23:18:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I put my computer in the back room and it goes from -10c to about +5. Never had problems until I moved my unix server out back. I know for solid state it&#39;s probably better to be cold - but these SMR/CMR disks whatever they are - could it just be the cold killing the drives?</p> <p>Long story: I had my computer in the house. moved about 4tb of data to the disks, Moved the computer to the back room for a long time and both drives had click of death after 4 month of no power. So I didn&#39;t let them idle with the click of death.</p> <p>Flipped them over, a trick I learned as a kid in the 80s (long story) and copied my data off but now I wonder what the root cause is.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cleuseau\"> /u/cleuseau </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmz1u2/two_disks_click_of_death_6tb_wd_i_recued_one_by/\">[link]</a></span> &#32; <span><a href=\"https:/",
        "id": 2440218,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmz1u2/two_disks_click_of_death_6tb_wd_i_recued_one_by",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Two disks, click of death (6tb) WD. I recued one by flipping it upside down. Could temperature be killing my disks?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T22:36:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone know how to download videos from Freeform?</p> <p>For example, my daughter likes Switched at Birth:</p> <p><a href=\"https://www.freeform.com/episode/1af334f4-a1b8-4bfe-abe4-bd1aa5f03d99\">https://www.freeform.com/episode/1af334f4-a1b8-4bfe-abe4-bd1aa5f03d99</a></p> <p>I&#39;ve tried quite a few downloaders, but none seem to work. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Wonton1111\"> /u/Wonton1111 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmy5m8/downloading_videos_from_freeform/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmy5m8/downloading_videos_from_freeform/\">[comments]</a></span>",
        "id": 2440216,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmy5m8/downloading_videos_from_freeform",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading Videos from Freeform",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T22:24:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I 100% know for a fact I uploaded / saved / backed them up. Infact, most things are uploaded twice. The cloud services I&#39;ve used / still use, in order of most to least: </p> <p>1) Google Drive</p> <p>2) pCloud</p> <p>3) OneDrive</p> <p>4) Samsung Notes (I own a Samsung laptop and phone, but the PDFs I&#39;m looking for would also show up in the above platforms)</p> <p>*) I also have a total of 10TB of local storage, with a strong liklihood of also being on local storage. During the times when I&#39;ve needed storage, PDFs are at the very bottom of the priority list of items to delete. Even duplicate PDFs don&#39;t get deleted. I&#39;ve completed indexing of all 10TB inside of Windows 11, but there&#39;s far too many documents to search though. Adobe Reader freezes then crashes when attempting to search. </p> <p>I&#39;ve manually looked. I&#39;ve searched &quot;checking account statements from &lt;date&gt;&quot;. I have my paystubs from that time ",
        "id": 2440217,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmxx8a/easily_searching_through_tens_of_thousands_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Easily searching through tens of thousands of PDFs hosted on cloud & local storage, based on contents?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T22:19:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m very passionate about archiving and new to data hoarding, but this is something tailor made for how my brain works. With the concerning trends of data disappearing in the US I feel panicked like I need to start grabbing everything, but I don&#39;t know where to start. What is in danger? Where are people needed? Can I get hooked up with other people doing the same thing so that I can work efficiently and not just duplicate someone else&#39;s efforts? </p> <p>I&#39;d appreciate a little crash course on how to get started on this. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ExtendedPlay7\"> /u/ExtendedPlay7 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmxt1s/what_can_i_do/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmxt1s/what_can_i_do/\">[comments]</a></span>",
        "id": 2439983,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmxt1s/what_can_i_do",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What can I do?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T21:44:18+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmx2gz/trouble_recovering_from_mini_cdrw_sony_mavica/\"> <img src=\"https://b.thumbs.redditmedia.com/sQSFN1jCkybAlqqn_SgJT7I-5571ehWZj1B1OpBbvTg.jpg\" alt=\"Trouble Recovering from mini CD-RW (Sony Mavica Camera)\" title=\"Trouble Recovering from mini CD-RW (Sony Mavica Camera)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>TLDR; I&#39;m trying to get data from my late papa&#39;s Sony MVC-CD350 Mavica-based (mini CD-RW) digital camera from ~2001. I&#39;ve tried everything I can think of and can not get the data onto a PC.</p> <p>Things I know so far:</p> <ul> <li>The data is perfectly readable from the camera itself (pics <em>and</em> vids, and plenty) - no read errors whatsoever from the camera&#39;s browser</li> <li>Windows sees absolutely nothing when connecting the cam directly via mini USB; the cam&#39;s manual states I should be able to connect and it should just pick it right up as a media or storage devi",
        "id": 2439984,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmx2gz/trouble_recovering_from_mini_cdrw_sony_mavica",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/sQSFN1jCkybAlqqn_SgJT7I-5571ehWZj1B1OpBbvTg.jpg",
        "title": "Trouble Recovering from mini CD-RW (Sony Mavica Camera)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T21:06:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello friends. Wondering about if I understand how an expander works in a jbod \\ server setup and limitations. If I understand it correctly you can use an expander on any SAS (example I have a LSI 9207 8i) and could use an expander (let&#39;s say a backplane I&#39;ve found that has 24 drive capacity LSI 2X36). From information I&#39;ve gathered looks like if you use one port of the SAS it won&#39;t be as fast but if use 2 to the SAS card it&#39;ll be faster. I&#39;m going to assume the speeds will be limited to the SAS capabilities?</p> <p>On the same vein of connectivity. Can you take two separate expanders and run them to the same SAS? Or is it better practice to run separate SAS for each expander. Also I see some specifications for the cables being mini SFF and I guess regular SFF? Also seems the standard is SFF 8087? Is that the port on the cards or the cable standard? </p> <p>Also it seems that expander cards only need power but are usually PCIE",
        "id": 2439675,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmw9kn/looking_for_clarity_on_sas_and_expanders",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for clarity on SAS and expanders",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T20:10:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all! I collect vintage magazines and want to digitize them before turning them into collage pieces. I&#39;d love to upload them to Pinterest/Internet Archive so others can enjoy them too.</p> <p>The catch is\u2014I&#39;m a teenager and don\u2019t have the time to scan them myself. \ud83d\ude05</p> <p>Would anyone be willing to help me scan them, or know of someone who offers affordable or even community-based scanning services? I\u2019m totally open to mailing them (if you\u2019re trusted or have a portfolio.)</p> <p>Thanks so much in advance\u2014I&#39;d really love to preserve and share these before they become part of my art!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Playful-Bank8870\"> /u/Playful-Bank8870 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmv1z8/looking_for_someone_to_help_scan_my_vintage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmv1z8/looking_for_so",
        "id": 2439404,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmv1z8/looking_for_someone_to_help_scan_my_vintage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for Someone to Help Scan My Vintage Magazines (Teen on a Budget)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T18:51:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there. I&#39;m trying to download all the replies in a single tweet on twitter/x but all gallery dl is doing is grabbing the main post&#39;s image and I want to grab all the images/videos in replies.</p> <p>I don&#39;t have a config file and find those confusing. I&#39;m just doing a command line in the command window.<br/> gallery-dl -o &quot;username=&lt;username&gt;&quot; -o &quot;password=&lt;password&gt;&quot; &quot;URL&quot; </p> <p>So what do I need to add to get all the replies to a single tweet? HELP!</p> <p>Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Imaginos9\"> /u/Imaginos9 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmtbqg/gallerydl_help_with_a_tweet_please/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmtbqg/gallerydl_help_with_a_tweet_please/\">[comments]</a></span>",
        "id": 2439674,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmtbqg/gallerydl_help_with_a_tweet_please",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Gallery-dl help with a tweet please!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T18:30:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m an organized digital hoarder and also have OCD. What has helped you overcome your digital hoarding?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Stormy1956\"> /u/Stormy1956 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmsv5n/i_was_not_raised_with_the_internet_and_just/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmsv5n/i_was_not_raised_with_the_internet_and_just/\">[comments]</a></span>",
        "id": 2439140,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmsv5n/i_was_not_raised_with_the_internet_and_just",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I was not raised with the internet and just became aware of digital hoarding.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T18:14:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi I have a decent volume of media files and also a decent volume of files and other data. I do &quot;software raid&quot;/sync across a pair of 24 TB Hdds and a pair of 14 TB Hdds on my main desktop which also acts as my Plex server for the time being. </p> <p>Backup wise, I am limited in means so I have 1 external 18TB Hdd which i want to act as the offline backup for the 24TB pair for the time being since I&#39;m not close to 18TB data on the 24TB yet. And I do have a 14TB external drive to act as offline backup for the 14TB mirror. </p> <p><strong>QUESTION:</strong></p> <p>For this offline data, is it better to just use macrium to image the drives/folders and this way allows me to have multiple images of the same drive/folder as a sort of time machine, storing different instances of thse drives (I assume this is possible because macrium compresses) image files? If not is there an app that <strong>creates compressed backups</strong> of folder/drive",
        "id": 2439141,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmshmx/which_backup_practice_is_better",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which backup Practice is Better?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T18:10:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have several 8TB external drives at home, was using Windows for years. Today I bought a MAC Mini and was trying to make the switch. Just for testing I connected all my drives onto MAC via powered USB Hub. Power should be enough bec this is how I was using it with Windows PC.</p> <p>Anyway later on I had to connect external drives to PC again. Then I realised there is a huge &quot;3TB free out of 8TB&quot; label on the drive. The disk was almost full, I know it. In the root of the drive I see a folder called &quot;Spotlight&quot; , also some MAC related folders. </p> <p>For the deleted files: Some are completely disappeared and some are showing as 0KB or 2MB, (normally they are much bigger)</p> <p>I don&#39;t know what the hell happened but I can&#39;t see these files now, they are gone. I didn&#39;t even do anything. All I did was plugging it into mac and thats it. Now is there a way I can recover this data? Maybe the files are still there but its ",
        "id": 2438795,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmse6d/connected_my_external_drive_to_mac_and_lost",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Connected my external drive to MAC and lost around 3TB of data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T17:45:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I am in need of more storage and am thinking of getting a ~14TB drive to replace one of my old 4TB ones. Is it a respectable strategy to grab a cheap refurb drive with a warranty and use BackBlaze as a backup? I have had very good luck with HDs over the years but want to be safe if possible.</p> <p>Edit: I could, for example, get a Seagate 20 TB refurb for $250, but I am aware that those are notoriously unreliable. The other option would be something like a new 16 TB Toshiba MN08 at the same price point.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GransurgBlackmore\"> /u/GransurgBlackmore </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmruby/storage_strategies/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmruby/storage_strategies/\">[comments]</a></span>",
        "id": 2438796,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmruby/storage_strategies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Storage Strategies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T16:19:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have all the information of nearly hundreds of lost media YouTube videos with all the information archived but I wonder if there\u2019s a chance if I can find them by using the description,like count, view count, name, thumbnail,date of creation, and links. It\u2019s just that I don\u2019t have the video I\u2019m looking for itself. (I originally posted this on <a href=\"/r/Archiveteam\">r/Archiveteam</a> but they suggested me post it here for more answers.) and no they aren\u2019t archived anywhere like on the web archive </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Funnyman959\"> /u/Funnyman959 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmpw3q/is_there_a_way_i_can_get_a_youtube_video_thats/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmpw3q/is_there_a_way_i_can_get_a_youtube_video_thats/\">[comments]</a></span>",
        "id": 2438431,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmpw3q/is_there_a_way_i_can_get_a_youtube_video_thats",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a way I can get a YouTube video that\u2019s lost media by having all the information about it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T16:15:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been banging my head with this for the last three days and I&#39;m coming at a bit of an impasse. My goal is to start moving to linux, and have a data pool/raid with my personal/game files being able to be freely used between a Linux and Windows installation on a DualBoot system. </p> <p>Things that I have ruled out for the following reasons/asumptions. </p> <p>Motherboard RAID: RAID may not be able to be read by another motherboard if current board fails. </p> <p>Snap RAID: This was the most promising, however, it all fell apart when i found there isn&#39;t a cross platform Merge/UnionFS solution to pool all the drives into one. You either have to use MergeFS/UnionFS on linux, or DrivePool on Windows. </p> <p>ZFS: This also looked promising, However, it looks like the Windows version of Open ZFS is not considered stable. </p> <p>BTRFS: Again, also looked promising. However, the Windows BTRFS driver is also not considered stable. </p> <p>",
        "id": 2438430,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmpsmq/shared_software_unionraid_array_between_a_windows",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Shared software Union/RAID array between a windows and linux dual boot.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T15:52:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m in the market to buy a new NAS for mainly storage and PLEX use. I know I want a 6-bay model (using 6 x 10TB drives) but am not sure which brand/model to go with. I&#39;m currently looking at the following;</p> <p><strong>QNAP TS-664-8G</strong> - I like that this model supports QuTS which allows me the ability to use the ZFS filesystem and have my drives in a Z2 array. I like the expansion options for memory, M.2 and PCI-E. I also like the inclusion of 2 10Gb/s USB ports and the 2.5G ethernet ports. I&#39;m less a fan of the older Celeron chip powering this NAS</p> <p><strong>TerraMaster F6-424 Max</strong> - I like that this model has much more modern hardware including a 12th gen Intel Core i5 CPU. I like all the expansion options and also that it&#39;s future proof with having 10G ethernet. Honestly, this is the model I&#39;d most likely buy for the hardware alone but I&#39;m not familiar with TerraMaster&#39;s TOS software. I assume it&#3",
        "id": 2438096,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmpanf/need_help_picking_out_a_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help picking out a NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T15:24:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there literally any way to possible to recover the media from old, deleted tumblrs? Are there any archives online I could search? Any info is helpful.<br/> I\u2019m not looking for the whole posts, simply any images or videos posted to any given deleted tumblr.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Separate-Lobster-806\"> /u/Separate-Lobster-806 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmoocc/deleted_tumblr_image_archives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmoocc/deleted_tumblr_image_archives/\">[comments]</a></span>",
        "id": 2438097,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmoocc/deleted_tumblr_image_archives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Deleted tumblr image archives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T15:02:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to back up about 300 GBs of photo from the OneDrive camera roll folder on my C drive.</p> <p>The destination is another drive, another drive letter.</p> <p>I have tried several utilities (including xcopy and) and none of them work. Every single one of them fills all available space on C drive. even 20 GB worth, with some unknown type of data. This is something that should not happen at all because this operation is creating new copies of files on the e-drive and only <em>looks</em> at what&#39;s on the C drive.</p> <p>FreeFileSync s nice on paper but it throws zillions of &quot;ffs&quot; errors, which I believe refer to the anger if the user instead of an acronym for the product. Other methods of copying give cloud errors and crash on them even though I&#39;m not touching the cloud whatsoever in this operation. </p> <p>I would like a reliable error-free file copy, utility suitable for this, and one that uses very little or no source st",
        "id": 2438094,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmo79w/backing_up_large_onedrive_photos_directory",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backing up large OneDrive photos directory.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T14:59:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Guys, how can i fetch the public_email field instagram on requests?</p> <pre><code>{ &quot;response&quot;: { &quot;data&quot;: { &quot;user&quot;: { &quot;friendship_status&quot;: { &quot;following&quot;: false, &quot;blocking&quot;: false, &quot;is_feed_favorite&quot;: false, &quot;outgoing_request&quot;: false, &quot;followed_by&quot;: false, &quot;incoming_request&quot;: false, &quot;is_restricted&quot;: false, &quot;is_bestie&quot;: false, &quot;muting&quot;: false, &quot;is_muting_reel&quot;: false }, &quot;gating&quot;: null, &quot;is_memorialized&quot;: false, &quot;is_private&quot;: false, &quot;has_story_archive&quot;: null, &quot;supervision_info&quot;: null, &quot;is_regulated_c18&quot;: false, &quot;regulated_news_in_locations&quot;: [], &quot;bio_links&quot;: [ { &quot;image_url&quot;: &quot;&quot;, &quot;is_pinned&quot;: false, &quot;link_type&quot;: &quot;external&quot;, &quot;lynx_url&quot;: &quot;https://l.instagram.com/?u=https%3A%2",
        "id": 2437716,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmo4p2/business_instagram_mail_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Business Instagram Mail Scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T13:10:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I know that obviously a harddrive would&#39;ve failed by now, but assuming that there was an effort to backup and such, what do you think?</p> <p>I know it&#39;s a weird hypothetical to engage with, because are we assuming that they otherwise were at the same technological level but just magically had digital storage? Idk, but it&#39;s something that has kept popping into my mind for a while now.</p> <p>Can digital data survive for two, or even one millennia? I kinda lean toward no in almost all cases because it requires constant diligence. I feel like if even one generation lacks the will or the tools to keep the data alive, that&#39;s it, game over. That&#39;s with wars and all that.</p> <p>Stuff like papyrus and tablets could get away with being rediscovered. But a rediscovered harddrive doesn&#39;t hold any data, though obviously it would blow some archeologist&#39;s mind.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.red",
        "id": 2437360,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmly2n/do_you_think_that_data_from_2000_years_ago",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you think that data from 2000+ years ago would've survived to today if they were in digital form?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T12:51:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Seagate refers to them in the documentation under the Exos Recertified Drive folder.</p> <p>Their transfer speed is significantly lower (&gt;20%) than the other X24 drives. What\u2019s uo with that?</p> <p>Elsewhere, I\u2019ve read these are HAMR drives, but that was not mentioned in the spec sheet.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EindhovenFI\"> /u/EindhovenFI </a> <br/> <span><a href=\"https://www.seagate.com/content/dam/seagate/en/content-fragments/products/datasheets/exos-recertified-drive/exos-recertified-drive-DS2045-2-2010US-October-2020-en_US.pdf\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmllm0/whats_the_deal_with_seagate_nm000c_drives/\">[comments]</a></span>",
        "id": 2437361,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmllm0/whats_the_deal_with_seagate_nm000c_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s the deal with Seagate NM000C drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T12:21:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a few inexpensive Linux KVM VPS servers that I&#39;d like to start backing up since they have become increasingly complex to setup as I&#39;ve tweaked and added functionality over time. The VPS providers charge a lot for adding backup functionality so I want to be able to perform backups/restores over the Internet. Preferably, store the backups on a Windows file server or a Linux server VM on my LAN. I currently have a SFTP server running on my network so I could forward a port on the gateway or maybe use an inexpensive backup service like BorgBackUp or whatever, depending on the price.</p> <p>I&#39;ve been using Veeam, for years, in my home lab and it is awesome but I was never able to get it to work with backing up those remote VPS&#39;s. I believe the only way is to get a license for Veeam Cloud Connect but I can&#39;t afford that.</p> <p>Being a tiny step above a Linux noob, I don&#39;t know what the best practice is for backing up and res",
        "id": 2438095,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jml2i1/best_choice_for_backing_uprestoring_linux_vps",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best choice for backing up/restoring Linux VPS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T11:52:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So every month or so i backup some of my laptops contents onto a external hdd for insurance, usually i just delete everything on the external and copy everything over from the laptop but i realise this isnt the best option for the external drives long term health, i change the folders around and add files to them on my laptop so i need software that can &quot;update&quot; my external so it mirrors my laptop without having to delete everything and copy over if that makes sense, im not too computer literate so any help would be much appreciated thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TransatlanticAB\"> /u/TransatlanticAB </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmklj7/best_waysoftware_to_backup_a_routinely_changed/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmklj7/best_waysoftware_to_backup_a_routinely_changed/\">[comments]</",
        "id": 2437068,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmklj7/best_waysoftware_to_backup_a_routinely_changed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way/software to backup a routinely changed folder to external HDD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T11:49:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi</p> <p>Been a while since I looked into this topic, and when I last built my home NAS 5 years ago all my research said don&#39;t use SSD for NAS as constant read / write is bad, and capacity of SSD will degrade a lot over time.</p> <p>My limited understanding is that SSD have improved, and especially if mainly reading from them that is very unlikely to degrade?</p> <p>I want to use my NAS in RAID 1 (mirrored single config) so it is backed up. I thought that will also reduce the number of read / write to the SSD as not striped?</p> <p>It will be connected via my switch 1000mbit to my Macstudio, Samsung TV and Apple laptop. </p> <p>I want SSD as its <strong>quite</strong> and this will live in my office room next to my Macstudio</p> <p>I want to use it for: </p> <p>1) Backup of my Macstudio (I also back up to iCloud and another external hard disk which I store in a fireproof safe)</p> <p>2) Hosting my Audiobooks, TV Shows and Movies on the LAN. Is i",
        "id": 2437069,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmkjko/ssd_for_simple_nas_setup_little_confused_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SSD for simple NAS setup - little confused from conflicting posts online on this topic",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T11:45:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If I use Staxrip and QTGMC medium, I get flickering on the smaller lines of the video; you can see this a bit better in motion but I hope it&#39;s clear enough here - but obviously I can&#39;t leave the MKV I get from the dvd ISO unencoded, because whatever program I put it into will interpret it differently- i think the default leads to the third image here. [The &#39;vlc with deinterlacing turned off&#39; is the same as this] </p> <p>Is there a better way to encode this than QTGMC medium?</p> <p><a href=\"https://imgur.com/a/rQ8Q086\">https://imgur.com/a/rQ8Q086</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lackadaisical37\"> /u/lackadaisical37 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmkhp1/trying_to_work_out_a_better_way_to_encode_dvd_rips/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmkhp1/trying_to_work_out_a_better_way_to_encode",
        "id": 2437070,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmkhp1/trying_to_work_out_a_better_way_to_encode_dvd_rips",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to work out a better way to encode DVD rips",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T11:19:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>In this moment, a large file transfer is running on my newly built PC. I am currently sitting on my old PC and doing other things in the meantime. In order to be aware of what went wrong (and when) (in case something goes wrong during the transfer), I have OBS set up to capture the screen.</p> <p>The content is being copied from my phone&#39;s internal memory to the new M.2 NVMe SSD (4TB Samsung 990 Pro, my new PCs main storage) via USB Type-C cable.</p> <p>Now my question: I don&#39;t know where on the SSD the capture is being saved, but the SSD is constantly being written to by the file transfer and by the capture. Does this result in a sort of alternating pattern in the file structure? Like, a few photos, then some MB of capture, then another photo or document, then some MB of capture, etc etc.? Something that would, once I delete the screen capture, make the transferred files be in an extremely unfavourable arrangement?</p> <p>I do know it&#39;s ",
        "id": 2436776,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmk3ca/will_screen_capture_during_file_transfer_do_weird",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Will screen capture during file transfer do weird things to the file structure?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T11:14:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>23andMe lets you build a family tree \u2014 but there\u2019s no built-in way to export it. I wanted to preserve mine offline and use it in genealogy tools like Gramps, so I wrote a Python scraper that: \u2022 Logs into your 23andMe account (with your permission) \u2022 Extracts your family tree + relatives data \u2022 Converts it to GEDCOM (an open standard for family history)</p> <p><strong>Totally local:</strong> runs in your browser, no data leaves your machine Saves JSON backups of all data Outputs a GEDCOM file you can import into anything (Gramps, Ancestry, etc.)</p> <p>Source + instructions: <a href=\"https://github.com/borsic77/23andMeFamilyTreeScraper\">https://github.com/borsic77/23andMeFamilyTreeScraper</a></p> <p>Built this because I didn\u2019t want my family history go down with 23andme, hope it can help you too!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/borsic\"> /u/borsic </a> <br/> <span><a href=\"https://www.reddit.com/r/",
        "id": 2436774,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmk10a/export_your_23andme_family_tree_as_a_gedcom_file",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Export your 23andMe family tree as a GEDCOM file (Python tool)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T11:13:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The same old question but search hasn&#39;t brought me yet (at least no recent) recommendation catered to my set of needs here.</p> <p>I thinking heavily about splitting my hoarding stash actually to make maintenance of it easier. I backuped heavily some years ago a lot of YT-Videos (Lets Plays, Political Shows, Lore Videos, documentations and such a stuff, primarly for saving content before it may vanish (and some has already vanished), also old Minecraft Savegames who took a lot of space but necessary also for server maintenance (sudden discoveries of corrupted biomes make it good to a have a lot of rollback alternatives). As well general system backups who provide some redundancy about my personal data. And preperations for having a &quot;off-grid&quot; old media library (especially GOG Game Files in case they close the platform or changing their NO-DRM-Policy). All of them have in common they are mostly cold storage I have touched rarely the last",
        "id": 2436775,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmk0fw/recommendations_for_affordable_cold_longterm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommendations for affordable cold longterm cloud storage solutions for private use?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T02:58:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Up-front costs are easy to measure. Buying a drive, rack, other parts, etc. Ongoing costs such as routine drive replacement and electricity, not so much (and yes, I understand electricity can vary <em>heavily</em> depending on location and setup).</p> <p>So I&#39;m curious, for those of you with larger setups especially (let&#39;s say 200TB+), what kind of routine ongoing costs do you have? How do you minimize these or make your setup more efficient?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kaptainkeel\"> /u/kaptainkeel </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmcx8a/for_those_with_larger_hoards_how_much_is_your/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jmcx8a/for_those_with_larger_hoards_how_much_is_your/\">[comments]</a></span>",
        "id": 2435160,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jmcx8a/for_those_with_larger_hoards_how_much_is_your",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "For those with larger hoards, how much is your routine/ongoing cost?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T00:40:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Here&#39;s what I want:</p> <ul> <li>See a single drive (eg. E:) in Windows.</li> <li>Single drive is two (or three) internal HDDs automatically cloned/duplicated. They&#39;re not the system drive.</li> <li>No BitLocker or any encryption, so I can just unplug and reconnect elsewhere if I ever care to or have to (whatever needs &#39;secrecy&#39; gets it through other means).</li> <li><strong>Main concern is local redundancy against hard drive failure. This is for long-term storage of rarely-accessed things and single-drive SATA 3 read speeds are presumed enough.</strong></li> <li>Secondary goal is user friendliness/simplicity. </li> </ul> <p>Here&#39;s what I wish to avoid:</p> <ul> <li>Command line.</li> <li>Anything Linux/FreeBSD.</li> <li>File systems other than NTFS.</li> <li>Protection from deleting files by mistake (for the sake of the solution&#39;s simplicity).</li> <li><strong>Having to learn skills and commands that I&#39;ll forget a year af",
        "id": 2434695,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jma9xz/is_drivepool_enough_for_automated_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Drivepool enough for automated backup duplication of internal HDDs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T00:16:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, as the NIST is being sacked: <a href=\"https://www.wired.com/story/nist-doge-layoffs-atomic-spectroscopy/\">https://www.wired.com/story/nist-doge-layoffs-atomic-spectroscopy/</a></p> <p>Do you know where/how to download the datasets? The atomic spectroscopy datasets has to be saved asap, but the rest of it is in danger. Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/maxdamon\"> /u/maxdamon </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jm9sse/nist_databases/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jm9sse/nist_databases/\">[comments]</a></span>",
        "id": 2434696,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jm9sse/nist_databases",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NIST databases",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-29T00:09:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I &#39;collect&#39; podcasts, and I have a back storage of the files off of my main drives due to space limitations. I annotate the file name with reference notes so I can recall them when needed.</p> <p>I tried making a smaller quality mp3 file for a smaller sized library, but that didn&#39;t work.</p> <p>Is there a way to copy all the filenames into a word or text document?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/qqererer\"> /u/qqererer </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jm9o60/how_do_i_create_a_searchable_database_of_my_mp3/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1jm9o60/how_do_i_create_a_searchable_database_of_my_mp3/\">[comments]</a></span>",
        "id": 2434697,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1jm9o60/how_do_i_create_a_searchable_database_of_my_mp3",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I create a searchable database of my mp3 files without having to actually have a complete version of the file itself?",
        "vote": 0
    }
]