# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Struggling to Scrape Pages Jaunes – Need Advice
 - [https://www.reddit.com/r/webscraping/comments/1j2ur45/struggling_to_scrape_pages_jaunes_need_advice](https://www.reddit.com/r/webscraping/comments/1j2ur45/struggling_to_scrape_pages_jaunes_need_advice)
 - RSS feed: $source
 - date published: 2025-03-03T22:01:03+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone,</p> <p>I’m trying to scrape data from <a href="https://www.pagesjaunes.fr/"><strong>Pages Jaunes</strong></a>, but the site is really good at blocking scrapers. I’ve tried rotating user agents, adding delays, and using proxies, but nothing seems to work.</p> <p>I need to extract <strong>name, phone number, and other basic details</strong> for shops in specific industries and regions. I already have a list of industries and regions to search, but I keep running into anti-bot measures. On top of that, some pages <strong>time out</strong>, making things even harder.</p> <p>Has anyone dealt with something like this before? Any advice or ideas on how to get around these blocks? I’d really appreciate any help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ReactNativeDevZ"> /u/ReactNativeDevZ </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2ur45/struggling_to_scrape_pages_jaune

## Best Approach for Solving Cloudflare Challenge page?
 - [https://www.reddit.com/r/webscraping/comments/1j2udn4/best_approach_for_solving_cloudflare_challenge](https://www.reddit.com/r/webscraping/comments/1j2udn4/best_approach_for_solving_cloudflare_challenge)
 - RSS feed: $source
 - date published: 2025-03-03T21:45:07+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone,</p> <p>I&#39;ve been running into issues with Cloudflare challenge page while scraping. I was using <strong>Puppeteer with a real browser</strong>, which worked decently, but since it&#39;s <strong>no longer receiving updates</strong>, I&#39;m looking for alternatives.</p> <p>I&#39;ve tried different approaches, but many seem unreliable or inconsistent. What are some effective strategies or open-source solutions that you’ve had success with?</p> <p>Would love to hear your thoughts—thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Riidv"> /u/Riidv </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2udn4/best_approach_for_solving_cloudflare_challenge/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2udn4/best_approach_for_solving_cloudflare_challenge/">[comments]</a></span>

## Help: Download Court Rulings (PDF) from Chilean Judiciary?
 - [https://www.reddit.com/r/webscraping/comments/1j2qbi4/help_download_court_rulings_pdf_from_chilean](https://www.reddit.com/r/webscraping/comments/1j2qbi4/help_download_court_rulings_pdf_from_chilean)
 - RSS feed: $source
 - date published: 2025-03-03T18:57:43+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1j2qbi4/help_download_court_rulings_pdf_from_chilean/"> <img src="https://a.thumbs.redditmedia.com/ILRw42nxPQilOpXXXFqTNcmfxt1ZvyYQ9RhYN6jtok4.jpg" alt="Help: Download Court Rulings (PDF) from Chilean Judiciary?" title="Help: Download Court Rulings (PDF) from Chilean Judiciary?" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hello everyone,</p> <p>I’m trying to automate the download of court rulings in PDF from the Chilean Judiciary’s Virtual Office (<a href="https://oficinajudicialvirtual.pjud.cl/">https://oficinajudicialvirtual.pjud.cl/</a>). I have already managed to search for cases by entering the required data in the form, but I’m having issues with the final step: opening the case details and downloading the PDF of the ruling.</p> <p>I have tried using Selenium and Playwright, but the main issue is that the website’s structure changes dynamically, making it difficult to access the PDF link.</p> <p>Man

## How to do google scraping on scale?
 - [https://www.reddit.com/r/webscraping/comments/1j2pucl/how_to_do_google_scraping_on_scale](https://www.reddit.com/r/webscraping/comments/1j2pucl/how_to_do_google_scraping_on_scale)
 - RSS feed: $source
 - date published: 2025-03-03T18:38:52+00:00

<!-- SC_OFF --><div class="md"><p>I have been try to do google scraping using requests lib however it is failing again and again. It says to enable the javascript. Any come around for thi? </p> <pre><code>&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;&lt;title&gt;Google Search&lt;/title&gt;&lt;style&gt;body{background-color:#fff}&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;noscript&gt;&lt;style&gt;table,div,span,p{display:none}&lt;/style&gt;&lt;meta content=&quot;0;url=/httpservice/retry/enablejs?sei=tPbFZ92nI4WR4-EP-87SoAs&quot; http-equiv=&quot;refresh&quot;&gt;&lt;div style=&quot;display:block&quot;&gt;Please click &lt;a href=&quot;/httpservice/retry/enablejs?sei=tPbFZ92nI4WR4-EP-87SoAs&quot;&gt;here&lt;/a&gt; if you are not redirected within a few seconds.&lt;/div&gt;&lt;/noscript&gt;&lt;script nonce=&quot;MHC5AwIj54z_lxpy7WoeBQ&quot;&gt;//# sourceMappingURL=data:application/json;charset=utf-8;base64, </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a

## Does anyone know how not to halt the rate limiting on Twítter?
 - [https://www.reddit.com/r/webscraping/comments/1j2mefo/does_anyone_know_how_not_to_halt_the_rate](https://www.reddit.com/r/webscraping/comments/1j2mefo/does_anyone_know_how_not_to_halt_the_rate)
 - RSS feed: $source
 - date published: 2025-03-03T16:19:20+00:00

<!-- SC_OFF --><div class="md"><p>Has anyone been scraping X lately? I&#39;m struggling trying to not halt the rate limits so I would really appreciate some help from someone with more experience on it.</p> <p>A few weeks ago I managed to use an account for longer, got it scraping nonstop for 13k twets in one sitting (a long 8h sitting) but now with other accounts I can&#39;t manage to get past the 100...</p> <p>Any help is appreciated! :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/jsandi99"> /u/jsandi99 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2mefo/does_anyone_know_how_not_to_halt_the_rate/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2mefo/does_anyone_know_how_not_to_halt_the_rate/">[comments]</a></span>

## Aliexpress welcome deals
 - [https://www.reddit.com/r/webscraping/comments/1j2ln66/aliexpress_welcome_deals](https://www.reddit.com/r/webscraping/comments/1j2ln66/aliexpress_welcome_deals)
 - RSS feed: $source
 - date published: 2025-03-03T15:47:30+00:00

<!-- SC_OFF --><div class="md"><p>Would it be possible to use proxys in some way to make aliexpress acounts and get a lot of welcome deal bonusses? Has something like this been done before?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Shot_Status2339"> /u/Shot_Status2339 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2ln66/aliexpress_welcome_deals/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2ln66/aliexpress_welcome_deals/">[comments]</a></span>

## Difficulty In Scraping website with Perimeter X Captcha
 - [https://www.reddit.com/r/webscraping/comments/1j2ffiz/difficulty_in_scraping_website_with_perimeter_x](https://www.reddit.com/r/webscraping/comments/1j2ffiz/difficulty_in_scraping_website_with_perimeter_x)
 - RSS feed: $source
 - date published: 2025-03-03T10:07:05+00:00

<!-- SC_OFF --><div class="md"><p>I have a list of around 3000 URLs, such as <a href="https://www.goodrx.com/trimethobenzamide"><code>https://www.goodrx.com/trimethobenzamide</code></a>, that I need to scrape. I&#39;ve tried various methods, including manipulating request headers and cookies. I&#39;ve also used tools like Playwright, Requests, and even <code>curl_cffi</code>. Despite using my cookies, the scraping works for about 50 URLs, but then I start receiving 403 errors. I just need to scrape the HTML of each URL, but I&#39;m running into these roadblocks. Even tried getting Google Caches. Any suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Super_Duck_2116"> /u/Super_Duck_2116 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2ffiz/difficulty_in_scraping_website_with_perimeter_x/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2ffiz/difficulty_in_scraping_website_

## How Do You Handle Selector Changes in Web Scraping?
 - [https://www.reddit.com/r/webscraping/comments/1j2e75q/how_do_you_handle_selector_changes_in_web_scraping](https://www.reddit.com/r/webscraping/comments/1j2e75q/how_do_you_handle_selector_changes_in_web_scraping)
 - RSS feed: $source
 - date published: 2025-03-03T08:35:12+00:00

<!-- SC_OFF --><div class="md"><p>For those of you who scrape websites regularly, how do you handle situations where the site&#39;s HTML structure changes and breaks your selectors?</p> <p>Do you manually review and update selectors when issues arise, or do you have an automated way to detect and fix them? If you use any tools or strategies to make this process easier, let me know pls</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/alimadat"> /u/alimadat </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2e75q/how_do_you_handle_selector_changes_in_web_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2e75q/how_do_you_handle_selector_changes_in_web_scraping/">[comments]</a></span>

## The First Rule of Web Scraping is... Dont Talk About Web Scraping
 - [https://www.reddit.com/r/webscraping/comments/1j2bq7a/the_first_rule_of_web_scraping_is_dont_talk_about](https://www.reddit.com/r/webscraping/comments/1j2bq7a/the_first_rule_of_web_scraping_is_dont_talk_about)
 - RSS feed: $source
 - date published: 2025-03-03T05:40:55+00:00

<!-- SC_OFF --><div class="md"><p>The first rule of web scraping? Don’t talk about web scraping… unless you’re here. Then, it’s basically your only conversation topic. All your friends are tired of hearing about headers, delays, and CAPTCHAs, but we live for it. So let’s gather and actually talk about it, or else we’ll start scraping the conversation. 😉</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/uvanwu"> /u/uvanwu </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2bq7a/the_first_rule_of_web_scraping_is_dont_talk_about/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2bq7a/the_first_rule_of_web_scraping_is_dont_talk_about/">[comments]</a></span>

## Web scraping and CLUSTERING
 - [https://www.reddit.com/r/webscraping/comments/1j2bb1h/web_scraping_and_clustering](https://www.reddit.com/r/webscraping/comments/1j2bb1h/web_scraping_and_clustering)
 - RSS feed: $source
 - date published: 2025-03-03T05:15:03+00:00

<!-- SC_OFF --><div class="md"><p>Hi guys, i am making an app that scrapes phones and ac units and compares their prices. The names on different sites are totally different even though its the same product. I cant seem to find a good match unless i clean them manually which isnt productive. I looked into clustering but i dont know how to do it correctly. The problem is that it matches iPhone 15 with iPhone 16 for example, or Vivax ACP-12CH35AERI+R32 with Vivax ACP-12CH35AEHI+R32. Any help? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Tall_Rabbit_5100"> /u/Tall_Rabbit_5100 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2bb1h/web_scraping_and_clustering/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2bb1h/web_scraping_and_clustering/">[comments]</a></span>

## Indigo website Scraping Problem
 - [https://www.reddit.com/r/webscraping/comments/1j2alxs/indigo_website_scraping_problem](https://www.reddit.com/r/webscraping/comments/1j2alxs/indigo_website_scraping_problem)
 - RSS feed: $source
 - date published: 2025-03-03T04:34:05+00:00

<!-- SC_OFF --><div class="md"><p>I just wanna Scrape Indigo website for getting Information about departure time,fare but i cannot scrape that data . idonot know why its happening as i think it works well i asked chatgpt and it said on logical level the code is correct but doesnt help in identifying the problem. so please help me out on this problem</p> <p>Link : <a href="https://github.com/ripoff4/Web-Scraping/tree/main/indigo">https://github.com/ripoff4/Web-Scraping/tree/main/indigo</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Perry_2013"> /u/Perry_2013 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j2alxs/indigo_website_scraping_problem/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j2alxs/indigo_website_scraping_problem/">[comments]</a></span>

## Help: Download Court Rulings (PDF) from Chilean Judiciary?
 - [https://www.reddit.com/r/webscraping/comments/1j294z8/help_download_court_rulings_pdf_from_chilean](https://www.reddit.com/r/webscraping/comments/1j294z8/help_download_court_rulings_pdf_from_chilean)
 - RSS feed: $source
 - date published: 2025-03-03T03:11:48+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1j294z8/help_download_court_rulings_pdf_from_chilean/"> <img src="https://b.thumbs.redditmedia.com/XsTxV0xiTbgQ9TuONMCMxrIiDUg-_eJfylWQyL4ZSAw.jpg" alt="Help: Download Court Rulings (PDF) from Chilean Judiciary?" title="Help: Download Court Rulings (PDF) from Chilean Judiciary?" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hello everyone,</p> <p>I’m trying to automate the download of court rulings in PDF from the Chilean Judiciary’s Virtual Office (<a href="https://oficinajudicialvirtual.pjud.cl/">https://oficinajudicialvirtual.pjud.cl/</a>). I have already managed to search for cases by entering the required data in the form, but I’m having issues with the final step: opening the case details and downloading the PDF of the ruling.</p> <p>I have tried using Selenium and Playwright, but the main issue is that the website’s structure changes dynamically, making it difficult to access the PDF link.</p> <p>Man

