# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## AI web scraping service?
 - [https://www.reddit.com/r/webscraping/comments/1j22b9u/ai_web_scraping_service](https://www.reddit.com/r/webscraping/comments/1j22b9u/ai_web_scraping_service)
 - RSS feed: $source
 - date published: 2025-03-02T21:42:49+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone, does anyone use AI-driven web scraping tools that don’t require manually selecting elements from a page?</p> <p>For example, instead of writing code, I’d love something where I could just type: <em>&quot;Extract the recipe and ingredients from this page&quot;</em> and get a structured response in JSON or CSV. Ideally it works well across multiple sites, even when layouts change frequently.</p> <p>I imagine this would be super useful for people collecting data from various sources without dealing with site-specific details. If you’ve used something like this, how well did it work? How much did you pay to use it? </p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/argent141"> /u/argent141 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j22b9u/ai_web_scraping_service/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j22b9u/ai_we

## Possible to search UPCs on a store website to see if they exist?
 - [https://www.reddit.com/r/webscraping/comments/1j1wxao/possible_to_search_upcs_on_a_store_website_to_see](https://www.reddit.com/r/webscraping/comments/1j1wxao/possible_to_search_upcs_on_a_store_website_to_see)
 - RSS feed: $source
 - date published: 2025-03-02T17:58:35+00:00

<!-- SC_OFF --><div class="md"><p>Hey guys! If I have a list of UPCs, is it possible to program something that searches for the UPC one by one on a website, and returns “True” if the site has the item or “False” if it doesn’t. I don&#39;t mean in stock or out of stock btw, literally just if the product exists at all.</p> <p>Do I need APIs? Or could I do this regularly? I’m a coding noob lol (basic Python/SQL knowledge), but I feel like the logic isn’t too complicated, just wondering if this is possible and how if it is. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/middleschoolyogurt"> /u/middleschoolyogurt </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j1wxao/possible_to_search_upcs_on_a_store_website_to_see/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j1wxao/possible_to_search_upcs_on_a_store_website_to_see/">[comments]</a></span>

## Puppy detection
 - [https://www.reddit.com/r/webscraping/comments/1j1r31t/puppy_detection](https://www.reddit.com/r/webscraping/comments/1j1r31t/puppy_detection)
 - RSS feed: $source
 - date published: 2025-03-02T13:41:09+00:00

<!-- SC_OFF --><div class="md"><p>Hey,<br/> Using stealth plugin, latest puppy version.</p> <p>Proxies</p> <p>UA agent randomization for desktop</p> <p>Locally it passes reddit scrapping but when running from docker/cloud it gets detected immediately. </p> <p>Ive moved to reddit API but im curious - do you know what they might use for detection?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/TheRepo90"> /u/TheRepo90 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j1r31t/puppy_detection/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j1r31t/puppy_detection/">[comments]</a></span>

## What Are Your Go-To Tools and Libraries for Efficient Web Scraping?
 - [https://www.reddit.com/r/webscraping/comments/1j1qe3o/what_are_your_goto_tools_and_libraries_for](https://www.reddit.com/r/webscraping/comments/1j1qe3o/what_are_your_goto_tools_and_libraries_for)
 - RSS feed: $source
 - date published: 2025-03-02T13:04:28+00:00

<!-- SC_OFF --><div class="md"><p>Hello fellow web scrapers!</p> <p>I&#39;m curious to know what tools and libraries you all prefer for web scraping projects. Whether it&#39;s a programming language, a specific library, or a tool that has made your scraping tasks easier, please share your experiences.</p> <p>For instance, I&#39;ve been using Python with BeautifulSoup and Requests for most of my projects, VPS, Visual Code and GitHub pilot but I&#39;m interested in exploring other options that might offer better performance or ease of use.</p> <p>Looking forward to your recommendations and insights!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ertostik"> /u/ertostik </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j1qe3o/what_are_your_goto_tools_and_libraries_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j1qe3o/what_are_your_goto_tools_and_libraries_for/">[comments]</a></span>

## Node (Puppeteer) Webscraping Advice
 - [https://www.reddit.com/r/webscraping/comments/1j1ic17/node_puppeteer_webscraping_advice](https://www.reddit.com/r/webscraping/comments/1j1ic17/node_puppeteer_webscraping_advice)
 - RSS feed: $source
 - date published: 2025-03-02T04:11:48+00:00

<!-- SC_OFF --><div class="md"><p>Been working on a web scraping project and I&#39;m just wondering if I&#39;m missing or over doing anything. Any advice is welcome. Alot of times I&#39;ll get a message saying that the the website I&#39;m trying to scrape knows something is weird but it eventually lets my through and I start scraping. But I&#39;m just not sure how it&#39;s catching something.</p> <p>Packages: Rebrowser-Puppeteer, User-Agents, Puppeteer-Proxy &amp; Proxy-Handler</p> <p>I&#39;m also using a Chrome Extension called WebRTC-Leak-Prevent since without a plugin, it seems pretty hopeless in node/chrome to stop any WebRTC leaks.</p> <pre><code>&quot;puppeteer&quot;: { &quot;headless&quot;: false, &quot;slowMo&quot;: 500, &quot;args&quot;: [ &quot;--start-maximized&quot;, &quot;--no-sandbox&quot;, &quot;--disable-setuid-sandbox&quot;, &quot;--disable-dev-shm-usage&quot;, &quot;--disable-dev-mode&quot;, &quot;--disable-debug-mode&quot;, &quot;--disable-blink-features=Automation

## Best Way to Scrape & Analyze 1000s of Products for eBay Automation
 - [https://www.reddit.com/r/webscraping/comments/1j1e99e/best_way_to_scrape_analyze_1000s_of_products_for](https://www.reddit.com/r/webscraping/comments/1j1e99e/best_way_to_scrape_analyze_1000s_of_products_for)
 - RSS feed: $source
 - date published: 2025-03-02T00:34:13+00:00

<!-- SC_OFF --><div class="md"><p>I’m completely new to web scraping and looking for the best way to extract and analyze thousands of product listings from an e-commerce website <a href="https://www.deviceparts.com">https://www.deviceparts.com</a>. My goal is to list them on ebay after i cheery picked the category.I dont want end up lisitng items manually one by one, as it will take ages for me.</p> <p>I need to scrape the following details for thousands of products:</p> <p>Product Title (from the category page)</p> <p>Product Image (from the category page)</p> <p>Product Description (which requires clicking on the product page)</p> <p>Since I don’t know how to code, I’d love to know:</p> <p>What’s the easiest tool to scrape 1000s of products? (No-code scrapers, browser extensions, or software recommendations?)</p> <p>How can I automate clicking on product links to get full descriptions efficiently?</p> <p>How do I handle large-scale scraping without getting blocked?</p> <p>Once I ha

## Are most scraping on the cloud? Or locally?
 - [https://www.reddit.com/r/webscraping/comments/1j1e8cy/are_most_scraping_on_the_cloud_or_locally](https://www.reddit.com/r/webscraping/comments/1j1e8cy/are_most_scraping_on_the_cloud_or_locally)
 - RSS feed: $source
 - date published: 2025-03-02T00:33:00+00:00

<!-- SC_OFF --><div class="md"><p>As an amateur scraper I am genuinely curious. I tried deploying a scraper to AWS and it became quite expensive, compared to being essentially free on my PC. Also, I find the need to use non-headless mode to get around many checks. Im using virtual monitor on linux to hide it. I feel like that would be very bulky and resource intensive on a cloud solution. </p> <p>Thoughts? Feelings? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/jgupdogg"> /u/jgupdogg </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1j1e8cy/are_most_scraping_on_the_cloud_or_locally/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1j1e8cy/are_most_scraping_on_the_cloud_or_locally/">[comments]</a></span>

