[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T22:37:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How do you separate webscraping traffic from the main network? I have a script that switches between VPN/Wireguard every few minutes, but it runs for hours and hours and this directly affects my main traffic.</p> <p>Any solutions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NicolasRS\"> /u/NicolasRS </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jlfxii/separate_webscraping_traffic_from_the_main_network/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jlfxii/separate_webscraping_traffic_from_the_main_network/\">[comments]</a></span>",
        "id": 2425927,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jlfxii/separate_webscraping_traffic_from_the_main_network",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Separate webscraping traffic from the main network?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T22:26:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>[I posted this in <a href=\"/r/Python\">r/Python</a> too]</p> <p>I use automated browsers a lot and sometimes I&#39;ll hit a situation and wonder &quot;would Selenium have perform this better than Playwright?&quot; or vice versa. But rewriting it all just to test it is... not gonna happen most of the time.</p> <p>So I wrote mahler!</p> <ul> <li>Project link: <a href=\"https://github.com/michaeleveringham/mahler\">https://github.com/michaeleveringham/mahler</a></li> <li>Documentation link: <a href=\"https://mahler.readthedocs.io/en/latest/index.html\">https://mahler.readthedocs.io/en/latest/index.html</a></li> </ul> <h1>What My Project Does</h1> <p>Offers the ability to write an automated browsing workflow once and change the underlying remote web browser API with the change of a single argument.</p> <h1>Target Audience</h1> <p>Anyone using browser automation, be it for tests or webscraping.</p> <p>The API is pretty limited right now to basic interactions (",
        "id": 2425928,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jlfnu1/i_wrote_a_wrapper_to_swap_automated_browser",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I wrote a wrapper to swap automated browser engines in Python.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T22:13:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Every month the FBI releases about 300 pages of files on the DB Cooper case. These are in PDF form. There have been 104 releases so far. The normal method for looking at these is for a researcher to take the new release, download it, add it to an already created PDF and then use the CTRL F to search. It\u2019s a tedious method. Plus at probably 40,000 pages, it\u2019s slow. </p> <p>There must be a good way to automate this and upload it to a website or have an app like R Shiny created and just have a simple search box like a Google type search. That way researchers would not be reliant on trading Google Docs links or using a lot of storage on their home computer. </p> <p>Looking for some ideas. AI method preferred. Here is the link. </p> <p><a href=\"https://vault.fbi.gov/D-B-Cooper%20\">https://vault.fbi.gov/D-B-Cooper%20</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Swimmer7777\"> /u/Swimmer7777 </a> <br/> <span><a h",
        "id": 2425478,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jlfd0p/web_scrape_on_fbi_files_pdf_question_db_cooper_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scrape on FBI files (PDF) question. DB Cooper or JFK etc.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T19:33:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Thinking to scrape posts and comments from subreddit and train it through notebook llm so that anyone can ask questions there </p> <p>Maybe a dumb idea but I want to give it a shot. </p> <p>But any free tool / extension. How to headstart </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/trying_to_improve45\"> /u/trying_to_improve45 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jlby29/scrape_a_particular_subreddit_with_posts_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jlby29/scrape_a_particular_subreddit_with_posts_and/\">[comments]</a></span>",
        "id": 2424543,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jlby29/scrape_a_particular_subreddit_with_posts_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "scrape a particular subreddit with posts and comments... For free",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T18:56:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Long time listener, first time caller \ud83d\ude01 </p> <p>I just took my first steps in coding and GitHub as a development tool, having published my first python project yesterday - scraping a quote from the Quotes to Scrape website, which was pretty awesome! </p> <p>For my second learning project I read documentation on mobile.de, Craigslist and Zillow scrapping, adapted the methodology for the Southern European equivalent OLX and built an interpretation of a scraper for used Car &amp; Motorcycle listings. </p> <p>Initial data fetches look promising, I&#39;m feeling positive for next steps as I learn how to use python to clean/process before analysis. </p> <p>For a current aggregated market view this will suffice (new listing alerts, potentially LLama, LangChain, Twillio as an AI agent for automated SMS to sellers etc), but such a small sample can&#39;t offer more than anecdotal insights due to lack of historical context.</p> <p>However, as a learning exercis",
        "id": 2424000,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jlbksg/project_guidance_selenium",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Project Guidance (Selenium)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T18:52:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There&#39;s a site I would like to scrape for large files. This site is pretty well protected and I have looked into external services to help bypass and rotate IPs. One service I&#39;ve tried is timing out when the resource url is being called. I am wondering if this is because the download itself is taking longer than the max timeout time (150s) or if seomthing anti-bot related is happening. </p> <p>Is there something I&#39;m missing when it comes to downloading large files while scraping?<br/> Also is there a thread where I can find popular scraping services?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/microflakes\"> /u/microflakes </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jlbhwg/downloading_large_files_on_a_heavily_botprotected/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jlbhwg/downloading_large_files_on_a_heavily_botprotected/\">[c",
        "id": 2424001,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jlbhwg/downloading_large_files_on_a_heavily_botprotected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading large files on a heavily bot-protected site",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T16:16:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to host the python script on the cloud for a one time scrape, because I don&#39;t have a stable internet connection at the moment.</p> <p>The scrape is a one time thing but will continuously run for 1.5-2 days. This is because i the website I&#39;m scraping is a relatively small website and i don&#39;t want to task their servers too much, the scrape is one request every 5-10 seconds(about 16800 requests).</p> <p>I don&#39;t mind paying but i also don&#39;t want to accidentally screw myself. What cloud service would be best for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Trobis\"> /u/Trobis </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl7q89/best_cloud_service_for_a_onetime_scrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl7q89/best_cloud_service_for_a_onetime_scrape/\">[comments]</a></span>",
        "id": 2422725,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jl7q89/best_cloud_service_for_a_onetime_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Cloud service for a one-time scrape.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T14:56:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Greetings \ud83d\udc4b\ud83c\udffb Noob here, I was given a task to find an official website for companies stored in database. I only have a name of the companies/persons that I can use. </p> <p>My current way of thinking is that I create a variations of the name that could be used in domain name. (e.g. Pro Dent inc. -&gt; pro-dent.com, prodent.com\u2026)</p> <p>I search the search engine of choice for results, I then get the URLs and check if any of them fits. When they do, I am done searching, otherwise I am going to check content of each of the results if it contains</p> <p>There is the catch, how do I evaluate the contents?</p> <p>Edit: I am using python with selenium, requests and BS4. For search engine I am using brave-search, it seems like there is no captcha. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Icount_zeroI\"> /u/Icount_zeroI </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl5tw9/programaticall",
        "id": 2422028,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jl5tw9/programatically_find_official_website_of_a_company",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Programatically find official website of a company",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T13:03:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve built a custom scraper for a client that extracts profiles in a given state from Psychology Today (a directory of therapists/psychologists) while safely bypassing their anti-bot measures using headless.</p> <p>Not sure what further use it has for me, so thought I\u2019d check to see if anybody is working with clients who want to target this demographic/or want to target this demographic themselves.</p> <p>Lemme know.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AcanthisittaOne2209\"> /u/AcanthisittaOne2209 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl3f8a/is_anybody_scraping_therapistspsychologists_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl3f8a/is_anybody_scraping_therapistspsychologists_data/\">[comments]</a></span>",
        "id": 2420746,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jl3f8a/is_anybody_scraping_therapistspsychologists_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is anybody scraping therapists/psychologists data from the US?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T12:49:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>edited without mentioned software.</p> <p>So, as title suggests, i am looking for easiest way to scrape result of google search. Example is, i go to <a href=\"http://google.com\">google.com</a>, type &quot;text goes here&quot; hit enter and scrape specific part of that search. I do this 15 times each 4 hours. I&#39;ve been using software scraper for past year, but since 2 months ago, i get captcha every time. Tasks run locally (since i can&#39;t get wanted results of pages if i run on cloud or different IP address outside of desired country) and i have no problem when i type in regular browser, only when using app. I would be okay with even 2 scrapes per day, or even 1. I just need to be able to run it without having to worry about captcha.</p> <p>I am not familiar with scraping outside of software scraper since i always used it without issues for any task i had at hand. I am open to all kinds of suggestions. Thank you!</p> </div><!-- SC_ON --> &#32; s",
        "id": 2420747,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jl3507/easiest_way_to_scrape_google_search_first_page",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Easiest way to scrape google search (first) page?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T11:54:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to make a shopping bot to buy Pok\u00e9mon cards. I\u2019m not trying to scalp I just want to buy packs and open them up myself but it\u2019s crazy difficult buy them. I have a cs background and have experience with web scraping and I\u2019ve even built a selenium program which can buy stuff off of target. Problem is that I think it is too slow to compete with the other bots. I\u2019m considering writing a playwright program in JavaScript, since ChatGPT said it would be faster than my python selenium program. My question is, how can I make a super fast shopping bot to compete with others out there?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Embarrassed_Fee_8327\"> /u/Embarrassed_Fee_8327 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl24qm/how_to_make_fast_shopping_bot/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jl24qm/how_to_make_fast_shopping_bot/\">[comm",
        "id": 2419854,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jl24qm/how_to_make_fast_shopping_bot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to make Fast shopping bot",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T08:56:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve seen in another post someone recommending very cool open source AI website scraping projects to have structured data in output!</p> <p>I am very interested to know more about this, do you guys have some projects to recommend to try? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DangerousFill418\"> /u/DangerousFill418 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jkzjy3/open_source_ai_website_scraping_projects/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jkzjy3/open_source_ai_website_scraping_projects/\">[comments]</a></span>",
        "id": 2419853,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jkzjy3/open_source_ai_website_scraping_projects",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open source AI website scraping projects recommandations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T07:53:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been learning web scraping lately, and it&#39;s pretty fascinating. I&#39;m starting to get pretty good at it, and I&#39;m wondering... is it actually possible to make REAL money with this skill? Not just a few bucks here and there, but like, actually rich?</p> <p>I know there are ethical considerations (and I&#39;m definitely aiming to stay on the right side of the law!), but assuming you&#39;re doing everything by the book, what are the possibilities? Are there people out there making a killing scraping data and selling it or using it for their own businesses?</p> <p>I&#39;ve seen some examples online, but they seem a bit... exaggerated. I&#39;d love to hear from anyone with real-world experience. What&#39;s the reality of making money with web scraping? What kind of projects are the most lucrative? And most importantly, how much hustle is actually involved?</p> <p>Thanks in advance for any insights! Let&#39;s keep it constructive and h",
        "id": 2418962,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jkyrsf/can_scrapping_skill_really_make_you_rich",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can scrapping skill REALLY make you rich ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T03:52:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1jkv8t2/realtorcom_blocks_me_even_just_opening_the_page/\"> <img src=\"https://b.thumbs.redditmedia.com/3T4aKxoMf_SM1XNe1OFJC7XqTbIghV-frowNbCGKewk.jpg\" alt=\"realtor.com blocks me even just opening the page in Chrome Dev tool?\" title=\"realtor.com blocks me even just opening the page in Chrome Dev tool?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Has anybody ever experience situations like this? A few weeks ago, I got my <a href=\"http://realtor.com\">realtor.com</a> scraper working, but yesterday when I tried it again, it got blocked (different IPs, and runs in docker container and the footprint should be different each run).</p> <p><a href=\"https://preview.redd.it/kyrgqx46n5re1.png?width=3006&amp;format=png&amp;auto=webp&amp;s=42cca658b07ae1de56d10a4771602542192c79f3\">https://preview.redd.it/kyrgqx46n5re1.png?width=3006&amp;format=png&amp;auto=webp&amp;s=42cca658b07ae1de56d10a4771602542192c79f3</a></p> <p>a",
        "id": 2418295,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jkv8t2/realtorcom_blocks_me_even_just_opening_the_page",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/3T4aKxoMf_SM1XNe1OFJC7XqTbIghV-frowNbCGKewk.jpg",
        "title": "realtor.com blocks me even just opening the page in Chrome Dev tool?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T02:09:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, </p> <p>I&#39;m looking to hire a developer for a web scraping project. The task involves writing clean and efficient Python code to automate a process on a website.</p> <p>If you&#39;re experienced with web scraping in Python, please DM me.</p> <p>Also are there better places or more appropriate subreddits to hire dev for this task?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tommy_Johnson_\"> /u/Tommy_Johnson_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jktbrv/looking_to_hire_a_programmer_for_a_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jktbrv/looking_to_hire_a_programmer_for_a_web_scraping/\">[comments]</a></span>",
        "id": 2417859,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jktbrv/looking_to_hire_a_programmer_for_a_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to Hire a Programmer for a Web Scraping Project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2025-03-27T02:05:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I posted and some people commented on my posting. I find it very valuable to me and would like a clean list of each comment. how do I scrape my posting?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IskenderunluCemal\"> /u/IskenderunluCemal </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jkt8vm/scraping_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1jkt8vm/scraping_reddit/\">[comments]</a></span>",
        "id": 2417860,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1jkt8vm/scraping_reddit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "scraping reddit",
        "vote": 0
    }
]