[
    {
        "age": null,
        "album": "",
        "author": "/u/Battler456",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T17:56:21.270347+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T17:03:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi</p> <p>Don&#39;t know if this is the right subreddit for this question however I&#39;m desperate. </p> <p>I\u2019m building a academic project for my bachelors and need access to real e-commerce product data (basic product catalog info only). I\u2019m looking for sources that provide structured product feeds or APIs exposing fields like:</p> <ul> <li>product name, description</li> <li>price / sale price</li> <li>sizes / variants (if applicable)</li> <li>image URLs</li> <li>stock/availability</li> <li>category / brand metadata</li> </ul> <p>Preferably: publicly accessible APIs or data feeds (XML/JSON) or affiliate feeds that are straightforward to use for prototyping. Bonus if they\u2019re EU/Europe-friendly, but I\u2019m open to global sources (including US).</p> <p>Things I want to avoid right now: scraping sites that explicitly forbid it, and proprietary vendor-only partner portals that require being a merchant.</p> <p>If you\u2019ve worked with any of the following or s",
        "id": 3886634,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oe8ulc/looking_for_public_productcatalog_apis_or_feeds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for public product/catalog APIs or feeds",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Many-Task-4549",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T16:42:21.653843+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T16:04:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019m sending a POST request to this endpoint: <a href=\"https://www.zoomalia.com/zearch/products/?page=1\">https://www.zoomalia.com/zearch/products/?page=1</a></p> <p>When I use a normal Python script with requests.post() and undetected-chromedriver to get the Cloudflare cookies, it works perfectly for keywords like &quot;dog&quot; , &quot;rabbit&quot;.</p> <p>But when I try the same request inside a Scrapy spider, it always returns 403 Forbidden, even with the same headers, cookies, and payload.</p> <p>Looks like Cloudflare is blocking Scrapy somehow. Any idea how to make Scrapy behave like the working Python version or handle Cloudflare better?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Many-Task-4549\"> /u/Many-Task-4549 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oe7a0i/scrapy_post_request_blocked_by_cloudflare_403_but/\">[link]</a></span> &#32; <span><a href=",
        "id": 3885940,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oe7a0i/scrapy_post_request_blocked_by_cloudflare_403_but",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scrapy POST request blocked by Cloudflare (403), but works in Python",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ivanalemunioz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T16:42:21.331133+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T15:34:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, I use Puppeteer (Node.js) to automate workflows in web pages and is easy to debug them in development cause I can do it in real time, but when something crashes in production I have to save the HTML in the moment of the crash, the URL, a screenshot and any other relevant information in s3 and my DB to be able to do a fast debugging, I end up creating my own &quot;tool&quot; to look at this reports but I don&#39;t think is the best way, do you know if is there a tool to do it better? How you do it? Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ivanalemunioz\"> /u/ivanalemunioz </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oe6hjj/how_do_you_debug_crashes_in_prod/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oe6hjj/how_do_you_debug_crashes_in_prod/\">[comments]</a></span>",
        "id": 3885939,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oe6hjj/how_do_you_debug_crashes_in_prod",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you debug crashes in prod?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Virtual-Wrongdoer137",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T10:22:37.085151+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T08:31:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,<br/> I&#39;m looking for a reliable solution to track when YouTube channels start and stop livestreaming. The goal is to monitor 1000+ channels in near real-time.</p> <p>The problem: <strong>YouTube API limits are way too restrictive</strong> for this use case. I\u2019m wondering if anyone has found a scalable workaround \u2014 maybe using webhooks or scrapers or any free tools?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Virtual-Wrongdoer137\"> /u/Virtual-Wrongdoer137 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1odxv38/need_a_way_to_detect_when_yt_channels_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1odxv38/need_a_way_to_detect_when_yt_channels_go/\">[comments]</a></span>",
        "id": 3882650,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1odxv38/need_a_way_to_detect_when_yt_channels_go",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need a way to detect when YT channels go live/offline (at scale)",
        "vote": 0
    }
]