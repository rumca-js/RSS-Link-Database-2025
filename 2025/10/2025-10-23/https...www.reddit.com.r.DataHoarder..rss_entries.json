[
    {
        "age": null,
        "album": "",
        "author": "/u/Public_Cress6584",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T23:39:19.846376+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T23:26:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a reliable SSD for my Time machine backups. I was going to get a 4tb T7 shield. But I&#39;m not sure if that&#39;s a good idea. I currently have no backup of my computer (only had it a short while) which is bad. Should I get that or will it slow over time? Will I be able to rewrite new backups over and over and not run into any problems? This is my only laptop and used for work. So I can&#39;t really afford to lose data. Any advice is great.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Public_Cress6584\"> /u/Public_Cress6584 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeihoj/best_ssd_for_time_machine_backups/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeihoj/best_ssd_for_time_machine_backups/\">[comments]</a></span>",
        "id": 3889003,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oeihoj/best_ssd_for_time_machine_backups",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best SSD for Time machine backups",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lucas_Zxc2833",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T23:39:20.321077+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T23:03:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>(Really sorry for the possibly duplicated post, I had to recreate the post because the previous one was deleted from the original location, so I had to delete it from there as well and i think I shouldn&#39;t have posted it there in the first place; it should have been here right away, and this time, I made a better one than the previous one, and this time, I focused on what I really wanted to say)</p> <p>I understand the reason for seeing out there why people be somewhat wary and uncomfortable with digital media, especially after recent news, like the removal of those three anime series from Crunchyroll (even though CR is a streaming service and not a store), and they must be saying that physical media is superior and digital media is terrible and should never exist<br/> but in reality, I always wonder, <strong>why instead, can&#39;t both of them coexist</strong></p> <p>like, i know the problem they always bring up is the issue of digital media versu",
        "id": 3889005,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oehznk/why_physical_media_and_digital_media_cannot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why physical media and digital media cannot coexist, helping each other? (better version of my previous post)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LeopoldBStonks",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T23:39:19.965308+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T22:23:21+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LeopoldBStonks\"> /u/LeopoldBStonks </a> <br/> <span><a href=\"/r/Trading/comments/1oeh185/looking_for_1_second_stock_and_crypto_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeh2h6/looking_for_1_second_stock_and_crypto_data/\">[comments]</a></span>",
        "id": 3889004,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oeh2h6/looking_for_1_second_stock_and_crypto_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for 1 second stock and crypto data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/karluvmost",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T23:39:19.557898+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T22:22:15+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeh1kr/it_doesnt_fit_drive_safe_deposit_box/\"> <img src=\"https://b.thumbs.redditmedia.com/OAwNjLqperu-1HS4deq2vTJl_pf7_1TecOaqJlutblc.jpg\" alt=\"It doesn't fit. Drive &gt;&gt; Safe Deposit Box\" title=\"It doesn't fit. Drive &gt;&gt; Safe Deposit Box\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Unbelievable. All that work to get a good backup of files + a Time Machine backup of my 8TB MBP.</p> <p>Final step to done: Put in safe deposit box.</p> <p>I thought for sure it would fit.</p> <p>Edit: The box was advertised to be 3&quot; tall x 5&quot; wide x 22&quot; deep. I swear I measured the drive and thought it would fit the 5&quot; wide part.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/karluvmost\"> /u/karluvmost </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1oeh1kr\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeh1kr",
        "id": 3889002,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oeh1kr/it_doesnt_fit_drive_safe_deposit_box",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/OAwNjLqperu-1HS4deq2vTJl_pf7_1TecOaqJlutblc.jpg",
        "title": "It doesn't fit. Drive >> Safe Deposit Box",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Such-Bench-3199",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T22:22:09.519174+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T22:03:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I might need to put an asterisk at the beginning of this question.</p> <p>Even though it seems trivial or \u201clow\u201d to other loyal heavy users on here, to me 10TB is a lot of space. Recently I decided to move/shuffle/offload/rob Pete to pay Paul, babushka doll a 10TB drive full of Podcasts (current) onto a larger 16TB drive, thus giving me 6TB free going forward. </p> <p>In order to do that I needed to first move/shuffle/offload/rob Pete to pay Paul, babushka doll 8TB from the 16TB, onto an 18TB temporarily. Then I realised I had 5TB\u2019s from the 10TB of old podcasts I needed to shift to cold storage, which I am in the process of doing now.</p> <p>Seeing as my plow horse/workhorse/battleaxe, transferring machine (my 2011 MacBook Pro) is more reliable than my 2015 iMac, in that it never randomly resets itself, and via Carbon Copy Cloner I can start and pause the transfer process whenever I want (can only transfer when I get home from work and on weekends) as",
        "id": 3888535,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oeglhc/is_it_potentially_dangerous_to_deletefree_so_much",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it potentially dangerous to delete/free so much all at once?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LifeIsLikeADumpster",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T22:22:09.792639+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T21:44:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It can download videos (images would be a plus!)</p> <p>It can download content from social media platforms (images would be a plus)</p> <p>It can accept a long list of links and download videos from that list without manually inputting each one - one at a time</p> <p>It can download entire facebook account albums</p> <p>It works on Mac OS</p> <p>Not known to have any viruses / malware</p> <p>NSFW Videos </p> <p>Would be legal (?) - Would like to know what&#39;s legal, what&#39;s not legal, what the consequences are for illegal actions, etc.</p> <p>Thank you for the help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LifeIsLikeADumpster\"> /u/LifeIsLikeADumpster </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeg5vx/looking_for_a_downloader_that_checks_a_few_boxes/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeg5vx/looking_for_a_downloader_that_",
        "id": 3888536,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oeg5vx/looking_for_a_downloader_that_checks_a_few_boxes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a downloader that checks a few boxes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BookShelfRandom",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T21:03:41.668900+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T20:49:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeerx7/internet_archive_reached_1_trillion_web_archives/\"> <img src=\"https://b.thumbs.redditmedia.com/1R4d4vPb1aYYpARXlulZKgymr8sQWOH3gMC6JGcifvQ.jpg\" alt=\"Internet archive reached 1 trillion web archives, internet archive day is on the 22nd of October every year.\" title=\"Internet archive reached 1 trillion web archives, internet archive day is on the 22nd of October every year.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BookShelfRandom\"> /u/BookShelfRandom </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1oeerx7\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oeerx7/internet_archive_reached_1_trillion_web_archives/\">[comments]</a></span> </td></tr></table>",
        "id": 3887867,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oeerx7/internet_archive_reached_1_trillion_web_archives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/1R4d4vPb1aYYpARXlulZKgymr8sQWOH3gMC6JGcifvQ.jpg",
        "title": "Internet archive reached 1 trillion web archives, internet archive day is on the 22nd of October every year.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wolfenstien98",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T21:03:42.656595+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T20:29:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Currently I use FreshRSS to backup local and world news. While it works well, each time i want to add a new source its a time consuming process, and its difficult to share/export my saved articles. Is there a better system for archiving news via RSS/web-scrapping? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wolfenstien98\"> /u/wolfenstien98 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oee9kj/better_system_to_archive_news/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oee9kj/better_system_to_archive_news/\">[comments]</a></span>",
        "id": 3887868,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oee9kj/better_system_to_archive_news",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Better system to archive news?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lucas_Zxc2833",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T21:03:44.681036+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T20:29:23+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lucas_Zxc2833\"> /u/Lucas_Zxc2833 </a> <br/> <span><a href=\"/r/animepiracy/comments/1oee6ug/is_digital_media_really_that_bad_is_physical/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oee9b3/is_digital_media_really_that_bad_is_physical/\">[comments]</a></span>",
        "id": 3887870,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oee9b3/is_digital_media_really_that_bad_is_physical",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is digital media really that bad? Is physical media superior? The truth",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ounaazh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T21:03:43.483808+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T19:03:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need another HDD &amp; I&#39;ve shucked in the past, but last time I bought storage, the Ultrastar DC HC550 was best bang for buck.</p> <p>At the moment, I see that 20TB prices for regular 3.5&quot; are priced OK, <em>but</em> then I checked Amazon.de (I&#39;m based in Europe) &amp; saw that <a href=\"https://www.amazon.de/dp/B0BSFRNQTF\">22TB WD Elements is priced the same as a 20TB internal one</a>, which is great bang for buck.</p> <p>I understand shucking is not such a big thing anymore, so there are no Youtube videos of 20/22/24TB shucking as there were earlier, so no speed tests over SATA aswell. The screenshots used to be directly in the Amazon reviews. I&#39;ve also read some scare-stories that the internal drives are gimped (even via. FW?) for SATA connections nowadays &amp; are inferior to internal ones in other terms aswell - is this true or is an external shucked WD still ~as good as the good old, but overpriced WD Red?</p> <p>The drive wi",
        "id": 3887869,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oec142/need_a_new_20tb_would_prefer_2224tb_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need a new 20TB (would prefer 22/24TB) drive - shucking in Q4 2025?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/InformalBiscotti2140",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T18:17:46.070644+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T18:09:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i am so over the digital consumption and have decided to start deleting as much of my data out there as much as possible. last year i used DeleteMe but didn&#39;t continue with it this year. I&#39;d like to get my data out of IG and FB which are the 2 main social media platforms I&#39;ve used in the last 20+ years. all i care about is the photos. not the messages and activity. first: is there no way just to get photos downloaded when going through Account Center?</p> <p>aside from social media, my next effort is to delete and move away from iCloud. but have NO idea how to get started or what to do there. </p> <p>lastly is email: what are folks doing with old emails? what should I consider is priority/most important?</p> <p>thanks for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/InformalBiscotti2140\"> /u/InformalBiscotti2140 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oea",
        "id": 3886770,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oealpe/need_guidance_on_getting_started",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "need guidance on getting started",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/urbanracer34",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T17:06:25.739086+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T16:40:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When I first started collecting I tried using a program to automate my &quot;acquisitions&quot; (more TV shows and Movies)</p> <p>Immediately my server crashed and hard locked. I had to force power down it for it to stop. </p> <p>So I decided to do it all manually: Go to sites, find the items individually, set them up to download, then move them by hand to the proper directories. </p> <p>Am I silly for doing this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/urbanracer34\"> /u/urbanracer34 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe88g6/does_anyone_else_acquire_and_process_their_linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe88g6/does_anyone_else_acquire_and_process_their_linux/\">[comments]</a></span>",
        "id": 3886128,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe88g6/does_anyone_else_acquire_and_process_their_linux",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone else acquire and process their \"linux isos\" completely by hand?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/khaf_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T17:06:25.824356+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T16:31:28+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/khaf_\"> /u/khaf_ </a> <br/> <span><a href=\"/r/datarecovery/comments/1oe50b2/need_help_for_a_lost_docx_file_on_macos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe802d/need_help_for_a_lost_docx_file_on_macos/\">[comments]</a></span>",
        "id": 3886129,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe802d/need_help_for_a_lost_docx_file_on_macos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help for a lost .docx file on MacOS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NachoAvgMurican",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T17:06:25.470722+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T16:30:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe7yzg/vhs_to_digital_best_practice/\"> <img src=\"https://b.thumbs.redditmedia.com/vb5RaI-llbPz_87YKzPenh4p0TQGxfQathNbG8ZrJqs.jpg\" alt=\"VHS to Digital Best Practice\" title=\"VHS to Digital Best Practice\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello everybody, got a question about the next step in processing about 80-90ish VHS video cassettes. </p> <p>I&#39;ve been asked to digitalize the videos by my family and luckily my parents kept some of the equipment that were intended for this back in the day. </p> <p>I have a Sanyo DvD/VHS (model DVW-6100)player/recorder and an Emerson VHS to DvD device (model EWR20V5) as pictured. Found some VHS rewinders too. </p> <p>Still at their house and not pictured is a Sony Handycam with a bunch more of those little tapes. </p> <p>The idea is to go straight to digital. </p> <p>Based on the wiki and a few youtube videos, would getting a BlackMagic UltraStudio Recorder ",
        "id": 3886127,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe7yzg/vhs_to_digital_best_practice",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/vb5RaI-llbPz_87YKzPenh4p0TQGxfQathNbG8ZrJqs.jpg",
        "title": "VHS to Digital Best Practice",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/craterean",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:23.530114+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T15:37:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a photographer and graphic designer, and I&#39;ve got tons of large files accumulating each year. I&#39;ve considered a NAS, but frequent power outages and slow internet in my area make that impractical.</p> <p>My current workflow: I work from an SSD for active projects, then archive completed work to two external HDDs + cloud storage.<br/> Speed is definitely a plus but not a must, given that I only work from SSDs. </p> <p>I&#39;ve had multiple LaCie drives fail on me after a year or so (despite careful handling), so I&#39;m steering clear from them.</p> <p>Any suggestions? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/craterean\"> /u/craterean </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe6kj7/best_8tb_external_hdd_for_file_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe6kj7/best_8tb_external_hdd_for_file_storage/\">[commen",
        "id": 3885377,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe6kj7/best_8tb_external_hdd_for_file_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best 8TB external HDD for file storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Taicore",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:23.662366+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T15:02:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I keep seeing rumors/fears about if an account get falsely flagged by the &quot;we couldnt verify you&#39;re an adult&quot; pop up, past videos of said account could get privated (pretty much the same as deleted ,good as gone if an account is inactive and cannot verify)<br/> So just to be safe--I suggest you download videos you like watching just to be safe. </p> <p>I did see an account get falsely flagged but their video remained public, and they verified after,so hopefully we won&#39;t get a massive amount of lost media but ,can never be too safe.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Taicore\"> /u/Taicore </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe5nd7/regarding_the_ai_age_verification_thing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe5nd7/regarding_the_ai_age_verification_thing/\">[comments]</a></span>",
        "id": 3885378,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe5nd7/regarding_the_ai_age_verification_thing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Regarding the ai age verification thing",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ThreatPriority",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:23.272871+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T14:42:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How would this help in a home PC? Would there be a noticeable difference in performance? What is the advantage here?</p> <p>I trying to figure out which drives, in the 24 TB - 30 TB range, I should buy, to store all the data I have which currently sits very precariously on only one single drive, a Seagate IronWolf Pro 18 TB. With no backups!</p> <p>Any insights or helpful thoughts would be most welcome. Thanks for reading.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ThreatPriority\"> /u/ThreatPriority </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe54p4/tons_of_cache_the_toshiba_24_tb_n300_drives_have/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe54p4/tons_of_cache_the_toshiba_24_tb_n300_drives_have/\">[comments]</a></span>",
        "id": 3885376,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe54p4/tons_of_cache_the_toshiba_24_tb_n300_drives_have",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tons of Cache! The Toshiba 24 TB N300 drives have 1024 mb of cache. That's TWICE the amount of all the other brands.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/needmind",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:23.786457+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T14:18:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was lucky to get a discount for this NAS which is similar price to their DAS but later on found out the spec is terrible - N5095 CPU and 4GB ram without Cache SSD. So I just want to get some advice as this is my first NAS - aiming to end my mess of external drives.</p> <p>I currently use an old laptop with 16GB ram and i3-7100u cup (repurposed as proxmox) to manage external hard drives with samba. It is also running home assistant and Plex server. But I wanted to have a proper NAS to replace my current setting and provide additional access outside the home network. But I really concerned with this machine. </p> <ol> <li><p>I know TOS can run Dockers for Plex and other software. But will the CPU be enough for running Plex? I don&#39;t have the need for multiple streaming. Just one TV only. Sometimes might run a 4K video. I am particularly annoyed by lacking of AV1 but maybe it is okay?</p></li> <li><p>Has anyone running home assistant on TOS? I haven",
        "id": 3885379,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe4i9s/terramaster_f2425",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Terramaster F2-425",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Annoyingly-Petulant",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T14:32:29.180733+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T14:15:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a lot of slides that I inherited from my grandpa. I would like to scan them to digital. But when shopping for scanners I can\u2019t justify spending $500+ on a scanner. </p> <p>I was wondering if anybody here has the equipment and what they would charge? Do you charge per slide or ? </p> <p>I have around 100 slides as a ballpark estimate. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Annoyingly-Petulant\"> /u/Annoyingly-Petulant </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe4f61/does_anybody_here_scan_slides_to_digital/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe4f61/does_anybody_here_scan_slides_to_digital/\">[comments]</a></span>",
        "id": 3884617,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe4f61/does_anybody_here_scan_slides_to_digital",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anybody here scan slides to digital?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mwomrbash",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T14:32:29.269631+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T14:14:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I have been running UnRaid for sme time and things are fine. I run a sync check every 30 days. But I am concerned about data corruption that is not caught by the sync check.</p> <p>Is there any kind of data verification I can run on my files regularly to verify that the data on disk is still good?</p> <p>I have begun to do backups onto tape but I am still working out issues in my workflow/automation so tape-backup is not &#39;ready&#39; yet.</p> <p>Thank you,</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mwomrbash\"> /u/mwomrbash </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe4epq/unraid_how_to_verify_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe4epq/unraid_how_to_verify_data/\">[comments]</a></span>",
        "id": 3884618,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe4epq/unraid_how_to_verify_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "UnRaid how to verify data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/throwawayyyyygay",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T14:32:28.957852+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T14:03:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Title</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/throwawayyyyygay\"> /u/throwawayyyyygay </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe44me/now_that_reddit_has_blocked_the_internet_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe44me/now_that_reddit_has_blocked_the_internet_archive/\">[comments]</a></span>",
        "id": 3884615,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe44me/now_that_reddit_has_blocked_the_internet_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Now that reddit has blocked the internet archive, how do you archive posts of interest?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Annoyingly-Petulant",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T14:32:29.399402+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T13:23:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><pre><code> Errors Corrected by Total Correction Gigabytes Total ECC rereads/ errors algorithm processed uncorrected fast | delayed rewrites corrected invocations [10^9 bytes] errors read: 586883129 1 0 586883130 1 2348978.638 0 write: 0 0 4 4 4 106336.379 0 verify: 1764976 0 0 1764976 0 3.626 0 Non-medium error count: 8 </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Annoyingly-Petulant\"> /u/Annoyingly-Petulant </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe35o8/so_im_guessing_my_parity_drive_is_no_good/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe35o8/so_im_guessing_my_parity_drive_is_no_good/\">[comments]</a></span>",
        "id": 3884619,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe35o8/so_im_guessing_my_parity_drive_is_no_good",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "So im guessing my parity drive is no good?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Toriality",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T14:32:29.088562+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T13:18:44+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Toriality\"> /u/Toriality </a> <br/> <span><a href=\"/r/selfhosted/comments/1oe304b/what_tool_or_platform_you_wish_existed/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe31t4/what_tool_or_platform_you_wish_existed/\">[comments]</a></span>",
        "id": 3884616,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe31t4/what_tool_or_platform_you_wish_existed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What tool or platform you wish existed?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/itsRennAgain",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:24.302048+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T13:06:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Context:<br/> - I&#39;m trying to script an archival system, but I&#39;m very much a beginner, and I can&#39;t find a satisfying answer to the problem in the title<br/> - I would use this solution only for important data, I think it&#39;s overkill for non-important one</p> <p>Encryption or compression can corrupt a file *while* transforming it, so I&#39;m searching for a way to detect that without too much computing overhead compared to my current method</p> <p>The method I currently prefer:<br/> 1. Checksum the original *and* encrypted version of the file/tarball<br/> 2. Decrypt (and decompress if needed) to verify the file against its checksum<br/> 3. Encrypt/compress again so I can verify the encrypted version of the file against the checksum I previously created in step 1</p> <p>The problem: too much compute and feels clunky. Not only do I need 2 checksums, but I also need to repeat the encryption and/or compression process</p> <p>I&#39;m searchin",
        "id": 3885380,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe2s1r/how_to_efficiently_check_for_corruption_during",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to efficiently check for corruption *during* transformations like encryption or compression?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Over-Term7939",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T12:01:30.555881+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T11:03:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe0at0/how_can_i_know_if_this_is_a_legit_wd_drive/\"> <img src=\"https://preview.redd.it/yvipq27nfuwf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f58fd4bc9de3b6994e5a0ab789db5774fb2c392c\" alt=\"How can I know if this is a legit WD drive?\" title=\"How can I know if this is a legit WD drive?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Pic of the Drive itself, it&#39;s completely sealed , they were selling it for $22</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Over-Term7939\"> /u/Over-Term7939 </a> <br/> <span><a href=\"https://i.redd.it/yvipq27nfuwf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oe0at0/how_can_i_know_if_this_is_a_legit_wd_drive/\">[comments]</a></span> </td></tr></table>",
        "id": 3883318,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oe0at0/how_can_i_know_if_this_is_a_legit_wd_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/yvipq27nfuwf1.jpeg?width=640&crop=smart&auto=webp&s=f58fd4bc9de3b6994e5a0ab789db5774fb2c392c",
        "title": "How can I know if this is a legit WD drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/InvestigatorThat4835",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T10:44:51.162747+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T10:39:26+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1odzvsw/hoarders_backup_your_github_repos_orgs_to/\"> <img src=\"https://a.thumbs.redditmedia.com/79r8raP_Pb2VswhaHTPoiwZ0nsg1VIpIiuaZP-wWRR0.jpg\" alt=\"Hoarders, Backup your Github repos, orgs, to self-hosted Gitea / Forgejo\" title=\"Hoarders, Backup your Github repos, orgs, to self-hosted Gitea / Forgejo\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Take backup of your Github with all your repos and their metadata issues, pr, release etc and store it in a self-hosted Gitea or Forgejo. So that when for whatever reason your github account is banned or hacked. Somehow you lost access you will still have all your super important work.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/InvestigatorThat4835\"> /u/InvestigatorThat4835 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1odzvsw\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comment",
        "id": 3882745,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odzvsw/hoarders_backup_your_github_repos_orgs_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/79r8raP_Pb2VswhaHTPoiwZ0nsg1VIpIiuaZP-wWRR0.jpg",
        "title": "Hoarders, Backup your Github repos, orgs, to self-hosted Gitea / Forgejo",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/t80_149",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:24.776680+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T10:38:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I need a scanner for higher photo quality because phone scanner app is kinda disappointed. I need to scan all of my family photos, album covers, music CD, journals and my paintings. I found these two scanners Canon Lide 400 and Epson V39II affordable to me for ~ $80 but cant decide what to buy since their resolution (according to the manufacture) is quite similar. Kindly share your review/opinion and help me to choose. \ud83d\ude42</p> <p>Last but not least, what do you think about scanner function of all-in-one family printers, with scan resolution of 1200x2400. Is it good enough? The resolution of 4800 of these two scanners are expressive, but i wonder what resolution do people usually choose, as 4800 might result in very big file.</p> <p>Thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/t80_149\"> /u/t80_149 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1odzvc7/seek_advice_canon_lide",
        "id": 3885381,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odzvc7/seek_advice_canon_lide_400_or_epson_v39ii",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seek advice | Canon Lide 400 or Epson V39II?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Artistic-Age-Mark2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T08:16:26.129975+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T07:35:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am considering buying an external hdd for storage and I cannot decide which one to buy from <a href=\"https://diskprices.com/?locale=ca&amp;condition=new&amp;capacity=2-2&amp;disk_types=external_hdd%2Cexternal_hdd25\">diskprices.com</a>. I went through 1 star amazon reviews of each drive and I noticed no matter which drive, there are reviewers who complained as if it were the worst drive they ever brought. I can&#39;t make decision at this point.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Artistic-Age-Mark2\"> /u/Artistic-Age-Mark2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1odx0sb/cant_decide_which_hdd_to_buy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1odx0sb/cant_decide_which_hdd_to_buy/\">[comments]</a></span>",
        "id": 3881903,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odx0sb/cant_decide_which_hdd_to_buy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can't decide which hdd to buy",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Rippedgeek",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T15:47:25.641061+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T05:12:01+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oduqre/unicode_file_renamer_a_free_little_tool_i_built/\"> <img src=\"https://b.thumbs.redditmedia.com/jCLjUMp56ji0YTNmcVzDyQri5BfTuKa-6-51sr91HfI.jpg\" alt=\"Unicode File Renamer, a free little tool I built (with ChatGPT) to fix weird filenames\" title=\"Unicode File Renamer, a free little tool I built (with ChatGPT) to fix weird filenames\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey folks, </p> <p>Firstly, I promise that I am not Satan. I know a lot of people are tired of \u201cAI-generated slop,\u201d and I get it, but in my very subjective opinion, this one\u2019s a bit different. </p> <p>I used ChatGPT to build something genuinely useful to me, and I hope it will benefit someone, somewhere. <br/> This is a Unicode File Renamer \u2013 I assume there\u2019s likely a ton of these out there, but this one\u2019s mine (and technically probably OpenAI\u2019s too). This small Windows utility (python based) fixes messy filenames with foreign cha",
        "id": 3885382,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oduqre/unicode_file_renamer_a_free_little_tool_i_built",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/jCLjUMp56ji0YTNmcVzDyQri5BfTuKa-6-51sr91HfI.jpg",
        "title": "Unicode File Renamer, a free little tool I built (with ChatGPT) to fix weird filenames",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/betachroniclesmod",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T05:41:53.443086+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T05:02:06+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/betachroniclesmod\"> /u/betachroniclesmod </a> <br/> <span><a href=\"/r/Annas_Archive/comments/1odhrqw/how_to_download_public_domain_book_from_national/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1odukom/how_to_download_public_domain_book_from_national/\">[comments]</a></span>",
        "id": 3881200,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odukom/how_to_download_public_domain_book_from_national",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download (public domain) book from National LIbrary of Australia?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/StrictWeb539",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T05:41:53.560363+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T04:51:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I thought the cloud service that profits it is dropbox, which is provides 10TB Storage in advanced trial, and do not delete files if you log in that account every year.<br/> I didn&#39;t think gsuite is good, because it can&#39;t share folders, and other accounts which is not admin can&#39;t approach files. and it&#39;s duration is too short(14 days), if i end up trial, my files are gone.<br/> but, there is way better service than these services? I want to know.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/StrictWeb539\"> /u/StrictWeb539 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1odudni/what_is_the_best_cloud_service_that_provides/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1odudni/what_is_the_best_cloud_service_that_provides/\">[comments]</a></span>",
        "id": 3881201,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odudni/what_is_the_best_cloud_service_that_provides",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the best cloud service that provides higher trial storage, and can share files after end of trial? (or under $5/month and provides up to 10TB storage)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BasedOnAir",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T04:27:34.296452+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T03:36:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>First off: I am mildly savvy but I am a n00b when it comes to advanced data management. What I am asking for is a way to do this with a simple windows program with a gui on my simple setup, which is just using a file sync program (FreeFileSync) to mirror some files to one external hard drive, and then sync that hard drive to a secondary drive. I have no file server, I don\u2019t understand Linux, am not good with command line and don\u2019t want to engineer a nas. </p> <p>I am looking for a simple way to do this on my two external hard drives in windows. </p> <p>What exactly am I looking to do? I know advanced enterprise solutions take hashes of every file at the time it is created, in addition to a parity file which can be used to reconstruct a file that suffers corruption. That hash is stored somewhere for long term use. Then later as time passes if bit rot happens, the file can be compared to this saved hash and repaired to the formerly hashed state. </p> <p",
        "id": 3880926,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odt1ie/how_do_i_get_started_with_longterm_integrity",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I get started with long-term integrity verification (hash/parity) on my simple setup (external hdd) in windows?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/xiao-e-yun",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-23T05:41:53.724674+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-23T03:05:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><blockquote> <p>NOW IS UNSTABLE, MAYBE IT WILL BREAK CHANGE.</p> </blockquote> <p>This (<a href=\"https://github.com/xiao-e-yun/PostArchiver\">PostArchiver</a>) is an interface that supports downloading various types of articles.</p> <p>Here is a tutorial on how to use it (you may need CLI skills) <a href=\"https://github.com/xiao-e-yun/FanboxArchive/wiki\">Get Started</a></p> <p>Supports importing from different platforms: * <a href=\"https://github.com/xiao-e-yun/FanboxArchive\">Fanbox</a> * <a href=\"https://github.com/xiao-e-yun/PatreonArchive\">Patreon</a> * <a href=\"https://github.com/xiao-e-yun/PixivArchive\">Pixiv</a> * <a href=\"https://github.com/xiao-e-yun/FanboxDLArchive\">FanboxDL</a></p> <p>You can browse through <a href=\"https://github.com/xiao-e-yun/PostArchiverViewer\">PostArchiverViewer</a>.</p> <p>But there is no editor now. ;(</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/xiao-e-yun\"> /u/xiao-e-yun </a> <b",
        "id": 3881202,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1odsghh/an_universal_post_downloader_post_archiver",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "An universal post downloader (Post Archiver)",
        "vote": 0
    }
]