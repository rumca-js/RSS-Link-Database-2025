[
    {
        "age": null,
        "album": "",
        "author": "/u/GarlicPrestigious715",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-26T20:36:52.999944+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-26T18:59:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I made a web scraper for a major grocery store&#39;s website using Playwright. Currently, I can specify a URL and scrape the information I&#39;m looking for.</p> <p>The logical next step seems to be simply copying their list of their products&#39; URLs from their sitemap and then running my program on repeat until all the products are scraped.</p> <p>I&#39;m guessing that the site would be able to immediately identify this behavior since loading a new web page each second is suspicious behavior.</p> <p>My questions is basically, &quot;What am I missing?&quot;</p> <p>Am I supposed to use a VPN? Am I supposed to somehow repeatedly change where my IP address supposedly is? Am I supposed to randomly vary my queries between one to thirty minutes? Should I randomize the order of the products&#39; pages I look at so that I&#39;m not following the order they provide?</p> <p>Thanks in advance for any help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a h",
        "id": 3908839,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ogstly/made_a_web_scraper_that_uses_playwright_am_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Made a web scraper that uses playwright. Am I missing anything?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/KrinkleSack",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-26T17:26:09.254243+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-26T16:43:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have a dataset in my hands, and I believe it has been compiled from multiple sources. I would also like to know how to obtain such data. I hope experienced technical personnel can contact me so we can discuss this further.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KrinkleSack\"> /u/KrinkleSack </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ogpcbv/largescale_data_collection_with_a_demand_of_tens/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ogpcbv/largescale_data_collection_with_a_demand_of_tens/\">[comments]</a></span>",
        "id": 3908389,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ogpcbv/largescale_data_collection_with_a_demand_of_tens",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Large-scale data collection with a demand of tens of millions.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Longjumping_Deal_157",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-26T20:36:53.176057+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-26T15:28:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to collect all \u201cPython Coding Challenge\u201d posts from <a href=\"https://www.clcoding.com/search/label/Python%20Coding%20Challenge\">here </a>into a CSV with title, URL, and content. I don\u2019t know much about web scraping and tried using ChatGPT and Copilot for help, but it seems really tricky because the site doesn\u2019t provide all posts in one place and older posts aren\u2019t easy to access. I\u2019d really appreciate any guidance or a simple way to get all the posts.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Longjumping_Deal_157\"> /u/Longjumping_Deal_157 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ognhib/help_needed_to_scrape_all_python_coding_challenge/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ognhib/help_needed_to_scrape_all_python_coding_challenge/\">[comments]</a></span>",
        "id": 3908840,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ognhib/help_needed_to_scrape_all_python_coding_challenge",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help needed to scrape all \u201cPython Coding Challenge\u201d posts",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kazazzzz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-26T20:36:52.527943+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-26T13:51:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I still can&#39;t understand why people choose to automate Web browser as primary solution for any type of scraping. It&#39;s slow, unefficient,......</p> <p>Personaly I don&#39;t mind doing if everything else falls, but...</p> <p>There are far more efficient ways as most of you know.</p> <p>Personaly, I like to start by sniffing API calls thru Dev tools, and replicate them using curl-cffi.</p> <p>If that fails, good option is to use Postman MITM to listen on potential Android App API and then replicate them.</p> <p>If that fails, python Raw HTTP Request/Response...</p> <p>And last option is always browser automating.</p> <p>--Other stuff--</p> <p>Multithreading/Multiprocessing/Async</p> <p>Parsing:BS4 or lxml</p> <p>Captchas: Tesseract OCR or Custom ML trained OCR or AI agents</p> <p>Rate limits:Semaphor or Sleep</p> <p>So, why is there so many questions here related to browser automatition ?</p> <p>Am I the one doing it wrong ?</p> </div>",
        "id": 3908838,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ogl57n/why_automating_browser_is_most_popular_solution",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 1,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why Automating browser is most popular solution ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BreathIndependent763",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-26T09:02:53.285160+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-26T07:12:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/webscraping\">r/webscraping</a>! \ud83d\udc4b</p> <p>If you&#39;re constantly hunting for fresh, working proxies for your scraping projects, we&#39;ve got something that might save you a ton of time and effort.</p> <p>The Proxy List is Updated Every 5 Minutes!</p> <p>This list is continuously checked from all public proxy list and refreshed by our incredibly fast validation system, meaning you get a high-quality, up-to-date supply of working proxies without having to run your own slow checks.</p> <p><a href=\"https://github.com/ClearProxy/checked-proxy-list\">https://github.com/ClearProxy/checked-proxy-list</a></p> <p>Stop wasting time on dead proxies! Enjoy!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BreathIndependent763\"> /u/BreathIndependent763 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oge7i4/free_validatedchecked_proxy_list_updated_every_5/\">[link]</a></span> &#32; <span>",
        "id": 3905695,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oge7i4/free_validatedchecked_proxy_list_updated_every_5",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Free Validated/Checked Proxy List (Updated Every 5 Minutes!)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Medical_Strawberry78",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-26T01:59:04.873833+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-26T01:29:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! Is there an easy way to build a Python automation script that detects the e-commerce platform my scraper is loading and identifies the site\u2019s HTML structure to extract product data? I\u2019ve been struggling with this for months because my client keeps sending me multiple e-commerce sites where I need to pull category URLs and catalog product data.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Medical_Strawberry78\"> /u/Medical_Strawberry78 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1og88yd/automating_ecommerce_platform_detection_for_web/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1og88yd/automating_ecommerce_platform_detection_for_web/\">[comments]</a></span>",
        "id": 3904489,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1og88yd/automating_ecommerce_platform_detection_for_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automating E-Commerce Platform Detection for Web Scraping",
        "vote": 0
    }
]