[
    {
        "age": null,
        "album": "",
        "author": "/u/WarmToasters",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T23:18:48.545701+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T23:14:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I wish to download and archive a number of defunct websites that are only present on archive.org, does a software tool exist that will create a full copy of the site locally for me to preserve?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WarmToasters\"> /u/WarmToasters </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojjslp/anyway_to_download_complete_websites_from/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojjslp/anyway_to_download_complete_websites_from/\">[comments]</a></span>",
        "id": 3937028,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojjslp/anyway_to_download_complete_websites_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyway to Download Complete Websites from archive.org For local use?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dranxis",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T22:03:23.153045+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T21:43:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a music hoarder, and recently decided that I wanted to try archiving the official website for each album I own. I&#39;m new to web archival and have been testing out SingleFile, but I&#39;m getting mixed results. I understand that some elements can&#39;t always be captured. </p> <p>To those of you who use SingleFile, what settings do you use to capture as much of a webpage as possible? Which boxes should I check/uncheck in the extension options? TIA!</p> <p>P.S. If there&#39;s a different tool y&#39;all prefer, I&#39;m open to suggestions! Must be idiot-friendly (I tried installing Docker today because I wanted to try out a different archival tool, but had no idea what I was doing, and uninstalled in shame).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dranxis\"> /u/dranxis </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojhkjs/recommended_settings_for_singlefile/\">[link]</a></s",
        "id": 3936554,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojhkjs/recommended_settings_for_singlefile",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommended settings for SingleFile?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DJ25380",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T22:03:22.728587+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T21:19:43+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojgz1i/goodwill_hard_drives_exist_go_get_yours/\"> <img src=\"https://preview.redd.it/0twz27waa4yf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cea464453774d10d9d07a0040e68371fd3d24f21\" alt=\"Goodwill hard drives exist go get yours\" title=\"Goodwill hard drives exist go get yours\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Went to my local goodwill and saw 4 sata drives taped together for $5. One was an SSD so I though ehh the SSD alone covers the price. First drive I plugged in and it looks like a brand new drive. Color me surprised. Mfg date is Dec 2015, surprised this thing is 10 years old and never used</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DJ25380\"> /u/DJ25380 </a> <br/> <span><a href=\"https://i.redd.it/0twz27waa4yf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojgz1i/goodwill_hard_drives_exist_go_get_you",
        "id": 3936553,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojgz1i/goodwill_hard_drives_exist_go_get_yours",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/0twz27waa4yf1.png?width=640&crop=smart&auto=webp&s=cea464453774d10d9d07a0040e68371fd3d24f21",
        "title": "Goodwill hard drives exist go get yours",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/stlalphanerd",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T20:38:50.828253+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T20:10:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>gauging interest - I became frustrated by the lack of ability to do tape dumps with tar and cpio - built a user space implementation - anyone care/interested? May implement rmt etc?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/stlalphanerd\"> /u/stlalphanerd </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojf73i/any_interest_in_being_able_to_use_tar_dd_cpio_etc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojf73i/any_interest_in_being_able_to_use_tar_dd_cpio_etc/\">[comments]</a></span>",
        "id": 3935901,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojf73i/any_interest_in_being_able_to_use_tar_dd_cpio_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any interest in being able to use tar , dd, cpio etc with tape drives on macos (getting tape devices back)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fliberdygibits",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T20:38:50.954653+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T20:08:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Building up parts list to finally rebuild my nas. Right now the Fractal Design 7 XL is the top runner but I had a few questions:</p> <p>a) I&#39;m not sure how much I like Fractal&#39;s mounting system that only supports the drive on 3 corners. Without putting my hands on one I&#39;m concerned it seems kinda flimsy. Anyone have any input here? I&#39;ve watched a few videos and read a few reviews but nothing&#39;s really addressed this properly.</p> <p>b) Is there another quality case I might look at if I want 12+ drives in a tower case? I have 9 drives currently with a couple more being added and would like room for future growth. I&#39;m ok spending upwards of 300 (or maybe more) if it&#39;s a known brand that ticks a bunch of boxes. I know there are some 100 and under cases with 10-12 drives but I&#39;m kinda meh on those for a few reasons. If Case Labs was still a thing I&#39;d be all over that.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a ",
        "id": 3935902,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojf568/looking_for_a_case_akin_to_the_define_7_xl",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a case akin to the Define 7 XL",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Meeseekslookatmee",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T20:38:51.965624+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T19:56:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 6 SATA drives already, so they are not included in the price I will be running Truenas Scale. I&#39;m probably going to build something right around Black Friday to see if I can get any deals.</p> <h1>\ud83d\udcb8 Estimated Build Cost (B660M\u2011ITX/ac + SATA Card)</h1> <table><thead> <tr> <th align=\"left\">Component</th> <th align=\"left\">Example Part</th> <th align=\"left\">Approx. Price (USD)</th> <th align=\"left\">Notes</th> </tr> </thead><tbody> <tr> <td align=\"left\"></td> <td colspan=\"3\" align=\"left\"></td> </tr> <tr> <td align=\"left\"><strong>Case</strong></td> <td align=\"left\">Fractal Design Node 304</td> <td align=\"left\">$110</td> <td align=\"left\">6\u00d7 3.5&quot; bays</td> </tr> <tr> <td align=\"left\"><strong>Motherboard</strong></td> <td align=\"left\">ASRock B660M\u2011ITX/ac</td> <td align=\"left\">$190\u2013$220</td> <td align=\"left\">4 SATA onboard, 2\u00d7 M.2</td> </tr> <tr> <td align=\"left\"><strong>CPU</strong></td> <td align=\"left\">Intel Core i5\u201113400 (10 cores: 6P+4E)</t",
        "id": 3935906,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojet2i/ai_gave_me_this_build_for_a_nas_any_thoughts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI gave me this build for a NAS, any thoughts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/armmrdn",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T20:38:51.050462+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T19:44:32+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/armmrdn\"> /u/armmrdn </a> <br/> <span><a href=\"/r/asustor/comments/1ojehl7/adm_issues_copying_data_off_failing_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojei6s/adm_issues_copying_data_off_failing_drive/\">[comments]</a></span>",
        "id": 3935903,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojei6s/adm_issues_copying_data_off_failing_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ADM Issues copying data off failing drive?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SplatinkGR",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T20:38:51.142144+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T19:31:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>No idea what subreddit this discussion belongs to, but since we all hold media libraries here I think it&#39;s a good place.</p> <p>So, H.254, H.265 and AV1 are the three big codecs these days and I commonly create my own encodes from my blu-ray remuxes eg to play on an old TV and such.</p> <p>I don&#39;t have fast CPUs, an i5-8350U on my thinkpad and i7-10700 on my desktop, but still, I&#39;ve tested the encode times on both x254 and x265 and compared them to their hardware counterpats (QSV on the i5 and AMD VCN on my RX6750XT) and what I&#39;ve noticed is that for so long we&#39;ve been mislead into beliving hardware encoders are inferior in quality.</p> <p>This is true if the bitrate is a set limit, say 6Mbit/s. In that case, the software encoders will be higher quality than their hardware counterparts because hardware encoders prioritize speed.</p> <p>However, in 90% of use cases you&#39;d be using CQP or the &quot;quality&quot; slider, which is c",
        "id": 3935904,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oje5m9/is_software_encoding_even_worth_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is software encoding even worth it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CaptainKen2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T19:22:07.044910+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T18:56:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t now this is correct sub to post this, but here goes.</p> <p>I&#39;ve been replacing my 4TB drives with 10TB. The only reason I&#39;m replacing is to increase storage space. To date I have 7 drives that I&#39;m looking to sell. I paid about $106-$200 between 2013-2020. Any offers?</p> <p>Now new on Amazon for $115 or eBay$97.<br/> <a href=\"https://www.amazon.com/dp/B00EHBERSE?th=1\">https://www.amazon.com/dp/B00EHBERSE?th=1</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CaptainKen2\"> /u/CaptainKen2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojd8ho/several_used_western_digital_4tb_wd_red_plus_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojd8ho/several_used_western_digital_4tb_wd_red_plus_nas/\">[comments]</a></span>",
        "id": 3935209,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojd8ho/several_used_western_digital_4tb_wd_red_plus_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Several used Western Digital 4TB WD Red Plus NAS Internal Hard Drive for sale",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Present-Oil-8408",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T19:22:07.205174+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T18:44:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all I have a Kingston X2000 portable SSD. </p> <p>It works ok but if I transfer large file batch. Say 100GB or more or if I leave it idle too long it gets very hot and disconnects. </p> <p>Is this typical of small form factor SSD\u2019s?</p> <p>What would be a good portable drive that has good heat management that I can use on the go. </p> <p>Please advise. </p> <p>Thank you. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Present-Oil-8408\"> /u/Present-Oil-8408 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojcwr5/need_reliable_portable_drive_iphone_16_pro_max/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ojcwr5/need_reliable_portable_drive_iphone_16_pro_max/\">[comments]</a></span>",
        "id": 3935210,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojcwr5/need_reliable_portable_drive_iphone_16_pro_max",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need reliable portable drive. iPhone 16 pro max compatible.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TKB21",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T19:22:07.344003+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T18:27:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all. Wanted to get some opinions on an app I have been pondering on building for quite some time. I&#39;ve seen Pluto adopt this and now Paramount+ where you basically have a slew of shows and movies moving in real-time where you, the viewer could jump in whenever or wherever, from channel to channel (i.e. like traditional cable television). Channels could either be created or auto-generated. Meta would be grabbed from an external API that in turn could help organize information. I have a technical background so now that I see proof of concept, I was thinking of pursuing this but in regards to a user&#39;s own personal collection of stored video.</p> <p>I&#39;ve come across a few apps that address this being <a href=\"https://getchannels.com/dvr-server/\">getchannels</a> and <a href=\"https://ersatztv.org/\">ersatv</a> but the former is paywalled out the gate while the other seems to require more technical know-how to get up and running. My solution i",
        "id": 3935211,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ojcfzp/creating_an_app_for_live_tvchannels_but_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Creating an App for Live TV/Channels but with personal media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dmanitoba",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T16:47:20.215983+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T16:29:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Current setup:</strong></p> <ul> <li>6\u00d724TB drives in Btrfs RAID10 (~72TB usable, 65TB used), bare-metal linux</li> <li>Loved the ability to add drives slowly, 2 at a time and various sizes, and expand the pool</li> <li>Rock solid reliability so far</li> </ul> <p><strong>The problem:</strong> 50% space efficiency is not ideal. With my collection growing, I am thinking ZFS RAIDZ2 for better space utilization while keeping dual-parity protection.</p> <p><strong>Current plan:</strong></p> <ol> <li>Buy 6 new 24TB drives</li> <li>Create ZFS RAIDZ2 pool with the new drives (6\u00d724TB \u2192 ~96TB usable)</li> <li>Copy 65TB of data over and test stability for a while</li> <li>Then either: <ul> <li>Add old 6\u00d724TB drives as second vdev (total ~192TB usable), or</li> <li>Test migrating old drives to Btrfs RAID6 (if stability has improved) and keep separate pools</li> </ul></li> </ol> <p><strong>Questions for the hive mind:</strong></p> <ul> <li>Anyone know of m",
        "id": 3933736,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj9877/migration_advice_btrfs_raid10_624tb_zfs_raidz2",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Migration advice: Btrfs RAID10 (6\u00d724TB) \u2192 ZFS RAIDZ2 - any unexplored options?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No-Event-6258",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T16:47:20.408703+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T16:26:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, Do any of you save the siterips of your favorite adult sites on bluray? Maybe broken down by year. I&#39;m thinking of freeing up space on the HDD by saving some complete series on Bluray (for example I have the entire pornstarpunishment siterip in 1080p, which was the maximum resolution at the time). At the same time I want to leave all the metadata associated with stashapp so that from the frontend I still have the possibility to see the scenes and send them to play by inserting the correct bluray. (maybe via symlinks). Does anyone use the same method or know of alternative methods? Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Event-6258\"> /u/No-Event-6258 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj94rk/siterips_18_backup_on_physical_media/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj94rk/siterips_18_backup",
        "id": 3933737,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj94rk/siterips_18_backup_on_physical_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Siterips 18+ backup on physical media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dlarsen5",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T16:47:20.732514+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T15:47:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hey all,</p> <p>I&#39;m planning on upgrading my local NAS from a 2-bay with 8TB drives to a new 6x18TB and was looking at maybe buying drives around black Friday to see if I could get a better price</p> <p>but with Seagate reporting earnings today/giving a higher forecast for demand the next quarter and seeing how DDR5 prices have increased lately, should I not expect a black Friday sale and buy drives now to avoid any price increase? feels like HDDs are the new GPUs with a potential demand frenzy approaching</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dlarsen5\"> /u/dlarsen5 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj83cf/should_i_buy_large_drives_now/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj83cf/should_i_buy_large_drives_now/\">[comments]</a></span>",
        "id": 3933738,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj83cf/should_i_buy_large_drives_now",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Should I buy large drives now?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Shank_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T15:35:40.583553+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T14:57:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, the private tracker I&#39;ve been apart of for a while and supported is now shutting down in late Feb and has made the entire site freeleach. I&#39;d like to download as much as I can, but I realize that my data limits are what&#39;s stopping me. Currently I run a Synology DS918+ with 2 12tb exos drives. They&#39;ve been great, but I&#39;m thinking about getting two 20tb drives. I understand if I plug two more in they&#39;ll only be recognized as 12tb? How can I get the most storage for my setup? Buy the two 20tb drives and transfer everything over, then buy another two 20tb?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Shank_\"> /u/Shank_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj6s9b/private_tracker_shutting_down_trying_to_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj6s9b/private_tracker_shutting_down_trying_to_arch",
        "id": 3932959,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj6s9b/private_tracker_shutting_down_trying_to_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Private tracker shutting down, trying to archive as many torrents as I can... how to best go about it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Locogooner",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T15:35:40.678379+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T14:48:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 2tb of video files I need backed up to the cloud in under a week.</p> <p>Is there a service where I can just give them an SSD and they upload on super fast wifi? </p> <p>Preferably somewhere in London, UK.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Locogooner\"> /u/Locogooner </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj6jh8/how_can_i_backup_2tb_to_the_cloud_quickly/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj6jh8/how_can_i_backup_2tb_to_the_cloud_quickly/\">[comments]</a></span>",
        "id": 3932960,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj6jh8/how_can_i_backup_2tb_to_the_cloud_quickly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can I backup 2tb to the cloud quickly?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/infiN1ty1337",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T16:47:21.180458+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T14:36:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t know whether this is an appropriate post for this sub, but I haven&#39;t had much luck with getting answers elsewhere, so here it goes. </p> <p>Just to give some context... I&#39;m working on an academical project. I have a panel dataset with temporal context at my disposal which is a product of a SaaS inside the AdTech space. It includes ad-based features (ad type, format, size etc.), request-based features (device type, OS etc.) as well as some details about the campaigns and accounts that were used. Additionally there are success metrics such as requested impression, loaded Impressions, rendered impressions and clicks present, which allow for click-through rate calculation. The core idea is to see whether it is possible to reliably forecast future CTR (or probability of future high CTR) using certain temporal aware machine learning methods solely on the internal data plus some relevant outside sources as the user-based data (which is ex",
        "id": 3933739,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj68rl/looking_for_advice_news_headlines_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for advice - news headlines data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Appropriate-Look-875",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T14:23:14.182997+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T14:18:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj5rp1/i_built_a_tool_that_lets_you_export_your_saved/\"> <img src=\"https://preview.redd.it/q5lcj6wr72yf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d98a5251cb4ab76273f9118454a514c698438845\" alt=\"I built a tool that lets you export your saved Reddit posts directly into Notion or CSV\" title=\"I built a tool that lets you export your saved Reddit posts directly into Notion or CSV\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Appropriate-Look-875\"> /u/Appropriate-Look-875 </a> <br/> <span><a href=\"https://i.redd.it/q5lcj6wr72yf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj5rp1/i_built_a_tool_that_lets_you_export_your_saved/\">[comments]</a></span> </td></tr></table>",
        "id": 3932340,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj5rp1/i_built_a_tool_that_lets_you_export_your_saved",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/q5lcj6wr72yf1.png?width=640&crop=smart&auto=webp&s=d98a5251cb4ab76273f9118454a514c698438845",
        "title": "I built a tool that lets you export your saved Reddit posts directly into Notion or CSV",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hlamblurglar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T20:38:51.715816+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T12:18:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have looked through this subreddit and have found the answer to &quot;How do you keep your own family photos&quot; - but I am asking a slightly different question. We have 6 members of our family, across multiple generations, and we&#39;re looking to create a data repository we all have access to. This is a shared vault with grandfather&#39;s pictures and dad&#39;s wedding photos that the kids can also access and contribute to. </p> <p>Our plan is to upload hundreds of family photos, upload family videos (converted from VHS) and family records. </p> <p>Has anyone else done this? What does your setup look like when distributing this across multiple families? </p> <p>My thought was to export photo libraries (mostly on Macs right now, but a few PCs) to files, organize them into folders and then include a copy of a VNC viewer or something similar. We would send everyone a hard drive and then have a cloud version, maybe via Dropbox.</p> </div><!-- SC_ON ",
        "id": 3935905,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj2ytc/how_are_you_managing_family_photo_archives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How are you managing family photo archives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mean_Start5021",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T13:06:16.817743+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T11:43:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all,</p> <p>My NAS died, I was very sick of Synology anyways, so I now have a OWC Thunderbay 4 and I transferred my two 16 TB Ironwolf Pro HDD. However I feel so confused now the best way to run these two drives redundantly in RAID. I may expand in the future but this is fine for now I&#39;m using about 7-8 TB.</p> <p>My goal is to backup all of my photos to these hard drives, don&#39;t worry I am not going to just have everything on these drives I will practice proper redundancy but I don&#39;t know what software to use or if i should just use windows Storage spaces and file history to do this?</p> <p>The basic goal is, two 16 TB drives are RAID 1 and redundant, second changes are updated once a day to these drives. What is best to use? I have gotten so confused!</p> <p>I see OWC and Softraid but I would love to limit monthly charges for software as best I can.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/us",
        "id": 3931398,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj29ef/ive_gotten_myself_confused_dead_nas_new_das_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've gotten myself confused - Dead NAS, New DAS and backing up Professional Photos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Busy-Chemical-6666",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T11:23:12.227311+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T10:15:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oj0o5f/i_have_made_an_app_which_downloads_entire_reddit/\"> <img src=\"https://b.thumbs.redditmedia.com/VkYsG83O3Rr4nWqcScRwRu4yse_ehBGBhcrG8tY9X7s.jpg\" alt=\"I have made an app which downloads entire Reddit Post and Comments and displays it in a beauiful page.\" title=\"I have made an app which downloads entire Reddit Post and Comments and displays it in a beauiful page.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/drkhiw3wz0yf1.png?width=1277&amp;format=png&amp;auto=webp&amp;s=a2d580450c9bc2514db26349ddf3614af9eceaa5\">https://preview.redd.it/drkhiw3wz0yf1.png?width=1277&amp;format=png&amp;auto=webp&amp;s=a2d580450c9bc2514db26349ddf3614af9eceaa5</a></p> <p><a href=\"https://preview.redd.it/f3hflil701yf1.png?width=1277&amp;format=png&amp;auto=webp&amp;s=70c70d880de5effbf3b208beb43b99a1889efe57\">https://preview.redd.it/f3hflil701yf1.png?width=1277&amp;format=png&amp;auto=webp&amp",
        "id": 3930779,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj0o5f/i_have_made_an_app_which_downloads_entire_reddit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/VkYsG83O3Rr4nWqcScRwRu4yse_ehBGBhcrG8tY9X7s.jpg",
        "title": "I have made an app which downloads entire Reddit Post and Comments and displays it in a beauiful page.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Federal_Bluebird_897",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T16:47:21.641861+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T10:09:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys,<br/> We are migrating our gsuit account to enterprise accounts. Because of that, we will have over 1.2 Po of pooled storage on google drive (we have over 200 gsuit accounts)</p> <p>We use AWS s3 and GCP bucket to store data, but as we will have so much free/included google drive storage included in our subscriptions, I&#39;d like to transfer our storage from those buckets as well as our enterprise dropbox accounts and centralise all on google drive in the shared drives. 1.2 Po is more than enough for our needs.</p> <p>When I try RClone, I can see the my drive of the account, but I can&#39;t see the team shared drive. I&#39;m not able to transfer in the team shared drive to be that one centralised location.</p> <p>Is there any reliable/easy way to transfer data to a shared drive instead of the my drive ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Federal_Bluebird_897\"> /u/Federal_Bluebird_897 </a> <b",
        "id": 3933740,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oj0jv7/google_drive_rsyncrclone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google Drive - RSync/RClone",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EchoGecko795",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:41.073026+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T08:30:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Videos from several creators have been taken down on topics including how to install Windows 11 without logging into a Microsoft account and how to install Windows 11 on unsupported hardware.</p> <p>CyberCPU Tech reports:</p> <ul> <li><p><a href=\"https://youtu.be/l6p6g0-JUNA\">https://youtu.be/l6p6g0-JUNA</a></p></li> <li><p><a href=\"https://youtu.be/jgU6Web4PPM\">https://youtu.be/jgU6Web4PPM</a></p></li> </ul> <hr/> <p>Saw this posted on another sub, download those videos if you want to keep them.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EchoGecko795\"> /u/EchoGecko795 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiz0v0/youtube_is_taking_down_videos_on_performing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiz0v0/youtube_is_taking_down_videos_on_performing/\">[comments]</a></span>",
        "id": 3929216,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiz0v0/youtube_is_taking_down_videos_on_performing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "YouTube is taking down videos on performing nonstandard Windows 11 installs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kitchen-Lab9028",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:41.721619+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T07:10:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It&#39;s limited to 50 accounts on the free version. It doesn&#39;t seem to know if you have concurrent sessions since I currently have 2 systems that run simultaneously so that gives me 100 accounts for free.</p> <p>However, I came across a comment on reddit saying its possible to bypass that limit if you have the knowhow, but they didn&#39;t say anything further than that. Hoping you guys can help if that is possible. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kitchen-Lab9028\"> /u/Kitchen-Lab9028 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oixvd3/how_to_bypass_myfavett_download_limit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oixvd3/how_to_bypass_myfavett_download_limit/\">[comments]</a></span>",
        "id": 3929219,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oixvd3/how_to_bypass_myfavett_download_limit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to bypass myfavett download limit?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Think_Criticism_3665",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:41.930915+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T06:02:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Let&#39;s assume that the Live were going for 1 hour and i just join, i can save the hour that i missed?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Think_Criticism_3665\"> /u/Think_Criticism_3665 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiwurx/what_tool_i_can_use_to_save_a_live_from_youtube/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiwurx/what_tool_i_can_use_to_save_a_live_from_youtube/\">[comments]</a></span>",
        "id": 3929220,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiwurx/what_tool_i_can_use_to_save_a_live_from_youtube",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What tool i can use to save a live from youtube, tiktok or instagram?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/napoleonbonaparte33",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:40.809257+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T05:21:17+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiw757/dvd_encoder_build/\"> <img src=\"https://preview.redd.it/a0yk6g60kzxf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=630fcccd73e2a7e925032168635aa695327ffd2a\" alt=\"DVD Encoder Build\" title=\"DVD Encoder Build\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello</p> <p>Not sure if this is the right sub, but I\u2019m trying to figure something out.</p> <p>Lately, I\u2019ve been getting into converting MP4 files to MPEG-2 (DVD Video Format) so they can be played easily at my aunt\u2019s/grandma\u2019s house. The idea is to make it simple for my nieces and nephews to use (and to steer them away from YouTube Kids brain-rot content$</p> <p>Here\u2019s my current workflow: 1. H.264 .MP4 \u2192 FFMPEG encode \u2192 MPEG-2 .MPG 2. DVDStyler \u2192 .ISO \u2022 Add menu screen \u2022 Set up chapters 3. Burn to DVD (5/9)</p> <p>Right now, I\u2019m using my XPS 13 9360 (i5-7200U) to handle the encoding. I\u2019ve been using software encoding (libx264), which isn\u2019t too sl",
        "id": 3929214,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiw757/dvd_encoder_build",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/a0yk6g60kzxf1.jpeg?width=320&crop=smart&auto=webp&s=630fcccd73e2a7e925032168635aa695327ffd2a",
        "title": "DVD Encoder Build",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Late-Cream-8872",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:42.295719+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T04:00:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I\u2019m working on a research and preservation project focused on collecting large amounts of Ayurvedic data \u2014 including classical texts, research papers, and government publications (AYUSH, CCRAS, Shodhganga, PubMed, etc.).</p> <p>My goal is to build a structured digital archive for study and reference. I already have a few sources, but I need guidance on the best methods and tools for: \u2022 Large-scale PDF or paper download management (with metadata) \u2022 Structuring and deduplicating datasets \u2022 Archival formats or folder systems used for large research collections</p> <p>I\u2019m not using AI or selling anything \u2014 just looking for technical advice from experienced data hoarders on how to efficiently organize and preserve this type of data.</p> <p>Thanks in advance for any insights or resources you can share!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Late-Cream-8872\"> /u/Late-Cream-8872 </a> <br/> <s",
        "id": 3929222,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiuslo/seeking_guidance_collecting_and_organizing_large",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seeking Guidance: Collecting and Organizing Large Ayurvedic Data for a Research Project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CrazyaboutSpongebob",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:42.427626+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T03:48:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Warner Home Video only released the complete first season of Unikitty on DVD. I would love to own the rest of the seasons, but they are never going to release them. I would like to make my own. I could always use files from special sites but they all have the Cartoon Network logo on the corner and I would love for it to look like a professional DVD.</p> <p>What website can I buy the episodes from and store them on my hard drive? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CrazyaboutSpongebob\"> /u/CrazyaboutSpongebob </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiukf1/i_would_like_to_make_my_own_unikitty_dvd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiukf1/i_would_like_to_make_my_own_unikitty_dvd/\">[comments]</a></span>",
        "id": 3929223,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiukf1/i_would_like_to_make_my_own_unikitty_dvd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I would like to make my own Unikitty DVD.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pokethey",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:42.554724+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T03:25:38+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiu4gr/clicktump_from_refurbished_hard_drive/\"> <img src=\"https://external-preview.redd.it/-rkdhN1ywFGtASw8PVILGJ05zvcXW8rUip4a6Aymit0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe901eb262e51e9e8df256d40f7dc9bcf26deaca\" alt=\"Click/tump from refurbished hard drive\" title=\"Click/tump from refurbished hard drive\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pokethey\"> /u/pokethey </a> <br/> <span><a href=\"https://v.redd.it/ixujhhj8zyxf1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiu4gr/clicktump_from_refurbished_hard_drive/\">[comments]</a></span> </td></tr></table>",
        "id": 3929224,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiu4gr/clicktump_from_refurbished_hard_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/-rkdhN1ywFGtASw8PVILGJ05zvcXW8rUip4a6Aymit0.png?width=640&crop=smart&auto=webp&s=fe901eb262e51e9e8df256d40f7dc9bcf26deaca",
        "title": "Click/tump from refurbished hard drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Simple_Information31",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:40.649979+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T01:52:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been slowly digitizing my DVD collection of about 400 discs so far, mostly older movies. My current goal isn&#39;t fancy menus or extras, just a single playable file that can live on a USB stick and play across a few devices:</p> <p>a 2019 Samsung TV, and an older Sony Blu-ray player with USB input</p> <p>Here are the friction points I keep running into:</p> <p>Quality vs size \u2013 My target is roughly 4\u20135 Mbps H.265 so the file fits on a 64 GB stick and plays well. It works for most films, but when I hit darker, grainier transfers or older masters the compression artifacts start showing up and make the movie look wrong on the big screen.</p> <p>Subtitle &amp; audio \u2013 I aim for &quot;forced subs only&quot; versions with stereo + 5.1 audio where applicable, but some older discs hide them in weird tracks. The result: the PC plays fine, the TV shows no subs or the wrong audio track, and I&#39;ve wasted time re-ripping.</p> <p>If you&#39;ve gone thr",
        "id": 3929213,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ois7yc/whats_your_workflow_for_ripping_dvds_to_usb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What's your workflow for ripping DVDs to USB drives for TV playback",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pingg-",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:42.907700+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T01:49:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As stated in the title, I was wondering if there are any public archiving sites where you can search through public Discord servers and find specific messages you\u2019re looking for.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pingg-\"> /u/pingg- </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ois599/any_public_archiving_sites_for_discord/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ois599/any_public_archiving_sites_for_discord/\">[comments]</a></span>",
        "id": 3929226,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ois599/any_public_archiving_sites_for_discord",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any public archiving sites for discord?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wickedephie",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:41.282264+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T01:17:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,<br/> I could really use some help finding an extension or free software that lets me download high-resolution or original-size images from Coppermine galleries on fansites.</p> <p>I\u2019m currently using ImageHostGrabber on an old version of Pale Moon, but Cloudflare has been making it impossible to access those sites without updating to the latest version. And if I do update, IHG stops working.</p> <p>I also have Bulk Image Downloader, but it seems Cloudflare is causing issues with that too.</p> <p>I\u2019ve tried almost every Chrome extension out there, as well as JDownloader and WFDownloader. They seem to work at first, but when I check the folder, all I find are thumbnails instead of the full-size images.</p> <p>Also, I\u2019m not familiar with Python, so if your suggestion involves using it, please explain it in simple terms\u2014I\u2019d really appreciate that!</p> <p>Can anyone please help me out?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href",
        "id": 3929217,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oirgw0/help_to_download_images",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help to download images",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Unlikely_Spread_7782",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:42.112849+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T01:15:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 2 old drives - both seagate expansions, 16TB and 20TB. Bought in 2023 and 2024. I&#39;ve had issues with losing data on both but want to check what the actual situation of both is. Is there any recommended free software for mac?</p> <p>I don&#39;t think I was using these drives properly in the past which led to the data loss. At times they wouldnt connect so I went thru disk utility and repaired the drives, which I now understand was not a good decision. Just want to check if they are still reliable</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unlikely_Spread_7782\"> /u/Unlikely_Spread_7782 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oirf0b/free_mac_software_to_check_health_of_external/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oirf0b/free_mac_software_to_check_health_of_external/\">[comments]</a></span>",
        "id": 3929221,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oirf0b/free_mac_software_to_check_health_of_external",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Free Mac software to check health of external Seagate expansion drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/the_tico_life",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:43.182252+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T01:01:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For context, I have 4 different Samsung SSD T7s and at least 2 T5s. </p> <p>I also have 2 different Macbooks (2025 M4 Macbook Pro and a soon to be retired 2020 M1). </p> <p>I work in video and these SSDs have been my go to for years. </p> <p>The drives are still able to write just fine (I can take footage off of them to my Macs) but they can&#39;t read worth a damn. I mean, a 1 GB clip will just spin and spin and maybe decide it wants to copy after 5 or 10 minutes. Sometimes it just never does. </p> <p>I noticed this on one drive a few months ago. Then I saw it on another. Now today, I&#39;m trying to get more organized and am trying to determine which drives and / or cables are having issues. </p> <p>Believe it or not I&#39;m not having extremely slow write speeds on 4 different drives testing 3 different cables!! Also tested on both my macs.</p> <p>I&#39;m a total newb at computers, but logically speaking this makes me wonder - is it a Mac problem? ",
        "id": 3929227,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oir4e8/is_mac_suddenly_having_problems_writing_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Mac suddenly having problems writing to Samsung SSDs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Saphsin",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-29T08:44:43.325743+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-29T00:25:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t remember exactly when I bought this SSD but I suspect it was more than a decade ago, but it still seems to work? I save files on it occasionally (ebooks, videos, etc) but I&#39;m concerned the data in can potentially wear down and become corrupted because of the physical aging of the SSD, because this isn&#39;t very modern updated SSD technology.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Saphsin\"> /u/Saphsin </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiqbtx/lacie_120_gb_porsche_design_how_many_years_does/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oiqbtx/lacie_120_gb_porsche_design_how_many_years_does/\">[comments]</a></span>",
        "id": 3929228,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oiqbtx/lacie_120_gb_porsche_design_how_many_years_does",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Lacie 120 GB Porsche Design, how many years does it last?",
        "vote": 0
    }
]