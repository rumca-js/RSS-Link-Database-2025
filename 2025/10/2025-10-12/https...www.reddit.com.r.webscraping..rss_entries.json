[
    {
        "age": null,
        "album": "",
        "author": "/u/Cuaternion",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T21:23:14.696618+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T18:59:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good morning, I want to download a series of data from my Mastodon social network account, text, images and video that I uploaded a long time ago. Any recommendations to do it well and quickly? Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cuaternion\"> /u/Cuaternion </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4xi2v/webscraping_a_mastodon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4xi2v/webscraping_a_mastodon/\">[comments]</a></span>",
        "id": 3797848,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4xi2v/webscraping_a_mastodon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webscraping a Mastodon",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Elegant-Fix8085",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T19:20:18.324371+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T18:07:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I\u2019m learning Python and experimenting with scraping publicly available business data (agency names, emails, phones) for practice. Most sites are fine, but some\u2014like <a href=\"https://www.prima.it/agenzie\">https://www.prima.it/agenzie</a>, give me trouble and I don\u2019t understand why.</p> <p>My current stack / attempts:</p> <p>Python 3.12</p> <p>Requests + BeautifulSoup (works on simple pages)</p> <p>Tried Selenium + webdriver-manager but I\u2019m not confident my approach is correct for this site</p> <p>Problems I see:</p> <p>-pages that load content via JavaScript (so Requests/BS4 returns very little)</p> <p>-contact info in different places (footer, \u201ccontatti\u201d section, sometimes hidden)</p> <p>-some pages show content only after clicking buttons or expanding elements</p> <p>What I\u2019m asking:</p> <ol> <li><p>For a site like prima.it/agenzie, what would you use as the go-to script/tool (Selenium, Playwright, requests+JS rendering service, o",
        "id": 3797387,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4w53b/cant_extract_data_from_this_site",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can\u2019t extract data from this site \ud83e\udee5",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dracariz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T18:05:53.412352+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T17:41:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1o4vgf8/browsers_stealth_performance_benchmark_open_source/\"> <img src=\"https://external-preview.redd.it/4WcWPjWWJuZrJw73UKYzPBLuk2BJaGQAz-_TNOS1nno.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ef9c496078d7df9549d30c0c97d61d8a75e716f\" alt=\"Browsers stealth &amp; performance Benchmark [Open Source]\" title=\"Browsers stealth &amp; performance Benchmark [Open Source]\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;ve seen a lot of people asking which browser is best for automation, so here&#39;s a quick reminder about my open-source project - <strong>browsers-benchmark</strong>.</p> <p>I&#39;ve added new browser engines and made several improvements since the last post, so feel free to check it out and share your feedback!</p> <p><strong>Github:</strong> <a href=\"https://github.com/techinz/browsers-benchmark\"><strong>https://github.com/techinz/browsers-benchmark</strong></a></p> <p>---</p> <p><strong>H",
        "id": 3796953,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4vgf8/browsers_stealth_performance_benchmark_open_source",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/4WcWPjWWJuZrJw73UKYzPBLuk2BJaGQAz-_TNOS1nno.png?width=640&crop=smart&auto=webp&s=4ef9c496078d7df9549d30c0c97d61d8a75e716f",
        "title": "Browsers stealth & performance Benchmark [Open Source]",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Atronem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T16:46:31.474069+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T16:13:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We are seeking an operator to extract approximately 300,000 book titles from AbeBooks.com, applying specific filtering parameters that will be provided.</p> <p>Once the dataset is obtained, the corresponding PDF files should be retrieved from the Wayback Machine or Anna\u2019s Archive, when available.</p> <p>The estimated total storage requirement is around 4 TB. Data will be temporarily stored on a dedicated server during collection and subsequently transferred to 128 GB Verbatim or Panasonic optical discs for long-term preservation.</p> <p>The objective is to ensure the archive\u2019s readability and transferability for at least 100 years, relying solely on commercially available hardware and systems.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Atronem\"> /u/Atronem </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4t63t/hiring_scrape_300000_pdfs_and_archive_to_128_gb/\">[link]</a></span> &#32; ",
        "id": 3796521,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4t63t/hiring_scrape_300000_pdfs_and_archive_to_128_gb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HIRING: Scrape 300,000 PDFs and Archive to 128 GB VERBATIM Discs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/grass0927",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T14:05:03.591269+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T13:21:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I just learned about webscraping &amp; have been trying out some various extensions. However, I don\u2019t think I understand how to get anything to work in my situation\u2026 so I just need to know if it\u2019s not possible </p> <p><a href=\"https://www.bkstr.com/uchicagostore/shop/textbooks-and-course-materials\">https://www.bkstr.com/uchicagostore/shop/textbooks-and-course-materials</a> Id like a spreadsheet of all of the Fall books under the course code LAWS but there\u2019s many course codes and each has sub sections. </p> <p>Is this something I can do with a chrome extension and if so is there one you recommend? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/grass0927\"> /u/grass0927 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4oxnn/can_i_webscrape_a_college_textbook_website_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4oxnn/can_i_webscrape_a_coll",
        "id": 3795563,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4oxnn/can_i_webscrape_a_college_textbook_website_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I webscrape a college textbook website with drop down options",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/apadjon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T14:05:03.716720+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T11:30:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks \ud83d\udc4b</p> <p>If you work with web scraping, REST APIs, or data analysis, you probably deal with tons of JSON and JSONL files. And if you\u2019ve tried to inspect or debug them, you know how annoying it can be to find a good viewer that:</p> <ul> <li>doesn\u2019t crash on big files,</li> <li>can handle malformed JSON,</li> <li>or supports JSONL (newline-delimited JSON).</li> </ul> <p>Most tools out there are either too basic (just a formatter) or too bloated (enterprise-level stuff). So\u2026 I built my own:</p> <p>\ud83d\udc49 JSON Treehouse</p> <p>A free online JSON viewer and inspector built specifically for developers working with real-world messy data.</p> <p>\ud83e\udde9 Core Features</p> <p>100% Free \u2014 no ads, no login, no paywalls</p> <p>JSON + JSONL support \u2014 handles standard &amp; newline-delimited JSON</p> <p>Broken JSON parser \u2014 gracefully handles malformed or invalid files</p> <p>Large file support \u2014 works with big data without freezing your browser</p> <p>\ud83d\udcbb Developer-F",
        "id": 3795564,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4modp/free_json_viewer_inspector_works_with_json_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Free JSON Viewer & Inspector - Works with JSON and JSONL Files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cooljs2005",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T11:27:18.927698+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T09:20:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks \ud83d\udc4b</p> <p>I\u2019m building something interesting at the intersection of AI + real-estate data \u2014 a system that scrapes, cleans, and structures large-scale property data to power intelligent recommendations.</p> <p>I\u2019m looking for a curious, self-motivated Python developer or web scraping enthusiast (intern/freelance/collaborator \u2014 flexible) who enjoys solving tough data problems using Playwright/Scrapy, MongoDB/Postgres, and maybe LLMs for messy text parsing.</p> <p>This is real work, not a tutorial \u2014 you\u2019ll get full ownership of one data module, learn advanced scraping at scale, and be part of an early-stage build with real-world data.</p> <p>\ud83d\udca1 Remote | Flexible | \u20b95k\u2013\u20b910k/month (or open collaboration) If this sounds exciting, DM me with your GitHub or past scraping work. Let\u2019s build something smart from scratch.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cooljs2005\"> /u/Cooljs2005 </a> <br/> <span><a h",
        "id": 3794813,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4kk7x/looking_for_a_web_scraper_to_join_an_ai",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\ud83d\ude80 Looking for a web scraper to join an AI + real-estate data project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/South-Mirror1439",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T07:42:04.990948+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T06:20:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i am trying to access a java wicket website , but during high traffic sending multiple request using rnet causes the website to return me a 500 internal server wicket error , this error is purely server sided. I used charles proxy to see the tls config but i don&#39;t know how to replicate it in rnet , is there any other http library for python for crafting the perfect the tls handshake http request so that i can bypass the wicket error. </p> <p>the issue is using the latest browser emulation on rnet gives away too much info , and the site uses akamai cdn which also has the akamai waf as well i assume , despite it not appearing in the wafwoof tool , searing the ip in censys revealed that it uses a waf from akamai , so is there any way to bypass it ? also what is the best way to find the orgin ip of a website without paying for security trails or censys</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/South-Mirror1",
        "id": 3792572,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4hqsc/how_to_make_a_11_copy_of_the_tls_fingerprint_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to make a 1:1 copy of the tls fingerprint from a browser",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Initial_Panda3090",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T07:42:04.707285+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T01:58:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I\u2019m trying to make Zendriver use a different browser fingerprint every time I start a new session. I want to randomize things like: User-Agent, Platform (e.g. Win32, MacIntel, Linux), Screen resolution and device pixel ratio, Navigator properties (deviceMemory, hardwareConcurrency, languages), Canvas/WebGL fingerprints. Any guidance or code examples on the right way to randomize fingerprints per run would be really appreciated. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Initial_Panda3090\"> /u/Initial_Panda3090 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4d2ch/zen_driver_fingerprint_spoofing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o4d2ch/zen_driver_fingerprint_spoofing/\">[comments]</a></span>",
        "id": 3792570,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4d2ch/zen_driver_fingerprint_spoofing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Zen Driver Fingerprint Spoofing.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hot_Tumbleweed5878",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T07:42:04.859079+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T00:23:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to find the API endpoint that returns the store list on this page:<br/> \ud83d\udc49 <a href=\"https://5hourenergy.com/pages/store-locator?utm_source=chatgpt.com\">https://5hourenergy.com/pages/store-locator</a></p> <p>It uses <strong>Destini /</strong> <a href=\"http://lets.shop\"><strong>lets.shop</strong></a> for the locator.<br/> When you search by ZIP, the first call hits <strong>ArcGIS</strong> (<code>findAddressCandidates</code>) \u2014 that gives lat/lng, but not the stores.</p> <p>The real request (the one that should return the JSON with store names, addresses, etc.) doesn\u2019t show up in DevTools \u2192 Network.<br/> I tried filtering for <code>destini</code>, <a href=\"http://lets.shop\"><code>lets.shop</code></a>, <code>locator</code>, even patched <code>window.fetch</code> and <code>XMLHttpRequest</code> to log all requests \u2014 still can\u2019t see it.</p> <p>Anyone knows how to capture that hidden fetch or where Destini usually loads its JSON from?<br/> I just n",
        "id": 3792571,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4b6ab/need_help_finding_the_json_endpoint_used_by_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help finding the JSON endpoint used by a Destini Store Locator",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AnonymousCrawler",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T07:42:05.193204+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T00:13:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to make my first production-based scrapper, but the code is not working as expected. Appreciate if anyone faced a similar situation and guide me how to go ahead!</p> <p>The task of the scrapper is to post a requests form behind a login page under favorable conditions. I tested the whole script on my system before deploying it on AWS. The problem is in the final steps of my task when it has to submit a form using requests, it does not fulfill the request. </p> <p>My code confirms if that form is submitted using the HTML text of redirect page (like &quot;Successful&quot;) after the form is submitted, The strange thing is my log shows even this test has passed, but when I manually log in later, it is not submitted! How can this happen? Anyone knows what&#39;s happening here?</p> <p><strong>My Setup:</strong></p> <p><em>Code:</em> Python with selenium, requests</p> <p><em>Proxy:</em> Datacenter. I know using Residential/ Mobile is better, but test ",
        "id": 3792573,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o4az9k/scrapper_not_working_in_vm_please_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scrapper not working in VM! Please help!",
        "vote": 0
    }
]