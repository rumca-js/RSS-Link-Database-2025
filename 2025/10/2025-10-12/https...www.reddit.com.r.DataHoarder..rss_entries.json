[
    {
        "age": null,
        "album": "",
        "author": "/u/TheWizardCat_YT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T21:07:39.405292+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T19:58:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, sorry if I this has been asked before, but I was wondering:<br/> With the events of censorship from institutions and organizations as well as the new capability of being able to generate huge amounts of misinformation in just a few seconds, is there anything that allows to have for example hoarded information by different people easily verificated to check that it wasn&#39;t tampered with or that it isn&#39;t completely made up?</p> <p>And yes I know that already exists techniques like checksum or blockchain technology to verify information in general, but for those mechanisms to work it is needed that someone or somewhere have the original value to compare to or that a system is already put in place, and given that the nature of data hoarding is that anyone can be the one downloading and uploading the information I wanted to know if there is any agreement already or such systems so that if the day comes where information can only be accessed from",
        "id": 3797744,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4z0rp/verification_of_hoarded_information_and_more",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Verification of hoarded information and more",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/whatcrawish",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T21:07:39.143153+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T19:43:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Since it sounds like it\u2019s in danger and there\u2019s a ton of public health info on there. I didn\u2019t see a similar post. Apologies if there is. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/whatcrawish\"> /u/whatcrawish </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4ymto/anyone_working_on_the_cdc_site/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4ymto/anyone_working_on_the_cdc_site/\">[comments]</a></span>",
        "id": 3797743,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4ymto/anyone_working_on_the_cdc_site",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone working on the CDC site ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/shawndw",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T17:52:16.356866+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T17:27:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Recently I have decided to create and maintain an offline repository of the archlinux repository (aprox 275 GB) and to maintain offline copies of arch repositories (multilib, extra, and core).</p> <p>I have done this to ensure that I can have a functioning linux distribution in the event of an internet shutdown and I figured it might be a good idea to document how I did this for anyone else that may be interested.</p> <p>First you will need to find a mirror that supports rsync. A list of mirrors can be found here <a href=\"https://archlinux.org/mirrors/status/\">https://archlinux.org/mirrors/status/</a> for my example I will be using &quot;ftp.acc.umu.se/mirror/archlinux&quot;</p> <p>Next you will want to ensure that rsync is available. It can be found in the arch repository &quot;sudo pacman -S rsync&quot;. rsync is a protocol designed to synchronize data stored in two different locations running the rsync command can be used to create a mirror as well",
        "id": 3796927,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4v392/backing_up_the_entire_arch_repository",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backing up the entire arch repository",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/404WalletNotFound",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T17:52:16.494754+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T16:58:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a ReFS Storage Space with 3 22TB drives (2 data + 1 parity).</p> <p>The write performance is not awesome and I have read that adding journal drives can speed it up as much as 150%.</p> <p>The problem is finding drives are are suitable for this. It seems like maybe this setup doesn&#39;t benefit from a very large Write Back Cache, with the docs mostly suggesting 1GB is good and 100GB is overkill. However, Storage Spaces wants to completely own the journal drives. So that puts me in the market for a low-capacity (like 128 GB) but high durability SSD. </p> <p>The number of options here is both over and underwhelming at the same time. My motherboard is a WRX90E. I am currently using all the M.2 and SATA ports. I have 2 SlimSAS and several PCIe slots that I could use to attach any number of drive technologies to use for this journal drives.</p> <p>It seems like maybe NVMe U.2 drives connected via SlimSAS has the fewest points of failure, but all the",
        "id": 3796928,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4ubod/journal_drives_for_refs_storage_spaces_w_parity",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Journal Drives for ReFS Storage Spaces w/ Parity, Looking for Hardware Recommendations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dukeman87",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T17:52:16.640667+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T16:44:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Planning ahead - trying to figure out which multi bay HDD enclosure I should get next. I currently have two 5 bay mobius raid-type enclosures (I don&#39;t have raid setup - I use stablebit drivepool and scanner) and I&#39;d like to get something with more than 5 bays when the time comes. What is a better than decent jbod type of enclosure that i should be looking at? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dukeman87\"> /u/Dukeman87 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4tyv7/external_enclosure/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4tyv7/external_enclosure/\">[comments]</a></span>",
        "id": 3796929,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4tyv7/external_enclosure",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "External enclosure",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/quinnshanahan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T15:08:12.350151+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T15:03:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, my current setup is as follows:</p> <p>* i use syncthing to keep data synced to a zfs server<br/> * zfs server contains RAIDZ2 pool of 5 drives<br/> * contents of zfs that i care about are all backed up using restic snapshots to b2</p> <p>Given I have local redundancy and remote backups here, I feel pretty good about this solution. However, there are a few areas I&#39;d like to improve:</p> <p>* remote redundancy<br/> * protect against bit rot (restic stores everything as &quot;content addressable&quot;, but no protection against potential changes in underlying data found at a content address)<br/> * no ransomware protection</p> <p>The solution i&#39;m looking at to solve all three is to replicate my b2 objects to aws glacier deep archive, the idea being I will basically never want to read the data back out, save for a disaster recover scenario. Here&#39;s the setup I&#39;m planning:</p> <p>* create dedicated AWS account<br/> * create a bucket con",
        "id": 3796027,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4rd6k/protecting_backups_against_ransom_attacks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Protecting backups against ransom attacks",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mariusmoga_2005",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T15:08:12.226402+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T14:02:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey,</p> <p>Sorry for maybe the stupid question, I am interested in getting the F8 SSD Plus. </p> <p>However I have one question - I don&#39;t need it running all the time. If I turn it off, when I turn it on again, will it rebuild anything? Or I can simply use it?</p> <p>Can I use it as a JBOD?</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mariusmoga_2005\"> /u/mariusmoga_2005 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4pv3g/terramaster_f8_ssd_plus_can_you_turn_it_off/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4pv3g/terramaster_f8_ssd_plus_can_you_turn_it_off/\">[comments]</a></span>",
        "id": 3796026,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4pv3g/terramaster_f8_ssd_plus_can_you_turn_it_off",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "TerraMaster F8 SSD Plus - Can you turn it off?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ghostchihuahua",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T12:29:27.768904+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T11:16:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone!</p> <p>First: title is fudged, i mean 2.5\u2019\u2019</p> <p>I&#39;ve recently moved and find myself going through boxes of HDD&#39;s, many, many of them being 2.5&quot; format SSD&#39;s, from 128GB to 4TB, there must be about 20-25 of them just under my eyes.</p> <p>Also, i only have that one imac Pro at home for now, the guy&#39;s happy as camper but seems to understandingly have trouble with feeding power to those when i start connecting them on hubs (normal...).</p> <p>Ideally, i&#39;d need to have most of them online so as to find dupes, make room, concatenate certain disks entirely to others in order to make free disks etc;</p> <p>Would you guys maybe share ideas and recommendations on that one?<br/> I thought daisy-chaining was possible but i find no self-powering hardware where i can just plop in 12 2.5&quot;ers and go with it...</p> <p>Many thanks in advance!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit",
        "id": 3795131,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4mfnw/just_moved_will_build_a_new_lab_need_advice_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Just moved, will build a new lab, need advice on managing hordes of 3.5 SSD drives on a single iMac in the meantime... Help! :/",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Regretnothing75",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T11:12:46.505112+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T10:36:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! So I was approached about a job for someone which I can do I just have no clue what the rate to charge would be. I\u2019m basically taking 7 hdd\u2019s from over the years and putting all the data on one big external hard drive basically consolidating everything for them. They have the external hdd so that\u2019s not included in price but what should I charge to do a job like this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Regretnothing75\"> /u/Regretnothing75 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4lr58/data_transfer_pricing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4lr58/data_transfer_pricing/\">[comments]</a></span>",
        "id": 3794796,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4lr58/data_transfer_pricing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data transfer pricing",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Additional_Flight522",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T09:57:54.854536+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T09:30:08+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4kpe1/cambridge_university_launches_project_to_rescue/\"> <img src=\"https://external-preview.redd.it/aEJmTZ_WINd9VIWRTGpByfiT4ehm_Vl9c_X0gGzg5II.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1a0d106cb438f7d29485d1cea1896e54487ca06\" alt=\"Cambridge University launches project to rescue data trapped on old floppy disks\" title=\"Cambridge University launches project to rescue data trapped on old floppy disks\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Additional_Flight522\"> /u/Additional_Flight522 </a> <br/> <span><a href=\"https://www.tomshardware.com/pc-components/storage/cambridge-university-rescues-data-from-old-floppy-disks\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4kpe1/cambridge_university_launches_project_to_rescue/\">[comments]</a></span> </td></tr></table>",
        "id": 3794457,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4kpe1/cambridge_university_launches_project_to_rescue",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/aEJmTZ_WINd9VIWRTGpByfiT4ehm_Vl9c_X0gGzg5II.png?width=640&crop=smart&auto=webp&s=b1a0d106cb438f7d29485d1cea1896e54487ca06",
        "title": "Cambridge University launches project to rescue data trapped on old floppy disks",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Malangsia",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T11:12:46.730658+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T09:24:23+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Malangsia\"> /u/Malangsia </a> <br/> <span><a href=\"/r/photography/comments/1o4kkmc/upgrading_storageworkstation_setup_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4km6x/upgrading_storageworkstation_setup_for/\">[comments]</a></span>",
        "id": 3794797,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4km6x/upgrading_storageworkstation_setup_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Upgrading storage/workstation setup for professional photographer - advice appreciated",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dobik7",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T09:57:54.660005+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T09:09:11+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4kdtx/inspired_by_recent_post_i_saw_my_newish_setup/\"> <img src=\"https://preview.redd.it/7intl656dnuf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9edc3dd4499c82b77eae27a1ecac415643a24279\" alt=\"Inspired by recent post i saw, my new(ish) setup.\" title=\"Inspired by recent post i saw, my new(ish) setup.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dobik7\"> /u/dobik7 </a> <br/> <span><a href=\"https://i.redd.it/7intl656dnuf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4kdtx/inspired_by_recent_post_i_saw_my_newish_setup/\">[comments]</a></span> </td></tr></table>",
        "id": 3794456,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4kdtx/inspired_by_recent_post_i_saw_my_newish_setup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/7intl656dnuf1.png?width=640&crop=smart&auto=webp&s=9edc3dd4499c82b77eae27a1ecac415643a24279",
        "title": "Inspired by recent post i saw, my new(ish) setup.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dreat15",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T11:12:47.056133+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T09:07:42+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4kczp/refurbished_seagate_exos_good_offer/\"> <img src=\"https://external-preview.redd.it/gfsrSwCpAy6lA9eTOckS98vxKITDe-To3dxn1Pgxtws.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4528520d5aef0f3e8b31113be76d7eb0d1574b6\" alt=\"Refurbished Seagate Exos good offer?\" title=\"Refurbished Seagate Exos good offer?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/as8s1lp7anuf1.png?width=1963&amp;format=png&amp;auto=webp&amp;s=0feec0b609714bc896d8f02ce59719da6f1f646e\">https://preview.redd.it/as8s1lp7anuf1.png?width=1963&amp;format=png&amp;auto=webp&amp;s=0feec0b609714bc896d8f02ce59719da6f1f646e</a></p> <p>I know it&#39;s in german, but is it safe to buy them? I would buy two of them, to start my data hoarding game :D</p> <p>It&#39;s 12\u20ac/TB or 13.95$/TB</p> <p><a href=\"https://www.alternate.de/Seagate/Seagate-Exos-24-TB-General%C3%BCberholt-Festplatte/html/product/100159279\">https://",
        "id": 3794798,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4kczp/refurbished_seagate_exos_good_offer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/gfsrSwCpAy6lA9eTOckS98vxKITDe-To3dxn1Pgxtws.jpeg?width=640&crop=smart&auto=webp&s=f4528520d5aef0f3e8b31113be76d7eb0d1574b6",
        "title": "Refurbished Seagate Exos good offer?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lolaids25",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T08:46:03.862630+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T08:45:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Any way to download media from <a href=\"https://www.fanvue.com/\">https://www.fanvue.com</a> ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lolaids25\"> /u/Lolaids25 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4k0sy/fanvue_download/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4k0sy/fanvue_download/\">[comments]</a></span>",
        "id": 3794167,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4k0sy/fanvue_download",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fanvue download",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Marmarasqw",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T11:12:47.444818+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T08:26:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p> <p>Si I&#39;ve been experiencing a problem while trying to burn DVDs using DVD Flick and ImgBurn. It ejects the tray either after 52% or before 80% on most of the movies I&#39;ve tried. </p> <p>I&#39;m using the Asus ZenDrive with all the drivers updated, the CDs i use are Verbatim Life Series DVD+R DL and in the settings I use create chapters every 1 minute, bitrate auto to get the highest possible, and when choosing the break point I&#39;ve tried going for 50/50 with the lowest padding, and I&#39;ve also tried 51/49 and 52/48 with as close to 0 padding i can find. </p> <p>I&#39;ve gotten lucky on some of the movies I&#39;ve burned and gotten a 100%, but most of the times it just ejects half way through resulting in a trashed dvd. </p> <p>Is there a way to get rid of this problem? Any tips would be appreciated if I&#39;m doing something wrong. I&#39;m new to this but it&#39;s kind of straightforward as a software.</p> <p>Thanks in",
        "id": 3794799,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4jpmh/dvd_flick_double_layer_writing_problem",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DVD Flick Double Layer Writing Problem",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Fantastic-Hair1554",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T08:46:03.994154+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T07:37:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I own a brand new SanDisk Portable SSD (1TB) for general use and storing my photos and videos, and a SeaGate Expansion Drive (2TB) for long-term archiving (also a backup), and two SanDisk USB\u2019s (also back-ups). Do I need to do anything else to keep my critical data for a long time?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fantastic-Hair1554\"> /u/Fantastic-Hair1554 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4iygr/how_to_do_the_321_backup_method/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4iygr/how_to_do_the_321_backup_method/\">[comments]</a></span>",
        "id": 3794168,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4iygr/how_to_do_the_321_backup_method",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to do the \u201c3-2-1 Backup\u201d method?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/asjarra",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T07:25:15.753175+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T05:22:10+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4gs4y/used_hd_on_ebay_talk_me_out_of_it/\"> <img src=\"https://a.thumbs.redditmedia.com/hx2pVcvqgKi_H8esrRfNLXs6_QTqWVlLXccBB5ecfJ4.jpg\" alt=\"USED HD on eBay - talk me out of it?\" title=\"USED HD on eBay - talk me out of it?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/5byolkc19muf1.jpg?width=449&amp;format=pjpg&amp;auto=webp&amp;s=289dd47734c23d8859e3982cbb0385cb5f48e8be\">https://preview.redd.it/5byolkc19muf1.jpg?width=449&amp;format=pjpg&amp;auto=webp&amp;s=289dd47734c23d8859e3982cbb0385cb5f48e8be</a></p> <p>USED 10TB Ironwolf Pro being sold by an individual (not a company) on eBay for $235 AUD ($155 USD) described as &quot;Excellent Condition&quot;</p> <p>I&#39;m not falling super hard, but it is in my bookmarks and I do keep coming back to it. Never bought a used drive. Talk me out of it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/u",
        "id": 3792438,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4gs4y/used_hd_on_ebay_talk_me_out_of_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/hx2pVcvqgKi_H8esrRfNLXs6_QTqWVlLXccBB5ecfJ4.jpg",
        "title": "USED HD on eBay - talk me out of it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Limp_Fig6236",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-12T07:25:16.284784+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-12T04:19:55+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4fpqc/beedrive_your_personal_backup_hub/\"> <img src=\"https://external-preview.redd.it/oxnZ23yqWiDcPipomENi9Q-kM05TJaS-TCZW5uK0nW4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ab14e8cbdd09558e281e52db4b5e895dcf4c69a\" alt=\"BeeDrive | Your personal backup hub\" title=\"BeeDrive | Your personal backup hub\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>what does everyone think about BeeDrive by Synology? Good? Bad? Meh? Would you use it? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Limp_Fig6236\"> /u/Limp_Fig6236 </a> <br/> <span><a href=\"https://bee.synology.com/en-us/BeeDrive\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1o4fpqc/beedrive_your_personal_backup_hub/\">[comments]</a></span> </td></tr></table>",
        "id": 3792442,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1o4fpqc/beedrive_your_personal_backup_hub",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/oxnZ23yqWiDcPipomENi9Q-kM05TJaS-TCZW5uK0nW4.jpeg?width=640&crop=smart&auto=webp&s=5ab14e8cbdd09558e281e52db4b5e895dcf4c69a",
        "title": "BeeDrive | Your personal backup hub",
        "vote": 0
    }
]