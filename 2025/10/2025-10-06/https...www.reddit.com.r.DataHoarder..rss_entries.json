[
    {
        "age": null,
        "album": "",
        "author": "/u/Cypher_Vorthos",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T23:14:49.770806+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T21:45:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019m working on a project for my wife, consolidating and archiving all of our photos and videos into a single, well-organized drive. I want to make sure they\u2019re safely stored for many years to come.</p> <p>I know this community values reliability and longevity, so I\u2019d really appreciate your advice. What hard drive brand/model do you recommend for long-term storage? I\u2019m mainly looking for something that\u2019s:</p> <ul> <li>Extremely reliable and durable</li> <li>Suitable for long-term, low-usage archival (not constant read/write)</li> <li>Ideally large enough (8TB+), but I\u2019m flexible</li> <li>Preferably an HDD, unless SSDs are now considered viable for decades-long storage</li> </ul> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cypher_Vorthos\"> /u/Cypher_Vorthos </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzwgs4/best_longterm_hard_drive_for_photo_archiv",
        "id": 3750800,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzwgs4/best_longterm_hard_drive_for_photo_archiving",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best long-term hard drive for photo archiving \u2014 looking for reliability above all else",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/didyousayboop",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T21:48:27.163680+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T20:32:57+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzuit4/a_billion_year_archive_of_human_knowledge_arch/\"> <img src=\"https://external-preview.redd.it/jfLsi7xhfq2_qFtO5aSwHXl4OS7zFPAjWFI3poX5LA0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c31e9b17b0c81080b612b790d681759bac010945\" alt=\"&quot;A Billion Year Archive Of Human Knowledge&quot; (Arch Mission Foundation)\" title=\"&quot;A Billion Year Archive Of Human Knowledge&quot; (Arch Mission Foundation)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/didyousayboop\"> /u/didyousayboop </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=V-50aPpjhuE\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzuit4/a_billion_year_archive_of_human_knowledge_arch/\">[comments]</a></span> </td></tr></table>",
        "id": 3750201,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzuit4/a_billion_year_archive_of_human_knowledge_arch",
        "manual_status_code": 0,
        "page_rating": 86,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/jfLsi7xhfq2_qFtO5aSwHXl4OS7zFPAjWFI3poX5LA0.jpeg?width=320&crop=smart&auto=webp&s=c31e9b17b0c81080b612b790d681759bac010945",
        "title": "\"A Billion Year Archive Of Human Knowledge\" (Arch Mission Foundation)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tyler----durden",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T20:09:49.371360+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T19:20:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for someone who likes a challenge and is willing to create a script/software to download/scrape full resolution images from eg. Photos.com, without watermarks. </p> <p>There used to be a way to fix that (below), but it didn\u2019t capture the images in full and the download method was extremely slow. </p> <p>All the images consist of tiles and they somehow have to be stitched together.</p> <p>Of course, I\u2019m willing to pay.</p> <p><a href=\"https://github.com/agmmnn/fineartdown\">https://github.com/agmmnn/fineartdown</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tyler----durden\"> /u/tyler----durden </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzsihy/photoscom_willing_to_pay/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzsihy/photoscom_willing_to_pay/\">[comments]</a></span>",
        "id": 3749533,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzsihy/photoscom_willing_to_pay",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Photos.com (willing to pay)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/arrthekiller",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T20:09:49.638407+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T18:58:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have recently been gifted a one month sub for a great musician&#39;s Patreon, and unfortunately don&#39;t have enough time this time of the year to use it properly. I need to dowload at least a part of the video collection (of course preferably everything), otherwise it will just go to waste. I&#39;ve tried installing yt-dlp (Windows 11), but just installing the .exe doesn&#39;t seem to be enough. I was hoping to get some help with dowloading the content since im quite new to this and not a native speaker. Either help with the yt-dlp or something easier and with more user friendly GUI would help a lot. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/arrthekiller\"> /u/arrthekiller </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzrxer/trying_to_dowload_content_from_patreon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzrxer/trying_to_dow",
        "id": 3749534,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzrxer/trying_to_dowload_content_from_patreon",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to dowload content from Patreon",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/viper7242",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T18:48:07.077800+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T17:55:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been using one of these for 10+ years now for a backup drive (internal 5.25 hot swap bay) and only recently stumbled across the fact that SATA to Molex could cause a fire. Should I stop using it?</p> <p><a href=\"https://a.co/d/daS8rby\">https://a.co/d/daS8rby</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/viper7242\"> /u/viper7242 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzq8a9/is_this_sata_to_molex_adapter_safe/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzq8a9/is_this_sata_to_molex_adapter_safe/\">[comments]</a></span>",
        "id": 3748953,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzq8a9/is_this_sata_to_molex_adapter_safe",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is this SATA to Molex adapter safe?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Turbulent_Math4498",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T20:09:50.054786+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T17:20:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>WinRar has a &quot;test&quot; option for corrupted files. Does this option only work for compressed rar files? Does it not efficiently test 7z, xz, and zip? Is it necessary to have multiple software from each format developer to test?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Turbulent_Math4498\"> /u/Turbulent_Math4498 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzpald/winrar_doubt/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzpald/winrar_doubt/\">[comments]</a></span>",
        "id": 3749535,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzpald/winrar_doubt",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "winrar doubt",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/p42io",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T20:09:50.224532+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T17:19:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I am currently starting to migrate from my old Windows 10/WSL based system towards the Apple ecosystem, and I am wondering about long term storage options there. I am not too keen about the prospect of the Apple tool <em>time mashine</em> due to the lock-in provided.</p> <p><strong>Current configration</strong>, Windows (WSL/Linux)</p> <p>a. Long term storage: <strong>Four 4 TB hard disks</strong>. Manual full sync via <em>robocopy</em>. Disks are formatted with NTFS and fully encrypted via <em>veracrypt.</em><br/> b. Partial working copy of this archive and backup of this copy on two 100 GB encrypted veracrypt volume formatted with NTFS. This encrypted volume is transferred to the long term storage disks from time to time. From there, point (a.) applies.</p> <p>I introduced a manual checksum check against bitrot, which I do 1 time per year. With my <strong>1.7 TB of data and 250K files</strong>, this takes about 6 hours to complete for one",
        "id": 3749536,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzp9my/backup_configuration_for_an_apple_based_setup",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup configuration for an Apple based setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/evildad53",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T17:26:31.479941+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T16:55:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Got an email &quot;Recertified high-capacity drives are now here!&quot; But the link <a href=\"https://www.seagate.com/seagate-recertified/\">https://www.seagate.com/seagate-recertified/</a> shows 3 recertified drives (22, 24 and 28TB), &quot;backed by a six month warranty.&quot; You get better than that at Go Hard Drive and Server Parts Deals.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/evildad53\"> /u/evildad53 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzombn/factory_recert_drives_at_seagate_website_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzombn/factory_recert_drives_at_seagate_website_with/\">[comments]</a></span>",
        "id": 3748241,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzombn/factory_recert_drives_at_seagate_website_with",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Factory recert drives at Seagate website with only 6 month warranty",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cody4king",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T17:26:31.557797+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T16:26:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I had a full copy of Niagara on a server at home and I nuked the server. To my dismay, I do not have a copy of the install media, does anyone in this sub happen to have a copy?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cody4king\"> /u/cody4king </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzntky/looking_for_a_copy_of_niagara/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzntky/looking_for_a_copy_of_niagara/\">[comments]</a></span>",
        "id": 3748242,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzntky/looking_for_a_copy_of_niagara",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a copy of Niagara",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Master_Camp_3200",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.598834+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T15:59:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using Win 11. Not sure how to do this but it seems like a thing Dupeguru should do.</p> <p>I have a folder with a bunch of subfolders, all of which are close (but not identical) copies of, effectively My Documents. So let&#39;s say:</p> <p>C:/copies</p> <p>C:/copies/marchfiles </p> <p>C:/copies/aprfiles </p> <p>C:/copies/mayfiles </p> <p>C:/copies/junefiles </p> <p>The vast majority of the files in all these folders are dupes. But there will be some in /aprfiles that aren&#39;t in /mayfiles, some that are in /marchfiles aren&#39;t in /junefiles, etc.</p> <p>I want to get rid of all the dupes in /copies overall, but make sure I&#39;ve kept any files that are in /marchfiles, /aprfiles, and /mayfiles but are *not* in /junefiles.</p> <p>In other words, I want to have /junefiles as reference where-ever any of its files are duplicated elsewhere. Then I&#39;ll amalgamate them manually as there won&#39;t be that many of these leftovers. </p> <p>I hope",
        "id": 3747438,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzn2oz/dupeguru_how_to_make_one_folder_and_subfolders",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Dupeguru - how to make one folder (and subfolders) the reference folder overall, without having to set it for every duplicate instance?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/happydaddeadinside",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.670712+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T15:51:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been a long time lurker here and looking for a simple nas/media server solution.</p> <p>I love tinkering and I&#39;m with a strong background in IT and networking, but in the past I would run these setups on large towers - now I&#39;m just looking for something I can stash away alongside my router.</p> <p>I&#39;m looking for something to:</p> <p>1) Handle time machine backups for 2 Macbooks over the network (1TB each).<br/> 2) Run Homebridge<br/> 3) Host a plex/jellyfin server<br/> 4) Immich or alternative as second on prem backup to iCloud (roughly 4TB of data there)<br/> 5) Possibly download torrents in the future.</p> <p>Beelink are having a sale on Amazon on all models (30% off) <a href=\"https://www.amazon.com/gp/product/B0DF2G11J6?th=1\">https://www.amazon.com/gp/product/B0DF2G11J6?th=1</a><br/> Would that be a good solution? Is that deal worth it (with prime day around the corner)? </p> <p>I&#39;ve been out of the loop for a couple of ye",
        "id": 3747439,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzmuun/beelink_are_having_a_sale_on_amazon_is_it_any",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Beelink are having a sale on Amazon - is it any good for nas/media streaming?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/_thetrue_SpaceTofu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.780732+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T15:09:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>as per title, I am going to NY for a holiday.<br/> I was thinking of buying a couple of WD HD for my NAS as they are 30/40% cheaper than back at home.</p> <p>Where to can I ship these hard drives?<br/> Is it safe to ship to the hotel we will be staying at? If this is the way, is it best to wait to be there physically before placing the order?</p> <p>or is it common in US / NY to have parcels shipped to a pickup point?</p> <p>I was going to use the official WD webshop, newegg and bhphotovideo. Or potentially Amazon if it offers any shipping option which could prove useful for my use case.<br/> Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_thetrue_SpaceTofu\"> /u/_thetrue_SpaceTofu </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzlp0e/i_am_going_to_ny_for_a_holiday_where_to_can_i/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzlp0e/i_am_g",
        "id": 3747440,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzlp0e/i_am_going_to_ny_for_a_holiday_where_to_can_i",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I am going to NY for a holiday - where to can I ship a couple of brand new hard drives to take back home?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SheriffRoscoe",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.452519+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T14:56:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t know how reliable the <em>Korea JoongAng Daily</em> is, but this is the first report I&#39;ve seen of this event. Apropos of <a href=\"/r/DataHoarder\">/r/DataHoarder</a>, the &quot;G-Drive&quot; so-called-cloud system had no off-site backups.</p> <p><a href=\"https://koreajoongangdaily.joins.com/news/2025-10-01/national/socialAffairs/NIRS-fire-destroys-governments-cloud-storage-system-no-backups-available/2412936\">https://koreajoongangdaily.joins.com/news/2025-10-01/national/socialAffairs/NIRS-fire-destroys-governments-cloud-storage-system-no-backups-available/2412936</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SheriffRoscoe\"> /u/SheriffRoscoe </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzlce2/nirs_fire_destroys_governments_cloud_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzlce2/nirs_fire_destroys_governments_cloud",
        "id": 3747437,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzlce2/nirs_fire_destroys_governments_cloud_storage",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NIRS fire destroys government's cloud storage system, no backups available",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Spiritus__Raptor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.852913+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T14:23:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am looking to start backing up old movies. I have been doing a little bit of research but wanted some outside opinion. </p> <p>Out of the these three which would yall recommend? </p> <p><a href=\"https://a.co/d/5sFksZr\">https://a.co/d/5sFksZr</a> ASUS</p> <p><a href=\"https://a.co/d/0D1vCrC\">https://a.co/d/0D1vCrC</a> Buffalo</p> <p><a href=\"https://eshop.macsales.com/item/OWC/MR3UBDRW16/\">https://eshop.macsales.com/item/OWC/MR3UBDRW16/</a> mercury pro</p> <p>And any other recommendations? Trying to keep it under $200. </p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Spiritus__Raptor\"> /u/Spiritus__Raptor </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzkhbu/best_external_blue_ray_player/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzkhbu/best_external_blue_ray_player/\">[comments]</a></span>",
        "id": 3747441,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzkhbu/best_external_blue_ray_player",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best external blue ray player?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Beneficial-Oil6759",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.925804+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T14:17:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been using <code>gallery-dl</code> to archive Twitter profiles\u2014specifically text tweets, retweets, quoted tweets, and associated media. My goal is to preserve meaningful public discourse and media before it gets lost to rate limits or platform changes.</p> <p>Here\u2019s a snippet of my current config:</p> <pre><code>{ &quot;extractor&quot;: { &quot;twitter&quot;: { &quot;text-tweets&quot;: true, &quot;download&quot;: false, &quot;retweets&quot;: true, &quot;include-retweet-media&quot;: true, &quot;replies&quot;: false, &quot;quoted&quot;: true, &quot;unique&quot;: true, &quot;filename&quot;: &quot;{author[name]}/{tweet_id}_{date|strftime(&#39;%Y%m%d_%H%M%S&#39;)}.{extension}&quot; } }, &quot;output&quot;: { &quot;directory&quot;: &quot;C:/Users/hello/Pictures/HAS-HOT/gallery-dl/twitter&quot;, &quot;size&quot;: &quot;&lt;200m&quot; }, &quot;postprocessors&quot;: [ { &quot;name&quot;: &quot;metadata&quot;, &quot;event&quot;: &quot;post&quot;, &quot;mod",
        "id": 3747442,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzkb4h/twitter_scraping_with_gallerydl_preserving",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Twitter scraping with gallery-dl: preserving threads and media for archival",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Belthazor82",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:27.997281+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T13:50:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone! I&#39;m new here, but have been reading this subreddit for a few weeks now.</p> <p>My situation is as follows: I have a desktop PC with 3 drives.</p> <ol> <li>A 1TB SSD (WD Blue) for Windows and apps. It currently also holds my Music, Pictures, Downloads and Documents folders. Pictures and Downloads are somewhat temporary, as I move the downloaded stuff from there to my second drive. Currently about 150GB of free space left.</li> <li>2TB HDD from Seagate. Here, all my pictures and videos are stored. Drive is about 67% full, but if I move the Music/Pictures/Downloads stuff from the SSD here, it&#39;s pretty much full.</li> <li>500GB HDD from Samsung, which is at least 14 years old. This one is just for movies and TV shows, but completely full at this point.</li> </ol> <p>The SSD is sufficient for me, since a few more hundred GB of space will free up once I empty/move the aforementioned folders onto the HDD. Both HDDs need to be replaced",
        "id": 3747443,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzjm8m/advice_for_storage_and_backup_setup_needed",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advice for storage and backup setup needed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bensummersx",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T14:38:37.098191+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T12:59:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We all have that one folder. Mine is 30GB of random ISO files from 2007 that I&#39;m terrified to delete. What&#39;s the most useless or bizarre thing you&#39;re inexplicably holding onto? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bensummersx\"> /u/bensummersx </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzidlw/whats_your_most_why_do_i_even_have_this_file/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzidlw/whats_your_most_why_do_i_even_have_this_file/\">[comments]</a></span>",
        "id": 3746630,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzidlw/whats_your_most_why_do_i_even_have_this_file",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "what's your most \"why do I even have this\" file?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NGAF2-lectricBugalou",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T14:38:37.778351+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T12:21:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Building out a Jonsbo N5 based Nas </p> <p>Looking to fire a big chunky cache on her to keep file movement fast to from the hdd array but dont need Nvme speeds. </p> <p>Saw integral v series 4tb drives for cheap. </p> <p>Any one got experiance using them for cache we have used the smaller 250/512 drives at work and never had an issue but curious if anyone has experiance with their higher capicty drives. </p> <p>Also have a seperate Jonsbo n5 backplate questions if theres any jonsbo bros about feel free to dm me. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NGAF2-lectricBugalou\"> /u/NGAF2-lectricBugalou </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzhimr/integral_v_series_4tb_quality/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzhimr/integral_v_series_4tb_quality/\">[comments]</a></span>",
        "id": 3746631,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzhimr/integral_v_series_4tb_quality",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Integral v series 4tb quality?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Antique-Ostrich-7853",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T16:01:28.202062+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T12:19:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been going through some of my old drives and cloud accounts lately, and it made me realize just how much random personal data I\u2019ve been holding onto without even thinking about it. Old backups, exported contacts, emails from accounts I don\u2019t even use anymore it\u2019s kind of insane how much digital footprint just sits there.<br/> So I had the idea to maybe upload everything to physical drives that I can keep and delete it from everywhere else, anyone have any idea how to do this? This felt like the right sub to ask. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Antique-Ostrich-7853\"> /u/Antique-Ostrich-7853 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzhguj/data_scattered_everywhere_want_to_congregate/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzhguj/data_scattered_everywhere_want_to_congregate/\">[comments]</a></span>",
        "id": 3747444,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzhguj/data_scattered_everywhere_want_to_congregate",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data scattered everywhere, want to congregate everything on physical drives, how to?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VisibleExercise5966",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T10:17:41.754594+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T09:33:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was looking at the Synology 12 bay DX1215II or the DX1222.. but I don&#39;t find specs anywhere.. like is one better on processor or memory or something?</p> <p>Or maybe I should build my own.. I kind of want something smaller.</p> <p>I found a website that has something I want to save/use for myself. It says 150TB+ but doesn&#39;t say exactly how large. 18x12 on Raid 6 will give me 180TB.. that sounds fine.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VisibleExercise5966\"> /u/VisibleExercise5966 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzegtt/is_there_a_master_thread_for_simple_questions/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzegtt/is_there_a_master_thread_for_simple_questions/\">[comments]</a></span>",
        "id": 3744798,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzegtt/is_there_a_master_thread_for_simple_questions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a master thread for \"simple questions\"? Regarding specs on Synology models or should I build my own NAS..",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ordwk2b",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T09:04:23.956323+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T08:53:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does something like this exist? I found that often IMDB is a great resource of high quality (behind the scenes) movie images.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ordwk2b\"> /u/ordwk2b </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzdv9d/existing_tool_to_scrape_all_images_from_an_imdb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzdv9d/existing_tool_to_scrape_all_images_from_an_imdb/\">[comments]</a></span>",
        "id": 3744419,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzdv9d/existing_tool_to_scrape_all_images_from_an_imdb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Existing tool to scrape all images from an IMDB movie page?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nicko170",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T07:54:46.722135+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T07:37:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A few hours ago there was a post about processing the Epstein files into something more readable, collated and what not. Seemed to be a cash grab. </p> <p>I have now processed 20% of the files, in 4 hours, and uploaded to GitHub, including transcriptions, a statically built and searchable site, the code that processes them (using a self hosted installation of llama 4 maverick VLM on a very big server. I\u2019ll push the latest updates every now and then as more documents are transcribed and then I\u2019ll try and get some dedupe. </p> <p>It processes and tries to restore documents into a full document from the mixed pages - some have errored, but will capture them and come back to fix. </p> <p>I haven\u2019t included the original files - save space on GitHub - but all json transcriptions are readily available. </p> <p>If anyone wants to have a play, poke around or optimise - feel free</p> <p>Total cost, $0. Total hosting cost, $0. </p> <p>Not here to make a buck, ju",
        "id": 3744103,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzcq31/epstein_files_for_real",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Epstein Files - For Real",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fern_whispers",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T06:38:22.857865+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T05:59:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,<br/> I\u2019m looking to buy an external hard disk (around 500GB to 1TB) mainly to store project files, Python scripts, and some personal data. Something reliable, durable, and compatible with both Windows and possibly Linux would be great. </p> <p>Any recommendations based on your experience? Bonus if it&#39;s lightweight and easy to carry around!<br/> Thanks in advance :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fern_whispers\"> /u/fern_whispers </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzb6my/looking_for_a_good_500gb1tb_external_hard_disk/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nzb6my/looking_for_a_good_500gb1tb_external_hard_disk/\">[comments]</a></span>",
        "id": 3743840,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nzb6my/looking_for_a_good_500gb1tb_external_hard_disk",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a good 500GB\u20131TB external hard disk \u2014 any recommendations?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gkanai",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T04:18:27.156178+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T03:53:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz8yqr/ssds_dram_and_hdd_prices_are_climbing_fast_as_ai/\"> <img src=\"https://external-preview.redd.it/uKWiSp09YvNcxNTKd7WBCUyZ07K-euIsqdDzls3pVX8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c854f47115b25c8e27c632bfd9037ce2189dc93e\" alt=\"SSDs, DRAM, and HDD prices are climbing fast as AI demand and constrained supply converge\" title=\"SSDs, DRAM, and HDD prices are climbing fast as AI demand and constrained supply converge\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gkanai\"> /u/gkanai </a> <br/> <span><a href=\"https://www.tomshardware.com/pc-components/storage/perfect-storm-of-demand-and-supply-driving-up-storage-costs\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz8yqr/ssds_dram_and_hdd_prices_are_climbing_fast_as_ai/\">[comments]</a></span> </td></tr></table>",
        "id": 3743298,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz8yqr/ssds_dram_and_hdd_prices_are_climbing_fast_as_ai",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/uKWiSp09YvNcxNTKd7WBCUyZ07K-euIsqdDzls3pVX8.jpeg?width=640&crop=smart&auto=webp&s=c854f47115b25c8e27c632bfd9037ce2189dc93e",
        "title": "SSDs, DRAM, and HDD prices are climbing fast as AI demand and constrained supply converge",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DarkAmaterasu58",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T04:18:27.321641+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T03:10:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Recently picked up a small CRT/VCR combo and have gone back and found a lot of old tapes including my parents wedding; we want to find a way to digitize these so they are preserved in the future. I figured there\u2019d be a device or service that could do it pretty easily, but when I started googling, I\u2019m hit with dozens of random adapters and a LOT of tech jargon that goes right over my head in most cases. Is there no one-stop solution for getting something like this done? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DarkAmaterasu58\"> /u/DarkAmaterasu58 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz851r/whats_gonna_be_my_simplest_way_to_preserve_some/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz851r/whats_gonna_be_my_simplest_way_to_preserve_some/\">[comments]</a></span>",
        "id": 3743299,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz851r/whats_gonna_be_my_simplest_way_to_preserve_some",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s gonna be my simplest way to preserve some old family VHS tapes?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mysticjazzius",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T03:11:03.936666+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T02:24:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz77m8/spotted_this_beautiful_beast_on_marketplace/\"> <img src=\"https://b.thumbs.redditmedia.com/ujGz5mYxdXfRdMkMRqfNm-hTE6lZWbjl-XS7VEU5GuQ.jpg\" alt=\"Spotted this beautiful beast on Marketplace\" title=\"Spotted this beautiful beast on Marketplace\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Yep found a Stacker at a crazy price. I am having thoughts about perhaps picking it up, but because I primarily want to use 3.5\u2019 HDD\u2019s with this, I will gave to get either 5.25\u2019 bay adapters, or those gadgets I have heard people talk about that let you mount multiple 3.5\u2019 drives across 3 x 5.25\u2019 bays.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mysticjazzius\"> /u/mysticjazzius </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1nz77m8\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz77m8/spotted_this_beautiful_beast_on_marketplace/",
        "id": 3743144,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz77m8/spotted_this_beautiful_beast_on_marketplace",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/ujGz5mYxdXfRdMkMRqfNm-hTE6lZWbjl-XS7VEU5GuQ.jpg",
        "title": "Spotted this beautiful beast on Marketplace",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PooperOfMoons",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T03:11:04.523223+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T02:24:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have 2 4TB drives mirrored in Storage Spaces. I&#39;d like to double that capacity. </p> <p>Will this work? :</p> <ol> <li><p>Purchase 8TB drive.</p></li> <li><p>Copy data from Storage Space drive to new 8TB drive</p></li> <li><p>Remove 2 4TB drives from Storage Spaces</p></li> <li><p>Make those 2 drives into a single DrivePool pool</p></li> <li><p>Add that pool to a second pool set as duplicated.</p></li> </ol> <p>Will that give me a single 8TB virtual drive with redundancy?</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PooperOfMoons\"> /u/PooperOfMoons </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz77io/will_this_plan_work/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz77io/will_this_plan_work/\">[comments]</a></span>",
        "id": 3743145,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz77io/will_this_plan_work",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Will this plan work?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lacrossedeamon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T02:03:54.517673+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T01:52:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hopefully this is the place for this.</p> <p>As a hobby I contribute to a fan wiki for Assassin&#39;s Creed. This franchise has had some mobile games come and go without them being properly archived. I happen to have one of those games still downloaded onto an old ipad from before I started editing the wiki. The game itself no longer opens but I am trying to see if I can extract any data from it to fill in the information that is missing from the wiki but have no idea where to start.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lacrossedeamon\"> /u/Lacrossedeamon </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz6il4/need_help_with_a_project/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz6il4/need_help_with_a_project/\">[comments]</a></span>",
        "id": 3742944,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz6il4/need_help_with_a_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help with a project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/account-suspenped",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T02:03:54.753237+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T01:46:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>They will make the thinking noises and spin up every 30 seconds. its very odd, I cant recall if this always happened with them or not, but ive only had them this year. I could attach an audio recording if need be but it does not seem to always happen just 75% of the time. if i unplug from computer they shut up. I have it backed up because 1) you should and 2) Im afraid they are broken. is this normal? how can I fix it? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/account-suspenped\"> /u/account-suspenped </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz6ew8/18_tb_wd_external_hard_drives_constantly_spinning/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz6ew8/18_tb_wd_external_hard_drives_constantly_spinning/\">[comments]</a></span>",
        "id": 3742945,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz6ew8/18_tb_wd_external_hard_drives_constantly_spinning",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "18 tb WD external hard drives constantly spinning up / down even when computer is afk",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hiremyhirschl",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T02:03:54.928263+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T01:29:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>ive tried for hours and i keep getting &#39;Unexpected error. class com.google.gson.JsonNull cannot be cast to class com.google.gson.JsonObject (com.google.gson.JsonNull and com.google.gson.JsonObject are in unnamed module of loader &#39;app&#39;)&quot;.</p> <p>I downloaded some new boards completely with no issue. it even updated one of the boards i downloaded exactly a year ago even though it didnt work when i tried to update the other ones i had from before. I tried importing my cache and logging out of my pinterest profile but nothing. just the most recent 73 or so pins.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hiremyhirschl\"> /u/hiremyhirschl </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz62ac/wfdownloader_issue_two_specific_boards_wont/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nz62ac/wfdownloader_issue_two_specific_boards_wont/",
        "id": 3742946,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz62ac/wfdownloader_issue_two_specific_boards_wont",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WFDownloader issue: two specific boards wont download more than 73-75 images",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/spookyapk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T00:52:09.218886+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T00:38:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! Hopefully this is the right spot to ask this. I&#39;m wondering if there are some technical steps I&#39;m missing or not grasping that need to be done. </p> <p>I&#39;m using ConvertXtoHD to make hard copies of my sister&#39;s home videos by uploading the MP4s straight to the program, converting, and then burning onto BD-Rs. However, once they&#39;re burned and in the blu-ray player, the disc&#39;s audio skips every few seconds and I&#39;m not sure what&#39;s causing it. Do I need to be converting the MP4s into a different format or something? Is there a setting that needs to be set a certain way? </p> <p>This is my first time doing something like this. I have read online guides and such for ConvertX but I have no clue what I&#39;m doing lol. Thank you in advance :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/spookyapk\"> /u/spookyapk </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1",
        "id": 3742695,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nz4zoq/beginner_using_convertxtohd_for_making_hard",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Beginner using ConvertXtoHD for making hard backups\u2014 audio issues?",
        "vote": 0
    }
]