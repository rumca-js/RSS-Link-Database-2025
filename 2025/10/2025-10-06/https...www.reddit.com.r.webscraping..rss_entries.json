[
    {
        "age": null,
        "album": "",
        "author": "/u/Icy_Cap9256",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T23:30:30.262780+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T22:45:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What would be the reasons to push or not push your scraping script to GitHub?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Icy_Cap9256\"> /u/Icy_Cap9256 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nzxy45/should_you_push_your_scraping_script_to_github/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nzxy45/should_you_push_your_scraping_script_to_github/\">[comments]</a></span>",
        "id": 3750928,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nzxy45/should_you_push_your_scraping_script_to_github",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Should you push your scraping script to GitHub?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/anantj",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T14:52:06.546171+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T14:48:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a set of 2000+ HTML files that contain certain digital product sales data. The HTML is, structurally a mess, to put it mildly. it is essentially a hornet&#39;s nest of tables with the information/data that I Want to extract contained in a. non-table text, b. in HTML tables (that are nested down to 4-5 levels or more), c. a mix of non-table text and the table. The non-table text is structured differently with non-obvious verbs being used as verbs (for example, product &quot;x&quot; was acquired for $xxxx, product &quot;y&quot; was sold for $yyyy, product &quot;z&quot; brought in $zzzz, product &quot;a&quot; shucked $aaaaa, etc. etc.). I can provide additional text of illustration purposes.</p> <p>I&#39;ve attempted to build scrapers in python using beautifulsoup and requests library but due to the massive variance in the text/sentence structures and the nesting of tables, a static script is simply unable to extract all the sales information reli",
        "id": 3746689,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nzl4b5/help_needed_in_information_extraction_from_over",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help needed in information extraction from over 2K urls/.html files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/InsuranceTerrible875",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T14:52:06.629334+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T09:41:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone \ud83d\udc4b</p> <p>I had a working scraper for OddsPortal written in Python (using requests + BeautifulSoup).</p> <p>It used to:</p> <ol> <li><p>Get the match page HTML.</p></li> <li><p>Find the `/ajax-user-data/e/&lt;eventId&gt;/...` script.</p></li> <li><p>Load that JSON and extract odds from `page_data[&quot;d&quot;][&quot;oddsdata&quot;][&quot;back&quot;][&quot;E-1-2-0-0-0&quot;]`.</p></li> </ol> <p>Since recently, the site changed completely \u2014 now:</p> <p>- The `ajax-user-data` endpoint doesn\u2019t return plain JSON anymore. </p> <p>It returns a JavaScript snippet with `JSON.parse(&quot;...&quot;)`, so my `json.loads()` fails.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/InsuranceTerrible875\"> /u/InsuranceTerrible875 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nzelay/need_help_fixing_my_old_scraper_python_requests/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/",
        "id": 3746690,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nzelay/need_help_fixing_my_old_scraper_python_requests",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "need help fixing my old scraper (Python + requests + BeautifulSoup)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Human-Mastodon-6327",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T09:20:09.243923+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T09:17:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>iis there any code that work in googlecollab to scarb data from googlemaps relative to a query</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Human-Mastodon-6327\"> /u/Human-Mastodon-6327 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nze844/googlemaps_scarper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nze844/googlemaps_scarper/\">[comments]</a></span>",
        "id": 3744495,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nze844/googlemaps_scarper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "googlemaps scarper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/optinsoft",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T09:20:09.368059+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T08:16:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I have a problem when I try to do <code>switch_to.frame</code> for the iframe inside shadow root. I&#39;m doing this: <code> frameElement = hostElement.shadow_root.find_element( By.CSS_SELECTOR, &#39;iframe[style*=&quot;display: block&quot;]&#39; ) browser.switch_to.frame(frameElement) </code> And I get error: <code>selenium.common.exceptions.InvalidArgumentException: Message: invalid argument: missing &#39;ELEMENT&#39;</code></p> <p>I also tried this without luck: <code> frameElement = browser.execute_script( &quot;return arguments[0].querySelector(&#39;iframe[style*=\\&quot;display: block\\&quot;]&#39;)&quot;, hostElement.shadow_root ) browser.switch_to.frame(frameElement) </code> This problem occurs for Chrome driver only. For Firefox everything works fine.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/optinsoft\"> /u/optinsoft </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nzdbeh/",
        "id": 3744496,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nzdbeh/selenium_chrome_switch_to_iframe_inside_shadow",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Selenium, Chrome: switch to iframe inside shadow root",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GoingGeek",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T06:53:32.803004+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T06:52:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hey there, so I&#39;m making a scraper for this website with and im looking for way to bypass google recaptcha v2 without using proxies or captcha solving service. is there any solid way to do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GoingGeek\"> /u/GoingGeek </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nzc0zx/bypass_google_recaptcha_v2_playwright/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nzc0zx/bypass_google_recaptcha_v2_playwright/\">[comments]</a></span>",
        "id": 3743911,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nzc0zx/bypass_google_recaptcha_v2_playwright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bypass Google recaptcha v2 playwright",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Aromatic_Succotash89",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T06:53:33.208458+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T02:52:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to call an API but Cloudflare keeps blocking the requests. My IP needs to be whitelisted to access it. Is there any workaround or alternative way to make the calls?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Aromatic_Succotash89\"> /u/Aromatic_Succotash89 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nz7rkd/need_a_bit_of_help_with_cloudflare/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nz7rkd/need_a_bit_of_help_with_cloudflare/\">[comments]</a></span>",
        "id": 3743912,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nz7rkd/need_a_bit_of_help_with_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need a bit of help with cloudflare",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/primeclassic",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-06T02:19:04.610697+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-06T02:09:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone \ud83d\udc4b I\u2019m new to Python and want to build a script that scrapes around 20 Indian news sites.</p> <p>Goal: \u2022 Input: list of site homepages or category pages (e.g. /india, /latest-news) \u2022 Script should: 1. Visit the page 2. Collect all today\u2019s article links 3. Open each link and extract \u2192 \u2022 Title \u2022 Full news article text \u2022 Published date (if available) \u2022 Source name 4. Save to CSV/JSON \u2022 Skip duplicates</p> <p>Constraints: \u2022 I don\u2019t want to use RSS feeds or APIs \u2192 HTML scraping only. \u2022 Each site has a different structure, so I\u2019m not sure what the best approach is.</p> <p>Questions: 1. What\u2019s the best way to handle multiple sites without writing 20 different scrapers? 2. Any libraries/repos you\u2019d recommend for full-article scraping (not just headlines)?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/primeclassic\"> /u/primeclassic </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nz6w6",
        "id": 3742977,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nz6w6g/how_to_scrape_todays_news_articles_from_20_sites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape today\u2019s news articles from 20 sites using Python?",
        "vote": 0
    }
]