[
    {
        "age": null,
        "album": "",
        "author": "/u/Big_Building_3650",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T22:56:03.676510+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T21:49:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How does shopee generate session_id is it server verifed can it be replicated without browser? The url in question is <a href=\"https://shopee.vn/api/v4/search/search_items?by=relevancy&amp;extra_params=%7B%22global_search_session_id%22%3A%22gs-1f7ea99e-91fd-405f-9298-f099eab05d5d%22%2C%22search_session_id%22%3A%22ss-3a6360bc-d961-49d1-b5fa-ece32e53ca09%22%7D&amp;keyword=nike&amp;limit=60&amp;newest=0&amp;order=desc&amp;page_type=search&amp;scenario=PAGE_GLOBAL_SEARCH&amp;source=SRP&amp;version=2&amp;view_session_id=dc014ac2-5021-4784-9b17-bdbc186e89ab\">https://shopee.vn/api/v4/search/search_items?by=relevancy&amp;extra_params=%7B%22global_search_session_id%22%3A%22gs-1f7ea99e-91fd-405f-9298-f099eab05d5d%22%2C%22search_session_id%22%3A%22ss-3a6360bc-d961-49d1-b5fa-ece32e53ca09%22%7D&amp;keyword=nike&amp;limit=60&amp;newest=0&amp;order=desc&amp;page_type=search&amp;scenario=PAGE_GLOBAL_SEARCH&amp;source=SRP&amp;version=2&amp;view_session_id=dc014ac2-502",
        "id": 3760182,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0sest/how_to_scrape_shopee_with_requests_can_i",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape Shopee with requests? Can i replicate session_id?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NoSweet158",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T18:14:40.510686+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T16:10:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019d like to share my Chrome extension that might help with web scraping tasks:<br/> <strong>Captcha Plugin: ReCaptcha Solver by Raptor</strong><br/> \ud83d\udd17 <a href=\"https://chromewebstore.google.com/detail/captcha-plugin-recaptcha/iomcoelgdkghlligeempdbfcaobodacg\">https://chromewebstore.google.com/detail/captcha-plugin-recaptcha/iomcoelgdkghlligeempdbfcaobodacg</a></p> <p>The extension automatically detects reCAPTCHAs on a page, clicks the checkbox, and solves the image challenges.<br/> It\u2019s completely <strong>free</strong>, doesn\u2019t require any registration, API keys, or external services.<br/> The image solving is done using a <strong>built-in neural network</strong> running locally.</p> <p>The only downsides for now:<br/> \u2013 It <strong>sends solved images</strong> to my server (after solving) to help build a dataset.<br/> \u2013 It\u2019s quite <strong>large (~300 MB)</strong> at the moment, since each image type has its own model.<br/> Once I\u2019ve collected enough d",
        "id": 3758281,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0j3sc/i_built_a_free_chrome_tool_to_automatically_solve",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I built a free Chrome tool to automatically solve reCAPTCHAs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PerspectiveTop5532",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T18:14:40.578390+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T14:33:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p> <p>I&#39;m facing a frustrating and complex issue trying to monitor a major B2B marketplace for time-sensitive RFQs (Request For Quotations). I need instant notifications, but the platform is aggressively filtering access based on session status.</p> <h1>\ud83c\udfaf The Core Problem: Paid Access vs. Bot Access</h1> <p>The RFQs I need are posted to the site instantly. However, the system presents two completely different versions of the RFQ page:</p> <ol> <li><strong>Authenticated (Manual View):</strong> When I log in <strong>manually</strong> with my paid seller account, I see the new RFQs <strong>immediately</strong>.</li> <li><strong>Unauthenticated (Bot View):</strong> When a monitoring tool (or any automated script) accesses the exact same RFQ page URL, the content is treated as public. Consequently, the time-sensitive RFQs are intentionally <strong>delayed by exactly one hour</strong> in the captured content.</li> </ol> <p>The immediate",
        "id": 3758282,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0gh7b/bypassing_delayed_content_filter_for",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bypassing Delayed Content Filter for Time-Sensitive Data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Atronem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T15:03:23.686723+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T14:22:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good evening everyone,</p> <p>I hope you are doing well.</p> <p><strong>Budget: 550$</strong></p> <p>We seek an operator to extract <strong>300,000 titles</strong> from <a href=\"http://Abebooks.com\">Abebooks.com</a>, using filtering parameters that will be provided. </p> <p>After obtaining this dataset, the <strong>corresponding PDF</strong> for each title should be <strong>downloaded</strong> from the Wayback Machine or Anna\u2019s Archive if available. </p> <p>Estimated raw storage requirement: approximately 7 TB.</p> <p>The data will be temporarily <strong>stored</strong> on a server during collection, then transferred to <strong>128 GB Sony optical discs.</strong></p> <p>My intention is to preserve this archive for 50 years and ensure that the stored material remains readable and transferable using commercially available drives and systems in the future.</p> <p><em>Thanks a lot for your insights and for your time!</em></p> <p><em>I wish you a pleasant ",
        "id": 3756422,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0g6ip/hiring_scrape_300000_pdfs_and_archival_to_128_gb",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HIRING: Scrape 300,000 PDFs and Archival to 128 GB Sony Optical Discs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nooob_hacker",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T15:03:23.563801+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T13:31:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to web scrape a dynamic website.</p> <p>The website: <a href=\"https://certificadas.gptw.com.br/\">https://certificadas.gptw.com.br/</a></p> <p>This web scraping needs to be from Information Technology companies.</p> <p>The website where I need to web scrape has a business sector field where I need to select Information Technology and then click search.</p> <p>I need links to the pages of all the companies listed below.</p> <p>There are many companies and there are exactly 32 pages. Keep in mind that the website is dynamic.</p> <p>How can I do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nooob_hacker\"> /u/nooob_hacker </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o0eulm/i_need_to_web_scrape_a_dynamic_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o0eulm/i_need_to_web_scrape_a_dynamic_website/\">[comments]</a></span>",
        "id": 3756421,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0eulm/i_need_to_web_scrape_a_dynamic_website",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need to web scrape a dynamic website.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T13:30:20.465968+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T13:01:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 3755520,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0e4yo/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Atronem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T11:50:55.410686+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T10:41:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good evening everyone,</p> <p>I hope you are doing well.</p> <p>I am planning to scrape and download approximatel<strong>y 300,000 books</strong> on PDF-format from open web archives (Anna\u2019s Archive and the Wayback Machine). </p> <p>The data will be temporarily stored on a server during collection, then transferred to Sony ODA 1.5TB cartridges for long-term archival storage. The objective is to utilize an <strong>Optical WORM device</strong> to ensure data integrity and immutability.</p> <p>I would like to confirm the suitability of the Sony ODA system for this scale of data storage, as well as any technical limitations, performance considerations, or long-term compatibility issues that may arise\u2014particularly regarding hardware support and BDXL compatibility in future decades. </p> <p>My intention is to preserve this archive for <strong>50 years</strong> and ensure that the stored material remains readable and transferable using commercially available",
        "id": 3754687,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o0b7u0/optical_worm_or_sony_oda_15tb_for_longterm",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Optical WORM or Sony ODA 1.5TB for Long-Term Storage of 300k Books",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lafftar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T11:50:55.603321+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T07:35:19+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1o089uu/a_20000_reqs_python_setup_for_largescale_scraping/\"> <img src=\"https://external-preview.redd.it/YnUxcXdoZDg3bnRmMd-SlRa-S-nN-NLn-o7-yN1yipMS0zcFR1YzfXGCWT3K.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7faa6cf9ca0c4a2e50b5812519d7e0e08ae0deb\" alt=\"A 20,000 req/s Python setup for large-scale scraping (full code &amp; notes on bypassing blocks).\" title=\"A 20,000 req/s Python setup for large-scale scraping (full code &amp; notes on bypassing blocks).\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I&#39;ve been working on a setup to tackle two of the biggest problems in large-scale scraping: <strong>speed</strong> and <strong>getting blocked</strong>. I wanted to share a proof-of-concept that can hit ~20,000 requests/sec, which is fast enough to scrape millions of pages a day.</p> <p>After a lot of tuning, I managed to get a stable <strong>~20,000 requests/second</strong> from a single",
        "id": 3754688,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o089uu/a_20000_reqs_python_setup_for_largescale_scraping",
        "manual_status_code": 0,
        "page_rating": 86,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/YnUxcXdoZDg3bnRmMd-SlRa-S-nN-NLn-o7-yN1yipMS0zcFR1YzfXGCWT3K.png?width=640&crop=smart&auto=webp&s=d7faa6cf9ca0c4a2e50b5812519d7e0e08ae0deb",
        "title": "A 20,000 req/s Python setup for large-scale scraping (full code & notes on bypassing blocks).",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Temporary_Minute_175",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T13:30:20.689957+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T07:13:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone know how to build a residential proxy network on their own? Does anyone have implemented it? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Temporary_Minute_175\"> /u/Temporary_Minute_175 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o07xi9/how_to_build_a_residential_proxy_network_on_own/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o07xi9/how_to_build_a_residential_proxy_network_on_own/\">[comments]</a></span>",
        "id": 3755521,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o07xi9/how_to_build_a_residential_proxy_network_on_own",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to build a residential proxy network on own?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Pirate_OOS",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-07T04:24:25.476087+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-07T04:13:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My Python script to scrape movie plots using requests and bs4 no longer works and it throws the exception AttributeError: NoneType object has no attribute find_all_next()</p> <p>The script was working fine till a month ago.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pirate_OOS\"> /u/Pirate_OOS </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o04w5s/has_wikipedia_implemented_some_sort_of_protection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o04w5s/has_wikipedia_implemented_some_sort_of_protection/\">[comments]</a></span>",
        "id": 3752289,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o04w5s/has_wikipedia_implemented_some_sort_of_protection",
        "manual_status_code": 0,
        "page_rating": 85,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has wikipedia implemented some sort of protection against scraping?",
        "vote": 0
    }
]