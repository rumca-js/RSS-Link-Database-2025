[
    {
        "age": null,
        "album": "",
        "author": "/u/Babastyle",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-05T20:33:38.373048+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-05T20:24:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi \u2014 I have a question. I\u2019m trying to scrape a website, but it keeps detecting that I\u2019m a bot. It doesn\u2019t always show an explicit \u201cyou are a bot\u201d message, but certain pages simply don\u2019t load. I\u2019m using Puppeteer in stealth mode, but it doesn\u2019t help. I\u2019m using my normal IP address.</p> <p>What\u2019s your current setup to convincingly mimic a real user? Which sites or tools do you use to validate that your scraper looks human? Do you use a browser that preserves sessions across runs? Which browser do you use? Which User-Agent do you use, and what other things do you pay attention to?</p> <p>Thanks in advance for any answers.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Babastyle\"> /u/Babastyle </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyz1ov/site_detects_my_scraper_even_with_puppeteer/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyz1ov/site_de",
        "id": 3741744,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nyz1ov/site_detects_my_scraper_even_with_puppeteer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "site detects my scraper even with Puppeteer stealth",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/koboy-R",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-05T18:09:46.552323+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-05T17:07:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>LLM&#39;s seem to strongly advice against automated circumvention of cloudflare or similars. When it comes to public data, it&#39;s against my understanding. I get that massive extraction of user data, even if public, can give you trouble, but is that also the case with small scale public data extraction? (for example, getting the prices of a catalogue of a website that&#39;s public, without login or anything, but with cloudflare protection enabled)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/koboy-R\"> /u/koboy-R </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nytss7/is_it_illegal_to_circumvent_cloudflare_or_similars/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nytss7/is_it_illegal_to_circumvent_cloudflare_or_similars/\">[comments]</a></span>",
        "id": 3741057,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nytss7/is_it_illegal_to_circumvent_cloudflare_or_similars",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it illegal to circumvent cloudflare or similars?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SuccessfulReserve831",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-05T15:47:50.528251+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-05T15:40:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Out of simple curiosity, I\u2019ve been trying to scrape some data from Upwork. I already managed to do it with Playwright, but I wanted to take it to the next level and reverse-engineer their API directly.</p> <p>So far, that\u2019s proven almost impossible. Has anyone here done it before?</p> <p>I noticed that the data on the site is loaded through a request called <code>suit</code>. The endpoint is:</p> <pre><code>https://www.upwork.com/shitake/suit </code></pre> <p>The weird part is that the response to that request is just &quot;ok&quot;, but all the data still loads only after that call happens.</p> <p>If anyone has experience dealing with this specific API or endpoint, I\u2019d love to hear how you approached it. It\u2019s honestly starting to make me question my seniority \ud83d\ude05</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SuccessfulReserve831\"> /u/SuccessfulReserve831 </a> <br/> <span><a href=\"https://www.reddi",
        "id": 3740341,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nyriwl/has_anyone_successfully_reverseengineered_upworks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone successfully reverse-engineered Upwork\u2019s API?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EloquentSyntax",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-05T14:31:40.485368+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-05T14:02:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there, does anyone have experience scraping the publicly available RPC endpoints that load on Google Maps at decent volume? For example their /listentity (place data) or /listugc (reviews) endpoints?</p> <p>Are they monitoring those aggressively and how cautious should I be in terms of antiscraping measures? </p> <p>Would proxies be mandatory and would datacenter ones be sufficient? any cautionary tale / suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EloquentSyntax\"> /u/EloquentSyntax </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyp1ff/scraping_google_maps_rpc_apis/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyp1ff/scraping_google_maps_rpc_apis/\">[comments]</a></span>",
        "id": 3739925,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nyp1ff/scraping_google_maps_rpc_apis",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Google Maps RPC APIs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Low-Watercress2524",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-05T13:22:37.742448+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-05T12:43:23+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Low-Watercress2524\"> /u/Low-Watercress2524 </a> <br/> <span><a href=\"https://www.producthunt.com/products/chatbrow\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyn7qt/ai_web_scraping_with_no_code/\">[comments]</a></span>",
        "id": 3739587,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nyn7qt/ai_web_scraping_with_no_code",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 1,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI Web scraping with no code",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FarYou8409",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-05T10:50:02.525161+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-05T09:50:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What is the best way to get around G\u00b4s search rate limits for scraping/crawling? Cant figure this out, please help. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FarYou8409\"> /u/FarYou8409 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyk33a/getting_around_googes_rate_limits/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nyk33a/getting_around_googes_rate_limits/\">[comments]</a></span>",
        "id": 3738974,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nyk33a/getting_around_googes_rate_limits",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting around Goog*e\u00b4s rate limits",
        "vote": 0
    }
]