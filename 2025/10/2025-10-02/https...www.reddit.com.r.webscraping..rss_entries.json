[
    {
        "age": null,
        "album": "",
        "author": "/u/Radiant-Wait5869",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T21:16:03.286431+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T20:28:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to use the script below to extract data from Uber Eats, such as restaurant names, menu items, and descriptions, but it&#39;s not working. Does anyone know what I might be doing wrong? Thanks!</p> <p><a href=\"https://github.com/tesserakh/ubereats/blob/main/ubereats.py\">https://github.com/tesserakh/ubereats/blob/main/ubereats.py</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Radiant-Wait5869\"> /u/Radiant-Wait5869 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nwf12j/uber_eats_data_extraction_can_anyone_help_me/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nwf12j/uber_eats_data_extraction_can_anyone_help_me/\">[comments]</a></span>",
        "id": 3721620,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nwf12j/uber_eats_data_extraction_can_anyone_help_me",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Uber Eats Data Extraction - Can anyone help me?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ZookeepergameNew6076",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T18:22:25.363978+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T17:31:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all \u2014 quick one. I\u2019m trying to get session cookies from <a href=\"http://send.now\">send.now</a>. The site normally <strong>doesn\u2019t show</strong> the Turnstile message:</p> <blockquote> <p>Verify you are human.</p> </blockquote> <p>\u2026but after I spam the site with ~10 GET requests the challenge appears. My current flow is:</p> <ol> <li>Spam the target a few times from my app until the Turnstile check appears.</li> <li>Call this service to solve and return cookies: <a href=\"https://github.com/iamyegor/Unflare/\">Unflare</a>. This works, but it\u2019s not scalable and feels fragile (wasteful requests, likely to trigger rate limits/blocks). Looking for short, practical suggestions:</li> </ol> <ul> <li>Better architecture patterns to scale cookie fetching without \u201cspamming\u201d the target.</li> <li>Ways to avoid tripping Cloudflare while still getting valid cookies (rate-limiting/backoff strategies, reuse TTL ideas). Thanks \u2014 any concise pointers or tools would be ",
        "id": 3720351,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nwa915/how_to_handle_invisible_cloudflare_captcha",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to handle invisible Cloudflare CAPTCHA?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Live_Baker_6532",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T21:16:03.411150+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T16:23:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Why is it that LLMs have not revolutionized webscraping where we can simply make a request or a call and have an LLM scrape our desired site?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Live_Baker_6532\"> /u/Live_Baker_6532 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nw8ejy/why_havent_llms_solved_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nw8ejy/why_havent_llms_solved_webscraping/\">[comments]</a></span>",
        "id": 3721621,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nw8ejy/why_havent_llms_solved_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why haven't LLMs solved webscraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/whiz_business",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T21:16:03.545863+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T15:28:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>reCAPTCHA Tokens work briefly then return 400 recaptcha verification failed. I\u2019ve tried: \u2022 Token harvester (intercept browser token) \u2014 sporadic (~5\u201310%). \u2022 2Captcha pool \u2192 Redis TTL=120s \u2014 worked days, then started failing. \u2022 Headful browsers + grecaptcha.execute (mimic humans) \u2014 intermittent, then stops. \u2022 Using rotating proxies and curl_cffi for requests.</p> <p>Failures are straight verification errors (not rate limits). Logs show tokens, timestamps, proxies, UAs; tokens that succeeded are later rejected with no clear pattern.</p> <p>I have a dev but if you find a solution to it you can dm me and we can try it and I can send you a payment for the possible fix as always comments are welcome to fix this asap.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/whiz_business\"> /u/whiz_business </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nw6x1p/recaptcha_v3_low_token_scores/\">[link]</a></s",
        "id": 3721622,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nw6x1p/recaptcha_v3_low_token_scores",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recaptcha v3 low token scores.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheImmortalHooman",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T15:13:27.066745+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T15:02:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need an ebay bot to fetch price for 15k products on 24 hourly basis. </p> <p>The product names exist in csv and output can be done in same csv or new csv whatever suits. </p> <p>Do hit me up if someone can do this for me. </p> <p>We can discuss pay in DM. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheImmortalHooman\"> /u/TheImmortalHooman </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nw67rw/ebay_bot_to_fetch_prices/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nw67rw/ebay_bot_to_fetch_prices/\">[comments]</a></span>",
        "id": 3718087,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nw67rw/ebay_bot_to_fetch_prices",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ebay bot to fetch prices",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Houseonthehill",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T15:13:27.360441+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T14:32:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been trying to scrape product data from crateandbarrel.com (specifically their Sale page) and I&#39;m hitting the classic Akamai Bot Manager wall. Looking for advice from anyone who&#39;s dealt with this successfully.</p> <p>I&#39;ve tried</p> <ul> <li>Puppeteer (both headless and headed) - blocked</li> <li>paid residential proxies with 7-day sticky sessions - still blocked</li> <li>&quot;Human-like&quot; behaviors (delays, random scrolling, natural navigation) - detected</li> <li>Priming sessions through Google/Bing search \u2192 both search engines block me</li> <li>Direct navigation to site \u2192 works initially, but blocks at Sale page navigation</li> <li><p>Attach mode (connecting to manually-opened Chrome) \u2192 connection works but navigation still triggers 403</p></li> <li><p>My cookies show Akamai&#39;s &quot;Tier 1&quot; cookies (basic <code>ak_bmsc</code>, <code>bm_sv</code>) but I&#39;m not getting the &quot;Tier 2&quot; trust level needed for",
        "id": 3718088,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nw5f8e/struggling_with_akamai_bot_manager",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Struggling with Akamai Bot Manager",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Human-Mastodon-6327",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T08:19:03.280595+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T07:11:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m using an <a href=\"http://expireddomain.net\">expireddomain.net</a> website that only shows 200 lines per page in search results. Inspect Element sometimes shows up to 2k lines, but not for every search type cause they refresh , and it&#39;s still not the full data.</p> <p>I want to extract **all results at once** instead of clicking through pages. Is there a way to:</p> <p>* Bypass the limit with URL params or a hidden API?</p> <p>* Use a script (Python/Selenium/etc.) to pull everything?</p> <p>Any tips, tools, or methods would help. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Human-Mastodon-6327\"> /u/Human-Mastodon-6327 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nvwz1v/how_to_bypass_200line_limit_on_expired_domain_site/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nvwz1v/how_to_bypass_200line_limit_on_expired_domain_site/\">[co",
        "id": 3716191,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nvwz1v/how_to_bypass_200line_limit_on_expired_domain_site",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to bypass 200-line limit on expired domain site?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/repeatingscotch",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-02T04:09:39.183477+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-02T03:08:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I built a scraper that downloads pdfs from a specific site, converts the document using OCR, then searches for information within the document. It uses Tesseract OCR and Poppler. I have it doing a double pass at different resolutions to try and get as accurate a reading as possible. It still is not as accurate as I would like. Has anyone had success with an accurate OCR? </p> <p>I\u2019m hoping for as simple a solution as possible. I have no coding experience. I have made 3-4 scraping scripts with trial and error and some ai assistance. Any advice would be appreciated. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/repeatingscotch\"> /u/repeatingscotch </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nvspnu/question_about_ocr/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nvspnu/question_about_ocr/\">[comments]</a></span>",
        "id": 3715439,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nvspnu/question_about_ocr",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question about OCR",
        "vote": 0
    }
]