[
    {
        "age": null,
        "album": "",
        "author": "/u/ayleustrendster",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T21:40:43.691940+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T20:59:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I need some advice.</p> <p>I&#39;ve got a crappy Seagate Expansion 6TB giving me clicks. Worrying stuff. I&#39;ve been putting off getting a proper DAS for years now and this has scared me enough to actually consider it.</p> <p>I&#39;m looking at getting a QNAP TR-004 and a Seagate IronWolf Pro 8TB and going from there. I don&#39;t exactly have the funds to be filling all the bays up with drives and raiding them together and doing it properly as I&#39;m a university student and money is pretty tight.</p> <p>For context: on the Seagate 6TB right now is a massive collection of music, a ton of raw footage from projects e.t.c. I&#39;m fully aware this is NOT how you go about storing data properly but again, money is tight and I&#39;ve gotta make do with what I&#39;ve got.</p> <p>What would you suggest I do?</p> <p>My aim with the DAS is to use it for video editing and data storage. </p> <p>Any advice, products you recommend e.t.c would be wo",
        "id": 3869727,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocovmf/do_i_das_or_do_i_continue_with_externals",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do I DAS or do I continue with externals?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/WeeklyBanEvasion",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T20:25:59.734575+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T19:36:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I currently have a bunch of harddrives jam-packed full of family photos and videos dating back to the dawn of consumer digital cameras. I have all the photos and videos I&#39;ve ever taken on all my phones and digital cameras, as well as many dozens of backup dumps from various family members&#39; phones and drives over the years. Altogether this probably approaches somewhere in the range of about 8 terabytes, but there&#39;s definitely lots of duplicates in there taking up space as well. I have all the files backed up on a FreeNAS, but it&#39;s time I get this mess organized. Most of the backup dumps are sorted by backup date, and any pictures taken on phones have the date/time as the file name, but that&#39;s about the extent of the organization at the moment.</p> <p>This might sound paranoid, but most of the pictures and videos are of a friends and family, with a large portion being my kids and other family members&#39; kids, and I don&#39;t fee",
        "id": 3869089,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocmo1i/does_anyone_know_of_an_offline_ai_image_sorter",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone know of an \"offline\" AI image sorter?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LiveChicken9",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T20:25:59.996322+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T19:08:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! </p> <p>I\u2019m looking for a digital (or scanned) copy of the Lost in Blue manual for the Nintendo DS. I don\u2019t mind which region it\u2019s from (NA, EU, or JP) I just really want to have a look at it for reference and nostalgia\u2019s/collecting sake, as they are not available to buy in my country.</p> <p>If anyone has it or knows where I can find it online (archive, scan, etc.), I\u2019d really appreciate your help!</p> <p>Thanks in advance \ud83d\ude4f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LiveChicken9\"> /u/LiveChicken9 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oclwst/request_lost_in_blue_nintendo_ds_manual_any_region/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oclwst/request_lost_in_blue_nintendo_ds_manual_any_region/\">[comments]</a></span>",
        "id": 3869090,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oclwst/request_lost_in_blue_nintendo_ds_manual_any_region",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Request: Lost in Blue (Nintendo DS) Manual any region",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/greenyellowandblue",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T18:31:47.311532+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T18:18:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ockkpx/old_hard_drive_question_wd800/\"> <img src=\"https://a.thumbs.redditmedia.com/imviGf9hVA8jz7u-GEeJDcKDShtyZlJZ9QU6-rope08.jpg\" alt=\"Old Hard drive question - WD800\" title=\"Old Hard drive question - WD800\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello everyone. Years ago I removed an old Western Digital WD800BB from a PC. It has some photos that I would like to retrieve from it. I believe/think it still works. I\u2019m getting lost picking the correct adaptor for it, and unsure how to power it. Any help would be greatly appreciated. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/greenyellowandblue\"> /u/greenyellowandblue </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1ockkpx\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ockkpx/old_hard_drive_question_wd800/\">[comments]</a></span> </td></tr></table>",
        "id": 3868321,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ockkpx/old_hard_drive_question_wd800",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/imviGf9hVA8jz7u-GEeJDcKDShtyZlJZ9QU6-rope08.jpg",
        "title": "Old Hard drive question - WD800",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/craftywizard1983",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T18:31:46.900100+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T17:23:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for advice if possible on creating a main movie blu-ray with TSMuxer? Would it be best to demux the raw streams with eac3to and then add the demuxed streams to TSMuxer to create the blu-ray or can I just add the m2ts/mpls straight from disc? I understand seamless branching can become a problem with overlapping frames so I assume it would be best to use eac3to first. I just want to make a simple main movie without all the bloat. Any help would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/craftywizard1983\"> /u/craftywizard1983 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocj28l/create_main_movie_bluray_disc_with_tsmuxer/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocj28l/create_main_movie_bluray_disc_with_tsmuxer/\">[comments]</a></span>",
        "id": 3868320,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocj28l/create_main_movie_bluray_disc_with_tsmuxer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Create main movie Blu-ray disc with TSMuxer?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SarcasticallyCandour",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T18:31:47.528970+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T17:13:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>TLDR:</strong> </p> <ol> <li><p>using an old Gigabyte z77x-ud5h I want backup HDDs with one primary spinning 8-12hrs a day seeding on torrent/emule/soulseek, while another is just a clone of this one which will only be turned on to copy the files across.</p></li> <li><p>Are Ironwolf/Barracuda better for each or one of each? The reason im going seagate is that&#39;s what&#39;s available in 8TB or 12TB. Western digital are available to me but only in 6TB and they are same cost as 8TB Ironwolf/Barracuda.</p></li> <li><p>Should I buy different drives for each use or just 2 of the same?</p></li> </ol> <p><strong>Lengthy:</strong> </p> <p>I&#39;m not sure about technology now, I&#39;ve not bought a HHD in 5 years or so.</p> <p>I have an old Gigabyte z77x-UD5H I want to load a few (2 for start) 8TB / or 12TB (if i can afford them) drives into it and I will use it as a backup store for general files, movies, music, old college lecture videos, porn etc",
        "id": 3868322,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocisyl/should_i_go_with_seagate_ironwolf_or_barracuda",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Should I go with Seagate IronWolf or Barracuda for a (possible raid based) home backup server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/karibuTW",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T17:00:36.263326+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T16:09:22+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/karibuTW\"> /u/karibuTW </a> <br/> <span><a href=\"/r/FractalDesign/comments/1och3gu/fractal_design_7_xl_with_6_drives_reco/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1och3t8/fractal_design_7_xl_with_6_drives_reco/\">[comments]</a></span>",
        "id": 3867353,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1och3t8/fractal_design_7_xl_with_6_drives_reco",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fractal Design 7 XL with 6 drives - Reco",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CelluloseNitrate",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T17:00:36.508054+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T15:57:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Running out of NVME slots and my PCIe slots are also full. Was wondering if anyone sold any NVME bifurcating boards that might take a single NVME PCIe4x4 and split it into two 4x2 NVME boards which would be plenty fast for my use. </p> <p>Vertical clearance isn\u2019t an issue. Heat might be. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CelluloseNitrate\"> /u/CelluloseNitrate </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocgrv1/anyone_make_a_nvme_multiplier_slash_bifurcator/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocgrv1/anyone_make_a_nvme_multiplier_slash_bifurcator/\">[comments]</a></span>",
        "id": 3867354,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocgrv1/anyone_make_a_nvme_multiplier_slash_bifurcator",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone make a NVME multiplier slash bifurcator?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wildjunkie",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T17:00:35.628280+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T15:47:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been using them for a few years now to store lots of things and was recently told by someone that anything I put there should be considered disposable because they could stop working at any time</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wildjunkie\"> /u/wildjunkie </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocgiuf/are_flash_drives_really_that_unreliable/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocgiuf/are_flash_drives_really_that_unreliable/\">[comments]</a></span>",
        "id": 3867351,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocgiuf/are_flash_drives_really_that_unreliable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are flash drives really that unreliable?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/skynetarray",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T17:00:35.853960+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T15:36:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just updated stash to the newest version on Unraid and now I can\u2019t login anymore, it says invalid username or password.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/skynetarray\"> /u/skynetarray </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocg8qw/stashapp_login_error/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocg8qw/stashapp_login_error/\">[comments]</a></span>",
        "id": 3867352,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocg8qw/stashapp_login_error",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Stashapp Login error",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Successful-Apricot81",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T15:32:22.070457+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T15:29:33+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Successful-Apricot81\"> /u/Successful-Apricot81 </a> <br/> <span><a href=\"/r/unRAID/comments/1ocg0o2/new_drive_woahs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocg1u7/new_drive_woahs/\">[comments]</a></span>",
        "id": 3866558,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocg1u7/new_drive_woahs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New Drive Woahs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DoubleoSavant",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T15:32:22.743734+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T15:00:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want something that will ideally last about 20 years at least, in order to save a digital copy of my children&#39;s sperm donor profile. It&#39;s not much information, so I find a lot of portable hard drives to be overkill. But a flashdrive doesn&#39;t seem reliable for that long in storage. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DoubleoSavant\"> /u/DoubleoSavant </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocfa2p/what_is_the_best_way_to_digitally_store_a_sperm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocfa2p/what_is_the_best_way_to_digitally_store_a_sperm/\">[comments]</a></span>",
        "id": 3866560,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocfa2p/what_is_the_best_way_to_digitally_store_a_sperm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the best way to digitally store a sperm donors profile?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dartmoo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T15:32:22.268460+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T14:48:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I was wondering if anyone has any experience with WD Recertified SSD&#39;s?</p> <p>The WD Black SN850X 4TB is on there for \u00a3209.99 which seems like quite a good deal in comparison to the retail price.</p> <p><a href=\"https://shop.sandisk.com/en-gb/products/recertified/ssd/internal-ssd/wd-black-sn850x-nvme-ssd-recertified?sku=A196-WDS400T2X0E\">https://shop.sandisk.com/en-gb/products/recertified/ssd/internal-ssd/wd-black-sn850x-nvme-ssd-recertified?sku=A196-WDS400T2X0E</a></p> <p>Is this a good enough deal to pick up, or should I wait for Black Friday deals for longer warranty?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dartmoo\"> /u/dartmoo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocez3g/wd_recertified_ssds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocez3g/wd_recertified_ssds/\">[comments]</a></span>",
        "id": 3866559,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocez3g/wd_recertified_ssds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WD Recertified SSD's",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No-Treat-2950",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T12:42:33.407155+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T12:06:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Yes. There is an extension called ESUIT but it is not free. Paywall after first 300 photos. I want to download 10000 photos from a page. Any other &quot;free&quot; way?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Treat-2950\"> /u/No-Treat-2950 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocb4m4/any_way_to_download_a_facebook_photo_album/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ocb4m4/any_way_to_download_a_facebook_photo_album/\">[comments]</a></span>",
        "id": 3865095,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocb4m4/any_way_to_download_a_facebook_photo_album",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any way to download a facebook photo album?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jerrydberry",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T12:42:33.697629+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T11:42:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an old laptop which is used as media server/NAS. OS is installed on M.2 nvme while SATA SSD is used for storage, so I needed more of the latter. </p> <p>I found <a href=\"https://serverpartdeals.com/products/samsung-pm883-mz7lh3t8hmlt-3-84tb-sata-6gb-s-2-5-aes-256-bit-ssd?utm_source=shop_app&amp;list_generator=link_to_storefront&amp;context=product&amp;user_id=734548051\">this</a> and from available data (3 years of 1.3 DWPD with 3.84 capacity, 704.58 TBW used) it looks like remaining resource is 1.3 x 3.84 x 365 x 3 - 704.58 = 4761.66 remaining TBW.</p> <p>At very similar price there is new consumer SATA SSD from the same manufacturer of similar capacity which is specified to have 2,400 TBW: <a href=\"https://a.co/d/bRcsVX8\">link</a></p> <p>With very similar price and capacity the used server SSD seems to have double the resource remaining which was a no brainer to me, so I bought it.</p> <p>Should have verifies my math before buying, but better ",
        "id": 3865096,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ocan63/i_bought_refurbished_server_25_sata_ssd_am_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I bought refurbished server 2.5 SATA SSD. Am I stupid?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AMPmusicals",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T12:42:34.075154+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T11:22:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There is another thread about such downloaders, but it appears to be PC only</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AMPmusicals\"> /u/AMPmusicals </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oca9v5/any_good_kemonocoomer_bulk_downloaders_for_mac/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oca9v5/any_good_kemonocoomer_bulk_downloaders_for_mac/\">[comments]</a></span>",
        "id": 3865097,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oca9v5/any_good_kemonocoomer_bulk_downloaders_for_mac",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any good Kemono/coomer bulk downloaders for MAC?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/InSearchOfUpdog",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T11:20:13.128140+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T11:07:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This is probably off most people&#39;s radars, but Medya News, one of the only English-language news websites focusing on Kurdistan and the Kurdish diaspora is <a href=\"https://medyanews.net/announcement-medya-news-closes-amid-shifting-political-landscape-makes-way-for-reimagined-kurdish-media/\">going offline</a>. Their website says it was due to go quiet September 2025, but current it still looks up, so I guess it could go quiet any day now.</p> <p>I&#39;m not in a position right now to do much scraping, and I&#39;m only really a lurker in this subreddit (I&#39;m looking forward to the day when I have a living situation that means I can do some hoarding and seeding). I am aware of Rule 8, but I hope others can see that this website is of importance far beyond myself (I&#39;m not even Kurdish). The website appears to be going down because of political developments in the Middle East. While I don&#39;t know the site admin&#39;s circumstances, it is not",
        "id": 3864479,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oca03f/englishlanguage_kurdish_news_website_medya_news",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "English-language Kurdish news website, Medya News, going offline.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cai_9us",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T11:20:13.228906+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T10:47:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you&#39;re planning to shop on AliExpress, don&#39;t miss out! I&#39;ve compiled a concise list of codes. Whether you&#39;re looking for electronics, fashion items, or home essentials, these codes make saving money easy! The following AliExpress codes are available. Plus, get an extra $40 off when your order reaches $269 and you pay with PayPal or PayPal Later..</p> <p>RDC2 $2 off $10 - 20% OFF</p> <p>RDC5A $5 off $25 - 20% OFF</p> <p>RDC7 $7 off $35 - 20% OFF</p> <p>RDC10A $10 off $50 - 20% OFF</p> <p>RDC14 $14 off $70 - 20% OFF</p> <p>RDC20 $20 off $100 - 20% OFF</p> <p>RDC25A $25 off $125 - 20% OFF</p> <p>RDC32C $32 OFF $160 20%OFF</p> <p>RDC56C $56 OFF $280 20%OFF</p> <p>RDC64C $64 OFF $320 20%OFF</p> <p>RDC80C $80 OFF $400 20%OFF</p> <p>RDC100C $100 OFF $500 20%OFF</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cai_9us\"> /u/cai_9us </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1",
        "id": 3864480,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oc9nk4/usa_paypal_spendandsave_and_aliexpress_20_off",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[USA] PayPal Spend-and-Save and Aliexpress 20% Off double Discounts on Aliexpress! Ends 11/19/25.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SpcT0rres",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T09:53:38.747342+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T09:51:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc8p2u/can_someone_explain_why_in_windows_storage_spaces/\"> <img src=\"https://preview.redd.it/9ce5krp1sfwf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e59c5b181435f2380b4934266788cd56aab8ead\" alt=\"Can someone explain why in windows storage spaces when i change the resiliency type to anything other than simple, the Size goes to 0.00? When i enter the size 56TB, i get a resiliency size of 84TB. What max size should i put for 8 x 8TB drives?\" title=\"Can someone explain why in windows storage spaces when i change the resiliency type to anything other than simple, the Size goes to 0.00? When i enter the size 56TB, i get a resiliency size of 84TB. What max size should i put for 8 x 8TB drives?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;m on windows 11, and have a QNAP JBOD with 8 x 8TB drives connected via SAS to my PC using a PCIe card. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=",
        "id": 3863902,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oc8p2u/can_someone_explain_why_in_windows_storage_spaces",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/9ce5krp1sfwf1.png?width=640&crop=smart&auto=webp&s=5e59c5b181435f2380b4934266788cd56aab8ead",
        "title": "Can someone explain why in windows storage spaces when i change the resiliency type to anything other than simple, the Size goes to 0.00? When i enter the size 56TB, i get a resiliency size of 84TB. What max size should i put for 8 x 8TB drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Smart_Design_4477",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T04:26:14.591870+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T03:07:39+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc1zm2/shpack_bundle_folder_of_scripts_to_single/\"> <img src=\"https://external-preview.redd.it/oU_Lu4j_Ehk3wSrQtXDEpv6wHRUwMZr9LhRRT21dzIo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed92632258a97b89c4a8c9c2ecee5c17680feb03\" alt=\"shpack: bundle folder of scripts to single executable\" title=\"shpack: bundle folder of scripts to single executable\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><strong>shpack</strong> is a Go-based build tool that bundles multiple shell scripts into a single, portable executable.<br/> It lets you organize scripts hierarchically, distribute them as one binary, and run them anywhere \u2014 no dependencies required.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Smart_Design_4477\"> /u/Smart_Design_4477 </a> <br/> <span><a href=\"https://github.com/luongnguyen1805/shpack/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoar",
        "id": 3862440,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oc1zm2/shpack_bundle_folder_of_scripts_to_single",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/oU_Lu4j_Ehk3wSrQtXDEpv6wHRUwMZr9LhRRT21dzIo.png?width=640&crop=smart&auto=webp&s=ed92632258a97b89c4a8c9c2ecee5c17680feb03",
        "title": "shpack: bundle folder of scripts to single executable",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Independent-Ball3215",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T03:10:07.548547+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T03:00:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Got quite a few videos I want to store, because they&#39;re taking up around 256 gb of storage. Anyone recommend any hdds? SSDs? Under $90 USD pls. Anything I should avoid?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Independent-Ball3215\"> /u/Independent-Ball3215 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc1uoo/recommend_any_under_90_usd_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc1uoo/recommend_any_under_90_usd_storage/\">[comments]</a></span>",
        "id": 3862197,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oc1uoo/recommend_any_under_90_usd_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Recommend any under $90 USD storage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/karrie0027",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T03:10:08.198691+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T02:47:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am looking to download and store all the playboy magazines, does anyone have a personal collection or where i can find it on internet?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/karrie0027\"> /u/karrie0027 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc1ki8/playboy_magazine_collection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc1ki8/playboy_magazine_collection/\">[comments]</a></span>",
        "id": 3862199,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oc1ki8/playboy_magazine_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Playboy magazine collection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/friendofelephants",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T03:10:08.426353+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T02:10:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, encountering an issue while downloading a ton of photos from iCloud: it ends up downloading each file individually and requires me to click save for each one. A pop-up will appear with &quot;Save&quot; or &quot;Cancel.&quot; And I&#39;m not able to cancel the whole process once started (so if I downloaded 300 photos, I&#39;m stuck in this nightmare of clicking save 300 times and can&#39;t even cancel the whole thing once started). </p> <p>Other times I&#39;ve been able to download all the photos I&#39;ve selected in my iCloud Photos and it downloads as a single zip file, which is exactly what I want. </p> <p>Does anyone know what I might be doing wrong? Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/friendofelephants\"> /u/friendofelephants </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1oc0stc/downloading_photos_from_icloud_but_theyre_not/\">[link]</a></span> &#32; <span><a",
        "id": 3862200,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1oc0stc/downloading_photos_from_icloud_but_theyre_not",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading photos from iCloud, but they're not packaging into one zip file but as individual files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/loganhuyy2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T03:10:07.820756+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T00:39:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have over a thousand old hand written letters in cursive that I am wanting to make into text or editable pdf files, so If something happens to the letters I will have a back up on my NAS. I was first thinking to use an optical character recognition software (OCR), but almost all the tools struggle with cursive or are too costly. I then went to use AI but it too seems to struggle sometimes with certain characters. I am wondering if anybody knows of a good and cheap software to get these letters onto my NAS. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/loganhuyy2\"> /u/loganhuyy2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1obyumu/need_to_covert_old_letters_to_text_for_better/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1obyumu/need_to_covert_old_letters_to_text_for_better/\">[comments]</a></span>",
        "id": 3862198,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1obyumu/need_to_covert_old_letters_to_text_for_better",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need to covert old letters to text for better redundancy.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ImmortalMacleod",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-21T01:44:51.952590+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-21T00:23:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Not sure if this is the right Sub for this question and will try cross posting to other subs that may have experience of dealing with the hardware and extracting the data.</p> <p>But here goes:</p> <p>Current hoarding project is to build a database of everything I&#39;ve watched at least in the past 5-10 years. So far have Netflix, Amazon, BBC iPlayer, Cinema Tickets (scraped from my Google Wallet), Any film I&#39;ve posted as a &quot;Watching&quot; status on Facebook and currently doing a second sweep for any post I&#39;ve made where I said I watched something. Still have to get data from Paramount+, Apple TV, Disney+, and Discovery+ but wanted to see how a Privacy Request to SKY would go down first - which is the basis of getting the information from these services (and how I got the BBC iPlayer information.</p> <p>The Subject Access Request to SKY came back telling me they had no data of that nature, and that&#39;s odd since the box knows what I&#3",
        "id": 3861879,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1obyiar/sky_q_viewingrecording_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SKY Q - Viewing/Recording Data",
        "vote": 0
    }
]