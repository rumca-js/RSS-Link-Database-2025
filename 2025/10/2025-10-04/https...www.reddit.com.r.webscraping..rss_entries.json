[
    {
        "age": null,
        "album": "",
        "author": "/u/One_Nose6249",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T21:39:18.071329+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T21:23:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there, I\u2019m using one of the well known scraping platforms scraper APIs. It tiers different websites from 1 to 5 with different pricing. I constantly get errors or access blocked oh 4th-5th tier websites. Is this the nature of scraping? No web pages guaranteed to be scraped even with these advanced APIs that cost too much? </p> <p>For reference, I\u2019m mostly scraping PDP pages from different brands </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/One_Nose6249\"> /u/One_Nose6249 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ny5z6y/web_scraper_apis_efficiency/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ny5z6y/web_scraper_apis_efficiency/\">[comments]</a></span>",
        "id": 3736450,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ny5z6y/web_scraper_apis_efficiency",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Scraper APIs\u2019 efficiency",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Embarrassed-Face-872",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T21:39:18.168498+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T20:14:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Are there any guides or repos out there that are optimized for location-based scraping of Amazon? Working on a school project around their grocery delivery expansion and want to scrape zipcodes to see where they offer perishable grocery delivery <strong>excluding Whole Foods</strong>. For example, you can get avocados delivered in parts of Kansas City via a scheduled delivery order, but I only know that because I changed my zipcode via the modal and waited to see if it was available. Looking to do randomized checks for new delivery locations and then go concentric when I get a hit.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Embarrassed-Face-872\"> /u/Embarrassed-Face-872 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ny48jl/amazon_location_specific_scrapes_for_scheduled/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n",
        "id": 3736451,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ny48jl/amazon_location_specific_scrapes_for_scheduled",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Amazon Location Specific Scrapes for Scheduled Delivery",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Yone-none",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T21:39:18.260747+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T19:38:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Let&#39;s say an user uploads a CSV file and it has 300 &quot;SKU&quot; , &quot;Title&quot; without <strong>URL of the SKU&#39;S websites</strong> but probably just domain like <a href=\"http://Amazon.com\">Amazon.com</a> , <a href=\"http://Ebay.com\">Ebay.com</a> that&#39;s it nothing like <a href=\"http://Amazon.com/product/id1000\">Amazon.com/product/id1000</a></p> <p>then somehow webscraping software it can track the price of the SKU on those websites.</p> <p>How is it possible to track without including URLS? </p> <p>I thought the user need to provide urls of all sku so the software can fetch and start to extract the price. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yone-none\"> /u/Yone-none </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ny3dcv/can_someone_tell_me_about_price_monitoring/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ny3dcv/can",
        "id": 3736452,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ny3dcv/can_someone_tell_me_about_price_monitoring",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can someone tell me about price monitoring software's logic",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Living-Window-1595",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T14:07:21.886522+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T12:56:45+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1nxta84/for_notion_not_able_to_scrape_the_page_content/\"> <img src=\"https://a.thumbs.redditmedia.com/edy29V0GZnN4KWXE3ARqWI3VimBWh6qcGsbcu3Hdpi4.jpg\" alt=\"for notion, not able to scrape the page content when it is published\" title=\"for notion, not able to scrape the page content when it is published\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey there!<br/> Lets say in Notion, I created a table with many pages as different rows, and published it publicly.<br/> Now I am trying to scrape the data, here the html content includes the table contents(page name)...but it doesnt include the page content...the page content is only visible when I hover on top of the page name element, and click on &#39;Open&#39;.<br/> Attached images here for better reference.</p> <p><a href=\"https://preview.redd.it/hpem2grke3tf1.png?width=564&amp;format=png&amp;auto=webp&amp;s=11cb6a358b5af80bf3d8ee50d32876a4addd1484\">https://pre",
        "id": 3734070,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxta84/for_notion_not_able_to_scrape_the_page_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/edy29V0GZnN4KWXE3ARqWI3VimBWh6qcGsbcu3Hdpi4.jpg",
        "title": "for notion, not able to scrape the page content when it is published",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/madredditscientist",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T10:29:03.538662+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T09:20:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A web scraping veteran recently told me that in the early 2000s, their scrapers were responsible for a third of all traffic on a big retail website. He even called the retailer and offered to pay if they\u2019d just give him the data directly. They refused and to this day, that site is probably one of the most scraped on the internet.</p> <p>It&#39;s kind of absurd: thousands of companies and individuals are scraping the same websites every day. Everybody is building their own brittle scripts, wasting compute, and fighting anti-blocking and rate limits\u2026 just to extract the very same data. </p> <p>Yet, we still don\u2019t see structured and machine-readable feeds becoming the standard. RSS (although mainly intended for news) showed decades ago how easy and efficient structured feeds can be. One clean, standardized XML interface instead of millions of redundant crawlers hammering the same pages.</p> <p>With AI, this inefficiency is only getting worse. Maybe it&#3",
        "id": 3733070,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxpcyg/why_are_we_all_still_scraping_the_same_sites_over",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why are we all still scraping the same sites over and over?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nseavia71501",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T09:17:02.910790+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T08:55:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just uncovered something that hit far closer to home than expected, even as an experienced scraper. I\u2019d appreciate any insight from others in the scraping community.</p> <p>I\u2019ve been in large-scale data automation for years. Most of my projects involve tens of millions of data points. I rely heavily on proxy infrastructure and routinely use thousands of IPs per project, primarily residential.</p> <p>Last week, in what initially seemed unrelated, I needed to install some niche video plugins on my 11-year-old son\u2019s Windows 11 laptop. Normally, I\u2019d use something like MPC-HC with LAV Filters, but he wanted something quick and easy to install. Since I\u2019ve used K-Lite Codec Pack off and on since the late 1990s without issue, I sent him the download link from their official site.</p> <p>A few days later, while monitoring network traffic for a separate home project, I noticed his laptop was actively pushing outbound traffic on ports 4444 and 4650. Closer inspe",
        "id": 3732724,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxoype/found_proxyware_on_my_sons_pc_time_to_admit_where",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Found proxyware on my son's PC. Time to admit where IPs come from.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Common_Western2300",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T09:17:03.197592+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T07:32:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hey everyone,</p> <p>so im basically trying to hit a API endpoint of a popular application in my country. A simple script using python(requests lib) works perfectly but ive been trying to implement this in nodejs using axios and i immediately get a forbidden 403 error. can anyone help me understand the underlying difference between 2 environments implementation and why am i getting varying results. Even hitting the endpoint from postman works just not using nodejs.</p> <p>what ive tried so far:<br/> headers: matched the headers from my netork tab into the node script.<br/> different implementations: tried axios, bun&#39;s fetch and got all of them fail with 403.<br/> headless browser: using puppeteer works, but im trying to avoid the overhead of a full browser. </p> <p>python code:</p> <pre><code>import requests url = &quot;https://api.example.com/data&quot; headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 ...&#39;, &#39;Auth_Key&#39;: &#39;some_key",
        "id": 3732725,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxnnii/scraping_api_gets_403_in_nodejs_but_works_fine_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping api gets 403 in Node.js, but works fine in Python. Why?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abdullah-shaheer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T03:13:28.048617+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T02:04:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone \ud83d\udc4b, I hope you are fine and good.</p> <p>Basically I am trying to automate:- </p> <p><a href=\"https://search.dca.ca.gov/\">https://search.dca.ca.gov/</a>. which is a website for checking authenticated license.</p> <p>Reference data:- Board: Accountancy, Board of License Type:CPA-Corporation License Number:9652</p> <p>My all approaches were failed as there was a Cloudflare on the page which I bypassed using pydoll/zendriver/undetected chromedriver/playwright but my request gets rejected each time upon clicking the submit button. May be due to the low success score of Cloudflare or other security measures they have in the backend.</p> <p>My goal is just to get the main page data each time I enter options to the script. If they allow a public/paid customizable API. That will also work.</p> <p>I know, this is a community of experts and I will get great help.</p> <p>Waiting for your reply in the comments box. Thank you so much.</p> </div><!-- SC_",
        "id": 3731619,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxhsk4/urgent_help_needed_for_web_automation_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "URGENT HELP NEEDED FOR WEB AUTOMATION PROJECT",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Valiantay",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T02:05:49.230404+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T02:00:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using Crawl4AI and allegedly it should be able to scrape everything - not so.</p> <p>I&#39;m trying to use the API to fetch the details from Google&#39;s new AI Flight Search but it doesn&#39;t seem to work.</p> <p>Another desktop web scraper can do it but doesn&#39;t have access to scheduling or auto-export. It&#39;s also $$$ so for personal use, that&#39;s a non-starter.</p> <p>Since the AI Search has no alerts, I wanted to set it Crawl4AI to scrape -&gt; n8n to send me an email about my specific query.</p> <p>Here&#39;s what I&#39;ve tried so far:</p> <p><code>&quot;magic&quot;: true,</code></p> <p><code>&quot;process_iframes&quot;: true,</code></p> <p><code>&quot;remove_overlay_elements&quot;: true,</code></p> <p><code>&quot;js_code&quot;: &quot;(async () =&gt; { await new Promise(r =&gt; setTimeout(r, 2000)); })();&quot;,</code></p> <p>Tried the following:</p> <ul> <li>Longer timeout</li> <li>LLM extraction</li> <li>Finding the CSS select",
        "id": 3731440,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxhp3o/google_flights_ai_search_impossible_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Google Flights AI Search - Impossible to Scrape?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/404mesh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T02:05:49.480080+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T01:42:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working with a TLS terminating proxy (mitmproxy on localhost:8080). The proxy presents its own cert (dev root installed locally). I&#39;m doing some HTTPS header rewriting in the MITM and, even though the obfuscation is consistent, login flows are breaking often. This usually looks something like being stuck on the login page, vague &quot;something went wrong&quot; messages, or redirect loops.</p> <p>I\u2019m pretty confident it\u2019s not a cert-pinning issue, but I\u2019m missing what else would cause so many different services to fail. How do enterprise products like Lightspeed (classroom management) intercept logins reliably on managed devices? What am I overlooking when I TLS-terminate and rewrite headers? Any pointers/resources or things to look for would be great.</p> <p>More: I am running into similar issues when rewriting packet headers as well. I am doing kernel level work that modifies network packet header values (like TTL/HL) using eBPF. Though ",
        "id": 3731441,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxhcav/oauth_and_other_signin_flows",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "OAuth and Other Sign-In Flows",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gvkhna",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-04T00:51:55.119886+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-04T00:51:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, i\u2019m working on an open source scraping tool and really am looking for feedback. Won\u2019t mention the project here but really looking to get examples of data you all are trying to scrape. </p> <p>Perhaps ones where it would be nice to have if it only took a few minutes but you don\u2019t have time. </p> <p>I can maybe post a scraper for you if feasible and you are willing to share the data you want to acquire. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gvkhna\"> /u/gvkhna </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nxgc01/what_dont_you_have_time_to_scrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nxgc01/what_dont_you_have_time_to_scrape/\">[comments]</a></span>",
        "id": 3731115,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nxgc01/what_dont_you_have_time_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What don\u2019t you have time to scrape?",
        "vote": 0
    }
]