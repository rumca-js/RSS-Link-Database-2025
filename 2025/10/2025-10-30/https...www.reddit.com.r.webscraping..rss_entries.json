[
    {
        "age": null,
        "album": "",
        "author": "/u/RabbitHoleGeorge",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-30T19:38:54.997094+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-30T18:31:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all \u2014 quick question:</p> <p>I\u2019ve got about 10 Google/Gmail accounts that I use when I manually QA our customers\u2019 websites. I want to log in to each account and have our agents automatically browse the customer sites. The browsing will be automated but very light \u2014 roughly what an obsessive web junkie would do on each account, not enough to create meaningful ad revenue or invalid impressions.</p> <p>Important: each of the 10 personas is supposed to be in a different country. We use static residential proxies in each of those countries, but we manage everything from our office in India.</p> <p>Questions:</p> <ol> <li>Would Google ban these Gmail accounts just for auto-browsing other people\u2019s sites, or would they mostly mark them as low-trust / flag them?</li> <li>Any suggestions for a minimal setup (proxies, device/browser fingerprints, login practices, recovery info, etc.) to keep operations running smoothly?</li> </ol> <p>Any pointers or experienc",
        "id": 3944524,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ok7pdk/crawling_nongoogle_sites_while_logged_in_to_google",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Crawling Non-Google Sites While Logged in to Google",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cloutboicade_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-30T09:46:29.655804+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-30T08:29:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for ways to upload to social media automatically but still look human, not an api. </p> <p>Anyone done this successfully using Puppeteer, Selenium, or Playwright? Ideas like visible Chrome instead of headless, random mouse moves, typing delays, mobile emulation, or stealth plugins. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cloutboicade_\"> /u/cloutboicade_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oju67i/humanlike_automated_social_media_uploading/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1oju67i/humanlike_automated_social_media_uploading/\">[comments]</a></span>",
        "id": 3939573,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oju67i/humanlike_automated_social_media_uploading",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Human-like automated social media uploading \u2022Puppeteer, Selenium, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/404mesh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-30T01:13:34.987218+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-30T00:01:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><em>Quick note, this is not a promotion post. I get no money out of this.</em> <a href=\"https://github.com/un-nf/404\">The repo is public.</a> <em>I just want feedback from people who care about practical anti\u2011fingerprinting work.</em></p> <p>I have a mild computer science background, but stopped pursuing it professionally as I found projects consuming my life. Lo-and-behold, about six months ago I started thinking long and hard about browser and client fingerprinting, in particular at the endpoint. TLDR, I was upset that all I had to do to get an ad for something was talk about it.</p> <p>So, I went down this rabbit hole on fingerprinting methods, JS, eBPF, dApps, mix nets, webscrabing, and more. All of this culminated into this project I am calling <strong>404</strong> (not found - duh).</p> <p>What it is:</p> <ul> <li>A TLS\u2011terminating mitmproxy script for experimenting with header/profile mutation, UA &amp; fingerprint signals, canvas/webGL hash sp",
        "id": 3937753,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ojkwhs/any_tips_on_localhost_tlstermination_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any tips on localhost TLS-termination for fingerprint evasion",
        "vote": 0
    }
]