[
    {
        "age": null,
        "album": "",
        "author": "/u/TeaFair8296",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T23:37:15.163497+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T22:47:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Building a Telegram bot that searches AliExpress products. I\u2019m using an LLM to extract search keywords from user requests, then using semantic search to match the right category ID before calling the aliexpress api. For this I need the full category tree in JSON format with: - category_id -category_name - parent_id - full hierarchy (root , children , leaf) Does anyone know where I can get this data?Is there an official API endpoint or should I scrape it? Thanks!! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TeaFair8296\"> /u/TeaFair8296 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1og4xsc/where_can_i_get_aliexpress_complete_category_tree/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1og4xsc/where_can_i_get_aliexpress_complete_category_tree/\">[comments]</a></span>",
        "id": 3903995,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1og4xsc/where_can_i_get_aliexpress_complete_category_tree",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where can I get AliExpress complete category tree with IDs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zaki_reg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T22:27:03.660352+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T21:56:10+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1og3toj/i_vibe_coded_an_ecommerce_web_scraper_to_scrape/\"> <img src=\"https://preview.redd.it/254b93gqwbxf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0cec28d4d5532f79b0a00b3a77f0a288cc024863\" alt=\"I vibe coded an ecommerce web scraper to scrape from +32 websites.\" title=\"I vibe coded an ecommerce web scraper to scrape from +32 websites.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone \ud83d\udc4b</p> <p>I built a web scraper for my e-commerce store and wanted to share how I solved a few scraping challenges.</p> <p><strong>Engine Detection</strong><br/> My scraper can automatically detect which platform a website is using for example, Shopify, WooCommerce, or another platform. Each platform has a different HTML structure, so the scraper detects the engine first, then uses the correct method to extract data.<br/> This saves me a lot of time because I scrape data from many suppliers. Before, I had to man",
        "id": 3903745,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1og3toj/i_vibe_coded_an_ecommerce_web_scraper_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/254b93gqwbxf1.png?width=640&crop=smart&auto=webp&s=0cec28d4d5532f79b0a00b3a77f0a288cc024863",
        "title": "I vibe coded an ecommerce web scraper to scrape from +32 websites.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/armanfixing",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T22:27:03.253214+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T21:50:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/webscraping\">r/webscraping</a>,</p> <p>I built a Chrome extension called Chromixer that helps bypass fingerprint-based detection. I&#39;ve been working with scraping for a while, and this is basically me putting together some of the anti-fingerprinting techniques that have actually worked for me into one clean tool.</p> <p><strong>What it does:</strong> - Randomizes canvas/WebGL output - Spoofs hardware info (CPU cores, screen size, battery) - Blocks plugin enumeration and media device fingerprinting - Adds noise to audio context and client rects - Gives you a different fingerprint on each page load</p> <p>I&#39;ve tested these techniques across different projects and they consistently work against most fingerprinting libraries. Figured I&#39;d package it up properly and share it.</p> <p><strong>Would love your input on:</strong></p> <ol> <li><p><strong>What are you running into out there?</strong> I&#39;ve mostly dealt with commercial",
        "id": 3903744,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1og3omu/built_a_fingerprint_randomization_extension",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Built a fingerprint randomization extension - looking for feedback",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Horror-Tower2571",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T21:17:18.908869+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T20:40:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys,</p> <p>Does anyone know of any good prompting tricks when getting an ai model like claude to code a scraper with bot evasion without it responding with &quot;I cAnT hElP wItH ThAt!!!&quot;, long story short I am trying to work quick and i need to code something quickly and all the ai models are giving me a pain in the ass. And please dont say &quot;code it yourself&quot; because i really dont have the superpower to write 10k lines of Python in 3 hours lol. Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Horror-Tower2571\"> /u/Horror-Tower2571 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1og21wv/getting_ai_to_code_a_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1og21wv/getting_ai_to_code_a_scraper/\">[comments]</a></span>",
        "id": 3903393,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1og21wv/getting_ai_to_code_a_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting ai to code a scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Negative-College-679",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T18:36:54.216100+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T16:51:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to see which companies have been given tenders for virtual tours, possibly make an automation out of this too. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Negative-College-679\"> /u/Negative-College-679 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ofwfwz/how_to_scrape_tendersontimecom_data_for_free/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ofwfwz/how_to_scrape_tendersontimecom_data_for_free/\">[comments]</a></span>",
        "id": 3902599,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ofwfwz/how_to_scrape_tendersontimecom_data_for_free",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape tendersontime.com data for free?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Global-Day9651",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T22:27:03.807801+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T14:57:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys, working on something really interesting in the AI B2B SAAS (and no it\u2019s just \u201canother one\u201d) space and looking for cofounders for the same. We\u2019re solving a real validated problem in the end to end sales space (something like clay but a lot better). Solving this is worth tens of thousands of dollars for our users, we have strong moats and a very early mover advantage.</p> <p>Little bit about us - Top tier team (PhD. Yale, IIT Madras) who have been working on this for months and developed a validated solution - we\u2019ve done a small angel round ($20k+) to keep things running, with a $250k pre-seed lined up in the next 4 months - The angels provide more than just capital, they are extremely successful entrepreneurs and one of them works in the space we\u2019re building for so access to first few customers as well as mentorship is a given - One of my mentors has over a billion dollars in PE/VC investments - Have a 100+ user waitlist filled up each user i",
        "id": 3903746,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1oftlwe/funded_startup_needs_another_technical_cofounder",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Funded startup needs another technical cofounder!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/henryhai0407",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T13:00:29.580967+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T09:35:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! My company is building an in-house AI using Microsoft Copilot (our ecosystem is mostly Microsoft). My manager wants us to collect competitor information from their official websites. The idea is to capture and store those pages as PDF or Word files in a central repository\u2014right now that\u2019s a SharePoint folder. Later, our internal AI would index that central storage and answer questions based on prompts.</p> <p>I tried automating the web-scraping with Power Automate to extract data from competitor sites and save files into the central storage, but it hasn\u2019t worked well. Each website uses different frameworks and CSS, so a single, fixed JavaScript to read text and export to Word/Excel isn\u2019t reliable.</p> <p>Could you advise better approaches for periodically extracting/ingesting this data into our central storage so our AI can read it and return results for management? Ideally Microsoft-friendly solutions would be great (e.g., SharePoint, Graph, Fabr",
        "id": 3900782,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ofndyz/web_scraping_for_ai_consumption",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scraping for AI consumption",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Even_Leading4218",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-25T13:00:29.424318+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-25T04:56:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ofiyad/i_built_a_free_nocode_scraper_for_social_content/\"> <img src=\"https://preview.redd.it/n6o50lpnv6xf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=51c1eb076bbd69913d60abeded8c156dbf255dfe\" alt=\"I built a free no-code scraper for social content\" title=\"I built a free no-code scraper for social content\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>hey everyone \ud83d\udc4b</p> <p>I found a lot of posts asking for a tool like this on this subreddit when I was looking for a solution, so I figured I would share it now that I made it available to the public.</p> <p>I can&#39;t name the social platform without the bot on this subreddit flagging it, which is quite annoying... But you can figure out which social platform I am talking about.</p> <p>With the changes made to the API\u2019s limits and pricing, I wasn&#39;t able to afford the cost of gathering any real amount of data from my social feed &amp; I wanted to store",
        "id": 3900781,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ofiyad/i_built_a_free_nocode_scraper_for_social_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/n6o50lpnv6xf1.png?width=640&crop=smart&auto=webp&s=51c1eb076bbd69913d60abeded8c156dbf255dfe",
        "title": "I built a free no-code scraper for social content",
        "vote": 0
    }
]