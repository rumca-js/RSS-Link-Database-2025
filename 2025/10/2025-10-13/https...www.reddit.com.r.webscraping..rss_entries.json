[
    {
        "age": null,
        "album": "",
        "author": "/u/heyoneminute",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T21:21:55.842660+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T21:00:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone!</p> <p>One of my first struggles when building CLI tools for end-users in Python was that customers always had problems inputting proxies. They often struggled with the <code>scheme://user:pass@ip:port</code> format, so a few years ago I made a parser that could turn any user input into Python&#39;s proxy format with a one-liner.<br/> After a long time of thinking about turning it into a library, I finally had time to publish it. Hope you find it helpful \u2014 feedback and stars are appreciated :)</p> <h1>What My Project Does</h1> <p>proxyutils parses any format of proxy into Python&#39;s niche proxy format with one-liner . It can also generate proxy extension files / folders for libraries Selenium.</p> <h1>Target Audience</h1> <p>People who does scraping and automating with Python and uses proxies. It also concerns people who does such projects for end-users.</p> <p>It worked excellently, and finally, I didn\u2019t need to handle complaints abou",
        "id": 3805050,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5w42y/proxy_parser_formatter_for_python_proxyutils",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Proxy parser / formatter for Python - proxyutils",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hour_Bit_2030",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T21:21:56.140039+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T20:46:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><h1>The State of Web Scraping in 2025: Code vs No-Code, Traditional vs AI-Powered</h1> <p>Hey <a href=\"/r/webscraping\">r/webscraping</a>! With all the new tools and AI-powered solutions hitting the market, I wanted to start a discussion about what the scraping landscape actually looks like in 2025. I&#39;ve been doing some research on what&#39;s available now, and the divide between traditional frameworks and modern solutions is getting really interesting.</p> <h2>The Big Shift I&#39;m Seeing</h2> <p>The ecosystem has basically split into three camps:</p> <p><strong>1. Traditional Code-First Tools</strong> - Still going strong with Scrapy, Beautiful Soup, Selenium, Puppeteer, etc. Full control, zero costs (minus infrastructure), but you&#39;re handling everything yourself including anti-bot measures.</p> <p><strong>2. API-Based Scraping Services</strong> - These handle proxies, rotating IPs, CAPTCHA solving, and anti-bot bypassing for you. You still writ",
        "id": 3805051,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5vqcr/web_scraping_in_2025_code_vs_nocode_ai_magic_vs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Scraping in 2025: Code vs No-Code, AI Magic vs Traditional Method",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DefiantScarcity3133",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T17:29:08.703245+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T16:35:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am currently using residential proxy for wallmart product scraping but it is still detecting them.<br/> Any idea how to pass this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DefiantScarcity3133\"> /u/DefiantScarcity3133 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o5opkb/how_to_bypass_wallmart_bot_protection_for_product/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o5opkb/how_to_bypass_wallmart_bot_protection_for_product/\">[comments]</a></span>",
        "id": 3803403,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5opkb/how_to_bypass_wallmart_bot_protection_for_product",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to bypass Wallmart BOT protection for product scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Much-Movie-695",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T17:29:08.517969+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T16:07:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been diving into Ai-powered scraping tools lately because I kept seeing them pop up everywhere. The pitch sounds great, just describe what you want in plain English, and it handles the scraping for you. No more writing selectors, no more debugging when sites change their layout.</p> <p>So I tested a few over the past month. Some can handle basic stuff like popups and simple CAPTCHAs , which is cool. But when I threw them at more complex sites (ones with heavy JS rendering, multi-step logins, or dynamic content), things got messy. Success rate dropped hard, and I ended up tweaking configs anyway.</p> <p>I&#39;m genuinely curious about what others think. Are these AI tools actually getting good enough to replace traditional scripting? Or is it still mostly marketing hype, and we&#39;re stuck maintaining Playwright/Puppeteer for anything serious?</p> <p>Would love to hear if anyone&#39;s had better luck, or if you think the tech just isn&#39;t t",
        "id": 3803402,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5ny1a/ai_scraping_tools_hype_or_actually_replacing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI scraping tools, hype or actually replacing scripts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Oh_Boy99",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T14:14:36.330778+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T13:37:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone \ud83d\udc4b</p> <p>I\u2019ve just finished coding a tool that scrapes listings from <strong>LeBonCoin</strong>, which is basically the French equivalent of Craigslist \u2014 it\u2019s one of the biggest online marketplaces in France for buying and selling anything (second-hand items, real estate, cars, etc.).</p> <p>You can check out a live demo here: <a href=\"http://monitorlbc.ohboy-domain.site/\">http://monitorlbc.ohboy-domain.site/</a></p> <p>Right now, it pulls listings that match specific filters. I originally built it for <strong>buyers and resellers</strong>, there\u2019s no data export yet, but I imagine there could be a ton of other use cases for a LeBonCoin scraper (lead generation, market research, price monitoring\u2026).</p> <p>I\u2019d love to get some <strong>beta testers</strong> to try it out and share feedback so I can iterate on it.</p> <p>Feel free to drop your thoughts in the comments or reach out via Discord \u2014 username: <strong>OhBoy</strong></p> <p>Thanks ",
        "id": 3800507,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5jxum/built_a_leboncoin_scraper_looking_for_beta_testers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Built a LeBonCoin scraper - looking for beta testers!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Tejas0_0Naik",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T18:42:21.663806+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T12:54:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi scrapers,</p> <p>I\u2019ve been working on a Playwright-based scraper for BigBasket\u2019s website and encountering some tough issues that I haven\u2019t seen with other sites like Blinkit and Zepto.</p> <p>What\u2019s happening:</p> <ul> <li>The first click (to open the location selector)only works if I reload the page first. Without reload, click attempts time out or get blocked.</li> <li>The second click (typing into the location search input)does not seem to work at all, no matter what I try (Playwright <code>.click()</code>, JavaScript click, <code>.press_sequentially()</code>).</li> <li>Using stealth mode made things worse \u2014 nothing works even after reload.</li> <li>Console logs reveal Zustand state management deprecated warnings**, strict Content Security Policy (CSP) errors blocking scripts, and blocked ads/scripts when using browsers with ad blockers.</li> <li>The <code>navigator.webdriver</code> flag is always <code>true</code> in Playwright, indicating auto",
        "id": 3803979,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5iwb8/struggling_to_automate_bigbasket_website_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Struggling to Automate BigBasket Website with Playwright",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Slow_Wait6550",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T22:40:12.458150+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T12:34:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I have a spider that scrapes data at scale using <strong>Scrapy + Playwright</strong>. I\u2019ve been trying to automate it on a schedule using <strong>cron</strong> or <strong>LaunchAgents</strong>, but both approaches have failed miserably. I\u2019ve wasted days trying to configure them, and they both seem to have issues running Playwright reliably.</p> <p>I\u2019m wondering how professional scrapers handle this efficiently. What\u2019s the <strong>most reliable way</strong> to schedule and automate Scrapy + Playwright jobs?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Slow_Wait6550\"> /u/Slow_Wait6550 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o5igaq/most_reliable_tool_to_automate_scrapy_playwright/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o5igaq/most_reliable_tool_to_automate_scrapy_playwright/\">[comments]</a></span>",
        "id": 3805607,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5igaq/most_reliable_tool_to_automate_scrapy_playwright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Most reliable tool to automate Scrapy + Playwright spiders?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SemperPistos",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T09:27:27.421118+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T08:21:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I can&#39;t find it anywhere in the documentation.<br/> I can only find filtering based on a domain, not url.</p> <p>Thank you :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SemperPistos\"> /u/SemperPistos </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o5e0jw/does_crawl4ai_have_an_option_to_exclude_urls/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1o5e0jw/does_crawl4ai_have_an_option_to_exclude_urls/\">[comments]</a></span>",
        "id": 3800146,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o5e0jw/does_crawl4ai_have_an_option_to_exclude_urls",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does crawl4ai have an option to exclude urls based on a keyword?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Initial_Panda3090",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-10-13T06:46:15.926960+00:00",
        "date_dead_since": null,
        "date_published": "2025-10-13T03:30:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! I\u2019ve been using a Namescheap catch-all email to create multiple accounts for automation, but the website blacklisted my domain despite using proxies, randomized user agents, and different fingerprints. I simulated human behavior such as delayed clicks, typing speeds, and similar interaction timing. I guarantee the blacklist is due to the lower reputation of catchall domains compared with major providers like Gmail or Outlook. I\u2019d prefer to continue using a catch-all rather than creating many Outlook/Gmail accounts or using captcha solving services. Does anyone have alternative approaches or suggestions for making catch-alls work, or ways to create multiple accounts without going through captcha solvers? If using a captcha solver is the only option, that\u2019s fine. Thank you in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Initial_Panda3090\"> /u/Initial_Panda3090 </a> <br/> <span><a href=\"https://www.r",
        "id": 3798990,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1o59258/catch_all_emails_for_automation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Catch All Emails For Automation.",
        "vote": 0
    }
]