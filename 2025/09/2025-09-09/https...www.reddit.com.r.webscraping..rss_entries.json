[
    {
        "age": null,
        "album": "",
        "author": "/u/jessejjohnson",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T00:04:39.019299+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T22:57:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ncxugp/hiring_senior_engineer_enterprise_scale_web/\"> <img src=\"https://external-preview.redd.it/vrROPwu0QymVA_zEIOnmmIPIzyiyALhD5f5n2FbnMzU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f45441183c4ce7d60f49606cfcf3902036247205\" alt=\"[Hiring] Senior Engineer, Enterprise Scale Web Scraping Systems\" title=\"[Hiring] Senior Engineer, Enterprise Scale Web Scraping Systems\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>We\u2019re seeking a senior engineer with extensive, proven experience in designing and operating enterprise scale web scraping systems. This role requires deep technical expertise in advanced anti-bot evasion, distributed and fault tolerant scraping architectures, large scale data streaming pipelines, and global egress proxy networks.</p> <p>Candidates must have a track record of building high throughput, production grade systems that reliably extract and process data at scale. <strong>This is a han",
        "id": 3536678,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncxugp/hiring_senior_engineer_enterprise_scale_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/vrROPwu0QymVA_zEIOnmmIPIzyiyALhD5f5n2FbnMzU.png?width=640&crop=smart&auto=webp&s=f45441183c4ce7d60f49606cfcf3902036247205",
        "title": "[Hiring] Senior Engineer, Enterprise Scale Web Scraping Systems",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SnooFloofs4038",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T00:04:39.503364+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T20:18:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am far from proficient in python. I have a strong background in Java, C++, and C#. I took up a little web scraping project for work and I&#39;m using it as a way to better my understanding of the language. I&#39;ve just carried over my knowledge from languages I know how to use and tried to apply it here, but I think I am starting to run into something of a language barrier and need some help.</p> <p>The program I&#39;m writing is being used to take product data from a predetermined list of retailers and add it to my company&#39;s catalogue. We have affiliations with all the companies being scraped, and they have given us permission to gather the products in this way.</p> <p>The program I have written relies on requests-html and bs4 to do the following</p> <ul> <li>Request the html at a predetermined list of retailer URLs (all get requests happen concurrently)</li> <li>Render the pages (every page in the list relies on JS to render)</li> <li>Find li",
        "id": 3536679,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nctwf8/struggling_with_requestshtml",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Struggling with requests-html",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Top-Journalist9785",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T21:23:12.140721+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T19:41:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Everyone,</p> <p>I&#39;m new to web scraping and recently learned the basics through tutorials on Scrapy and Playwright. I&#39;m planning a project to scrape Amazon product listings and would appreciate your feedback on my approach.</p> <p>My Plan:</p> <p>*Forward Proxy: to avoid IP blocks.</p> <p>*Browser Automation: Playwright (is selenium better? I used AI, and it told playwright is just as good but not sure)</p> <p>*Data Processing: Scrapy data pipelines and cleaning.</p> <p>*Storage: MySQL</p> <p>Could you advise me on the type of thing I should look out for, like rate limiting strategies, Playwright&#39;s stealth modes against Amazon detection or perhaps a better proxy solutions I should consider.</p> <p>Many Thanks</p> <p>p.s. I am doing this to learn </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Top-Journalist9785\"> /u/Top-Journalist9785 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 3535513,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncsv9e/1st_time_scrapping_amazon_any_helpful_tips",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "1st Time scrapping Amazon, any helpful tips",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Few-Tie-55",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T00:04:40.088854+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T14:57:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i was wondering if there are any script or tools for the job, 10x!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Few-Tie-55\"> /u/Few-Tie-55 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ncl7b4/any_tools_that_map_geo_location_to_websites/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ncl7b4/any_tools_that_map_geo_location_to_websites/\">[comments]</a></span>",
        "id": 3536680,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncl7b4/any_tools_that_map_geo_location_to_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any tools that map geo location to websites ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vitmaster001",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T22:46:22.036707+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T13:41:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ncj9c4/camoufox_unable_to_add_other_fonts/\"> <img src=\"https://b.thumbs.redditmedia.com/nuRKhdqZU3fb8OLXyiR6-E3ZDE-HCBeuxpLif_kVqSQ.jpg\" alt=\"[camoufox] Unable to add other fonts\" title=\"[camoufox] Unable to add other fonts\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I am attempting to add other fonts as described here <a href=\"https://camoufox.com/fingerprint/fonts/\">https://camoufox.com/fingerprint/fonts/</a></p> <p>But fonts not loaded. I have copied UbuntuCondensed-Regular.ttf to camoufox/fonts and camoufox/fonts/windows. Also added to /usr/share/fonts, launched <code>sudo fc-cache -fv</code>, <code>fc-list :family</code> shows installed Ubuntu but NOT Ubuntu Condensed font</p> <pre><code>config = { &#39;fonts&#39;: [&quot;Ubuntu&quot;, &quot;Ubuntu Condensed&quot;], &#39;fonts:spacing_seed&#39;: 2, } </code></pre> <p>But only Ubuntu loads. Ubuntu Condensed - not.<br/> I also tried Arial, Times New R",
        "id": 3536207,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncj9c4/camoufox_unable_to_add_other_fonts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/nuRKhdqZU3fb8OLXyiR6-E3ZDE-HCBeuxpLif_kVqSQ.jpg",
        "title": "[camoufox] Unable to add other fonts",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/psy_com",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T14:12:57.188842+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T13:29:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am working on a research project for my university, for which we need a knowledge base. Among other things, this should contain transcripts of various YouTube videos on specific topics. For this purpose, I am using a Python program with the YouTubeTranscriptApi library.</p> <p>However, YouTube rejects further requests after 24, so that I am timed out or banned from my IP (I don&#39;t know exactly what happens there).</p> <p>In any case, my professor is convinced that there is an official API from Google (which probably costs money) that can be used to download such transcripts on a large scale. As I understand it, the YouTube Data API v3 is not suitable for this purpose.</p> <p>Since I have not found such an API, I would like to ask if anyone here knows anything about this and could tell me which API he specifically means.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/psy_com\"> /u/psy_com </a> <br/> <span><a ",
        "id": 3531905,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nciy6f/get_subtitles_via_youtube_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Get subtitles via Youtube API",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T13:04:46.787265+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T13:01:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 3531342,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nciaps/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/steven1379_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T14:12:57.329723+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T12:59:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>any idea on how to make it works in .net httpclient ? it works on postman standalone or C# console with http debugger pro turned on.</p> <p>i encounter 403 forbidden whenever it runs alone in .net core. </p> <pre><code>POST /v2/search HTTP/1.1 Host: bff-mobile.propertyguru.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Content-Type: application/json Cookie: __cf_bm=HOvbm6JF7lRIN3.FZOrU26s9uyfpwkumSlVX4gqhDng-1757421594-1.0.1.1-1KjLKPJvy89RserBSSz_tNh8tAMrslrr8IrEckjgUxwcFALc4r8KqLPGNx7QyBz.2y6dApSXzWZGBpVAtgF_4ixIyUo5wtEcCaALTvjqKV8 Content-Length: 777 { &quot;searchParams&quot;: { &quot;page&quot;: 1, &quot;limit&quot;: 20, &quot;statusCode&quot;: &quot;ACT&quot;, &quot;region&quot;: &quot;my&quot;, &quot;locale&quot;: &quot;en&quot;, &quot;regionCode&quot;: &quot;2hh35&quot;, &quot;_floorAreaUnits&quot;: &quot;sqft&quot;, &quot;_landAreaUnits&quot;: &quot;sqft&quot;, &q",
        "id": 3531906,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nci8tp/api_scrapping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "API Scrapping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Tequila-Giesskanne",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T11:55:12.003272+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T11:17:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>Quick question about &quot;Gutefrage.net&quot; \u2014 kind of like the quirky, slightly lackluster German cousin of Reddit. I\u2019m using some tools to track keywords on Reddit so I can stay updated on topics I care about.</p> <p>Does anyone know if there\u2019s a way to do something similar for Gutefrage.net? I\u2019d love to get automated notifications whenever one of my keywords pops up, without having to check the site manually all the time.</p> <p>Any tips would be really appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tequila-Giesskanne\"> /u/Tequila-Giesskanne </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ncg30n/keyword_tracking_on_gutefragenet/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ncg30n/keyword_tracking_on_gutefragenet/\">[comments]</a></span>",
        "id": 3530747,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncg30n/keyword_tracking_on_gutefragenet",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Keyword tracking on Gutefrage.net",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SunnyShaiba",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T11:55:12.134148+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T10:32:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I recently set up a Docker container for the open-source project Scrapegraph AI, and now I&#39;m testing its different functions, like web search. The Search Graph uses DuckDuckGo as the engine, and you can just pass your prompt. This is my first time using a crawler, so I have no idea what\u2019s under the hood. Anyway, the search results are shit af, 3 tries with 10 urls each to find out if my fav kebab diner is open lol. It scrap weird urls my smart google friend would never show me. Should I switch to other engines, or do I need to parameterize them (region etc.) or wtf should I do? Probably search manually right... </p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SunnyShaiba\"> /u/SunnyShaiba </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ncfaic/scrapegraphai_duckduckgo/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ncfaic/scr",
        "id": 3530748,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncfaic/scrapegraphai_duckduckgo",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ScrapeGraphAi + DuckDuckGo",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vroemboem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T10:45:00.676343+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T10:02:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ncescp/bypassing_cloudflare_turnstile/\"> <img src=\"https://preview.redd.it/2kdzamsl44of1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=daa5d89b95a9b3c32d4a7aff5539897a005ec1b6\" alt=\"Bypassing Cloudflare Turnstile\" title=\"Bypassing Cloudflare Turnstile\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I want to scrape an API endpoint that&#39;s protected by Cloudflare Turnstile. </p> <p>This is how I think it works: 1. I visit the page and am presented with a JavaScript challenge. 2. When solved Cloudflare adds a cf_clearance cookie to my browser. 3. When visiting the page again the cookie is detected and the challenge is not presented again. 4. After a while the cookie expires and a new challenge is presented.</p> <p>What are my options when trying to bypass Cloudflare Turnstile?</p> <p>Preferably I would like to use a simple HTTP client (like curl) and not use full fledged browser automation (like seleniu",
        "id": 3530204,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncescp/bypassing_cloudflare_turnstile",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/2kdzamsl44of1.jpeg?width=640&crop=smart&auto=webp&s=daa5d89b95a9b3c32d4a7aff5539897a005ec1b6",
        "title": "Bypassing Cloudflare Turnstile",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/brewpub_skulls",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T08:21:52.326805+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T07:44:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Any recommendation for Indian VM provider or VM with Indian IP that allows to run bot and scraping.</p> <p>Please PM for suggestions.</p> <p>Appreciate any help.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/brewpub_skulls\"> /u/brewpub_skulls </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nccprd/vm_for_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nccprd/vm_for_scraping/\">[comments]</a></span>",
        "id": 3529331,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nccprd/vm_for_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "VM for scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ransixi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-09T08:21:52.704929+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-09T07:08:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks \ud83d\udc4b</p> <p>Over the past year, we\u2019ve been turning our weekend project into a SaaS tool called <strong>Capalyze</strong>.<br/> It\u2019s an AI-powered agent that helps you: - Capture data from multiple websites (APIs, CSVs, even web scraping) - Run sentiment analysis &amp; other text insights - Generate interactive dashboards automatically</p> <p>But the real journey has been in <strong>building the business</strong>, not just the tech:</p> <p><strong>What we got right</strong> - Started with a simple, natural language interface (\u201csay it, Capalyze it\u201d) - Found a surprising early audience: small e-commerce operators &amp; solo founders</p> <p><strong>What we struggled with</strong> - Our initial pricing was too complicated (moved to a simple <strong>freemium</strong> model) - Integrations took longer than expected - Balancing \u201ccool AI features\u201d with \u201creal user needs\u201d was harder than we thought</p> <p><strong>Where we are now</strong> - Freemium is li",
        "id": 3529332,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ncc62y/lessons_learned_building_an_aipowered_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "lessons learned building an AI-powered webscraping agent",
        "vote": 0
    }
]