[
    {
        "age": null,
        "album": "",
        "author": "/u/Horror-Tower2571",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T16:11:48.069206+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T15:14:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you&#39;ve ever heard of <a href=\"https://buckets.grayhatwarfare.com/\">Greyhat Warfare</a> you might know that they scrape public data in S3 and other public bucket types, but how would they just enumerate buckets from scratch? Thats a question I&#39;ve been trying to find for a while, if anyone knows or has suggestions feel free to throw them down in the comments, anything helps.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Horror-Tower2571\"> /u/Horror-Tower2571 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr40dl/anyone_managed_to_scrape_public_s3_buckets/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr40dl/anyone_managed_to_scrape_public_s3_buckets/\">[comments]</a></span>",
        "id": 3671643,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nr40dl/anyone_managed_to_scrape_public_s3_buckets",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone managed to scrape public s3 buckets?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No_Development_5561",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T15:10:34.162287+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T14:15:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello folks, i have been doing webscraping for 3 years from my senior university year. I am doing it for a hobby. I am a software engineer by the way.<br/> I use selenium. If it doesn&#39;t work, i customize headers. Generally this is enough.<br/> After I realized that there are other tools and methods (scrapy, proxies etc.) I wonder that is using selenium outdated and do i have to learn the other ways to be a good scraper?<br/> Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Development_5561\"> /u/No_Development_5561 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr2htb/about_selenium_and_other_tools/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr2htb/about_selenium_and_other_tools/\">[comments]</a></span>",
        "id": 3671111,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nr2htb/about_selenium_and_other_tools",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "About selenium and other tools",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/unteth",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T14:05:01.397120+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T13:46:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><ul> <li>What\u2019s your stack / setup?</li> <li>What data are you scraping (if you don\u2019t mind answering, or even CAN answer)</li> <li>What problems have you ran into?</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/unteth\"> /u/unteth </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr1s6s/anyone_here_scraping_at_a_large_scale_millions_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr1s6s/anyone_here_scraping_at_a_large_scale_millions_a/\">[comments]</a></span>",
        "id": 3670619,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nr1s6s/anyone_here_scraping_at_a_large_scale_millions_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone here scraping at a large scale (millions)? A few questions.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/apple713",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T14:05:01.930434+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T13:25:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to build a scraper that will provide me all of the new publications, announcements, press releases, etc from given domain. I need help with the high level methodolgy I&#39;m taking, and am open to other suggestions. Currently my approach is</p> <ol> <li>To use crawl4ai to seed urls from sitemap and common crawl, filter down those urls and paths using remove tracking additions, remove duplicates, positive and negative keywords, to find the listing pages (what im calling the pages that link to the articles and content I want to come back for).,</li> <li>Then it should use deep crawling to crawl an entire depths to find URLs not discovered in step one, ignoring paths it elimitated in step 1. remove tracking, duplicates, filter negative and positive keywords in paths, identify the listing pages again.,</li> <li>Then use llm calls to validate the pages it identified as listing pages by downloading content and understanding and then present t",
        "id": 3670620,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nr1b6b/need_help_feedback_on_my_approach_to_my_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "need help / feedback on my approach to my scraping project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kailtis",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T13:03:28.301295+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T13:00:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone!</p> <p>Figured I&#39;d ask here and see if someone could give me any pointers where to look at for a solution.</p> <p>For my business I used to rely heavily on a scraper to get leads out of a famous database website.</p> <p>That scraper is not available anymore, and the only one left is the overpriced $30/1k leads official one. (Before you could get by with $1.25/1k).</p> <p>I&#39;m thinking of attempting to build my own, but I have no idea how difficult it will be, or if doable by one person.</p> <p>Here&#39;s the main challenges with scraping the DB pages :</p> <p>- The emails are hidden, and get accessed by consuming credits after clicking on the email of each lead (row). Each unblocked email consumes one credit. The cheapest paid plan gets 30k credits per year. The free tier 1.2K.<br/> - On the free plan you can only see 5 pages. On the paid plans, you&#39;re limited to 100 (max 2500 records).<br/> - The scraper I mentioned allowed",
        "id": 3670062,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nr0qhy/how_would_you_scrape_from_a_db_website_that_has",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How would you scrape from a DB website that has these constraints?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/namalleh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T14:05:02.610021+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T12:28:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Curious for the defenders - what&#39;s your preferred stack of defense against web scraping?</p> <p>What are your biggest pain points?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/namalleh\"> /u/namalleh </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr00tu/kind_of_an_antipost/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nr00tu/kind_of_an_antipost/\">[comments]</a></span>",
        "id": 3670621,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nr00tu/kind_of_an_antipost",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Kind of an anti-post",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ddlatv",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-26T02:20:37.895410+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-26T01:35:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is this some kind of spam we are not aware of? Just asking. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ddlatv\"> /u/ddlatv </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nqoo62/whats_with_all_this_im_new_on_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nqoo62/whats_with_all_this_im_new_on_scraping/\">[comments]</a></span>",
        "id": 3666888,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nqoo62/whats_with_all_this_im_new_on_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What's with all this \"I'm new on scraping\"?",
        "vote": 0
    }
]