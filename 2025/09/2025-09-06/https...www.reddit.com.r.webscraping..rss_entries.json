[
    {
        "age": null,
        "album": "",
        "author": "/u/Lafftar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-06T23:05:28.006436+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-06T22:30:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1nacryn/the_python_library_you_need_to_get_past_amazon/\"> <img src=\"https://external-preview.redd.it/bWMxZjEzbTVmbW5mMfq0X2A3-FHz2brh9Vzwjo8myQR_sMGIxb0f9sIRSfff.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3785251f97654cb496c59e94494ddd4157eedd2d\" alt=\"The Python library you need to get past Amazon and cloudflare blocks\" title=\"The Python library you need to get past Amazon and cloudflare blocks\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>\u200bI used to sell this exact insight for $300. Now, I&#39;m sharing it for free.</p> <p>\u200bThis video breaks down the number one reason data collection scripts get blocked by sites like Amazon: the client fingerprint. </p> <p>I show a quick test that proves why some tools fail instantly while others succeed.</p> <p>\u200bIf you&#39;re building a scraping or automation solution, understanding this is critical.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https:/",
        "id": 3512639,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nacryn/the_python_library_you_need_to_get_past_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/bWMxZjEzbTVmbW5mMfq0X2A3-FHz2brh9Vzwjo8myQR_sMGIxb0f9sIRSfff.png?width=640&crop=smart&auto=webp&s=3785251f97654cb496c59e94494ddd4157eedd2d",
        "title": "The Python library you need to get past Amazon and cloudflare blocks",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DpsEagle",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-06T17:06:47.377985+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-06T16:26:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, I started selling on eBay recently and decided to make my first web scraper to give me notifications if any competition is undercutting my selling price. If anyone would try it out to give feedback on the code / functionality I would be really grateful so that I can improve it! </p> <p>Currently you type your product name with its prices inside the config file with a couple more customizable settings, after it searches for the product on eBay and lists all products which were cheaper with desktop notifications, can be run as a background process and comes with log files</p> <p><a href=\"https://github.com/Igor-Kaminski/ebay-price-monitor\">https://github.com/Igor-Kaminski/ebay-price-monitor</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DpsEagle\"> /u/DpsEagle </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1na3v48/first_scarper_ebay_price_monitor_uk/\">[link]</a></span> &#32; <span>",
        "id": 3510889,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1na3v48/first_scarper_ebay_price_monitor_uk",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "First scarper - eBay price monitor UK",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AdditionMean2674",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-06T17:06:47.788170+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-06T16:22:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How do companies like Google or Perplexity build their Scrapers? Does anyone have an insight into the technical architecture?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AdditionMean2674\"> /u/AdditionMean2674 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1na3r1l/how_are_large_scale_scrapers_built/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1na3r1l/how_are_large_scale_scrapers_built/\">[comments]</a></span>",
        "id": 3510890,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1na3r1l/how_are_large_scale_scrapers_built",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How are large scale scrapers built?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Impossible-Rub-3067",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-06T17:06:48.244890+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-06T16:12:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Part of my new job is ridiculous busy work That involves browsing specific websites to identify certain events in the area, copying and pasting the What, when, where, why and the URL to that relevant webpage into a email. In the email those 5 W&#39;s are formatted into a very simple easy to read text block.</p> <p>This isn&#39;t something I want to automate entirely, I need to make sure that the webpage that I copy from is actually relevant, so I need a tool that I can manually activate when I find the relevant webpage.</p> <p>Would an extension like Web Scraper be the most applicable for a relatively simple task like this? Build a sitemap and export the data? It seems Web Scraper only exports to a csv. What I would like is to export that data scraped from the site into a simple txt or doc with a specific format. </p> <p>Maybe this would require 2 tools or python, which is outside of my capabilities.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <",
        "id": 3510891,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1na3ilv/complete_beginner_trying_to_automate_busy_work",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Complete beginner trying to automate busy work",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/madredditscientist",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-06T15:57:51.534099+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-06T15:52:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What would you consider a fair and effective take-home task to test real-world scraping skills (without being too long or turning into free work)?</p> <p>Curious to hear what worked well for you, both as a candidate and as a hiring team.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/madredditscientist\"> /u/madredditscientist </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1na30m5/whats_a_good_takehome_assignment_for_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1na30m5/whats_a_good_takehome_assignment_for_scraping/\">[comments]</a></span>",
        "id": 3510525,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1na30m5/whats_a_good_takehome_assignment_for_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s a good take-home assignment for scraping engineers?",
        "vote": 0
    }
]