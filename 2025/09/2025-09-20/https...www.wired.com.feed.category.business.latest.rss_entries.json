[
    {
        "age": null,
        "album": "",
        "author": "Amos Zeeberg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-20T11:26:02.824638+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-20T11:00:00+00:00",
        "description": "A fundamental technique lets researchers use a big, expensive model to train another model for less.",
        "id": 3622257,
        "language": "en-US",
        "link": "https://www.wired.com/story/how-distillation-makes-ai-models-smaller-and-cheaper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 396,
        "source_url": "https://www.wired.com/feed/category/business/latest/rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://media.wired.com/photos/68ca9371156a7f15ecfea1d5/master/pass/Distillation-Explainer-crNico-H.-Brausch-Lede.jpeg",
        "title": "Distillation Can Make AI Models Smaller and Cheaper",
        "vote": 0
    }
]