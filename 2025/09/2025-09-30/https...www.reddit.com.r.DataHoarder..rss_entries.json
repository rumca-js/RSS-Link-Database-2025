[
    {
        "age": null,
        "album": "",
        "author": "/u/al3arabcoreleone",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T23:41:56.551413+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T22:59:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Based on this <a href=\"https://www.reddit.com/r/math/comments/1nt60s8/what_books_or_articles_should_i_download_before/\">post</a> in <a href=\"/r/math\">r/math</a>, Springer is giving &quot;free access to almost all of their books, research papers, and articles. Unfortunately, this agreement will end on December 31, 2025, and it doesn\u2019t look like it will be renewed.&quot; by OP. </p> <p>Any Egyptian here willing to do the job ? Please it is a rare opportunity for such a huge amount of materials from one of the big publishers to be freely and legaly be downloaded, we need to strike the iron while hot.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/al3arabcoreleone\"> /u/al3arabcoreleone </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nus04c/any_egyptian_data_hoarders_here_its_your_time_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nus04c/any_egypt",
        "id": 3703954,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nus04c/any_egyptian_data_hoarders_here_its_your_time_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any Egyptian Data Hoarders here ? it's your time for the duty!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Same_Platypus1629",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T23:41:57.291595+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T22:53:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve currently got 2x8TB hdd setup as individual disks on my server. I can&#39;t actually remember my reason for this as it was a while ago, but I&#39;m currently running Proxmox with one of the disks acting as a NAS, and the other for movies, etc, through Jellyfin.</p> <p>I don&#39;t need instant access to things if the server goes down, as long as my files are backed up. I&#39;m planning to mirror the server at a family member&#39;s house with a basic setup, but I thought about buying a bunch of disks to make the server bigger anyway. </p> <p>I&#39;ve still got about 6 TB of space free, but I want to ensure I&#39;m doing things right. My question is, how do i best switch from ext4 to a more futureproof system? What I mean by that is, so I can just adding more disks in the future without configuring a bunch of stuff. Perhaps a pool or something? I&#39;m pretty sure I&#39;ve read somewhere that I&#39;d have to reformat everything if I decide to",
        "id": 3703955,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nurv8t/change_ext4_to_raid_pool",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Change ext4 to raid pool",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PoultryTechGuy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T22:25:45.741471+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T22:19:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019m brand new here and could use some guidance. I\u2019ve suddenly run into a storage crunch and need to download somewhere in the ballpark of 7 million images (~7TB) with room to grow, so I\u2019m realistically looking at 10 to 12TB+.</p> <p>Here\u2019s my situation:</p> <p>Right now, I\u2019m completely out of space on my Windows PC.</p> <p>Long term, I want to build a NAS with RAID, but that\u2019s not realistic in the immediate future.</p> <p>Short term, I just need something cheap that won\u2019t die on me after a few months.</p> <p>Budget-wise, I\u2019m hoping to find something in the $100 to $150 range if possible.</p> <p>I keep seeing people here talk about shuckable drives (WD Easystore, Elements, Seagate Expansion, etc.), but I\u2019ve never done this before and don\u2019t know what models are good vs. SMR \u201cgotchas.\u201d</p> <p>So I\u2019d love your advice on:</p> <p>What\u2019s the cheapest, safest option right now for 10 to 12TB?</p> <p>Are there any current deals worth jumpin",
        "id": 3703555,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nur2lm/new_to_data_hoarding_need_7_to_10tb_cheap_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New to data hoarding, need ~7 to 10TB cheap storage fast (shuckable or otherwise?)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Final-Desk-5630",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T22:25:45.923783+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T21:04:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nup7nu/minisforum_n5_n5_pro_22tb_hdd_limit/\"> <img src=\"https://b.thumbs.redditmedia.com/xGOnL33QrGrnAY3IIzmkxJHZMjJba-4J96bWiqR5zrM.jpg\" alt=\"Minisforum N5, N5 Pro - 22TB HDD Limit\" title=\"Minisforum N5, N5 Pro - 22TB HDD Limit\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I was planning on buying the Minisforum N5 Pro to run as my Home Server + NAS.</p> <p>Was potentially getting a good deal on 26tb WD UltraStar HC590s but luckily stumbled upon the 22TB limit mentioned. Most other OTS NASes allow 30TBs now...</p> <p><a href=\"https://preview.redd.it/t9tulqkucdsf1.png?width=1930&amp;format=png&amp;auto=webp&amp;s=8dab3df4cbc70500f78954f71824d27fa3cb467d\">https://preview.redd.it/t9tulqkucdsf1.png?width=1930&amp;format=png&amp;auto=webp&amp;s=8dab3df4cbc70500f78954f71824d27fa3cb467d</a></p> <p>Thinking it&#39;s only at OS level or whatever max they used to test it, decided to reach out to their support to co",
        "id": 3703556,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nup7nu/minisforum_n5_n5_pro_22tb_hdd_limit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/xGOnL33QrGrnAY3IIzmkxJHZMjJba-4J96bWiqR5zrM.jpg",
        "title": "Minisforum N5, N5 Pro - 22TB HDD Limit",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SR_RSMITH",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T20:59:27.517335+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T20:11:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, I&#39;m hoping some of you experts can help me out here because I&#39;ve truly hit a wall. I&#39;m trying to download a 10-minute embedded video for some &quot;personal analysis&quot;, but the protection on it is unlike anything I&#39;ve dealt with before. I&#39;m no expert but here&#39;s an analysis: the video is an HLS stream, served from an iframe on a domain like `lauchacohete.top`.</p> <p>My first thought was the usual browser extensions, of course. Video DownloadHelper detects the stream and all the different resolutions perfectly, but when I try to download, it just gives the generic &quot;Sadly we failed...&quot; error. The CoApp is installed and running fine. I also tried grabbing the `.m3u8` URL and feeding it directly to VLC, but that just gives a network error, so I figured the server is blocking non-browser requests.</p> <p>So I moved on to dedicated apps. I first tried Open Video Downloader, and after sorting out a missing Pytho",
        "id": 3702934,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nunt4i/struggling_to_download_a_heavily_protected_hls",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Struggling to download a heavily protected HLS stream. Tried Downie, VDH, yt-dlp with Referer, but it still fails",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jaydarealone",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T20:59:25.936643+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:52:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nunbeu/looking_for_techtvg4_anime_unleashed_recordings/\"> <img src=\"https://b.thumbs.redditmedia.com/IvV_pL36MSilgckMc2NMQFFcL7V765M4rtOX0fsYjkM.jpg\" alt=\"Looking for TechTV/G4 Anime Unleashed Recordings 2002-2006,\" title=\"Looking for TechTV/G4 Anime Unleashed Recordings 2002-2006,\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey so I&#39;ve been looking for these for years, and have actually been able to find the complete anime of Dual parallel trouble, 11 episodes of silent mobius &amp; 17 episodes of last exile, 1 episode of serial experiments lain, + the movie Appleseed which I recorded myself but I would love to find more &amp; if anyone sees this and has tapes I would be willing to pay up to 40$ for a tape with anything from the anime unleashed block, hell if you have any Lain or any gun grave episodes I would pay more than that amount, Armitage III is another one I&#39;ve been searching for TechTV ",
        "id": 3702931,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nunbeu/looking_for_techtvg4_anime_unleashed_recordings",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/IvV_pL36MSilgckMc2NMQFFcL7V765M4rtOX0fsYjkM.jpg",
        "title": "Looking for TechTV/G4 Anime Unleashed Recordings 2002-2006,",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cohacq",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T20:59:26.287574+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:43:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve found a CD image of one of my favorite games from the 90&#39;s, but i no longer have a program to mount it. Back in day I used Daemon Tools, but im checking if a better program has come along in the almost 20 years since I used that.</p> <p>It&#39;s a .IMG, a .SUB and a .CCD file, so Windows cant natively mount it like an ISO. Right clicking and selecting Mount on the .IMG file gives the error message that the file is damaged.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cohacq\"> /u/Cohacq </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nun2m8/in_2025_what_programs_are_used_to_mount_old_cd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nun2m8/in_2025_what_programs_are_used_to_mount_old_cd/\">[comments]</a></span>",
        "id": 3702932,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nun2m8/in_2025_what_programs_are_used_to_mount_old_cd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "In 2025, what programs are used to mount old cd images from the 90's?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/1e6",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T20:59:27.919534+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:40:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a program that catalogs files on various internal and external disks. I&#39;ve had the &quot;problem&quot; of copying stuff to a newer, larger disk, and never going back to clean up the smaller disk. That and disorganization and procrastination. The end goal is getting &quot;everything&quot; in one place and deduplication, and maybe even some organization.</p> <p>I haven&#39;t worked through the logic, but maybe if the file size matches, and the name is &quot;close&quot;, do some sort of CRC/hashing/fingerprinting, and record that.</p> <p>I wouldn&#39;t mind writing a program that does this, but it is likely there is something that already exists and is debugged.</p> <p>This would probably run on my Ubuntu server, as it has the best access to various file systems. I&#39;m reading through the results for searching &quot;linux deduplication&quot;, but what do people use?</p> <p>Update: I need to watch using quotes for &quot;emphasis&",
        "id": 3702935,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nun0gx/making_a_db_of_files_for_deduplication",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Making a db of files, for de-duplication",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/runes911",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T20:59:28.227649+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:40:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hopefully this would be a good place to ask about this. I figured some of you may have run into this problem. </p> <p>I bought a Supermicro 2U 6028R-E1CR24N on eBay. I currently have a m.2 pci adapter, from Amazon, that I am trying to boot from. It shows up fine as a drive, because I installed Proxmox on it, however I cannot boot from it. I have researched all the bios settings that I might need to change and have adjusted many of them with no success. It seems at one point it acts like it is going to boot, I get the ready to boot message that Proxmox does, but it doesn\u2019t boot. </p> <p>Any ideas what I may be missing?</p> <p>The adapter: <a href=\"https://a.co/d/1KNqxxy\">https://a.co/d/1KNqxxy</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/runes911\"> /u/runes911 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1numzno/booting_from_pci_m2_drive_adapter/\">[link]</a></span> &#32; <span><a ",
        "id": 3702936,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1numzno/booting_from_pci_m2_drive_adapter",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Booting from PCI m.2 drive adapter",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tyranicalspud",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T20:59:26.889949+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:28:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does this drive have a standby mode?</p> <p>I cannot find this information on the datasheet it just mentions idle power. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tyranicalspud\"> /u/tyranicalspud </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1numo4l/help_with_western_digital_ultrastar_dc_hc530/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1numo4l/help_with_western_digital_ultrastar_dc_hc530/\">[comments]</a></span>",
        "id": 3702933,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1numo4l/help_with_western_digital_ultrastar_dc_hc530",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with Western Digital Ultrastar DC HC530",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Icediamondshark",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.229730+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:17:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So like a year back I&#39;ve bought one of these dell 3.5 inch 3tb sas hdds cause it seemed like really good value per gigabyte. However I wasn&#39;t able to put it in my pc and at the time I didn&#39;t have enough money for a bus card so I just forgot about it. Now I&#39;m wondering what to do with it? Sell it? If yes, then for how much? Is it even worth getting a bus card when they cost so much just for a single drive? Or perhaps is there a cheap way for me to connect it to my pc?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Icediamondshark\"> /u/Icediamondshark </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nume00/bought_a_sas_drive_on_accident_what_step_to_take/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nume00/bought_a_sas_drive_on_accident_what_step_to_take/\">[comments]</a></span>",
        "id": 3702219,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nume00/bought_a_sas_drive_on_accident_what_step_to_take",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bought a SAS drive on accident, what step to take next?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Moonwolf_222",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.368288+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T19:10:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What&#39;s the best way you&#39;ve found to backup and sync data across different storage formats (i.e. cloud, hard drive) since it&#39;s recommended to keep 3 backups of different storage types/locations? How do you keep track of what you&#39;ve backed up already?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Moonwolf_222\"> /u/Moonwolf_222 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1num78l/best_way_to_sync_data_across_multiple_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1num78l/best_way_to_sync_data_across_multiple_storage/\">[comments]</a></span>",
        "id": 3702220,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1num78l/best_way_to_sync_data_across_multiple_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to sync data across multiple storage formats?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AsterionVT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.556821+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T18:23:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nukypl/i_just_saved_myself_1200_euros_in_data_recovery/\"> <img src=\"https://b.thumbs.redditmedia.com/YU7Yq9uN858xZYanTzoY1fbJOkmsXIaqBwpRhdHGR1g.jpg\" alt=\"I just saved myself 1200 euros in data recovery (DIY)\" title=\"I just saved myself 1200 euros in data recovery (DIY)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AsterionVT\"> /u/AsterionVT </a> <br/> <span><a href=\"/r/datarecovery/comments/1nukpv7/i_just_saved_myself_1200_euros_in_data_recovery/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nukypl/i_just_saved_myself_1200_euros_in_data_recovery/\">[comments]</a></span> </td></tr></table>",
        "id": 3702221,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nukypl/i_just_saved_myself_1200_euros_in_data_recovery",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/YU7Yq9uN858xZYanTzoY1fbJOkmsXIaqBwpRhdHGR1g.jpg",
        "title": "I just saved myself 1200 euros in data recovery (DIY)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/scene_missing",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.050202+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T18:09:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good afternoon fellow hoarders. I feel like I&#39;m picking a bad time to expand from my PC Plex solution to a dedicated NAS device due to vile hard drive prices. I keep looking around to make sure I&#39;m not screwing something up. I want 2 larger SATA drives to go with my existing 2x8TB into a 4 bay NAS that&#39;ll run Unraid. </p> <p>I hate the idea of paying $175 US for 2 used 14TB drives, but every other option seems worse somehow? If I&#39;m buying in the next month, is this the play?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/scene_missing\"> /u/scene_missing </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nukktm/are_used_hc_530_drives_really_the_best_deal_going/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nukktm/are_used_hc_530_drives_really_the_best_deal_going/\">[comments]</a></span>",
        "id": 3702218,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nukktm/are_used_hc_530_drives_really_the_best_deal_going",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are used HC 530 drives really the best deal going currently?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/didyousayboop",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.673264+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T17:59:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nukage/a_vault_in_svalbards_arctic_frost_wants_to/\"> <img src=\"https://external-preview.redd.it/fSXmxN9NxtgUgHsSzCkZ9Q2YwAPjGmaxxloR7oR9G4s.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad0ca4d7afd8f4b243d86c9d5782d6c8402a2249\" alt=\"&quot;A vault in Svalbard's Arctic frost wants to protect your data&quot; (video about the Arctic World Archive)\" title=\"&quot;A vault in Svalbard's Arctic frost wants to protect your data&quot; (video about the Arctic World Archive)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/didyousayboop\"> /u/didyousayboop </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=5fwyGbnD9LM\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nukage/a_vault_in_svalbards_arctic_frost_wants_to/\">[comments]</a></span> </td></tr></table>",
        "id": 3702222,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nukage/a_vault_in_svalbards_arctic_frost_wants_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/fSXmxN9NxtgUgHsSzCkZ9Q2YwAPjGmaxxloR7oR9G4s.jpeg?width=320&crop=smart&auto=webp&s=ad0ca4d7afd8f4b243d86c9d5782d6c8402a2249",
        "title": "\"A vault in Svalbard's Arctic frost wants to protect your data\" (video about the Arctic World Archive)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Veronw_DS",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T17:52:18.772882+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T17:30:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there! So Robert&#39;s decided to stop creating new videos on his science channel but is keeping the content up. With the internet going the way that it is, I thought it would be a good idea to archive his accessible content. He&#39;s really fantastic and my students love his stuff!</p> <p>Buuuut it turns out I really underestimated just how much he&#39;d put out there over the years! While I sort through that side of things, I thought it would be good to let folks know about the channel here in case anyone else wanted to also archive his exceptional work: <a href=\"https://www.youtube.com/@ThinkingandTinkering/playlists\">https://www.youtube.com/@ThinkingandTinkering/playlists</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Veronw_DS\"> /u/Veronw_DS </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nujhxk/archiving_robert_murraysmiths_youtube_channel/\">[link]</a></span> &#32; <span><",
        "id": 3701087,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nujhxk/archiving_robert_murraysmiths_youtube_channel",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Archiving Robert Murray-Smith's Youtube Channel",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Atronem",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:27.197970+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T17:27:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We seek an operator to download metadata (titles) and cover images for ~1,000,000 books from a website (it&#39;s an online library).<br/> For each recorded title, retrieve the corresponding PDF when available from the Wayback Machine.<br/> Estimated raw storage requirement: ~20 TB; required disk capacity will be supplied.</p> <p>The project is dedicated solely to the preservation of knowledge and carries no commercial intent.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Atronem\"> /u/Atronem </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nujfrc/download_1_million_pdfs_from_way_back_machine/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nujfrc/download_1_million_pdfs_from_way_back_machine/\">[comments]</a></span>",
        "id": 3702225,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nujfrc/download_1_million_pdfs_from_way_back_machine",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Download 1 million PDFs from Way Back Machine",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FajnBrambor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.850698+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T17:18:20+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FajnBrambor\"> /u/FajnBrambor </a> <br/> <span><a href=\"/r/ffmpeg/comments/1nugc4z/is_amds_sam_even_worth_enabling/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuj6ql/is_amds_sam_even_worth_enabling_for_ffmpeg/\">[comments]</a></span>",
        "id": 3702223,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuj6ql/is_amds_sam_even_worth_enabling_for_ffmpeg",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is AMD's SAM even worth enabling for FFMPEG",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CountryRaptor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T17:52:18.931963+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T17:09:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all, l&#39;ve recently been looking for a way to download through csv format and analyse all my data from TikTok (comments/ likes / new followers etc) as the analytics on the app are quite limiting wanted to be able to extract that data and create more interactive and visually appealing data sheets to see what I would need to optimise (think I could do this from some AI and quick vibe coding so that\u2019s not a problem). Especially understanding ratios and other relationships I may miss in the data. Any advice would be appreciated or some websites I could access for free :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CountryRaptor\"> /u/CountryRaptor </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuiydy/tiktokinstagram_statistic_downloader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuiydy/tiktokinstagram_statistic_downloader/\">[comments]<",
        "id": 3701088,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuiydy/tiktokinstagram_statistic_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tiktok/instagram statistic downloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Real_Cap_4982",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T19:27:26.966552+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T17:06:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all, I\u2019ve recently been looking for a way to download and analyse all my data from TikTok (comments/ likes / new followers etc) as the analytics on the app are quite limiting wanted to be able to extract that data and create more interactive and visually appealing data sheets to see what I would need to optimise. Especially understanding ratios and other relationships I may miss in the data. </p> <p>Any advice would be appreciated or some websites I could access for free :) </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Real_Cap_4982\"> /u/Real_Cap_4982 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuiv0u/tiktokinstagram_statistic_downloader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuiv0u/tiktokinstagram_statistic_downloader/\">[comments]</a></span>",
        "id": 3702224,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuiv0u/tiktokinstagram_statistic_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tiktok/instagram statistic downloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abbrechen93",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T17:52:19.175654+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T16:21:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I would like to use around 100GB of my storage to preserve data that are almost lost, not existing as a complete set, or similar. But I have no idea what could be fitting into my requirements, because all ideas I had take too much space (e.g. all films of the 19th Century), are well preserved by big websites (e.g. almost everything that has to do with copyright-free media), or are more or less nonsense (e.g. a collection of dog poo images).</p> <p>What do you think would be a good use case?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abbrechen93\"> /u/abbrechen93 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuhnzt/ideas_for_tiny_data_preservation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuhnzt/ideas_for_tiny_data_preservation/\">[comments]</a></span>",
        "id": 3701089,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuhnzt/ideas_for_tiny_data_preservation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ideas for tiny data preservation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/03stevensmi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T17:52:18.426528+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T16:04:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a list of 5000 animated movies from wco that I would like to search through via a phrase or spoken word. I have a Samsung galexy A9 Tab, a raspberry pi 5 and a lenovo legion 5 AMD ryzen 4000 5 cpu with a nvidia gtx 1650Ti gpu running linux mint!</p> <p>Would it be possible to do this locally using the fastest (not insanely shit model) for free using one of those devices (if possible, the raspberry pi 5). I&#39;m looking for somthing not major like whisper-large-v3... just somthing fast enough for results simular to youtube&#39;s automatic subtitles. If there is somthing open source that does an OK job, could someone help by providing a link? If that can run fine on the rpi5... how long would you say it would take to go through 5000 animated movies and transcribe them all? I&#39;m aiming for around 1 week. Any help would be massively appreciated! Thanks guys!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/",
        "id": 3701086,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuh8hg/i_need_to_transcribe_5000_movies_to_txt_is_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need to transcribe 5000 movies to txt. Is it possible?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DeckardTBechard",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T17:52:18.271339+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T15:49:44+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nugu52/trove_of_manuals/\"> <img src=\"https://preview.redd.it/zmyzcr5ppbsf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9de4a6fe18d6e1b18f8a4f270f93e07bf9c234d\" alt=\"Trove of manuals?\" title=\"Trove of manuals?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Anyone in the Evansville area interested in what seems to be a large collection of free electronics service manuals? Spotted on Facebook with some other equipment, (cameras, vcrs, transistors...) but the manuals were what really caught my eye. It&#39;s too far from me and I don&#39;t have the equipment to archive them properly, but I figured it wouldn&#39;t hurt to make a shout out into the ether in case archiving this sort of thing was someone&#39;s passion. Seems they&#39;re only available Saturday. I can send a link if interested. There are more pictures in the listing. I just don&#39;t want to be posting someone&#39;s stuff onto reddit like that",
        "id": 3701085,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nugu52/trove_of_manuals",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/zmyzcr5ppbsf1.jpeg?width=320&crop=smart&auto=webp&s=d9de4a6fe18d6e1b18f8a4f270f93e07bf9c234d",
        "title": "Trove of manuals?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/originalQazwsx",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T15:45:14.331613+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T14:49:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was about to start building a dedicated desktop for hoarding/Plex and wanted to repurpose as much old items I had laying around. What I was considering:</p> <ul> <li>Case: Obsidian Series 750D (<a href=\"https://www.corsair.com/us/en/p/pc-cases/cc-9011035-ww/obsidian-series-750d-full-tower-atx-case-cc-9011035-ww\">https://www.corsair.com/us/en/p/pc-cases/cc-9011035-ww/obsidian-series-750d-full-tower-atx-case-cc-9011035-ww</a>)</li> <li>Motherboard: ASRock H110 Pro BTC+ (<a href=\"https://www.asrock.com/mb/intel/h110%20pro%20btc+/\">https://www.asrock.com/mb/intel/h110%20pro%20btc+/</a>)</li> <li>CPU: Intel Celeron G3930 (<a href=\"https://www.intel.com/content/www/us/en/products/sku/97452/intel-celeron-processor-g3930-2m-cache-2-90-ghz/specifications.html\">https://www.intel.com/content/www/us/en/products/sku/97452/intel-celeron-processor-g3930-2m-cache-2-90-ghz/specifications.html</a>)</li> <li>RAM: 8GB (1x8GB) DDR4</li> <li>GPU: OEM RTX 3070 (if necessa",
        "id": 3700051,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuf8ph/using_an_old_gpu_mining_mobo",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using an old GPU mining Mobo?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VideoGame_Trtle",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:43.481046+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T13:49:05+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nudpr7/what_can_i_do_about_this/\"> <img src=\"https://preview.redd.it/au6m1za64bsf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=24bdb804b7b9964a19ecd03a82f3bb4d40ffe40e\" alt=\"What can I do about this?\" title=\"What can I do about this?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I want to fix this so I have more storage on my iPhone. Is there any app or something that I can transport large videos to or a place to dump all my photos?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VideoGame_Trtle\"> /u/VideoGame_Trtle </a> <br/> <span><a href=\"https://i.redd.it/au6m1za64bsf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nudpr7/what_can_i_do_about_this/\">[comments]</a></span> </td></tr></table>",
        "id": 3698850,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nudpr7/what_can_i_do_about_this",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/au6m1za64bsf1.jpeg?width=640&crop=smart&auto=webp&s=24bdb804b7b9964a19ecd03a82f3bb4d40ffe40e",
        "title": "What can I do about this?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Undecided79",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:42.803163+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T13:20:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I found this encyclopedia very useful and well written. Apart for buying the book series as a whole (that is the best way to archive imo but its hard because of house space issues), can someone buy officially their encyclopedia as a file like pdf for example? I am not interested in subscriptions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Undecided79\"> /u/Undecided79 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nud0qs/encyclopedia_brittanica/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nud0qs/encyclopedia_brittanica/\">[comments]</a></span>",
        "id": 3698845,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nud0qs/encyclopedia_brittanica",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Encyclopedia Brittanica",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cowmootoe",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:42.919538+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T12:10:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi so I bought a WB passport 4 terrabyte and I didnt realise that I should of changed its file system before downloading 3 terrabytes worth of movies ane tv shows so im looking to change the system but obviously I need to move the data or it will get wiped so Im considering compressing it all but ive been searching and I see different things about how it can compromise quality (does this mean potentially when I uncompress the file it could be shit?) And that compressing media usually doesnt compress it by much cause its already pretty compressed (essentially a waste of time) my computer can hold about 900gb of it and my flatmate has a computer with a terrabyte so rough plan is to compress and move it onto them temporarily and then change the file system.</p> <p>Thank you in advance for any tips or answers</p> <p>And if you have any other plans (on the chance that my plan is just stupid and hopeless)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a",
        "id": 3698846,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nubg3f/compressing_media",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Compressing media",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JuniferBerries",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:42.999764+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T11:20:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there,</p> <p>I&#39;ve been lurking here for a few years and have a question for you all. After an unexpected HDD failure where I lost some data, I&#39;m really wanting to set things up right to have clones of important drives. I have two things setup right now:</p> <p><strong>1: PC:</strong></p> <p>4x 4TB NVME Drives</p> <p>1x 24TB SATA HDD</p> <p><strong>2. NAS*:</strong></p> <p>3x 16 TB Drives</p> <p>1x 24 TB Drive</p> <p>* Currently set in single drive configuration for each drive</p> <p><strong>What I want to do:</strong></p> <p>= Drives 1+2 of NAS (2x16TB) be direct clones of each other. Meaning if I add, delete, or edit a file on one, it adds/deletes/edits on both.</p> <p>= Drive 3 of NAS to be backups of the 4 NVME Drives in the PC. Although, for 2 of the 4 drives, I&#39;d ideally like to backup only specific folders and not the full drive, making it 2 full backups and 2 partial backups.</p> <p>= Drive 4 of the NAS (24 TB) to be a direct cl",
        "id": 3698847,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuagx5/alwaysactive_directcloning_of_disks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Always-active direct-cloning of disks?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Apricot7902",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:42.584379+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T11:17:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuaf4o/software_for_external_seagate_drive/\"> <img src=\"https://a.thumbs.redditmedia.com/ev4Sqw9cMb8nuMdC19frd5QRD9seacnZqlwo-pi-KE4.jpg\" alt=\"Software for external Seagate drive\" title=\"Software for external Seagate drive\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Found this FireWire drive that will compliment my digital video capture setup nicely. I can hack together a power cable, but I can&#39;t find the Disc Utility for external drives (probably), on the Internet archive there only are their utilities for internal drives.</p> <p>Does anyone have the CD and could share an image? Thx</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Apricot7902\"> /u/Ok_Apricot7902 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1nuaf4o\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuaf4o/software_for_external_seagate_drive/\">[co",
        "id": 3698844,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuaf4o/software_for_external_seagate_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/ev4Sqw9cMb8nuMdC19frd5QRD9seacnZqlwo-pi-KE4.jpg",
        "title": "Software for external Seagate drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DandyLion23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:43.080490+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T11:14:33+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DandyLion23\"> /u/DandyLion23 </a> <br/> <span><a href=\"https://ivo.palli.nl/2025/09/30/re-encoding-movies-in-powershell-with-ffmpeg/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nuad87/reencoding_movies_in_powershell_with_ffmpeg_a/\">[comments]</a></span>",
        "id": 3698848,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nuad87/reencoding_movies_in_powershell_with_ffmpeg_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Re-encoding movies in Powershell with ffmpeg; a script",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FaceGreat2625",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T14:18:43.160302+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T08:30:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As the title suggests, I am planning on creating time capsules to preserve human knowledge by placing them at the base of Olympus Mons and Valles Marineris. I made a Google sheet with 50 important human works in books, music and movies. Feel free to add more! The limit is 1 million pages. I plan on using nanofiche microfilm for storage as electronic is unreliable for space. Please do not add personal favorites that are not beneficial for the future of humanity, or add memes and jokes. This is my first Reddit post so i hope this is good enough! I&#39;m 14 so I&#39;m probably not that equipped to curate the content on the capsule, so help would be wonderful</p> <p><a href=\"https://docs.google.com/spreadsheets/d/1p8V4oAYCo_MhD8tCxGMVZxSu6khZCAsHzKBIk7fq99o/edit?usp=drivesdk\">Google sheets link</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FaceGreat2625\"> /u/FaceGreat2625 </a> <br/> <span><a href=\"https://www.r",
        "id": 3698849,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nu7qko/human_data_preservation_on_mars",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Human Data Preservation on Mars",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Toczke",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T09:15:20.045488+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T08:13:29+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Toczke\"> /u/Toczke </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1nu7ckq\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nu7hd3/finished_my_new_homelab_cleanest_job_in_history/\">[comments]</a></span>",
        "id": 3696373,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nu7hd3/finished_my_new_homelab_cleanest_job_in_history",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Finished my new Homelab - cleanest job in history",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/14132",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T09:15:20.350190+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T08:09:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I&#39;m a member of a small and dedicated community who loves an obscure pet site/avatar dress up site, similar to Neopets or Flight Rising. The website is shutting down on October 20th and if at all possible the fans would like to save as much of it as we can.</p> <p>I&#39;ve been looking into the logistics of using HTTracker, as well as done research on this subreddit and the wiki, and while it seems like HTTracker would work well for the text and image heavy parts of the website, I don&#39;t think it would work well for downloading the various clothing items or avatar system, or really any kind of dynamic content that might call from a database and isn&#39;t a static page. A few of the fans have been manually saving files and data so that we might be able to recreate the avatar system by hand, but it&#39;s slow going. And we aren&#39;t sure of solutions for backing up the games or other interactive elements, but on the bright side one of the",
        "id": 3696374,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nu7ezw/backing_up_imageheavy_avatar_site",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backing up image-heavy avatar site",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kedlub",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T07:48:49.859763+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T07:26:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello!<br/> I am searching for quiet drives into my DS923+ NAS. A few months ago I grabbed the WD Red Plus 12TB (WD120EF<strong>G</strong>X), but I didn&#39;t notice that they released a new model, and that this model is now air filled. And it&#39;s basically the loudest drive I ever had, both in idle and in seek. I tried applying the Velcro trick to the disk trays, but it didn&#39;t do much of a difference for these drives.<br/> Noise is a huge concern, as the NAS is in the room I sleep in. There is sadly no other place in home where it could be placed.<br/> I tried twice buying only by reviews and data sheets, the first time it was 16TB Synology drives, but they were extremely loud and also came defective, and now these WD Red Plus where I was really unlucky as it looked like the hellium filled got discontinued basically the month I was getting them, so the new ones didn&#39;t have any reviews yet. So I don&#39;t want to make that mistake again.</p>",
        "id": 3695978,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nu6r5p/searching_quiet_812tb_drives_with_the_best",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Searching quiet 8-12TB drives with the best noise/capacity ratio",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thegameksk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T04:56:46.820774+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T03:54:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Western Digital Ultrastar HC570 WUH722222ALE604 0F48152 22TB 7.2K RPM SATA 6Gb/s 512e Power Disable 3.5in Recertified Hard Drive. Its 13.40 per TB on serverpartsdeals recertified. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thegameksk\"> /u/thegameksk </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nu362e/would_this_ultrastar_be_good_for_a_nas_that_will/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nu362e/would_this_ultrastar_be_good_for_a_nas_that_will/\">[comments]</a></span>",
        "id": 3695308,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nu362e/would_this_ultrastar_be_good_for_a_nas_that_will",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Would this Ultrastar be good for a NAS that will be used for media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dti85",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T03:35:07.161700+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T02:06:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Currently, I store everything using mirrored ZFS backed up with Restic. It protects me from bit rot, a local disaster, and a cloud disaster.</p> <p>I realized I&#39;m spending too much time maintaining my own computers, so I&#39;d like to switch to storing everything in the cloud. Fortunately, my internet connection is gigabit fiber, so transfer speeds aren&#39;t an issue, but latency might be.</p> <p>What are some options for storing data? I need to</p> <ul> <li>access random files occasionally</li> <li>encrypt data prior to transit</li> <li>have cheap file moves/renames</li> <li>be able to do integrity checks</li> <li>recover from file management mishaps</li> </ul> <p>I don&#39;t trust a single cloud because accounts get deleted, and I have to keep the two synced.</p> <p>I briefly considered still using local zfs and a cloud-backed virtual block device, but the latency is probably too much. My current idea is using an rclone mount and continuing to ",
        "id": 3695001,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nu0ylm/fsbackup_options_for_storing_with_cloud_providers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "FS/Backup options for storing with cloud providers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Glum_Award9379",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-30T02:04:30.814476+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-30T01:11:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>MEGA window client for some reason (no exclusions setup) refuses to fully sync a massive backup folder. Resulting in skipping/missing a few folders and dozens of files. </p> <p>Other provider&#39;s windows client does not have this issue...</p> <p>Their Ubuntu client though does not have this issue and properly syncs the entire massive backup folder. </p> <p>Their useless bot support isn&#39;t going anywhere for months. </p> <p>Does anyone know why this is the case? Why is Ubuntu working as expected but the windows client skips folders/files? Any way to check what folders and files specifically the windows client is skipping?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Glum_Award9379\"> /u/Glum_Award9379 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ntzsh6/mega_windows_client_skipping_foldersfiles_but_not/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/c",
        "id": 3694737,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ntzsh6/mega_windows_client_skipping_foldersfiles_but_not",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "MEGA windows client skipping folders/files but not Ubuntu client",
        "vote": 0
    }
]