[
    {
        "age": null,
        "album": "",
        "author": "/u/Gordian_Smegma",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T22:54:37.318864+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T22:30:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I have alot of music stored on my old Ex-Hard Drive, some of it is FLAC, but the bulk is m4a format, and because m4a is not lossless, I wanted to port it over to an online cloud storage like Mega or Drive. Now with Drive I know I&#39;ve tried every which way to cut/paste all my albums over to no avail, and I was wondering if there was a cloud storage service out there that maybe does allow full transfers of audio files instead of just copies of them. And if it isn&#39;t possible, then oh well I guess, but still any answer is a big help for me.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Gordian_Smegma\"> /u/Gordian_Smegma </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfgyk9/moving_my_music_to_the_cloud_instead_of_copying/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfgyk9/moving_my_music_to_the_cloud_instead_of_copying/\">[comments]</a></s",
        "id": 3561618,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfgyk9/moving_my_music_to_the_cloud_instead_of_copying",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Moving my music to the cloud instead of copying?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/awolfwearingabanana",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T22:54:37.439470+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T22:01:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I have collected, catalogued and archived about 4TB&#39;s of data from a niche that I am a part of, and that collection is still growing. I was wondering what the options are for the best, and most cost effective way of sharing it with other people was? Because In my opinion It ain&#39;t an archive unless it&#39;s available to others.</p> <p>I want options other than the Internet Archive because I don&#39;t want to centralize my collection on one service (and I don&#39;t want to burden the Internet Archive with unnecessary data).</p> <p>I don&#39;t feel like spending a lot of money on a cloud service like mediafire or mega (they also don&#39;t keep files reliably for long term which is a priority of mine).</p> <p>I know of self hosted services like apache open directories or copyparty servers (I am familiar with self hosting but I haven&#39;t hosted a publicly accessible file server and would like some tips if that&#39;s the best route).</p> <p",
        "id": 3561619,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfg9pl/what_is_the_best_and_most_cost_effective_way_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the best, and most cost effective way to share a large amount of data (Terabytes worth) online?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PricePerGig",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:07.611053+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T21:20:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Last week I asked for your recommendations on the best and most trusted eBay sellers for hard drives to add on the <a href=\"Https://pricepergig.com\">price aggregation site pricepergig.com</a>. The response was fantastic, and I wanted to say a massive thank you to everyone who contributed!</p> <p>Well, I&#39;ve listened and I&#39;m excited to announce that a whole bunch of your recommendations have been added. This means every single listing from these community-vetted sellers is now indexed on the site.</p> <p>The goal is to help all of us snag those great deals\u2014especially on used or recertified drives, and with sellers that accept returns \u2014with a lot more peace of mind. You can now browse with confidence, knowing you&#39;re looking at inventory from sellers that others here trust.</p> <p>Here is the list of sellers that have been added based on your feedback: * goharddrive * serverpartdeals * stxrecerthdd * seagatestore * wd * dbskyusa88 * deals2day3",
        "id": 3561192,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nff983/i_added_your_favourite_hard_drive_ebay_sellers_so",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I added your favourite Hard Drive eBay sellers so you can get the best deals from the best sellers now",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rexyuan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:07.003953+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T21:02:08+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nferxo/i_finally_got_my_grail_intel_optane_p5800x_16t/\"> <img src=\"https://b.thumbs.redditmedia.com/vPleExVqxAQlWvL4kwgFy48c3Ftl5-UoZFuX6AR2DfQ.jpg\" alt=\"I finally got my grail. Intel Optane P5800X 1.6T. This is gonna be a family heirloom\" title=\"I finally got my grail. Intel Optane P5800X 1.6T. This is gonna be a family heirloom\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rexyuan\"> /u/rexyuan </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1nferxo\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nferxo/i_finally_got_my_grail_intel_optane_p5800x_16t/\">[comments]</a></span> </td></tr></table>",
        "id": 3561189,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nferxo/i_finally_got_my_grail_intel_optane_p5800x_16t",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/vPleExVqxAQlWvL4kwgFy48c3Ftl5-UoZFuX6AR2DfQ.jpg",
        "title": "I finally got my grail. Intel Optane P5800X 1.6T. This is gonna be a family heirloom",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Anonymoose8D",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:08.018774+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T20:59:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im still learning about hardware so any help is appreciated.</p> <p>I have been running a media server for a while now and im running into physical limitations for how many 3.5&quot; HDDs i can fit in my PC case and connect on my motherboard (Asus Prime Z690-A). </p> <p>Im not worried about running backups or setting up any raid atm. </p> <p>But I need help finding a good approach to connecting more 3.5&quot; HDDs for simply streaming through my media server. I understand my 4 sata connections are limited to 6Gb/s and I only have 1 more free. Is there a good enclosure/dock that i could connect through USB-A/C gen 3.2 to connect say 4 more HDDs? I&#39;ve read that USB gen 3.2 is capable of up to 10Gb/s regardless or type A or C connection and that ultimately they will be limited by the sata connections at 6 Gb/s</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Anonymoose8D\"> /u/Anonymoose8D </a> <",
        "id": 3561193,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfep4r/i_need_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Temporary_Potato_254",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:07.258012+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T20:51:45+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfei2l/another_bomberman_game_for_japanese_feature/\"> <img src=\"https://external-preview.redd.it/sxWW2dm4EeZCilAx6uN1t85FWjrQH56MkO2YWnDihs0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=128b7274413253dec31ab5778d50c8d6aeeb7c53\" alt=\"Another Bomberman Game For Japanese Feature Phones Has Been Preserved\" title=\"Another Bomberman Game For Japanese Feature Phones Has Been Preserved\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Temporary_Potato_254\"> /u/Temporary_Potato_254 </a> <br/> <span><a href=\"https://www.timeextension.com/news/2025/09/another-bomberman-game-for-japanese-feature-phones-has-been-preserved\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfei2l/another_bomberman_game_for_japanese_feature/\">[comments]</a></span> </td></tr></table>",
        "id": 3561191,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfei2l/another_bomberman_game_for_japanese_feature",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/sxWW2dm4EeZCilAx6uN1t85FWjrQH56MkO2YWnDihs0.jpeg?width=640&crop=smart&auto=webp&s=128b7274413253dec31ab5778d50c8d6aeeb7c53",
        "title": "Another Bomberman Game For Japanese Feature Phones Has Been Preserved",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HornyArepa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T20:36:08.024505+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T20:26:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone!<br/> I created a zim from <a href=\"https://archive.fart.website/archivebot/viewer/job/20240901213047bvqa8\">this Anandtech archive</a>. </p> <p>Link to zim: <a href=\"https://archive.org/details/anand-tech-2024-09\">https://archive.org/details/anand-tech-2024-09</a></p> <p>With this you can browse and search AnandTech (mostly) as it was. It doesn&#39;t include some things like the forum, other content not hosted directly on the site, or anything else the original crawl simply didn&#39;t capture.</p> <p>-<br/> It is viewable using Kiwix - you can download a viewer from <a href=\"https://kiwix.org/en/applications/\">here</a>.</p> <p>You can also donate to them <a href=\"https://kiwix.org/en/get-involved/#donate\">here :)</a></p> <p>-</p> <p>I created the zim file locally using kiwix&#39;s <a href=\"https://github.com/openzim/zimit\">zimit</a>. Zimit is usually used for scraping + zim creation, but it can be used to create the zim from existing warc ",
        "id": 3560794,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfduzy/anandtech_zim_file_available",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AnandTech zim file available",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MyRecklessHabit",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T20:36:07.530795+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T20:13:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Filled my Linux laptop. Mostly old clips, games, some service manuals. 100k songs. 100 1440 movies. </p> <p>Offload to T7 and start again. </p> <p>Very new to this (10mo). Went from windows to macOS to PopOS to Linux mint. Been a hell of a journey. Aged me 5 years. </p> <p>Paid for with crypto trades. Lost a few on the end, went flat and got a Mac mini 64gb OTW. </p> <p>Probably should have went with a framework with the AMD 395+ but we live. We learn. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MyRecklessHabit\"> /u/MyRecklessHabit </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfdjw9/first_4tb_full/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfdjw9/first_4tb_full/\">[comments]</a></span>",
        "id": 3560793,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfdjw9/first_4tb_full",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "First 4tb full.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ambitious_Shirt_598",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T20:36:08.151650+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T20:04:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there a way to pull all of an account\u2019s posts on X into a file? I\u2019m interested in pulling stats on what various accounts post about/word usage/etc. Does something like this exist? TIA!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ambitious_Shirt_598\"> /u/Ambitious_Shirt_598 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfdbig/downloading_x_posts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfdbig/downloading_x_posts/\">[comments]</a></span>",
        "id": 3560795,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfdbig/downloading_x_posts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading X Posts",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/That-Way-5714",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T19:24:06.644342+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T19:22:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I know this song definitely has a creepy stalker/serial killer vibe, but it also reminds me of this sub. </p> <p>&quot;Hoard<br/> Collect<br/> File<br/> Index<br/> Catalogue<br/> Preserve<br/> Amass<br/> Index&quot; </p> <p><a href=\"https://www.youtube.com/watch?v=-UoKIiw-p2g\">https://www.youtube.com/watch?v=-UoKIiw-p2g</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/That-Way-5714\"> /u/That-Way-5714 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfc9d7/steven_wilson_index/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfc9d7/steven_wilson_index/\">[comments]</a></span>",
        "id": 3560312,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfc9d7/steven_wilson_index",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Steven Wilson - Index",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RinShiroJP",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T19:24:06.879405+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T18:59:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently bought a LTO 5 External tape drive. Model is <strong>HP EH958B LTO5 Ultrium 3000</strong>. After looking online I&#39;ve found I need a SAS HBA card and SFF 8088 to SFF 8088 cable but I&#39;m confused on which ones to get. Could someone link me to some ebay postings or amazon links for these.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RinShiroJP\"> /u/RinShiroJP </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfboas/how_do_you_connect_a_lto5_external_drive_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfboas/how_do_you_connect_a_lto5_external_drive_to/\">[comments]</a></span>",
        "id": 3560313,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfboas/how_do_you_connect_a_lto5_external_drive_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you connect a LTO-5 external drive to desktop pc?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/plunki",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:08.694487+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T18:24:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://cazadora.substack.com/p/why-do-we-collect-things\">https://cazadora.substack.com/p/why-do-we-collect-things</a></p> <p>Why do we collect things?</p> <p>An interesting essay on collecting (hoarding!), with some history and notable hoarders.</p> <p>Sadly it sticks to physical hoarding, but thought it would still be of interest to folks here. There is data in the physical, and much of the physical can be (at least partially) digitized, so I&#39;m sure there is more data throughout the essay to be uncovered and hoarded. (Yes, I intend to hoard photos, etc of hoarding related things haha - check out those hand drawn butterfly wings!)</p> <p>Description via The Browser (<a href=\"https://thebrowser.com/):\">https://thebrowser.com/):</a></p> <p>Over 100,000 years ago in the Kalahari, people were collecting crystals. Today, people collect everything from labubus to jigsaw pieces. Artists are especially prone to the habit: Joan Didion collected s",
        "id": 3561195,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfasbk/why_do_we_collect_things_an_essay",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why do we collect things? (An Essay)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/alex11263jesus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T19:24:08.118150+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T18:15:24+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfajs8/the_datahoarder_effect_after_the_hype_has/\"> <img src=\"https://preview.redd.it/0uydu3yuyrof1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d28a179009141c871331b150c241cf8aa977a151\" alt=\"The &quot;Datahoarder Effect&quot; after the hype has newscycle passed? At least a somewhat net benefit.\" title=\"The &quot;Datahoarder Effect&quot; after the hype has newscycle passed? At least a somewhat net benefit.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I think I just thought it&#39;d hold longer</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/alex11263jesus\"> /u/alex11263jesus </a> <br/> <span><a href=\"https://i.redd.it/0uydu3yuyrof1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nfajs8/the_datahoarder_effect_after_the_hype_has/\">[comments]</a></span> </td></tr></table>",
        "id": 3560314,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nfajs8/the_datahoarder_effect_after_the_hype_has",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/0uydu3yuyrof1.png?width=320&crop=smart&auto=webp&s=d28a179009141c871331b150c241cf8aa977a151",
        "title": "The \"Datahoarder Effect\" after the hype has newscycle passed? At least a somewhat net benefit.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Wilson1218",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T18:07:32.600604+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T17:05:55+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Wilson1218\"> /u/Wilson1218 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1nf8ptp\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nf8s3g/identifying_drive_chassis/\">[comments]</a></span>",
        "id": 3559712,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf8s3g/identifying_drive_chassis",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Identifying drive chassis",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nski884",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:09.197084+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T16:46:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Help with using Goodsync software would be much appreciated. I have two disks and copied the contents from the primary disk (hdd1) to a fresh second one (hdd2). Mostly manually within the Finder app on Mac. I restructured the directories on the hdd2 and moved some of the files and folders into these newly structured folders. </p> <p>Now I want to compare the files so that contents are the same without including the folders. When I run Analyze in Goodsync with 2-way job (Sync mode + enabled &quot;Compare Checksum of All Files&quot;) it wants to make changes for most of the files and folders on the right side (hdd2), approx. 6k files. There are about 15k+ total files. </p> <p>Should I just format the hdd2 and copy all files onto it again, then compare checksums and at the very end restructure the directories again? Or is there another, more elegant way of doing this? </p> <p>Cheers, N</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://ww",
        "id": 3561197,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf8a24/backup_done_need_to_compare_contents_now_for_15k",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup done, need to compare contents now for 15k+ files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Temporary_Potato_254",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T16:42:52.000181+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T15:58:15+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nf70z1/dna_cassette_tape_can_store_every_song_ever/\"> <img src=\"https://external-preview.redd.it/YWT_hYweSH2Klq7Yk2znvhAzYgrR77EvGxSklVni26k.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6211d783936de5df07395cd3bb88519ac2938b7f\" alt=\"DNA cassette tape can store every song ever recorded\" title=\"DNA cassette tape can store every song ever recorded\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Temporary_Potato_254\"> /u/Temporary_Potato_254 </a> <br/> <span><a href=\"https://www.newscientist.com/article/2495758-dna-cassette-tape-can-store-every-song-ever-recorded/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nf70z1/dna_cassette_tape_can_store_every_song_ever/\">[comments]</a></span> </td></tr></table>",
        "id": 3559107,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf70z1/dna_cassette_tape_can_store_every_song_ever",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/YWT_hYweSH2Klq7Yk2znvhAzYgrR77EvGxSklVni26k.jpeg?width=640&crop=smart&auto=webp&s=6211d783936de5df07395cd3bb88519ac2938b7f",
        "title": "DNA cassette tape can store every song ever recorded",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Wishbone768",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:08.446165+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T15:34:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks,</p> <p>I\u2019m exploring digiKam as a DAM for a large photo team (~20 users) and wanted to see if anyone here has real-world experience at scale.</p> <p><strong>Our setup:</strong></p> <ul> <li>~2.5M images, grows by 150\u2013200K per year</li> <li>Central server with MariaDB + shared storage</li> <li>Team workflows: searching, tagging, labeling, renaming, editing (mostly in Adobe), ingest/export</li> </ul> <p><strong>Concern:</strong><br/> IT warned us digiKam isn\u2019t really built for true multi-user setups. Concurrent writes to the DB could risk corruption. Possible workaround: only one user writes at a time (maybe enforced via scripting).</p> <p><strong>Questions:</strong></p> <ol> <li>Has anyone successfully run digiKam with 2M+ images?</li> <li>Any examples of multi-user setups (or workarounds) that actually work?</li> <li>What hardware specs (server + workstations) would you recommend for this scale?</li> </ol> <p>Would love to hear from anyone ",
        "id": 3561194,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf6egz/anyone_running_digikam_at_2m_images_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone running digiKam at 2M+ images with multi-user access?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Blueowl1717",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T15:26:23.229222+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T14:08:54+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Blueowl1717\"> /u/Blueowl1717 </a> <br/> <span><a href=\"/r/learnpython/comments/1nf48c4/using_python_to_download_text_to_pdf/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nf48vi/using_python_to_download_text_to_pdf/\">[comments]</a></span>",
        "id": 3558348,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf48vi/using_python_to_download_text_to_pdf",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using python to download text to pdf",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Wrong_Swimming_9158",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T21:45:08.907361+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T12:04:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nf1d6s/paperion_a_selfhosted_academic_search_engine_to/\"> <img src=\"https://b.thumbs.redditmedia.com/FKZqoPdA3VhSXQ4-nszDXYgSaRKkYo8cy3JxaOf3qdk.jpg\" alt=\"Paperion : A self-hosted Academic Search Engine (to DWNLD all papers)\" title=\"Paperion : A self-hosted Academic Search Engine (to DWNLD all papers)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;m not in academia, but I use papers constantly especially thos related to AI/ML. I was shocked by the lack of tools in the academia world, especially those related to Papers search, annotation, reading ... etc. So I decided to create my own. It&#39;s self-hosted on Docker.</p> <p><strong>Paperion contains 80 million papers</strong> in Elastic Search. What&#39;s different about it, is I digested a big number of paper&#39;s content into the database, thus making the recommendation system the most accurate there is online. I also added a section for annotation,",
        "id": 3561196,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf1d6s/paperion_a_selfhosted_academic_search_engine_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/FKZqoPdA3VhSXQ4-nszDXYgSaRKkYo8cy3JxaOf3qdk.jpg",
        "title": "Paperion : A self-hosted Academic Search Engine (to DWNLD all papers)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Exodusllc",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T11:40:51.912654+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T11:15:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am fairly new to HDD harddrives so this might be an easy to solve question.</p> <p>As you all probably know everytime i transfer files the plugged in Seagate HDD kind of \u201cstarts up\u201d wich takes a few seconds for the disc spinning sound and then it starts writing the data. </p> <p>My situation is: i use lots of large CAD, mp4 etc files and i dont have enough space on my laptop so i sometimes use the Expansion plugged in for like an hour and edit the files directly there. So the Drive starts up, writes ( saves my data) and goes \u201coff\u201d ( no more sounds obviously) after almost 30 sec. Not in active use. This repeats until i finished the work like 100 times. Cant imagine this is good at all for the drive. I cant edit them in my laptop though because i work on multiple files at once and i dont have enough space.</p> <p>My Question: is it possible to keep the HDD running the whole time ready to instantly save/write without it shutting into the standby mode a",
        "id": 3556486,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nf0eo5/seagate_expansion_16_tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate expansion 16 TB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Emotional_Dust2807",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T10:30:00.313939+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T09:51:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1neyz1w/help_modify_this_code/\"> <img src=\"https://b.thumbs.redditmedia.com/8DN9mwfhQZPq037BDYbyBhd3ZQHZ8BMht_rlhTWW0es.jpg\" alt=\"Help modify this code\" title=\"Help modify this code\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Emotional_Dust2807\"> /u/Emotional_Dust2807 </a> <br/> <span><a href=\"/r/PythonLearning/comments/1neywj6/help_modify_this_code/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1neyz1w/help_modify_this_code/\">[comments]</a></span> </td></tr></table>",
        "id": 3555958,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1neyz1w/help_modify_this_code",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/8DN9mwfhQZPq037BDYbyBhd3ZQHZ8BMht_rlhTWW0es.jpg",
        "title": "Help modify this code",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Prudent_Impact7692",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T10:30:00.448934+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T09:50:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I started to download the entire of Anna\u2019s archive and as others have already pointed out there are files with the exact same content but sometimes not a matched MD5 summ. So as far as I know deduplication with ZFS is not possibile in this case. Files are only deduplicated if their MD5 hash matches. So, they would have to be exactly identical files to be deduplicated.</p> <p>Sometimes books don\u2019t have the identical MD5 but the content is the same although in a different format or just little bit different in file composition. So manually deceiding which books are duplicates would be a nightmare. </p> <p>Isn\u2019t there an AI App that can go through a bunch of files and register which one have the identical content (not based on MD5 but the content of the book itself) and then determine based on your setting which one to keep?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Prudent_Impact7692\"> /u/Prudent_Impact7692 <",
        "id": 3555959,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1neyyjg/using_ai_to_detect_and_remove_duplicate_ebooks_by",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using AI to Detect and Remove duplicate ebooks by their content?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vaesuis",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T09:17:21.008459+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T08:57:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, everyone! I hope this is the right place to ask this question.</p> <p>I have several TBs of books in various formats (PDF, ePub, DVJU, TXT) that I have accumulated over the years (and continue to accumulate), but while until a few years ago I had the time to sort them manually (by subject and author), lately the rate of accumulation is greater than what I would need to sort them manually as I have always done.</p> <p>So I wanted to ask if there is a tool that, for example, by mass-analyzing the files can create a library that can be sorted and filtered according to tags automatically assigned.</p> <p>From what I&#39;ve seen, paperless-ngx should be able to do this, but since it was created to digitalize physical documents I don&#39;t know if it&#39;s perfect for my needs, also because I&#39;d need very specific tags, basically one for each sub-sub-branch of human knowledge in sciences, arts, and humanities.</p> <p>Thanks in advance for your ans",
        "id": 3555552,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ney4d4/seeking_advice_on_books_sorting_and_management",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seeking advice on books sorting and management",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/roller182",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T06:36:28.785302+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T06:35:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The show from 1990 called \u2018Dream On\u2019 is streaming on TheRokuChannel. It seems to be the uncut versions of the episodes, whereas the majority of the episodes available online are from re-runs, which weren\u2019t complete, and also in bad quality. I understand it is protected by DRM, just wondered if anyone was capable of ripping them? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/roller182\"> /u/roller182 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nevz9o/is_it_possible_to_download_from_therokuchannel/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nevz9o/is_it_possible_to_download_from_therokuchannel/\">[comments]</a></span>",
        "id": 3554708,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nevz9o/is_it_possible_to_download_from_therokuchannel",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it possible to download from TheRokuChannel?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lohre2000s",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T05:18:56.551563+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T04:41:58+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1neu47v/how_do_you_guys_organize_your_games_looking_for/\"> <img src=\"https://preview.redd.it/u1lk31c4ynof1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1455acb108a88e6bf634c382a482a285420e445\" alt=\"How do you guys organize your games? Looking for advice on my current method (LINUX USER)\" title=\"How do you guys organize your games? Looking for advice on my current method (LINUX USER)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi. After switching to Linux I got into the habit of storing and organizing a proper game library, buying more stuff on GOG (rather than Steam) and always trying to keep ALMOST &quot;ready to play&quot; for whenever I need them.</p> <p><strong>Right now here&#39;s my system:</strong></p> <p><strong>For steam games</strong>, they are located on the <strong>SteamLibrary</strong> folder, as it seems to be impossible to change that. Not much to do here besides some <strong>simple no",
        "id": 3554407,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1neu47v/how_do_you_guys_organize_your_games_looking_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/u1lk31c4ynof1.png?width=640&crop=smart&auto=webp&s=b1455acb108a88e6bf634c382a482a285420e445",
        "title": "How do you guys organize your games? Looking for advice on my current method (LINUX USER)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/val_in_tech",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T04:13:29.732019+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T03:05:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.ebay.ca/itm/205667292840\">https://www.ebay.ca/itm/205667292840</a></p> <p>What do you guys think? 400$ US 3y warranty. Free delivery, same price as eBay refubrished from US 2y warranty minus deliver cost.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/val_in_tech\"> /u/val_in_tech </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nesbue/exos_28tb_from_china/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nesbue/exos_28tb_from_china/\">[comments]</a></span>",
        "id": 3554103,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nesbue/exos_28tb_from_china",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Exos 28TB from China?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/toraleii",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T03:03:46.584386+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T02:04:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ner3ss/digitizing_family_albums_should_i_upgrade/\"> <img src=\"https://preview.redd.it/fcihg9py5nof1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b79e6bf652eed82ea94dfda8cc52cab8004c367\" alt=\"Digitizing family albums, should I upgrade equipment?\" title=\"Digitizing family albums, should I upgrade equipment?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I\u2019m currently taking on the task of backing up all of my family albums, from my mother and grandmother\u2019s collections. There are probably 15-20k photos total. </p> <p>My process so far for this has been a set up with a ring light, a tripod and a Canon EOS Rebel SL2. I\u2019m not so concerned with getting the photos cropped, just having some type of digital archive of them. If I want to print them later, cropping is a problem for future me. I also don\u2019t mind the time it takes, it\u2019s nice to review the photos one by one and revisit memories.</p> <p>I attached one",
        "id": 3553865,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ner3ss/digitizing_family_albums_should_i_upgrade",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/fcihg9py5nof1.jpeg?width=640&crop=smart&auto=webp&s=8b79e6bf652eed82ea94dfda8cc52cab8004c367",
        "title": "Digitizing family albums, should I upgrade equipment?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fabiorzfreitas",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T03:03:46.907014+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T02:01:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m part of a family plan for 2TB on Google, but I&#39;ll temporarily need twice as much: for compatibility reasons, I&#39;ll have to wipe a 4TB HDD to format it to exFAT, but I don&#39;t have sufficient local storage.</p> <p>From what I got in older posts, it seems Google doesn&#39;t really enforce their storage quota. Is that still true? And does it mean I can get away with uploading 4TB as a temporary backup?</p> <p>I know there are far better and more reliable options, but I really need to avoid spending any money (currency exchange rates means everything is expensive).</p> <p>Thanks in advance for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fabiorzfreitas\"> /u/fabiorzfreitas </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ner1rz/is_google_still_not_enforcing_their_storage_quotas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/",
        "id": 3553866,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ner1rz/is_google_still_not_enforcing_their_storage_quotas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Google still not enforcing their storage quotas?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jiggyleaves930",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-12T01:48:46.905535+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-12T00:58:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>On September 30th, 2025, any unrated (without a content maturity label) Roblox game with less than 1,000 plays will become unplayable. As you can imagine, this is going to lead to a lot of hidden gems becoming lost to time. Apparently this is to \u201cprotect children\u201d but I don\u2019t think it\u2019s worth it when parents should just be monitoring their kids. How do you feel about this? What could be done? This is actually making me kinda sad. I\u2019ll link the devforum post in the comments then.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jiggyleaves930\"> /u/Jiggyleaves930 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1neprhr/robloxs_unrated_experiences_update/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1neprhr/robloxs_unrated_experiences_update/\">[comments]</a></span>",
        "id": 3553558,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1neprhr/robloxs_unrated_experiences_update",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Roblox\u2019s Unrated experiences Update",
        "vote": 0
    }
]