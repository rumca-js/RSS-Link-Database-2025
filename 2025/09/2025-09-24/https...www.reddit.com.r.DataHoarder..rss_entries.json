[
    {
        "age": null,
        "album": "",
        "author": "/u/hupo224",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T23:42:33.540933+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T23:06:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I seem to have issues with this. From the guides it only allows me to &quot;move&quot; stuff from my library to my library-ext. I want to use in the built in webpage to select content I want to download directly my my external hdd. It shows only my internal storage with &quot;not enough&quot; storage. Any ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hupo224\"> /u/hupo224 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npqxsc/iiab_how_do_i_move_the_library_to_my_external/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npqxsc/iiab_how_do_i_move_the_library_to_my_external/\">[comments]</a></span>",
        "id": 3657925,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npqxsc/iiab_how_do_i_move_the_library_to_my_external",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "IIAB: How do I move the library to my external drive so that I may install content directly there?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TotezCoolio",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T23:42:33.710338+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T22:50:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Was running total ~10 TB, but now with homelab, a new 8 TB NVMe and 22 TB toshiba drive I realised I don&#39;t have any free space left. I don&#39;t really want to bust out another 8 TB nvme, maybe 16 TB comes soon or at least gets cheaper. </p> <p>Nevermind, the question is what is a good strategy here? Buy the same drives? Buy different manufacturers? Size-wise I like the 22 TB drives, because they are around 20 TiB. Ffor now 2 machines replicate, inside machine there is also replication + everything goes to Backblaze from main lab + critical also somewhero elso, so no raid/unraid (do not see the time cost/benefit with my small drives, I can just use partitions/mounts and anyway I need to automate Rsync/FreeFileSync/FastCopy).</p> <p>Usage: lot of small file reads/writes </p> <p>2.5M MBTF is a plus, almost a requirement (got used to Toshiba MG10 screeching quite quickly).</p> <p>Also, is seagate / seagate barracuda any good? Or should avoid cheaper/",
        "id": 3657926,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npqkut/drive_sourcing_strategy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Drive sourcing strategy",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/depeesz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T23:42:33.840822+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T22:47:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks,</p> <p>I\u2019m planning to build a NAS with ZFS. Unfortunately, due to financial reasons I can\u2019t afford to buy 4 drives at once. My plan is to buy them one by one, roughly every 2 months, until I have all 4. They will all be the same model and manufacturer.</p> <p>Since I live in the EU and have a 14-day return window, I\u2019d like to make sure each disk is properly tested right after purchase. My worry is that after several months, when I finally have all 4 drives, I could end up with one (or more) bad disks that already had issues from day one.</p> <p>So my questions are: - What\u2019s the best way to stress-test or burn-in each new drive right after I buy it? - Are there specific tools or workflows you recommend (Linux/Windows)? - What\u2019s \u201cgood enough\u201d testing to be confident the drive is solid before the return window closes?</p> <p>Thanks in advance for any advice!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/use",
        "id": 3657927,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npqi0w/how_to_properly_test_hdds_when_buying_them_one_by",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to properly test HDDs when buying them one by one for a future NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/md2mbb",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T22:39:04.406271+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T22:11:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there,</p> <p>I have an Asustor NAS running 3x8TB HDDs. They&#39;re loud as hell, and I have acquired 4x3,84TB second hand enterprise SSDs for a solid price, to make this quieter.</p> <p>I&#39;m struggling to find how best to do the migration. I thought I would be able to hot swap the drives out 1 for 1, and then add the fourth to the same volume, with the RAID rebuilding to the new drives as I replaced them. But I think it&#39;s not letting me do that.</p> <p>As far as I can tell, there is only one option: back up the data, start from scratch with the 4 SSDs. I have about 4TB of data on this NAS (yes, it&#39;s not a very big hoard yet), and I guess the only way to do this with the new array being 4 SSDs is to back it up to some kind of external drive attached to a computer temporarily.</p> <p>Could any kind soul help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/md2mbb\"> /u/md2mbb </a> <br/> <span><a href",
        "id": 3657553,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nppo2x/any_way_to_upgrade_a_raid_5_hdd_server_to_run_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any way to upgrade a RAID 5 HDD server to run on SSDs, without total rebuild?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/martijn208",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T22:39:04.513251+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T22:02:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>so i extracted the video using make MKV, but when i play it back it&#39;s just flashes a bunch of images and then plays the music. it probably uses one image per chapter but extracted like this it doesn&#39;t play it properly. trying to use MKV toolnix to split it into chapters results into unusable files. i even tried handbrake to resolve it but that doesn&#39;t work either. and ditching the video stream all together doesn&#39;t work either, i just get empty non functioning files again. is there some kind of obscure DVD spec to make slideshows with audio that nothing supports? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/martijn208\"> /u/martijn208 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nppg9j/i_have_a_dvd_that_uses_a_slideshow_setup_and_i/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nppg9j/i_have_a_dvd_that_uses_a_slideshow_setup_an",
        "id": 3657554,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nppg9j/i_have_a_dvd_that_uses_a_slideshow_setup_and_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "i have a DVD that uses a slideshow setup and i can't figure out how to rip it",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cyborg762",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T21:37:05.788709+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T21:08:53+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cyborg762\"> /u/cyborg762 </a> <br/> <span><a href=\"/r/makemkv/comments/1npnz6j/forcing_subtitles/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npo4js/forcing_subtitles/\">[comments]</a></span>",
        "id": 3657100,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npo4js/forcing_subtitles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Forcing subtitles",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/2012ctsv",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T21:37:05.527074+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T20:52:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve filled up my 2TB drives and I&#39;d like to move to 8TB primary and backup. I found Western DigitalBWLG0080HBK-NESN on Amazon. These would be used on a Raspberry Pi plex server. Would the Western Digital Elements series be good for my uses?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/2012ctsv\"> /u/2012ctsv </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npnp2o/looking_for_2_8tb_hard_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npnp2o/looking_for_2_8tb_hard_drives/\">[comments]</a></span>",
        "id": 3657099,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npnp2o/looking_for_2_8tb_hard_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for 2 8TB hard drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/supinator1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T22:39:04.712907+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T20:49:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t want to sync to Google because of privacy concerns and having to pay for additional storage. It would be nice if I could have nightly backups of my text messages and photos to a dedicated backup server at my home. Right now, I have to do it via USB cable and manually copy the data. It would be nice if this could be done over WiFi or cellular network.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/supinator1\"> /u/supinator1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npnmtf/is_there_a_way_to_periodically_back_up_text/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npnmtf/is_there_a_way_to_periodically_back_up_text/\">[comments]</a></span>",
        "id": 3657555,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npnmtf/is_there_a_way_to_periodically_back_up_text",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a way to periodically back up text messages and photos to a personal server on Android?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mediocre-Leg9025",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T21:37:05.968786+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T20:49:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey I\u2019m looking into nas for myself. I\u2019ve found these two at around a $700cad mark which is the higher end of what I\u2019d like to spend + drives. Looking for advice on which to actually go forward with. </p> <p>Questions: 1. Is the software Asustor really that dated and old and annoying to use as I\u2019ve heard. But really great hardware.</p> <ol> <li>Qnap has better software but slightly worse hardware by the sounds of it. So which trade off is more worth it/ is it even a trade off with recent updates?</li> </ol> <p>Tasks: ~I\u2019m a photographer and engineer so it would see common tasks associated with those (Lightroom and AutoCAD files) </p> <p>~Back up my whole iPhone and laptop. </p> <p>~Store music/songs/movies</p> <p>~Remote in on my phone from anywhere to look at photos stored</p> <p>~Give a friend access so we can share files (them via vpn) easily for business purposes and make use of all this storage I\u2019ll have available.</p> <p><a href=\"https://www.red",
        "id": 3657101,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npnm7r/nas_asustor_5404t_v2_vs_qnap_ts462",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NAS Asustor 5404t v2 vs qnap ts-462",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/plasmo_falciparum",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T20:34:20.315941+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T20:24:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is anyone here aware of any attempts being made to archive the lists of people being arrested and deported by ICE and DHS? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/plasmo_falciparum\"> /u/plasmo_falciparum </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npn07q/ice_detainee_records/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npn07q/ice_detainee_records/\">[comments]</a></span>",
        "id": 3656671,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npn07q/ice_detainee_records",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ICE Detainee Records",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Warhost",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T20:34:20.599829+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T20:21:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey,</p> <p>I am using a 10TB WD Red Plus since January, it\u2019s in a USB Housing (FANTEC Qb-X2US3R) and connected to a HTPC, which puts it to sleep when not used (most of the day, only use it 2 hours in the evening).</p> <p>Now after about 9 months of use, I noticed all of a sudden one day it\u2019s reading noise is significantly louder. The usual sub-second noises it makes i.e. when a folder is accessed just got more louder and grinder/rougher.</p> <p>Is this cause for concern?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Warhost\"> /u/Warhost </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npmx71/wd_red_plus_changed_how_it_sounds_while_reading/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npmx71/wd_red_plus_changed_how_it_sounds_while_reading/\">[comments]</a></span>",
        "id": 3656672,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npmx71/wd_red_plus_changed_how_it_sounds_while_reading",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WD Red Plus changed how it sounds while reading",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IndividualLucky",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T20:34:20.105094+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T20:20:09+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IndividualLucky\"> /u/IndividualLucky </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1npml45\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npmvsh/my_plex_server_has_started_an_addiction/\">[comments]</a></span>",
        "id": 3656670,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npmvsh/my_plex_server_has_started_an_addiction",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My Plex server has started an addiction",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mobo_deli",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T19:32:06.072166+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T19:02:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve checked a few posts about blu-ray/4k ripping, but a lot of them recommend specific drives that can flash the firmware necessary to rip 4k discs. I&#39;m fine with just being able to rip blu-ray, so is <strong>any</strong> external blu-ray drive sufficient for those purposes? I see some ranging from <a href=\"https://www.amazon.com/PIONEER-External-Blu-ray-BDR-XD08B-Portable/dp/B0BN6721NC\">$145</a> to <a href=\"https://www.amazon.com/wintale-External-Portable-Blu-ray-Suitable/dp/B0CKSXY18G?th=1\">$39</a>, but are there any major differences? Do all of them work with MakeMKV?<br/> Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mobo_deli\"> /u/mobo_deli </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npkw69/external_drive_for_ripping_only_blurays/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npkw69/external_drive_for_ripping_only_blu",
        "id": 3656257,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npkw69/external_drive_for_ripping_only_blurays",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "External drive for ripping ONLY Blu-Rays recommendation?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Fit-Wafer-6074",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T22:39:04.881066+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T17:43:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, in order to repair &quot;my cloud home &quot; device from WD, i replaced the hardrive but even with the reset proc\u00e9dure the partition are not created... Do you know i can proceed because even the official procedure is not working.</p> <p>Regards </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fit-Wafer-6074\"> /u/Fit-Wafer-6074 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npis6x/looking_for_a_firmware/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npis6x/looking_for_a_firmware/\">[comments]</a></span>",
        "id": 3657556,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npis6x/looking_for_a_firmware",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a firmware",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lm8888888",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T22:39:04.986933+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T17:10:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hello</p> <p>i dont know if im at the right place, but i saw a post of this group a while ago, about &quot;how to grab all facebook comments of a publication, in once.&quot;</p> <p>well, i&#39;d like to do the same with some newspaper&#39;s comments of articles. Here it&#39;s about french newspapers, eg Le Figaro, Le Point, etc, and i dont see at all how to get them.</p> <p>Eg : <a href=\"https://www.lefigaro.fr/conso/l-ufc-que-choisir-demande-a-la-justice-l-arret-de-la-commercialisation-des-bouteilles-perrier-20250924\">https://www.lefigaro.fr/conso/l-ufc-que-choisir-demande-a-la-justice-l-arret-de-la-commercialisation-des-bouteilles-perrier-20250924</a></p> <p>or :</p> <p><a href=\"https://www.lepoint.fr/people/permis-invalide-et-vehicule-non-assure-revelations-sur-l-accident-d-isabelle-nanty-18-09-2025-2598953_2116.php\">https://www.lepoint.fr/people/permis-invalide-et-vehicule-non-assure-revelations-sur-l-accident-d-isabelle-nanty-18-09-2025-2598953_2",
        "id": 3657557,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nphxnm/grab_all_comments_of_a_newspaperarticle_in_one",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "grab all comments of a newspaper/article, in one operation?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/essentialaccount",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T17:22:38.547812+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T16:56:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I move every three to five years it seems, and often internationally. I am contemplating once such move now, and don&#39;t know what to do with my NAS and other equipment. Short of buying a <a href=\"https://business.pelican.com/us/en/discover/rack-mount-server-cases/supermac\">Supermac</a> there doesn&#39;t seem to be a good solution. I have considered just buying a Pelican clase and stashing my server in there. Is this a viable option? What concerns should I have with respect to HDD integrity? </p> <p>Also, when will SSDs be cheap enough I can stop worrying about all this nonsense. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/essentialaccount\"> /u/essentialaccount </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nphjfr/travelling_with_a_server_hand_luggage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nphjfr/travelling_with_a_server_hand_luggag",
        "id": 3654971,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nphjfr/travelling_with_a_server_hand_luggage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Travelling with a Server: Hand Luggage?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/KnuckleMonkey_782",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T17:22:39.490667+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T14:12:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was trying to search for some peoples insta profiles, but I accidentally took snapshots of them instead. How do I go about getting them taken off. Ik I&#39;m going to get flack, so go ahead and bring out whips for invasion of privacy </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KnuckleMonkey_782\"> /u/KnuckleMonkey_782 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npd8r5/i_screwed_up/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npd8r5/i_screwed_up/\">[comments]</a></span>",
        "id": 3654973,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npd8r5/i_screwed_up",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I screwed up",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/madisonjar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T17:22:38.687971+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T14:07:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good morning, I have a Mycloudultra 2 2 bay NAS drive. It originally had a ten TB Drive and I bought a 12 TB to use in Raid 1 configuration. It of course told me I could only use ten but at the time I had no choice. The original drive has since failed and I bought another 12 TB drive to use as backup.....any tips on how to not lose my 7 TB of Data on the drives but expand both drives to use the 12 TB? Thank you for your time and help</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/madisonjar\"> /u/madisonjar </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npd3mu/12_tb_drive_formatted_as_10_tb_in_ultracloud/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npd3mu/12_tb_drive_formatted_as_10_tb_in_ultracloud/\">[comments]</a></span>",
        "id": 3654972,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npd3mu/12_tb_drive_formatted_as_10_tb_in_ultracloud",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "12 TB drive formatted as 10 TB in ultracloud, looking to do full 12",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CombinationLive3973",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T14:05:00.166790+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T13:49:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How to? I bought a fake 2 TB which is one of the most foolish things I did, but since it&#39;s still has memory to use (even it only has 8GB), its still usable, I even used it for my games and others (around 5GBs but it easily corrupts files when copying files into the MicroSD Card at high speeds (maybe even at 15 mbps).</p> <p>So what way I could lower it? Is there a software to limit it or do I have to do something on the MicroSD card itself?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CombinationLive3973\"> /u/CombinationLive3973 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npcno4/how_to_lower_the_readwrite_speed_of_my_microsd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npcno4/how_to_lower_the_readwrite_speed_of_my_microsd/\">[comments]</a></span>",
        "id": 3653982,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npcno4/how_to_lower_the_readwrite_speed_of_my_microsd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How To Lower The Read/Write Speed Of My MicroSD Card?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PartInevitable6290",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T12:20:27.731975+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T11:50:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1npa0rj/i_made_the_fastest_adfree_way_to_download_videos/\"> <img src=\"https://external-preview.redd.it/2bbDGQWrLg7p9pFUCq1uL4hYRglEBtVGIX5NpQ338tI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=62e879a1b50c9ace563349f64a91005659172f01\" alt=\"I made the fastest, ad-free way to download videos from X/Twitter\" title=\"I made the fastest, ad-free way to download videos from X/Twitter\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>All the other sites annoyed me, so I built a better one. </p> <p>Will always be ad-free and free. Enjoy</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PartInevitable6290\"> /u/PartInevitable6290 </a> <br/> <span><a href=\"https://xdownload.org/?ref=reddit\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npa0rj/i_made_the_fastest_adfree_way_to_download_videos/\">[comments]</a></span> </td></tr></table>",
        "id": 3652653,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npa0rj/i_made_the_fastest_adfree_way_to_download_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/2bbDGQWrLg7p9pFUCq1uL4hYRglEBtVGIX5NpQ338tI.png?width=640&crop=smart&auto=webp&s=62e879a1b50c9ace563349f64a91005659172f01",
        "title": "I made the fastest, ad-free way to download videos from X/Twitter",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PartInevitable6290",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T12:20:27.472413+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T11:23:39+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1np9imt/downloading_every_amazon_invoice_ever_by_pressing/\"> <img src=\"https://external-preview.redd.it/dTJpbTdwbGprM3JmMbfLfWYzWB7WFtZT59QtOVfJumqFhSm6TND4yP24zDcX.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b3b7c9255579474d47cd9ae1b74f8f3498bab71b\" alt=\"Downloading every Amazon invoice ever by pressing one button\" title=\"Downloading every Amazon invoice ever by pressing one button\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I needed to do this for Amazon and other sites, so I built a tool.</p> <p>This would have taken me hours to do manually for Amazon alone. Now I press a button, go for a walk, it&#39;s done when I get back.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PartInevitable6290\"> /u/PartInevitable6290 </a> <br/> <span><a href=\"https://v.redd.it/uafwunljk3rf1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1np",
        "id": 3652652,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1np9imt/downloading_every_amazon_invoice_ever_by_pressing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/dTJpbTdwbGprM3JmMbfLfWYzWB7WFtZT59QtOVfJumqFhSm6TND4yP24zDcX.png?width=640&crop=smart&auto=webp&s=b3b7c9255579474d47cd9ae1b74f8f3498bab71b",
        "title": "Downloading every Amazon invoice ever by pressing one button",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CarelessChain6999",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T12:20:28.442344+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T10:34:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a 4-bay NAS enclosure that accepts existing, NTFS-formatted SATA drives with data already populated. I want to be able to add existing data drives to it without having to reformat them, and have no need for RAID. Can anyone recommend a product that meets this requirement?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CarelessChain6999\"> /u/CarelessChain6999 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1np8ndu/4bay_sata_nas_enclosure_that_accepts_preformatted/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1np8ndu/4bay_sata_nas_enclosure_that_accepts_preformatted/\">[comments]</a></span>",
        "id": 3652655,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1np8ndu/4bay_sata_nas_enclosure_that_accepts_preformatted",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "4-bay SATA NAS enclosure that accepts pre-formatted NTFS drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dethrall",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T12:20:28.069421+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T08:42:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am looking to build a custom NAS / Homeserver combo. Some mini ITX board, probably an SFX PSU and 5x 3.5&quot; HDD. It should be near silent and will have a capable CPU for plex and the *rr stack.<br/> So far I found some cases like the Node 304 or the Jonsbo N1. However, I don&#39;t really like any of those for different reasons.</p> <p>The N1 is almost there, but from what I read it is too hot and/or loud. Also, too round for my taste.<br/> The Node is a cube. I am looking for a tower.<br/> ATX cases are too big for my use case.</p> <p>It should be a &quot;tower&quot; like a Loque S1 or Dan A4 or the N1 for that matter.<br/> It should have space for 5 HDDs. (ideally with hotswap, but optional)<br/> It should be cool and silent.<br/> It should be metal and/or wood.</p> <p>I do not need a GPU, but will have a 10G NIC in the expansion (if the MB does not have one).<br/> Is there any case I am missing?</p> <p>EDIT: added reasons against some cases</p>",
        "id": 3652654,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1np6xti/what_cases_are_you_using_for_a_custombuilt",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What cases are you using for a custom-built NAS/Server combo?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tinney97",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T08:07:57.061830+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T08:01:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I was thinking about how to back up my files today and asked myself: what is the benefit of a raid? I read more than one time that a raid is not a back up, so why not just store the files on an unplugged HDD? The only thing I could think of is when you keep adding files regulary.</p> <p>Thanks in advance :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tinney97\"> /u/tinney97 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1np6cr3/regarding_backups/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1np6cr3/regarding_backups/\">[comments]</a></span>",
        "id": 3651826,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1np6cr3/regarding_backups",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Regarding Backups",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tic-tac135",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T02:38:30.510829+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T02:23:52+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tic-tac135\"> /u/tic-tac135 </a> <br/> <span><a href=\"/r/techsupport/comments/1nnwkj1/zfsecc_on_local_machine/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1np0jkv/zfsecc_on_local_machine/\">[comments]</a></span>",
        "id": 3650437,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1np0jkv/zfsecc_on_local_machine",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ZFS/ECC on local machine",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/KHSebastian",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T02:38:30.893900+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T02:08:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am in the US, and I have a blu-ray boxed set that I want to back up, but it&#39;s region B, so I have limited options for ripping it. The only way I&#39;ve found is to put the disc into my modded PS4 and use the FTP functionality to browse the files in the BD drive. </p> <p>I&#39;ve successfully pulled the files, so now I have a folder with 3 subfolders, AACS, BDMV and CERTIFICATE. There is a STREAM folder in the BDMV folder that has .m2ts files in it, and it looks like the episodes I am trying to get, but VLC won&#39;t play them. As I understand it, I probably need to decrypt the files first. I&#39;ve tried MakeMKV and that says the files are corrupt. I&#39;ve tried the free version of DVDFab, and that sees the streams and their lengths and stuff, but when I convert them it just doesn&#39;t do anything.</p> <p>Since DVDFab is picking up on the times, I suspect that the files aren&#39;t actually corrupt and it&#39;s some weirdness of the region mism",
        "id": 3650438,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1np07sa/help_decrypting_region_b_bluray",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help Decrypting Region B Blu-Ray",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/1alessandrolol",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-24T02:38:31.112805+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-24T01:54:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So... I just wanted to know how do I archive a Wattpad story because the site is banned from the Wayback Machine + there are no stories archived on the Internet Archive so I have no clue what to put on the identifier, I thought putting wattpad-id but I don&#39;t know if doing it or not.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/1alessandrolol\"> /u/1alessandrolol </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nozx2w/archiving_wattpad_stories/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nozx2w/archiving_wattpad_stories/\">[comments]</a></span>",
        "id": 3650439,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nozx2w/archiving_wattpad_stories",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Archiving Wattpad stories???",
        "vote": 0
    }
]