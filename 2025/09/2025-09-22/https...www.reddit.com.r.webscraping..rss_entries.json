[
    {
        "age": null,
        "album": "",
        "author": "/u/Ok-Depth-6337",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T19:38:23.221798+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T19:10:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi scrapers,</p> <p>I actually have a python script that use asyncio, aiohttp and scrapy to do massive scraping on various e-commerce really fastes, but not enough.</p> <p>i do around of 1gbit/s</p> <p>but python seems to be at the max of is possible implementation.</p> <p>im thinking to move in another language like C#, i have a little knowledge of it because i ve studied years ago.</p> <p>im searching the best stack to do the same project i have in python.</p> <p>my requirements actually are:</p> <p>- full async</p> <p>- a good library to make async call to various endpoint massively (crucial get the best one) AND possibility to bind different local ip in the socket! this is fundamental, because i ve a pool of ip available and rotating to use</p> <p>- best scraping library async.</p> <p>No selenium, browser automated or like this.</p> <p>thx for your support my friends.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.co",
        "id": 3638877,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nnvldd/best_c_stack_to_do_scraping_massively_around_10k",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best c# stack to do scraping massively (around 10k req/s)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/arnabiscoding",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T17:16:33.587997+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T16:17:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to <strong>scrape and format all the data</strong> from <a href=\"https://git-scm.com/docs/git#_git_commands\">Complete list of all commands</a> <strong>into a RAG</strong> which I intend to use as a info source for playful mcq educational platform to learn GIT. How may I do this? I tried using clause to make a python script and the result was not well formatted, lot of &quot;\\n&quot;. Then I feed the file to gemini and it was generating the json but something happened (I think it got too long) and the whole chat got deleted??</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/arnabiscoding\"> /u/arnabiscoding </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnqxah/how_to_convert_git_commands_into_rag_friendly_json/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnqxah/how_to_convert_git_commands_into_rag_friendly_json/\">[comments]</a></span>",
        "id": 3637764,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nnqxah/how_to_convert_git_commands_into_rag_friendly_json",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to convert GIT commands into RAG friendly JSON?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MasterpieceSignal914",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T16:04:58.922312+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T15:50:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey is there anyone who is able to scrape from websites protected by Akamai Bot Manager. Please guide on what technologies still work and if there are any reliable paid solutions. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MasterpieceSignal914\"> /u/MasterpieceSignal914 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnq6zw/getting_blocked_by_akamai_bot_manager/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnq6zw/getting_blocked_by_akamai_bot_manager/\">[comments]</a></span>",
        "id": 3637084,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nnq6zw/getting_blocked_by_akamai_bot_manager",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting Blocked By Akamai Bot Manager",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Intelligent-Cap-4022",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T16:04:59.097364+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T15:41:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello people.</p> <p>So, I think I went too hard on the website and now apparently ive been blocked.</p> <p>What you suggest me doing now? Can you help me, please? \ud83d\ude30</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Intelligent-Cap-4022\"> /u/Intelligent-Cap-4022 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnpye8/noob_here_apparently_ive_been_ip_blocked_what_now/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnpye8/noob_here_apparently_ive_been_ip_blocked_what_now/\">[comments]</a></span>",
        "id": 3637085,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nnpye8/noob_here_apparently_ive_been_ip_blocked_what_now",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Noob here. Apparently I've been IP blocked.. What now?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Upstairs-Public-21",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T08:07:20.080842+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T07:36:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Lately, my scrapers keep getting blocked by Cloudflare, or I run into a ton of captchas\u2014feels like my scraper wants to quit \ud83d\ude02</p> <p>Here\u2019s what I\u2019ve tried so far:</p> <ul> <li>Puppeteer + stealth plugin, but some sites still detect it \ud83d\udc40</li> <li>Rotating proxies (datacenter/residential IPs), helps a bit \ud83c\udf00</li> <li>Solving captchas manually or outsourcing, but costs are crazy \ud83d\udcb8</li> </ul> <p>How do you usually handle these issues?</p> <ul> <li>Any lightweight and reliable automation solutions?</li> <li>How do you manage IP/request strategies for high-frequency scraping?</li> <li>Any practical, stable, and legal tips you can share?</li> </ul> <p>Let\u2019s share experiences\u2014promise I\u2019ll bookmark every suggestion\ud83d\udccc</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Upstairs-Public-21\"> /u/Upstairs-Public-21 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nng56p/scrapers_vs_cloudflare_captchastips/\">",
        "id": 3633789,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nng56p/scrapers_vs_cloudflare_captchastips",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\ud83e\udd2f Scrapers vs Cloudflare & captchas\u2014tips?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/maloneyxboxlive",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T06:57:46.511519+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T06:31:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am currently in the process of trying to develop a social media listening scraper tool to help me automate a totally dull task for my job.</p> <p>I have to view certain social media groups every single day to look out for relevant mentions and then gauge brand sentiment in a short plain text report.</p> <p>Not going to lie, it&#39;s a boring process. To speed things up at the min, I just copy and paste relevant posts and comments into a plain text doc then run the whole thing through ChatGPT</p> <p>It got me thinking that surely this could be an automated process to free me up to do something useful.</p> <p>So far, my extension plugin is doing a half decent job of pulling in most of the data of the social media groups, but can&#39;t help help wondering if there&#39;s a much better way already out there that can do it all in one go.</p> <p>Thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/malone",
        "id": 3633508,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nnf49l/want_to_automate_a_social_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Want to automate a social scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Naht-Tuner",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T06:57:46.852374+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T06:03:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Has anyone used Crawl4AI to generate CSS extraction schemas fully automatically (via LLM) for scaling up to around 50 news webfeeds, without needing to manually tweak selectors or config for each site?</p> <p>Does the auto schema generation and adaptive refresh actually keep working reliably if feeds break, so everything continues to run without manual intervention even when sites update? I want true set-and-forget automation for dozens of feeds but not sure if Crawl4AI delivers that in practice for a large set of news websites.</p> <p>What&#39;s your real-world experience?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Naht-Tuner\"> /u/Naht-Tuner </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nneoip/crawl4ai_autogenerated_schemas_for_largescale/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nneoip/crawl4ai_autogenerated_schemas_for_largescale/\">[",
        "id": 3633510,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nneoip/crawl4ai_autogenerated_schemas_for_largescale",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Crawl4AI auto-generated schemas for large-scale news scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gvkhna",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T06:57:46.691567+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T05:24:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been working on a vibe scraping tool. The idea is you tell the agent the website you want to scrape, and it will take care of the rest for you. It has access to all of the right tools and a system that gives it enough information for it to figure out how to get the data you&#39;re looking for. Specifically code generation.</p> <p>It generates an extraction script currently, and a crawler script. Both scripts are run in a sandbox. The extraction script is given cleaned html, and the llm writes something like cheerio code to turn the html into json data. The crawler script also runs on the html to return urls repeatedly until it&#39;s done.</p> <p>The llm also generates a json schema so the json data can be validated.</p> <p>It does this repeatedly until the scraper is working. Currently it only scrapes one url and may or may not be working. But I have a working test example where the entire crawling process works and should have it working wit",
        "id": 3633509,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nne0v8/im_working_on_an_open_source_vibescraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm working on an open source vibescraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dragonyr",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-22T03:39:58.783999+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-22T03:37:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We have tried pydoll (headful/headless), rnet, regular requests of course on residential proxies with retries, at best we can get around 10% success rate. Any tips people have would be greatly appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dragonyr\"> /u/dragonyr </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnc40s/any_tips_on_crawling_nordstrom/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nnc40s/any_tips_on_crawling_nordstrom/\">[comments]</a></span>",
        "id": 3632779,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nnc40s/any_tips_on_crawling_nordstrom",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any tips on crawling nordstrom?",
        "vote": 0
    }
]