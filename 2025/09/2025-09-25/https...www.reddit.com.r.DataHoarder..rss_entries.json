[
    {
        "age": null,
        "album": "",
        "author": "/u/Just4Hero",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T23:00:18.044016+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T22:07:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am using MakeMKV to rip BlueRays and DVDs. But it wont get passed the protections. What do I do?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Just4Hero\"> /u/Just4Hero </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqk509/ripping_dvds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqk509/ripping_dvds/\">[comments]</a></span>",
        "id": 3666020,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqk509/ripping_dvds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ripping DVDs,",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kooky-Bandicoot3104",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T21:59:59.004676+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T21:54:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hi, i need molex to molex safe cables which wont melt and or pins bending.</p> <p>I found these currently:</p> <p><a href=\"https://www.amazon.com/dp/B09Z2M77KL\">https://www.amazon.com/dp/B09Z2M77KL</a></p> <p>or</p> <p><a href=\"https://www.startech.com/en-eu/cables/lp4powext12\">https://www.startech.com/en-eu/cables/lp4powext12</a></p> <p>also</p> <p>I need to find a crimped style sata extender, i find none that are currently crimped</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kooky-Bandicoot3104\"> /u/Kooky-Bandicoot3104 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqjt94/need_help_on_finding_safe_molex_to_molex/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqjt94/need_help_on_finding_safe_molex_to_molex/\">[comments]</a></span>",
        "id": 3665597,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqjt94/need_help_on_finding_safe_molex_to_molex",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help on finding SAFE molex to molex extension cables",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dancurranjr",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T21:59:59.282653+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T21:51:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My Father&#39;s wife just passed away, and he was so distraught that he had a panic attack and could not attend her service so I am trying to get a copy.</p> <p>I read other responses and they essentially said:</p> <p>Chrome &gt; F12 &gt; Dev tools &gt; Network</p> <ol> <li>Play video</li> <li>Locate .m3u8 file (might help to sort files by name) and right click &gt; copy link</li> <li>Open VLC &gt; file &gt; convert/save &gt; network/url &gt; paste url &gt; follow prompts to convert/save as .mp4</li> </ol> <p>However, I do not see the .m3u8 file anywhere. </p> <p><strong>Can anyone lend a hand? There are two videos at:</strong> <a href=\"https://view.oneroomstreaming.com/index.php?data=MTc1NzA4NTgyMDM3OTI0NiZvbmVyb29tLWFkbWluJmNvcHlfbGluaw==\"><strong>Jane Doe Service</strong></a></p> <p>Or maybe a screenshot if you see the file? Perhaps I am looking in the wrong place, but I am pretty computer savvy. </p> <p>THANK YOU IN ADVANCE</p> </div><!-- SC_ON --",
        "id": 3665598,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqjr2m/how_to_download_funeral_service_video_from_one",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How To Download Funeral Service Video from One Room",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abbrechen93",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T21:59:58.674184+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T21:12:35+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abbrechen93\"> /u/abbrechen93 </a> <br/> <span><a href=\"/r/4kdownloadapps/comments/1nq5b2r/new_update_released_for_4k_download_apps_version/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqissw/youtube_downloads_with_4k_download_apps_should/\">[comments]</a></span>",
        "id": 3665596,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqissw/youtube_downloads_with_4k_download_apps_should",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "YouTube downloads with 4K Download Apps should now be possible again",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/utsnik",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T21:59:59.601678+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T21:06:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey! </p> <p>So i bought some cheap but very lightly used NetAPP drives (HGST air 10TB drives) And i had some troubles getting them to work as they were formatted as 520 bytes drives, as well as they were encrypted.. </p> <p>What worked was this:</p> <p>Unlocking &amp; Reformatting NetApp HGST SAS Drives (520B \u2192 512B)</p> <p>These pulled drives (e.g. <strong>NETAPP X378_WVAXE10TA07</strong>) ship with:</p> <ul> <li><strong>SED lock enabled (OPAL2)</strong></li> <li><strong>520-byte sectors</strong> with Protection Information</li> <li>NetApp firmware that blocks normal <code>sg_format</code></li> </ul> <h1>1. Install sedutil (binary release)</h1> <ol> <li>Download the pre-built Linux package (from <a href=\"https://github.com/Drive-Trust-Alliance/sedutil/wiki/Executable-Distributions\">Executable Distributions</a>):wget <a href=\"https://github.com/Drive-Trust-Alliance/sedutil/releases/download/v1.20.0/sedutil_LINUX.tgz\">https://github.com/Drive-Trust-Al",
        "id": 3665599,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqinu8/formatted_and_fixed_my_netapp_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Formatted and fixed my NetAPP drives..",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ffpg2022",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T20:58:20.788220+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T20:56:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there any software that will do an on-the-fly hash based duplicate check and skip writing the file if a copy already exists anywhere on the disk/volume?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ffpg2022\"> /u/ffpg2022 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqierx/on_the_fly_duplicate_checker/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqierx/on_the_fly_duplicate_checker/\">[comments]</a></span>",
        "id": 3665293,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqierx/on_the_fly_duplicate_checker",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "On the fly duplicate checker",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DrSteam04",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T20:58:20.950335+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T20:53:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqic6l/need_a_hard_drive_desperately/\"> <img src=\"https://preview.redd.it/4ipqigt4idrf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff14b26288a7453763a061217db7f10ccaad2ace\" alt=\"Need a hard drive desperately.\" title=\"Need a hard drive desperately.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>As you can see i am in need of a hard drive. I don&#39;t need anything big 1 or 2 tb will be enough, and i don&#39;t know what to get. I&#39;m on a budget so less than $75 would be preferred. It needs to be able to connect to either USB or USB type-c.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DrSteam04\"> /u/DrSteam04 </a> <br/> <span><a href=\"https://i.redd.it/4ipqigt4idrf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqic6l/need_a_hard_drive_desperately/\">[comments]</a></span> </td></tr></table>",
        "id": 3665294,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqic6l/need_a_hard_drive_desperately",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/4ipqigt4idrf1.png?width=640&crop=smart&auto=webp&s=ff14b26288a7453763a061217db7f10ccaad2ace",
        "title": "Need a hard drive desperately.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jukerix",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T20:58:21.166273+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T20:50:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, ive been googling for a while noticed this sub coming up a lot, but the questions were not rlly answered for my problem. So i have a unlisted youtube video link which says its unavailable. Most sites show that the video is only available in Kosovo, Somaliland or N. Cyprus. Couldn&#39;t find a free vpn, dont wana pay for vpn just to open one link one time lol. Tried proxy but for some reason it just says port timed out. Vid in question is just a silly meme song. <a href=\"https://youtu.be/ohKcckRHY4U\">https://youtu.be/ohKcckRHY4U</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jukerix\"> /u/Jukerix </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqi8mg/youtube_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqi8mg/youtube_help/\">[comments]</a></span>",
        "id": 3665295,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqi8mg/youtube_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Youtube help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Invoyail",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T18:34:24.937922+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T18:30:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>According to my system info it&#39;s been 9 months since I did into this rabbit hole. Well to be frank it&#39;s more accurate to say I&#39;ve only had my first computer for that long. In that time I have acquired three 10tb refurbished hdd (one got corrupted and died save the data). Ran my first PC on a cheap 500gb ssd before getting a 2tb ssd nvme 4.0. Got jellyfin and all the arrs working. </p> <p>I&#39;ve asked this to my fellow elders. How do I evolve from here. In my a3 matx officially there room for 2 2.5 slots and 1 hdd slots. And in my motherboard that I have to replace because it asrock has three slots nvme. I don&#39;t know what tracker or usenet I&#39;m supposed move towards from having (TL/NZBfinder). Everything is starting to feel cramped and unorganized both quality and size. Please help. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Invoyail\"> /u/Invoyail </a> <br/> <span><a href=\"https://www.re",
        "id": 3664202,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqemm3/noobs_survival_guide",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Noobs Survival Guide",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Correct_Detective_35",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T16:31:26.733008+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T16:10:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to download several artist&#39;s arts, I merely want what&#39;s in the &quot;media&quot; section, but so many of them exceed +1,000 arts (and some of them even have like +10,000 arts) and AFAIK Twitter (X) only allows scrolling up to 1,000 posts (at least in the &quot;media&quot; section) if you don&#39;t have Premium.</p> <p>How do I get past the scrolling limit without paying for Premium? Any tools that allow me to do that? I tried WFDownloader and JDownloader, but they don&#39;t pierce through the 1,000 posts limit if your account doesn&#39;t have Premium.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Correct_Detective_35\"> /u/Correct_Detective_35 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqaxd0/how_do_i_get_past_twitter_xs_scroll_limit_without/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqaxd0/how_do_i_get_past_twitter",
        "id": 3663315,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqaxd0/how_do_i_get_past_twitter_xs_scroll_limit_without",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How Do I Get Past Twitter (X)'s Scroll Limit Without Premium?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/D3MZ",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T16:31:26.457641+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T16:03:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>No judgment, I think it\u2019s important work regardless. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/D3MZ\"> /u/D3MZ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqar5s/do_you_hoard_in_real_life_or_is_just_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqar5s/do_you_hoard_in_real_life_or_is_just_data/\">[comments]</a></span>",
        "id": 3663314,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqar5s/do_you_hoard_in_real_life_or_is_just_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you hoard in real life or is just data?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bobloblaw-87",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T17:32:41.832809+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T15:49:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>These new 24TB Barracuda drives (ST24000DM001) are confirmed <strong>CMR</strong>, which makes them look tempting for big media libraries.</p> <p>But I see the community split some say they\u2019re fine in ZFS, others won\u2019t use Barracudas at all since they\u2019re \u201cdesktop drives.\u201d</p> <p>For a <strong>ZFS NAS (RAIDZ2) mainly serving Plex/media</strong>, are these CMR Barracudas a reasonable option, or still too risky compared to IronWolf/Exos?</p> <p>If they are too risky, what is the best option and what kind of deals should I be on the hunt for?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bobloblaw-87\"> /u/bobloblaw-87 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqae24/using_cmr_barracuda_drives_in_zfs_nas_safe_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nqae24/using_cmr_barracuda_drives_in_zfs_nas_safe_for/\">[comments]</a></span>",
        "id": 3663755,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nqae24/using_cmr_barracuda_drives_in_zfs_nas_safe_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using CMR Barracuda drives in ZFS NAS safe for Plex/media?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Temporary_Potato_254",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T15:29:11.404995+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T15:20:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I often see comments on TikTok videos and sometimes YouTube and some of the pc reddits about nas devices and you see people in the comments being like using hdds in the big 25 or imagine using hdds which doesn\u2019t make sense to me ssds wear out too and they don\u2019t have the same price value per tb especially for cold storage, am I missing something?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Temporary_Potato_254\"> /u/Temporary_Potato_254 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq9mke/whats_up_with_the_hate_the_hdds_get/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq9mke/whats_up_with_the_hate_the_hdds_get/\">[comments]</a></span>",
        "id": 3662841,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq9mke/whats_up_with_the_hate_the_hdds_get",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s up with the hate the HDDs get",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/I_LIKE_RED_ENVELOPES",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T17:32:41.971641+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T14:57:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been cataloging my cold storage drives and NAS boxes with NeoFinder, but I can\u2019t figure out if it has a way to <em>compare</em> catalogs. Basically I want to cross-check 2 NAS units against ~20 cold drives and see what\u2019s missing on either side (not just duplicates).</p> <p>I tried Googling, asked ChatGPT (which sent me on a wild goose chase), and the official forums don\u2019t look too active.</p> <p>Does NeoFinder even support this? Or should I switch to another app that can handle this kind of verification?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/I_LIKE_RED_ENVELOPES\"> /u/I_LIKE_RED_ENVELOPES </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq90mr/can_neofinder_compare_catalogs_for_missing_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq90mr/can_neofinder_compare_catalogs_for_missing_files/\">[comments]</a></span>",
        "id": 3663756,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq90mr/can_neofinder_compare_catalogs_for_missing_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can NeoFinder compare catalogs for missing files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No_Outside_7468",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T17:32:42.077725+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T14:06:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Which software download the image in best quality jdownloader or wfdownloader? And how can I get best image quality out of any software </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Outside_7468\"> /u/No_Outside_7468 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq7qb8/jdownloader_vs_wfdownloader_quality/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq7qb8/jdownloader_vs_wfdownloader_quality/\">[comments]</a></span>",
        "id": 3663757,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq7qb8/jdownloader_vs_wfdownloader_quality",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Jdownloader vs wfdownloader (quality)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Then_Newspaper_7379",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T11:15:54.131998+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T10:28:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A few months ago I relistened to this parody song using the Wayback Machine, but now it won\u2019t seem to work. Does anyone have any advice for getting the video.</p> <p><a href=\"https://www.youtube.com/watch?v=d7sJG_Fz1hg\">https://www.youtube.com/watch?v=d7sJG_Fz1hg</a></p> <p>Yes, it\u2019s a stupid song </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Then_Newspaper_7379\"> /u/Then_Newspaper_7379 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq34s9/can_someone_help_me_with_this_video/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq34s9/can_someone_help_me_with_this_video/\">[comments]</a></span>",
        "id": 3661086,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq34s9/can_someone_help_me_with_this_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can someone help me with this video",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NoTimeToKink",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T10:13:09.787807+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T09:53:38+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NoTimeToKink\"> /u/NoTimeToKink </a> <br/> <span><a href=\"https://archive.org/details/@magitompg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq2khf/magipack_games_official_repository_in_internet/\">[comments]</a></span>",
        "id": 3660689,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq2khf/magipack_games_official_repository_in_internet",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "MagiPack Games Official Repository in Internet Archive (As the main Site is down)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Iamzhugelaocunfu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T09:11:46.918989+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T08:39:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Start from next month I may need a storage device to store some important data for work (some backup files for BMS system).</p> <p>the use pattern is that:</p> <p>I will go to 5-10 different building in the end of each month, copy the backup file (each around 1-2 GB) to that drive and put it inside where I work and won&#39;t touch it until next collection, or in case something messed up and need the backup.</p> <p>Since it is for work, I dont think shucking is a good idea, and I think a 2TB drive would be enough. But I also found people said external drives are not that good?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Iamzhugelaocunfu\"> /u/Iamzhugelaocunfu </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq1ftj/external_drive_recommendation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq1ftj/external_drive_recommendation/\">[comments]</a></spa",
        "id": 3660394,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq1ftj/external_drive_recommendation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "external drive recommendation?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ClindAff",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T12:20:19.070583+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T08:09:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I searched all over Reddit and had to find the solution myself, so here it is:</p> <ol> <li><p>Download the extension &quot;Video DownloadHelper&quot; <a href=\"https://v10.downloadhelper.net/fr\">(here is the link)</a></p></li> <li><p>Go on the video you want to download</p></li> <li><p>Download it on the extension</p></li> <li><p>If you have reached the limit of downloads, delete and reinstall the extension</p></li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ClindAff\"> /u/ClindAff </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq0zsa/how_to_download_paid_whop_courses/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nq0zsa/how_to_download_paid_whop_courses/\">[comments]</a></span>",
        "id": 3661486,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nq0zsa/how_to_download_paid_whop_courses",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download paid Whop courses",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TechnicalLMAO",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T04:57:56.041566+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T04:53:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>An account on TikTok who I archive now for some reason has turned everything into followers only. The third-party website that I use to get the metadata of their videos does not work anymore. Is there any other way that I could get the metadata? Possibly using chrome extensions or inspect element.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TechnicalLMAO\"> /u/TechnicalLMAO </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npxx58/how_do_i_get_the_metadata_of_a_video_that_is_only/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npxx58/how_do_i_get_the_metadata_of_a_video_that_is_only/\">[comments]</a></span>",
        "id": 3659327,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npxx58/how_do_i_get_the_metadata_of_a_video_that_is_only",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I get the metadata of a video that is only accessible for followers only",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EchoGecko795",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T06:02:31.839071+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T04:32:58+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1npxkek/google_will_soon_break_all_thirdparty_yt_clients/\"> <img src=\"https://external-preview.redd.it/7hrTWaOEHw7ZKapzaCYbe9q7lihvu2Q0PCRue6jaFRk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0de2b936e79f779fc7bb33ff9abde3988823de09\" alt=\"Google will soon break all third-party YT clients, including yt-dlp; a full JS implementation is now required.\" title=\"Google will soon break all third-party YT clients, including yt-dlp; a full JS implementation is now required.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EchoGecko795\"> /u/EchoGecko795 </a> <br/> <span><a href=\"https://github.com/yt-dlp/yt-dlp/issues/14404\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npxkek/google_will_soon_break_all_thirdparty_yt_clients/\">[comments]</a></span> </td></tr></table>",
        "id": 3659635,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npxkek/google_will_soon_break_all_thirdparty_yt_clients",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/7hrTWaOEHw7ZKapzaCYbe9q7lihvu2Q0PCRue6jaFRk.png?width=640&crop=smart&auto=webp&s=0de2b936e79f779fc7bb33ff9abde3988823de09",
        "title": "Google will soon break all third-party YT clients, including yt-dlp; a full JS implementation is now required.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/eyebeesea",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T04:57:56.442113+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T04:23:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My dvd drive in my computer just stopped working so I need to get a new one. I&#39;m thinking of upgrading so I can rip blue ray and uhds. I have seen a lot of conflicting posted about what is the best internal drive to get. Any help is appreciated. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/eyebeesea\"> /u/eyebeesea </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npxeex/what_dvd_drive_should_i_get/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npxeex/what_dvd_drive_should_i_get/\">[comments]</a></span>",
        "id": 3659328,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npxeex/what_dvd_drive_should_i_get",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What dvd drive should I get?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/livinin82",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T03:55:26.400028+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T03:40:10+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1npwl6f/i_remember_when_i_thought_1tb_was_a_lothow_far/\"> <img src=\"https://preview.redd.it/kx8ru4gre8rf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c27e286c4b72143af463737efc25e415108a1100\" alt=\"I remember when I thought 1tb was a lot...how far we've come. It's wild to see it expand with time.\" title=\"I remember when I thought 1tb was a lot...how far we've come. It's wild to see it expand with time.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Not pictured: 7tb Plex Server and 2tb Steam Deck (among other things)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/livinin82\"> /u/livinin82 </a> <br/> <span><a href=\"https://i.redd.it/kx8ru4gre8rf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npwl6f/i_remember_when_i_thought_1tb_was_a_lothow_far/\">[comments]</a></span> </td></tr></table>",
        "id": 3659003,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npwl6f/i_remember_when_i_thought_1tb_was_a_lothow_far",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/kx8ru4gre8rf1.jpeg?width=640&crop=smart&auto=webp&s=c27e286c4b72143af463737efc25e415108a1100",
        "title": "I remember when I thought 1tb was a lot...how far we've come. It's wild to see it expand with time.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/2020_2904",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T02:54:04.006673+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T02:46:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Title</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/2020_2904\"> /u/2020_2904 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npvjwx/is_scraping_zillow_and_selling_it_legal/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npvjwx/is_scraping_zillow_and_selling_it_legal/\">[comments]</a></span>",
        "id": 3658727,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npvjwx/is_scraping_zillow_and_selling_it_legal",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is scraping Zillow and selling it legal?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Disgrace2029",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T01:52:15.881295+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T01:44:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have deleted all volumes in Disk Management set it up as GPT and it still only shows 2tb. What can I use to get it to see the full drive space? (I know its gonna be less than 10tb). The model WD101EFBX</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Disgrace2029\"> /u/Disgrace2029 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npuade/bought_a_10tb_from_auction_wd_red_plus/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npuade/bought_a_10tb_from_auction_wd_red_plus/\">[comments]</a></span>",
        "id": 3658475,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npuade/bought_a_10tb_from_auction_wd_red_plus",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bought a 10TB from auction WD Red Plus",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/iNebulaiNinjai",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T01:52:16.070494+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T01:21:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was looking to see if there was somewhere to go to play &amp; possibly download these games. I haven&#39;t see them. With the exception of neopets. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iNebulaiNinjai\"> /u/iNebulaiNinjai </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nptsxp/classic_neopets_disney_online_games/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nptsxp/classic_neopets_disney_online_games/\">[comments]</a></span>",
        "id": 3658476,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nptsxp/classic_neopets_disney_online_games",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Classic Neopets/ disney online games.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tianq11",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T06:02:32.548929+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T00:55:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1npt9os/redditgrab_automatic_image_video_reddit_downloader/\"> <img src=\"https://b.thumbs.redditmedia.com/OeNRTH0YXKzi4uafTp4ZsZi1jj1geU2VhFBBDNiClxQ.jpg\" alt=\"RedditGrab - automatic image &amp; video Reddit downloader\" title=\"RedditGrab - automatic image &amp; video Reddit downloader\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Built a browser extension that helps you archive media from subreddits.</p> <p>It works within Reddit\u2019s infinite scroll (as far as Reddit allows). Here\u2019s what it does:</p> <ul> <li>One-click downloads for individual posts</li> <li>Mass downloads with auto-scrolling</li> <li>Works with images (JPG, PNG) and videos (MP4, HLS streams)</li> <li>Supports RedGIFs and Reddit&#39;s native video player</li> <li>Adds post titles as overlays on media</li> <li>Customizable folder organization</li> <li>Download button appears on every Reddit post</li> <li>Filename patterns with subreddit/timesta",
        "id": 3659636,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npt9os/redditgrab_automatic_image_video_reddit_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/OeNRTH0YXKzi4uafTp4ZsZi1jj1geU2VhFBBDNiClxQ.jpg",
        "title": "RedditGrab - automatic image & video Reddit downloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kryakozavr",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T00:47:10.904075+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T00:44:04+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kryakozavr\"> /u/Kryakozavr </a> <br/> <span><a href=\"/r/homelab/comments/1npt07u/how_actually_bad_use_sas_and_sata_on_same/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npt0s6/how_actually_bad_use_sas_and_sata_on_same/\">[comments]</a></span>",
        "id": 3658150,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npt0s6/how_actually_bad_use_sas_and_sata_on_same",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How actually bad use SAS and SATA on same backplane?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ktbsupremo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-25T00:47:11.058072+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-25T00:42:17+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ktbsupremo\"> /u/ktbsupremo </a> <br/> <span><a href=\"/r/homelab/comments/1npsysd/zfs_raid_on_proxmox_host_or_in_vm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1npszf8/zfs_raid_on_proxmox_host_or_in_vm/\">[comments]</a></span>",
        "id": 3658151,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1npszf8/zfs_raid_on_proxmox_host_or_in_vm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ZFS RAID on Proxmox Host or in VM?",
        "vote": 0
    }
]