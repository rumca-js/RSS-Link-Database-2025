[
    {
        "age": null,
        "album": "",
        "author": "/u/SirFine7838",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-19T19:31:24.148838+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-19T19:18:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you develop and open source a tool for scraping or downloading content from a bigger platform, are there any likely negative repercussions? For example, could they take down your GitHub repo? Should you avoid having this on a GH profile that can be linked to your real identity? Is only doing the actual scraping against TOS?</p> <p>How are the well known GH projects surviving?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SirFine7838\"> /u/SirFine7838 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nlcrk4/can_you_get_into_trouble_for_developing_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nlcrk4/can_you_get_into_trouble_for_developing_a/\">[comments]</a></span>",
        "id": 3618141,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nlcrk4/can_you_get_into_trouble_for_developing_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can you get into trouble for developing a scraping tool?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/afeyedex",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-19T16:57:32.184584+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-19T16:32:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys, I&#39;m looking for a tool to scrape google search results. Basically I want to insert the link of the search and the results should be a table with company name and website url. There is a free tool for it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/afeyedex\"> /u/afeyedex </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nl8e2c/how_can_i_scrape_google_search/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nl8e2c/how_can_i_scrape_google_search/\">[comments]</a></span>",
        "id": 3616946,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nl8e2c/how_can_i_scrape_google_search",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can I scrape google search?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Satobarri",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-19T16:57:31.909380+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-19T15:58:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all, </p> <p>I talked to a competitor of ours recently. Through the nature of our competitive situation, he did not tell me exactly how they do it, but he said the following:</p> <p>They scrape 3000-4000 real estate platforms in real-time. So when a new real estate offer comes up, they directly find it within 30 seconds. He said, they add about 4 platforms every day. </p> <p>He has a small team and said, the scraping operation is really low cost for them. Before they did it with Thor browser apparently, but they found a new method. </p> <p>From our experience, it is lots of work to add new pages, do all the parsing and maintain them, since they change all the time or ad new protection layers. New anti-bot detections or anti-captchas are introduced regularly, and the pages change on a regular basis, so that we have to fix the parsing and everything manually. </p> <p>Does anyone here know, what the architecture could look like? (e.g. automating ma",
        "id": 3616945,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nl7g7u/how_to_create_reliable_high_scale_real_time",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to create reliable high scale, real time scraping operation?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Excellent-Yam7782",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-19T10:42:07.426646+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-19T08:28:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m using Capsole to get a CF turnstile token to be able to submit a form on a site, when I run in local host I get a successful form post request with the correct redirect</p> <p>When I run on proxy (multiple) I still get 200 code but the form doesn\u2019t get submitted correctly</p> <p>I\u2019ve tried running the proxys on browser with a proxy switch and it works completely fine which makes me think the proxy isn\u2019t blocked, I\u2019m just not sure as to why I can do it with sole requests?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Excellent-Yam7782\"> /u/Excellent-Yam7782 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nkxzwh/proxy_issue_turnstile/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1nkxzwh/proxy_issue_turnstile/\">[comments]</a></span>",
        "id": 3613224,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nkxzwh/proxy_issue_turnstile",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Proxy issue/ turnstile",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Upstairs-Public-21",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-19T10:42:07.194519+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-19T08:23:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m currently working on a large scraping project with millions of records and have run into some challenges:</p> <ul> <li>Inconsistent data formats that need cleaning and standardization</li> <li>Duplicate and missing values</li> <li>Efficient storage with support for later querying and analysis</li> <li>Maintaining scraping and storage speed without overloading the server</li> </ul> <p>Right now, I\u2019m using Python + Pandas for initial cleaning and then importing into PostgreSQL, but as the dataset grows, this workflow is becoming slower and less efficient.</p> <p>I\u2019d like to ask:</p> <ul> <li>What tools or frameworks do you use for cleaning large-scale scraped data?</li> <li>Are there any databases or data warehouses you\u2019d recommend for this use case?</li> <li>Do you know of any automation or pipeline tools that can optimize the scrape \u2192 clean \u2192 store process?</li> </ul> <p>Would love to hear your practical tips or lessons learned to make my data pro",
        "id": 3613223,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1nkxx1r/how_do_you_clean_largescale_scraped_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How Do You Clean Large-Scale Scraped Data?",
        "vote": 0
    }
]