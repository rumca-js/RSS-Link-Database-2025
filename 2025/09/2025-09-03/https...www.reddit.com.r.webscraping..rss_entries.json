[
    {
        "age": null,
        "album": "",
        "author": "/u/New_Manufacturer_977",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-03T22:58:50.480705+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-03T22:18:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working on a project where I run a tournament between cartoon characters. I have a CSV file structured like this:</p> <pre><code> contestant,show,contestant_pic Ricochet,Mucha Lucha,https://example.com/ben.png The Flea,Mucha Lucha,https://example.com/ben.png Mo,50/50 Heroes,https://example.com/ben.png Lenny,50/50 Heroes,https://example.com/ben.png </code></pre> <p>I want to automatically populate the contestant_pic column with reliable image URLs (preferably high-quality character images).</p> <p>Things I\u2019ve tried:</p> <p>Scraping Google and DuckDuckGo \u2192 often wrong or poor-quality results.</p> <p>IMDb and Fandom scraping \u2192 incomplete and inconsistent.</p> <p>Bing Image Search API \u2192 works, but limited free quota (I need 1000+ entries).</p> <p>Requirements:</p> <p>Must be free (or have a generous free tier).</p> <p>Needs to support at least ~1000 characters.</p> <p>Ideally programmatic (Python, Node.js, etc.).</p> <p>Question: What would be a relia",
        "id": 3489960,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n7ssc7/automatically_fetch_images_for_large_list_from_csv",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Automatically fetch images for large list from CSV?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/_do_you_think",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-03T19:56:48.955703+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-03T19:46:02+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/\"> <img src=\"https://preview.redd.it/xu5ho4v870nf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a76ca2faa43fc3a62a089ee8d1a3d2dd7f785861\" alt=\"Browser fingerprinting\u2026\" title=\"Browser fingerprinting\u2026\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Calling anybody with a large and complex scraping setup\u2026</p> <p>We have scrapers, ordinary ones, browser automation\u2026 we use proxies for location based blocking, residential proxies for data centre blockers, we rotate the user agent, we have some third party unblockers too. But often, we still get captchas, and CloudFlare can get in the way too.</p> <p>I heard about browser fingerprinting - a system where machine learning can identify your browsing behaviour and profile as robotic, and then block your IP. </p> <p>Has anybody got any advice about what else we can do to avoid being \u2018identified\u2019 while scraping?</p> <p>Also, I heard abo",
        "id": 3488706,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/xu5ho4v870nf1.jpeg?width=216&crop=smart&auto=webp&s=a76ca2faa43fc3a62a089ee8d1a3d2dd7f785861",
        "title": "Browser fingerprinting\u2026",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/deduu10",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-03T15:51:45.965328+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-03T15:33:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Wonder where you host your scrapers and let them auto run?<br/> How much does it cost? To deploy on for example github and let them run every 12h? Especially with like 6gb RAM needed each run?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/deduu10\"> /u/deduu10 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n7i0re/where_do_you_host_your_web_scrapers_and_auto/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n7i0re/where_do_you_host_your_web_scrapers_and_auto/\">[comments]</a></span>",
        "id": 3486428,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n7i0re/where_do_you_host_your_web_scrapers_and_auto",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where do you host your web scrapers and auto activate them?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Certain_Vehicle2978",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-03T15:51:46.085216+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-03T15:17:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, I\u2019ve been dabbling in network analysis for work, and a lot of times when I explain it to people I use social networks as a metaphor. I\u2019m new to scraping but have a pretty strong background in Python. Is there a way to actually get the data for my \u201csocial network\u201d with people as nodes and edges being connectivity. For example, I would be a \u201chub\u201d and have my unique friends surrounding me, whereas shared friends bring certain hubs closer together and so on. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Certain_Vehicle2978\"> /u/Certain_Vehicle2978 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n7hlrl/building_a_literal_social_network/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n7hlrl/building_a_literal_social_network/\">[comments]</a></span>",
        "id": 3486429,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n7hlrl/building_a_literal_social_network",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Building a Literal Social Network",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dense_Educator8783",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-03T08:25:50.873618+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-03T07:20:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Right now, I can scrape the product name, price, and the main thumbnail image, but I\u2019m struggling to capture the entire image gallery(specfically i want back panel image of the product)</p> <p>I\u2019m using Python with Crawl4AI so I can already load dynamic pages and extract text, prices, and the first image</p> <p>will anyone please guide it will really help </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dense_Educator8783\"> /u/Dense_Educator8783 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n7888e/how_to_extract_all_back_panel_images_from_amazon/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n7888e/how_to_extract_all_back_panel_images_from_amazon/\">[comments]</a></span>",
        "id": 3482953,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n7888e/how_to_extract_all_back_panel_images_from_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to extract all back panel images from Amazon product pages?",
        "vote": 0
    }
]