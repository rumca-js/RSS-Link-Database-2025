[
    {
        "age": null,
        "album": "",
        "author": "/u/therealwalterwhiter",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T19:32:38.566612+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T19:22:31+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/therealwalterwhiter\"> /u/therealwalterwhiter </a> <br/> <span><a href=\"/r/startrek/comments/1ndnodi/where_to_listen_to_and_download_star_trek_kahn/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndnpp8/where_to_listen_to_and_download_star_trek_kahn/\">[comments]</a></span>",
        "id": 3544184,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndnpp8/where_to_listen_to_and_download_star_trek_kahn",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where to listen to and download star trek kahn podcast?[2025]",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JokaGaming2K10",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T19:32:38.197920+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T19:21:05+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndnoc8/hot_take_we_should_make_525_inch_hdd_again/\"> <img src=\"https://preview.redd.it/bce5o3zbpdof1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7832bd4090525f3654bcef01ac9144d0e825bb17\" alt=\"HOT TAKE! We should make 5.25 inch hdd again\" title=\"HOT TAKE! We should make 5.25 inch hdd again\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><strong>DISCLAIMER! I&#39;M NOT A HDD EXPERT OR ENGINEER, THIS IS JUST A DISCUSSION OR POTENTIALLY A IDEA! I MIGHT BE WRONG, SO PLEASE REACH OUT TO ME AND CORRECT ME!</strong></p> <p>We are hitting the physical limitations of HDDs data density, and we would have to innovate A LOT to get an extra 10Tb of storage, not saying it&#39;s bad, but imagine how many tb could a new 5.25&#39;&#39; HDD hold, with current tech, we can fit 372GB into a cm2, and a 5.25&quot; platter is approximately 132.73cm2, it might be a crappy calculation, but we could fit roughly <strong>50TB per ",
        "id": 3544183,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndnoc8/hot_take_we_should_make_525_inch_hdd_again",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/bce5o3zbpdof1.jpeg?width=640&crop=smart&auto=webp&s=7832bd4090525f3654bcef01ac9144d0e825bb17",
        "title": "HOT TAKE! We should make 5.25 inch hdd again",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/IxBetaXI",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T18:20:11.371401+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T17:55:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I am looking for the best option to save 100TB, maybe more in the Future. I need to be able to access the data at any time and any order. So no Tape. I don\u2019t access the data often, maybe once a month. So i don\u2019t need a 24/7 NAS. I don\u2019t need a raid. If parts of it fail its not the end of the world.</p> <p>What is my best and cheapest option? Just buying 5x20TB HDD and connecting them to my pc once i need something?</p> <p>I am open for any idea</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IxBetaXI\"> /u/IxBetaXI </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndldon/offline_storage_100_tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndldon/offline_storage_100_tb/\">[comments]</a></span>",
        "id": 3543560,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndldon/offline_storage_100_tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Offline Storage 100 TB+",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chaks243",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T18:20:11.658362+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T17:37:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 4x14TB Western Digital Ultrastar DC HC530 SATA drives that recquired a Molex to SATA adapter to turn on. I found some Toshiba N300 SATA drives going for a similar price and am considering picking up some soon. Will they recquire a similar adapter or can can I connect them directly to my corsair PSU&#39;s SATA power cables? Thank you guys for the help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Chaks243\"> /u/Chaks243 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndkvm7/toshiba_n300_power_connector/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndkvm7/toshiba_n300_power_connector/\">[comments]</a></span>",
        "id": 3543561,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndkvm7/toshiba_n300_power_connector",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Toshiba N300 Power Connector",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DraugTheWhopper",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T18:20:11.898513+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T17:20:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Especially as we move into TLC and QLC flash, there&#39;s <a href=\"https://www.tomshardware.com/pc-components/storage/unpowered-ssd-endurance-investigation-finds-severe-data-loss-and-performance-issues-reminds-us-of-the-importance-of-refreshing-backups\">plenty of warnings</a> that it&#39;s not safe to just put your SSDs on a shelf for 10 years and expect the data to be OK. As a long-time user of WD Gold and Ultrastar disks, I note that they are now adding &quot;Optinand&quot; to their top end models. My understanding is that it allows more DRAM cache flushing in the event of sudden power loss, and that it holds some &quot;metadata&quot;, but I can&#39;t find many details.</p> <p>Are there any writeups (or even manufacturer&#39;s statements!) on whether this affects the ability of these disks to stay powered off for long periods? If I&#39;m dumping data onto an array of disks and shoving them into a bunker, do I need to power them back up once a year j",
        "id": 3543562,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndkf3z/are_wd_drives_with_optinand_susceptible_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Are WD drives with \"OptiNAND\" susceptible to unpowered retention issues?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MattTheCasual",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T18:20:12.135572+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T17:18:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does Anyone Know of a suitable Multi Download capable Alternative to YT Downloader HD </p> <p>Hey so, I&#39;m having issues with the Downloader HD, it will download 1 video and then just sit there. Does nothing. Could be cause of recent YT changes, some downloaders online work just fine but my problem is, for discussing a topic i need multiple clips, and i need to for speed of process to dload multiple videos at once. rather than just doing them 1 by 1. Is there any alternatives out there that can dload multiple YT videos at once?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MattTheCasual\"> /u/MattTheCasual </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndkd0p/does_anyone_know_of_a_suitable_multi_download/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndkd0p/does_anyone_know_of_a_suitable_multi_download/\">[comments]</a></span>",
        "id": 3543563,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndkd0p/does_anyone_know_of_a_suitable_multi_download",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does Anyone Know of a suitable Multi Download capable Alternative to YT Downloader HD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Malesto",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T17:09:23.508536+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T16:22:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been trying to move around my backups and storage and I finally got the funds for a 4TB SSD, so I&#39;m wanting to shift the last of my old HDDs stuff onto it and just use the HDD as cold storage for now. It&#39;s a mixture of storage and gaming, so I would like it be okay as far as speeds go but my primary goal is for it to last a long time, if anyone has recommendations. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Malesto\"> /u/Malesto </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nditw7/looking_to_convert_a_pcie_slot_into_a_m2_slot_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nditw7/looking_to_convert_a_pcie_slot_into_a_m2_slot_for/\">[comments]</a></span>",
        "id": 3542827,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nditw7/looking_to_convert_a_pcie_slot_into_a_m2_slot_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to convert a PCIE slot into a M.2 slot for a secondary NVME SSD, can someone point me in the direction for a solid 4-6TB option for longevity(and an adapter if theres ones that are better!), wanting to move the last of my data from my 8 year old HDD!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ronnygiga",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T15:55:30.726706+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T15:20:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Maybe this post is not entirely on point with this community but a few years back, someone posted here a video from youtube about why people hoard. I&#39;m trying to find it but i cant :( . It was directly focused on digital hoard, it also speaks about this community. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ronnygiga\"> /u/ronnygiga </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndh48l/finding_a_video_posted_in_this_community_about/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndh48l/finding_a_video_posted_in_this_community_about/\">[comments]</a></span>",
        "id": 3541886,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndh48l/finding_a_video_posted_in_this_community_about",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Finding a video posted in this community about \"Why we hoard\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MooseMouse12",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T15:55:30.984866+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T14:52:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m planning on putting together my first NAS PC and looking for any input or thoughts on my build.</p> <p>The main purpose is purely a 4k media library of movies and shows that I can stream at home over the local home network. I won&#39;t be streaming outside my home and all my devices are modern so I don&#39;t believe I will be transcoding or doing anything like VMs.</p> <p>My main concern is really power consumption and efficiency since it will be on 24/7. I estimate that idle is about 50-60W with active being about 70 or 100W at the highest.</p> <p>Case: Fractal Design Define R5</p> <p>MB/CPU: ASrock N100M</p> <p>RAM: Crucial 16Gb DDR4 3200 MHz</p> <p>Boot drive: WD Blue SA510 250 Gb</p> <p>HBA: LSI 9207-8i</p> <p>PSU: Corsair RM750x</p> <p>HDD: starting with 4x18-24tb Exos Drives (might expand in future)</p> <p>OS: Thinking about UNRAID, but open to TrueNAS or others</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddi",
        "id": 3541887,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndgcn8/thoughts_or_input_on_my_first_nas_build",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Thoughts or input on my first NAS build?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Soggy_Bottle_5941",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T15:55:30.204091+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T13:49:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Getting old brings anxiety, thinking &quot;How will my wife and children manage life after i&#39;ve gone?&quot;. So i thought to have a document with all my passwords, digital structure and devices, bank and government details, investments, taxes, house, how to access my datahoard, how to manage everything after me; knowing since i do all these things, they&#39;ve got no clue how to handle anything. Now comes the problem:</p> <ol> <li>Writing in a physical notebook: Advantage-They do not need any device or app or password to read it. Disadvantage- Any person also do not need any device or app or password to read it, huge security problem.</li> <li>Writing in a device with .txt format: Advantage- The .txt format will keep long term compatability with any app. Disadvantage- Security problem with .txt file and the location of the file on device will be hard to find.</li> <li>Writing in a journal application: Advantage- Password security and text formatti",
        "id": 3541885,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndeq6a/the_life_after_me_for_a_datahoarder",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\"The Life After Me\" for a datahoarder",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Austechprep",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T15:55:31.321768+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T13:21:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I&#39;ve setup Manyfold and it&#39;s pretty good, I&#39;ve started to download some of the models I like into a zip folder and uploading it to Manyfold which does a good job at that and it seems to be the most efficient way to do it manually as I can open a heap of models and start the download, wait until they are done and then just drop all the files in at once and let manyfold sort it out.</p> <p>But this still a manual process and could take ages to hoard, I&#39;m keen to see if anybody has any repositories with a large amount of different models that can be uploaded to manyfold?</p> <p>I figured the DataHoarder sub was a great place to start to see if anybody has hoarded a lot of different models.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Austechprep\"> /u/Austechprep </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nde28n/collecting_3d_print_modelfiles/\">[link]</a",
        "id": 3541888,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nde28n/collecting_3d_print_modelfiles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Collecting 3D print model/files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MooseMouse12",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T11:58:04.935052+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T11:55:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m planning on putting together my first NAS PC and looking for any input or thoughts on my build. </p> <p>The main purpose is purely a 4k media library of movies and shows that I can stream at home over the local home network. I won&#39;t be streaming outside my home and all my devices are modern so I don&#39;t believe I will be transcoding or doing anything like VMs. </p> <p>My main concern is really power consumption and efficiency since it will be on 24/7. I estimate that idle is about 50-60W with active being about 70 or 100W at the highest.</p> <p>Case: Fractal Design Define R5</p> <p>MB/CPU: ASrock N100M</p> <p>RAM: Crucial 16Gb DDR4 3200 MHz</p> <p>Boot drive: WD Blue SA510 250 Gb</p> <p>HBA: LSI 9207-8i</p> <p>PSU: Corsair RM750x</p> <p>HDD: starting with 4x18-24tb Exos Drives (might expand in future)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MooseMouse12\"> /u/MooseMouse12 </a> <br/> <span><a ",
        "id": 3540136,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndc4co/first_time_building_a_nas_thoughts_or_input_on_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "First time building a NAS. Thoughts or input on my build?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AlgaeicAmalgam",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T11:58:05.104458+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T11:02:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am fairly new to this, but from what I managed to find it looks like it is only able to find duplicates in two different folders, not within the same one.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AlgaeicAmalgam\"> /u/AlgaeicAmalgam </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndb43k/how_do_i_use_czkawka_to_detect_duplicate_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndb43k/how_do_i_use_czkawka_to_detect_duplicate_files/\">[comments]</a></span>",
        "id": 3540137,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndb43k/how_do_i_use_czkawka_to_detect_duplicate_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I use Czkawka to detect duplicate files within the same folder",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zarevskaya",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T11:58:04.625242+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T10:51:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndawwt/syncin_your_opensource_platform_to_store_share/\"> <img src=\"https://external-preview.redd.it/HGWCjUhfQ6EyUP1aHxfL59qlWJR6cArwBIe6e6DJK5k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71fd049643e3b9b095000b05b92a110661deb4ae\" alt=\"Sync-in, your open-source platform to store, share, collaborate, and sync your files.\" title=\"Sync-in, your open-source platform to store, share, collaborate, and sync your files.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zarevskaya\"> /u/zarevskaya </a> <br/> <span><a href=\"/r/selfhosted/comments/1nco6w0/syncin_your_opensource_platform_to_store_share/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ndawwt/syncin_your_opensource_platform_to_store_share/\">[comments]</a></span> </td></tr></table>",
        "id": 3540135,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ndawwt/syncin_your_opensource_platform_to_store_share",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/HGWCjUhfQ6EyUP1aHxfL59qlWJR6cArwBIe6e6DJK5k.png?width=640&crop=smart&auto=webp&s=71fd049643e3b9b095000b05b92a110661deb4ae",
        "title": "Sync-in, your open-source platform to store, share, collaborate, and sync your files.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LErich112",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T10:44:29.088342+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T09:31:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd9jzl/old_2000s_cd_game_possible_copy_protection/\"> <img src=\"https://b.thumbs.redditmedia.com/96AD7d3VXlH_MX_xFkAC7iSE1xKbJRrBpNFQ_LhqUGQ.jpg\" alt=\"Old 2000s CD game - possible copy protection?\" title=\"Old 2000s CD game - possible copy protection?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/4x28wg6oxaof1.png?width=769&amp;format=png&amp;auto=webp&amp;s=9c7c688a94336be08a384c55b15f9bf0e62e40c8\">https://preview.redd.it/4x28wg6oxaof1.png?width=769&amp;format=png&amp;auto=webp&amp;s=9c7c688a94336be08a384c55b15f9bf0e62e40c8</a></p> <p>I&#39;ve been going through old CD educational games and I decided to make backups of them. I am from Romania and a bunch of those games were sold to kids in schools for different subjects. They are in good condition and are being able to be read good and properly.</p> <p>Trying to make a copy of them using a USB reader/writer and ImgBurn but t",
        "id": 3539602,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd9jzl/old_2000s_cd_game_possible_copy_protection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/96AD7d3VXlH_MX_xFkAC7iSE1xKbJRrBpNFQ_LhqUGQ.jpg",
        "title": "Old 2000s CD game - possible copy protection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RecoverLimp2397",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T07:06:52.414138+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T06:54:34+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd779c/cinepolis_vs_bharath_where_should_i_watch_demon/\"> <img src=\"https://b.thumbs.redditmedia.com/P6LJavwfaHC-GtFTXIvocqI9O6NNxbKpdCGdQod0mMo.jpg\" alt=\"Cinepolis vs Bharath: Where Should I Watch Demon Slayer?\" title=\"Cinepolis vs Bharath: Where Should I Watch Demon Slayer?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><strong>Hey everyone,</strong></p> <p>So <em>Demon Slayer</em> is finally releasing in India on <strong>September 12</strong>, and I\u2019m super excited to catch it in theaters! The catch? I\u2019ve <em>never been to a movie theater in Mangalore before</em> so this is all new territory for me.</p> <p><a href=\"https://preview.redd.it/ocaoeluubaof1.png?width=535&amp;format=png&amp;auto=webp&amp;s=18347e9dd060729e0f5607d1c04aac821dfafc44\">https://preview.redd.it/ocaoeluubaof1.png?width=535&amp;format=png&amp;auto=webp&amp;s=18347e9dd060729e0f5607d1c04aac821dfafc44</a></p> <p>I\u2019m stuck between two opti",
        "id": 3538385,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd779c/cinepolis_vs_bharath_where_should_i_watch_demon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/P6LJavwfaHC-GtFTXIvocqI9O6NNxbKpdCGdQod0mMo.jpg",
        "title": "Cinepolis vs Bharath: Where Should I Watch Demon Slayer?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/venturemedia",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T07:06:52.064811+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T06:50:41+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd74zq/clicking_hdd_startup_sounds_is_it_dead_any_ideas/\"> <img src=\"https://external-preview.redd.it/cm54MjdtZG5hYW9mMfg4jINoawYdJy5Ug7HTQ3xKpFz15vylIiHrLBACrrTp.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4e2a9830f13a9cf15d3e94fcbacf13a718a6fad\" alt=\"Clicking HDD Startup Sounds: Is it dead? Any Ideas on how to access?\" title=\"Clicking HDD Startup Sounds: Is it dead? Any Ideas on how to access?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I got this HDD which makes this clicking sounds on Startup and does not show up in the File System. Any Ideas if this is still recoverable? It\u00b4s not the end of the World if not, but i would like to grab some single Files if possible. But i would not send this to a recovery service.<br/> If you have any DIY-Ideas on how to get this accessed one last time, please let me know. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/u",
        "id": 3538384,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd74zq/clicking_hdd_startup_sounds_is_it_dead_any_ideas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/cm54MjdtZG5hYW9mMfg4jINoawYdJy5Ug7HTQ3xKpFz15vylIiHrLBACrrTp.png?width=640&crop=smart&auto=webp&s=a4e2a9830f13a9cf15d3e94fcbacf13a718a6fad",
        "title": "Clicking HDD Startup Sounds: Is it dead? Any Ideas on how to access?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lionofearth",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T07:06:51.896173+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T06:33:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Last month I moved into a much smaller apartment and finally had to admit defeat: there&#39;s just no room for my DVD shelf anymore. I&#39;ve got around 200 discs boxed up now. My plan is to convert them to MP4 so I can keep everything on an external drive. But I&#39;m kind of stuck: some tools I tried make the files huge, others drop subtitles, and the encoding time feels endless.</p> <p>Ideally, I&#39;d like something that would save me time by not having to bounce between different programs. Sometimes I want to merge a bonus disc into the main one, so having editing options would be a huge plus. Really appreciate any practical tips before I spend weeks re-ripping everything.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lionofearth\"> /u/lionofearth </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd6v9x/dvd_to_mp4_conversion_tips_trying_to_keep_quality/\">[link]</a></span> &#32; <span>",
        "id": 3538383,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd6v9x/dvd_to_mp4_conversion_tips_trying_to_keep_quality",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DVD to MP4 conversion tips? Trying to keep quality and save time",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/stingrayjerk11211",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T05:51:30.144457+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T05:41:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for the right program to fit my needs when it comes to downloading my Instagram posts. I&#39;m totally fine with something real simple right now and going one post at a time as tedious and time consuming as that might be. Of course there are dozens of these sites online and they almost work well enough for me. But I need one that will &quot;carry over&quot; the info/details I had on my original posts caption/description. If that makes sense. Once the .jpg image is downloaded to my computer I want to be able to right click, go to properties and to details and read all the info/details I had on the caption/description. </p> <p>I&#39;ve been a 4kstogram customer around 8 years. Which I&#39;ve always loved. But I finally got a warning for something on Instagram so I don&#39;t want to risk my account getting banned. In addition to that the program was giving me issues with &quot;stacked posts&quot; that contained multiple pictures. So I thi",
        "id": 3538047,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd60u1/looking_for_simple_1_by_1_instagram_post",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for simple 1 by 1 Instagram post downloader that will carry over my original posst description/caption.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No_Professional_582",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T04:31:54.533875+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T04:27:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all. I am expecting a shipment of refurbished HDDs from goharddrives (Seagate Exos X16s) and would like to test them prior to replacing the drives in my current NAS. </p> <p>Can you recommend a single or dual HDD dock and software solutions (preferably Windows) for ensuring drive health prior to full install? My NAS does not have any open bays to use.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Professional_582\"> /u/No_Professional_582 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd4rt1/dock_for_testing_hdds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd4rt1/dock_for_testing_hdds/\">[comments]</a></span>",
        "id": 3537757,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd4rt1/dock_for_testing_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Dock for testing HDDs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SmartAverageCanadian",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T03:21:20.151685+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T03:06:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Recently, I saw Anna&#39;s archive&#39;s incident and it reminded me of twitch&#39;s purge, veoh&#39;s shutdown and the list goes on... I appreciate and always admire those who backup these valuable sites and their irreplaceable data before the plug is pulled (heroes don&#39;t always wear capes!) </p> <p>But I think it is time for me to not just sit on the sidelines admiring. I want to learn how to archive and backup video sites, and master that skill. I had tried searching the subreddit and couldn&#39;t find a guide.</p> <p>I know yt-dlp will be crucial for this and I have spent time to learn the basics of it, but when the site isn&#39;t supported, it becomes more challenging and I want to learn to do it myself, have my own setup instead of waiting around for yt-dlp to add support for new sites.</p> <p>yt-dlp uses FFMPEG. Maybe that&#39;s a tool I will need to learn also.</p> <p>Normally, we would check chrome tools and network and find URLs to the v",
        "id": 3537489,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd37sr/ytdlp_newbie_any_tutorial_for_best_setup_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "yt-dlp newbie, any tutorial for best setup for archiving unsupported video sites?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dwhit7",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T03:21:21.316573+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T02:33:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m having a difficult time finding this &quot;in between&quot; offsite data backup solution; was hoping someone could help. I feel like I&#39;m missing an obvious solution, but in my research (of which has been extensive at this point), I haven&#39;t found a solution yet.</p> <p>I&#39;m looking for a low cost, offsite backup solution for my family&#39;s documents, photos, etc storage. Sorta &quot;cold storage&quot; in the sense that I don&#39;t really need frequent access, this is basically archived data. I wouldn&#39;t expect to ever recover / retrieve unless my onsite storage solution fails.</p> <p>I don&#39;t need all the bells and whistles that current cloud based providers provide (iDrive, Backblaze, etc). I don&#39;t need it synced to multiple devices, I don&#39;t need to retrieve one file here, or one file there. Just strictly to serve as an offsite, redundant storage.</p> <p>However, I do want it to be managed / autonomous with synced cha",
        "id": 3537491,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd2juf/cloud_backup_storage_without_all_the_bells_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "\"Cloud\" Backup Storage without all the bells and whistles?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/This-Ship",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T07:06:52.631444+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T02:31:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all,</p> <p>So I\u2019ve been slowly hoarding DVDs over the years (I\u2019m close to 100 now \ud83d\ude05). I still love the physical discs, but I also like having backups \u2014 both for convenience and in case any of the discs die on me.</p> <p>I\u2019ve mostly been using MakeMKV, and honestly it does the job\u2026 but last month the license key thing dragged on and I was stuck waiting. Kinda killed the flow and now I\u2019m wondering if I should just pay for something more stable instead of relying on the beta key situation.</p> <p>Problem is, I don\u2019t really know what else is out there. I\u2019ve barely tried other tools.</p> <p>What do you guys use for DVD backups if you\u2019re paying for it? Anything worth switching to? I\u2019m fine paying a reasonable price if it saves me from the random headaches.</p> <p>Would love to hear what works for you all.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/This-Ship\"> /u/This-Ship </a> <br/> <span><a href=\"https://www",
        "id": 3538386,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd2i9u/any_good_dvd_tools_tired_of_makemkvs_license",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any good DVD tools? Tired of MakeMKV\u2019s license hiccups",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FalconSteve89",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T03:21:19.739665+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T02:20:07+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd29q7/synology_read_only_mode/\"> <img src=\"https://b.thumbs.redditmedia.com/3T9lPWGW_aFEagDdKeceezWpP3_vapoWYZoXXHU9dws.jpg\" alt=\"Synology read only mode\" title=\"Synology read only mode\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I had a drive in failure mode, but I replaced it and resynced it and the array will still not stay in read/write mode for more than a few minutes. Any idea how I fix it without starting over?</p> <p>I need this for digital hoarding</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FalconSteve89\"> /u/FalconSteve89 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1nd29q7\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd29q7/synology_read_only_mode/\">[comments]</a></span> </td></tr></table>",
        "id": 3537488,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd29q7/synology_read_only_mode",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/3T9lPWGW_aFEagDdKeceezWpP3_vapoWYZoXXHU9dws.jpg",
        "title": "Synology read only mode",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/myfufu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T03:21:20.741534+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T02:14:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Edit: config for four drives, or seven drives.</p> <p>Picked up two of those 26TB Barracudas. Then, randomly in my electronics box I found I had a pair of un-shucked 14TB WD Reds.</p> <p>I have three of those 14TB drives currently in service; two in ZFS Mirror, backing up to the third.</p> <p>Meanwhile, I was wondering the best solution for only 2x 26TB drives, and was just going to do the ZFS mirror with them, but THEN I realized I could slap the two WDs in and ZFS Stripe those as the backup destination. But then I thought maybe another configuration there would make more sense, like DRAID...</p> <p>So: How would you reconfigure 2x 26TB drives and 5x 14TB drives for max storage and redundancy? Recognize that I&#39;m not going to be able to put them *all* in DRAID in one go (for example) because I&#39;ll need one drive safe with my data...</p> <p>Edit: Now that I think about it, I also have 5x 2TB drives and 1x 4TB drive sitting around I could throw i",
        "id": 3537490,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd25jd/your_opinions_please_best_configuration_for_four",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Your opinions, please: Best configuration for four drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Wooden-Implement-499",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T07:06:52.781120+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T01:42:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, Im a researcher looking into how political parasocialism can have an effect on voter turnouts, and how political parasocialism develops in social media and its effects.</p> <p>I got two facebook posts to scrape comments from, each have around 12.9K~ comments. I&#39;ve tried Facepager, ive also tried manually scrolling and extracting them through browser page console, Ive tried easyAPI&#39;s facebook comment scraper on apify, and still, none wasn&#39;t successful. Any advice? </p> <p>Is there a scraping program I can use that recognizes Facebook&#39;s new alphanumeric ID codes for their posts? or any program at all that can turn the alphanumeric ID code into the old numeric format? coz thats what I really have trouble with. If anything, is there any other alternatives so that I can collect all the 12.9k comments from both posts?</p> <p>P.S. If you&#39;re offering to do the job, dm me an offer so i can consider it.</p> </div><!-- SC_ON --> &#32; sub",
        "id": 3538387,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd1h8j/need_help_to_scrape_26k_facebook_comments",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help to scrape 26k Facebook Comments",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PeppPizzaPie",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T02:08:31.519852+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T01:11:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all! Newbie here trying to figure out the best way to tackle a data organization project. My main concern is photos and my end goal is to have a single source of data that can then be backed up in a 3-2-1 system, and also create printed family &#39;yearbook&#39; photo albums. </p> <p>My sources of data include a 1 current main laptop, multiple small SD cards from cameras, 1 external hard drive, 4 nonworking laptops, and Google storage (currently paying for 2TB). I&#39;ve had Geek Squad transfer the data from 2 of the old laptops to the external HD so I can actually access the files, and I have 2 more laptops to go. I started going through some of the files that were recovered and realized a majority of the photos are already on Google, but there are some that are not. I believe the current laptop and the SD cards are backed up to Google as well. </p> <p>I&#39;m stumped on the best way to go forward to create a single source of data without duplicat",
        "id": 3537215,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd0twg/creating_one_source_of_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Creating one source of data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FewAct2027",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T02:08:31.123086+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T01:01:33+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd0m2h/what_drives_do_you_have_that_are_egregiously_past/\"> <img src=\"https://preview.redd.it/3ouc4vvbj8of1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=beca644acfcd9eec7dc77a154dfcdc7e62f55177\" alt=\"What drives do you have that are egregiously past their lifespan? My Seagate that refuses to die (101500 hours)\" title=\"What drives do you have that are egregiously past their lifespan? My Seagate that refuses to die (101500 hours)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>It&#39;s been used for a bit of everything over the years, and every now and then I&#39;m reminded of how much I&#39;m surprised it&#39;s still alive. It&#39;s the second last Seagate drive I ever purchased (The last was a couple years ago, and failed at 300 hours)</p> <p>Don&#39;t worry, nothing on it is a singular copy - although even if it was, I trust it more than most of my SSD&#39;s at this point.</p> </div><!-- SC_ON --> &#32;",
        "id": 3537214,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd0m2h/what_drives_do_you_have_that_are_egregiously_past",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/3ouc4vvbj8of1.png?width=640&crop=smart&auto=webp&s=beca644acfcd9eec7dc77a154dfcdc7e62f55177",
        "title": "What drives do you have that are egregiously past their lifespan? My Seagate that refuses to die (101500 hours)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FunMisteryGuy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T02:08:31.638338+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T01:01:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to get a Yellow Magic Orchestra tribute performance from WOWOW Japan&#39;s video on demand service.</p> <p><a href=\"https://wod.wowow.co.jp/watch/160794\">https://wod.wowow.co.jp/watch/160794</a> &quot; MUSIC AWARDS JAPAN - A Tribute to YMO #1 &quot;</p> <p>I think it requires a subscription, but I&#39;m not fully sure. The other thing is that while the link above shows the full title in web searches, opening it results in &quot;content not found&quot; errors...</p> <p>YT-DLP does not support WOWOW WOD, and I don&#39;t see any closed issues requesting it. I&#39;m guessing it&#39;s a paid thing and probably has DRM then...</p> <p>Anyone checked this? thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FunMisteryGuy\"> /u/FunMisteryGuy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd0lnm/anyone_have_ideas_for_grabbing_wowow_wod_content/\">[link]</a></span> &#32; <span><a href=\"",
        "id": 3537216,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd0lnm/anyone_have_ideas_for_grabbing_wowow_wod_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone have ideas for grabbing WOWOW WOD content?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/christopher123454321",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-10T00:58:48.870161+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-10T00:50:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://archive.org/details/myspace_dragon_hoard_2010\">https://archive.org/details/myspace_dragon_hoard_2010</a></p> <p>Trying to find some some music from my emo days. There was a lot of really good bands. I never even got record deals in exclusively. Had stuff on on MySpace music. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/christopher123454321\"> /u/christopher123454321 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd0duw/myspace_music_archives_from_the_early_2000s_other/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nd0duw/myspace_music_archives_from_the_early_2000s_other/\">[comments]</a></span>",
        "id": 3536938,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nd0duw/myspace_music_archives_from_the_early_2000s_other",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "MySpace music archives from the early 2000s other than The MySpace Dragon Hoard?",
        "vote": 0
    }
]