[
    {
        "age": null,
        "album": "",
        "author": "/u/AskGolfNut",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T23:26:15.222041+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T22:51:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>1st time poster and really am unraid newbie really, only ever used it&#39;s basic functions for backing up my data and also storing films etc for Jellyfin. My troubles started after building a new to me server from a new Phanteks Enthoo Pro 2 server edition case with a second hand Aorus Ultra X570 and 3950x 32gb 3600 ddr4 and rtx 3060.. I moved over the 10g nic and sas controller also and the 4 x 16tb drives. 1 parity and the others collectively just a touch over 50% full. After removing the drives out of the old server and placing them down on the anti stay bag the motherboard came in, after about 5 mins I was ready to put them in the new case... Got everything finally booting with a little messing around in Bios to get it booting from the Unraid USB and unraid reports that disk 2 and 3 are missing even though they are connected the same way as Parity and Disk 1. I&#39;ve tried all the possible combinations of swap cables, ports etc to no avail... Th",
        "id": 3611141,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkmymd/disk_drives_missing_after_unraid_migration",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Disk Drives Missing after Unraid Migration",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Low_Prompt1337",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T23:26:15.429004+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T22:10:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there,</p> <p>decided that today is a good evening to finally get organized and burn my photos on M-Disk.</p> <p>4 Hours later, I am pretty deep into this rabbit-hole and stumbled upon this subreddit. </p> <p>I want to archive my family fotos and some documents, all about 1TB to 100GB disks.</p> <p>My strategy will be to </p> <ol> <li><p>identify &quot;cold&quot; data</p></li> <li><p>partition that cold data so that it fits on the MDisks (using WinDirStat)</p></li> <li><p>create par2 files</p></li> <li><p>burn it (probably two copies, verify written data)</p></li> <li><p>find some good place to store the written discs</p></li> </ol> <p>But there are some hurdles and i struggle to select some reliable tools.</p> <p>Burning:</p> <p>ImgBurn seems abandoned</p> <p>CdBurnerXP seems to ship adware (selected the portable version though, hopefully without adware)</p> <p>BurnAware seems nice but lacks the ability to split the data</p> <p>Payd options. (Ash",
        "id": 3611142,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nklzqc/archive_workflow_in_2025_on_windows",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Archive Workflow in 2025 (on windows)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jewpiter",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T22:20:36.783230+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T21:56:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>My birthday was last week and a friend gifted me a really nice OLED digital photo frame. After playing with it, I&#39;ve been using it to display photos off my phone, some silly memes, etc. But what I&#39;d really like to use it for is to display classical art paintings. I went on Wikipedia and downloaded a bunch of famous paintings but I&#39;m not really satisfied with the variety. I&#39;d like to download thousands of them and just randomly display them and discover new favorites this way and just expose myself to new art.</p> <p>Does anyone have any sources of high-resolution art? Any torrents? Any art sites that need to be archived or backed up? Hit me up with some ideas! I&#39;m willing to contribute back.</p> <p>Many thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jewpiter\"> /u/Jewpiter </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkln33/sources_",
        "id": 3610716,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkln33/sources_of_high_resolution_art_paintings_that_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sources of high resolution art / paintings that I can backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SurgicalMarshmallow",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T22:20:37.497757+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T21:14:46+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SurgicalMarshmallow\"> /u/SurgicalMarshmallow </a> <br/> <span><a href=\"/r/HomeNAS/comments/1nkkix6/click_hmmm_clic_click/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkklkl/click_hmmm_clic_click/\">[comments]</a></span>",
        "id": 3610717,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkklkl/click_hmmm_clic_click",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Click... Hmmm... Clic click..",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nauxiv",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T19:55:53.637274+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T18:05:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We have about 60TB of data across 6 HDDs (3-14TB each). All NTFS. They&#39;re installed in an old Sandy Bridge i3-2100 box running Windows and shared over the LAN with SMB. This setup sort of organically accumulated over time without any advance planning. </p> <p>I&#39;d like to add additional capacity, and also set up a duplicate array at a secondary location that will be synchronized using Syncthing or similar. This would allow efficient access at both sites, and also provide some redundancy. About 80% of the data (highest priority) was copied to another set of drives already. Unfortunately they are dissimilar drive sizes from the first set, so they won&#39;t be able to be synced directly.</p> <p>I think the most straightforward way to handle this would be to simply pool all drives into a single logical volume (Drivepool?) and then add additional drives for more capacity as necessary. However, I&#39;m not sure if that&#39;s the best plan.</p> <p>I d",
        "id": 3609371,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkfmrc/help_updating_60tb_jbod",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help updating 60TB JBOD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MOMOxKAWAII",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T19:55:53.909638+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T17:53:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>does anyone have a direct download (that it isnt nitroflare, because i cannot afford a premium account right now) or even a torren/magnet of it please? i have searched everywhere, and since all the breached forums are down, there is no way for me to find it for free...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MOMOxKAWAII\"> /u/MOMOxKAWAII </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkfbee/searching_for_this_wattapad_leak/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkfbee/searching_for_this_wattapad_leak/\">[comments]</a></span>",
        "id": 3609372,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkfbee/searching_for_this_wattapad_leak",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Searching for this wattapad leak",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/franklinzunge",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T17:25:37.338991+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T17:06:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I have only recently begun repurposing my old gaming pc for data hoarding, mostly movies. This pc had a blu ray drive, and a larger ATX case and had a drive cage with slots for six 3.5 hdds. The system was originally 10 years old with a i7 5820k CPU, 16 Gb ddr4 ram. I upgraded the GPU a couple years ago to EVGA RTX 3070 and the boot drive is an m.2 Samsung ssd. </p> <p>The psu is an 850w Corsair cs850m. </p> <p>I\u2019ve added to the original PC which included a 2tb hdd an Ultrastar 14gb hdd and I have another one exactly like it that I haven\u2019t installed yet. Also I put in a second internal Blu ray disc drive because I was having issues with the first but was able to resolve them so both drives work now, the newer one can read 4k discs. </p> <p>Everything is working good right now, but I\u2019m wondering since you guys also are doing this kind of stuff, how far do you think I can go adding hdds with 850w psu? </p> </div><!-- SC_ON --> &#32; submit",
        "id": 3608426,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nke1jd/media_pc_recommendations",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Media PC Recommendations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nashu2k",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T17:25:37.616888+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T17:04:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkdzl6/how_nostalgic_are_you_about_old_stuff/\"> <img src=\"https://a.thumbs.redditmedia.com/kuuz03V4ZZ0kAGu2_gs03XybF5LrdOOeyvRFPryng38.jpg\" alt=\"How nostalgic are you about old stuff?\" title=\"How nostalgic are you about old stuff?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Answer: I still keep these...</p> <p><a href=\"https://preview.redd.it/ssyx2aozfypf1.png?width=572&amp;format=png&amp;auto=webp&amp;s=d59893f642cb7bc76df027b5e19d9f2068ac883b\">https://preview.redd.it/ssyx2aozfypf1.png?width=572&amp;format=png&amp;auto=webp&amp;s=d59893f642cb7bc76df027b5e19d9f2068ac883b</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nashu2k\"> /u/nashu2k </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkdzl6/how_nostalgic_are_you_about_old_stuff/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkdzl6/how_nostalgic_",
        "id": 3608427,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkdzl6/how_nostalgic_are_you_about_old_stuff",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/kuuz03V4ZZ0kAGu2_gs03XybF5LrdOOeyvRFPryng38.jpg",
        "title": "How nostalgic are you about old stuff?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MudAffectionate3490",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T17:25:38.677423+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T16:31:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Imagine that you have <strong>200 GB</strong> of images, audios, and videos.<br/> Wouldn&#39;t it be better if that got reduced to <strong>100 GB</strong>? Giving you space for more files?<br/> I found out a couple of really easy and cheap ways to give me a Bite per buck approach. But not only that, it also helped me secure my data, and protecting my privacy.</p> <p><strong>1. Strip the Metadata</strong></p> <p>When I&#39;m viewing videos and images, I don&#39;t want to know what &quot;exposure&quot; it has.<br/> Or what camera it was taken with. I only want to see the pixels.<br/> The same goes with audios; I want to listen to them, not view the codec stuff and all that gibberish text.</p> <p><strong>The pros</strong>: Slightly more storage capacity (for 200 GB with 1000 files, you could free up to 500 MB if you have aggressive metadata logged into them), also privacy, and security of your files.</p> <p><strong>The cons</strong>: I found people menti",
        "id": 3608428,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkd3i5/my_approach_to_halving_my_media_storage_from_200",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My Approach to Halving My Media Storage: From 200 GB to 100 GB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mr_goodbear",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T16:11:37.656159+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T15:48:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This may not be the right sub, but I\u2019m genuinely curious. </p> <p>So obviously your photo roll turns into memes, random screenshots, fifteen photos of the kids, only one is good. </p> <p>My current setup, I have a 16tb, synology nas. I backup frequently. I was better at sifting through before backing up but I\u2019ve gotten so busy lately it\u2019s been months. Anywho, as data is getting cheaper and cheaper and seems that always will be the case, is the time worth even sorting, deleting, properly categorizing?</p> <p>I just feel like in the very soon future we will maybe even have an ai tool that, with some parameters can sort through decades of photos and clear it up on it own at that point? </p> <p>I mean what\u2019s another 8th hard drive? 80 bucks? Haha. It doesn\u2019t seem worth my time to sort. It only satiates my ocd but I can put that aside if I\u2019m saving literally three or four days of my time. </p> <p>What do yall think?</p> <p>What works for you?</p> <p>Thanks",
        "id": 3607677,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkbxrv/personal_photos_and_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Personal photos and videos.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Foreign-Werewolf-202",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T16:11:37.372987+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T15:48:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>One of the things I\u2019ve learned while hoarding TBs of data is that the clutter doesn\u2019t just come from files it also comes from old, half-broken software installs. Over the years I\u2019ve noticed random leftover drivers, registry entries, and even old utilities still hanging around long after I stopped using them.</p> <p>Lately I\u2019ve been experimenting with different ways to track down and remove this \u201csoftware cruft\u201d while keeping my main archive drives safe. I\u2019ve seen people use scripts, registry cleaners, and even manual tracking in spreadsheets. I personally tested a few tools and guides (even stumbled across resources like uninstaller ipcmaster that talk about cleaning methods).</p> <p>For me, the challenge is doing this without breaking dependencies for older programs that I still need to run once in a while (think legacy video converters or backup software). Curious how do other hoarders here manage the software side of the hoard? Do you sandbox, VM, ",
        "id": 3607676,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkbxl3/trying_to_keep_my_archive_clean_without_breaking",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to keep my archive clean without breaking dependencies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AdditionalType3415",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T16:11:37.813895+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T15:31:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I have a couple of 14TB Toshiba MG drives. They are great drives when they are set up properly, but I have always had issues with running them formated to ext4. Whenever I tried they would just make a constant noise as if they were seeking. Formated to XFS, NTFS, and even set up in ZFS they never had these issues. So I&#39;m wondering if anyone else has noticed this, or if I just did something wrong when trying it out on Ubuntu.</p> <p>The reason I&#39;m asking is that I am currently in the process of rebuilding my setup, and I need to reformat the MGs and set them up in a RAID 1 (It&#39;s just what I prefer for the simplicity of it, all drives are paired and set up like that in my server).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AdditionalType3415\"> /u/AdditionalType3415 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkbh21/question_for_toshiba_mg_drives_users/\">[link]</a></s",
        "id": 3607678,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkbh21/question_for_toshiba_mg_drives_users",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Question for Toshiba MG drives users",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/very_undeliverable",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T16:11:38.002980+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T15:10:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I cant get their website to cough up the management software they use. I will probably end up doing software RAID 1, but I thought I would at least evaluate their software. Does anyone have it handy?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/very_undeliverable\"> /u/very_undeliverable </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkax03/windows_software_for_cenmate_803sn23raid/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nkax03/windows_software_for_cenmate_803sn23raid/\">[comments]</a></span>",
        "id": 3607679,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nkax03/windows_software_for_cenmate_803sn23raid",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Windows software for Cenmate 803SN23RAID?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Arkaddian",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T14:59:07.801149+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T14:36:05+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Arkaddian\"> /u/Arkaddian </a> <br/> <span><a href=\"https://torrentfreak.com/internet-archive-vs-music-labels-693m-copyright-battle-ends-with-confidential-settlement/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nka0g1/internet_archive_vs_music_labels_693m_copyright/\">[comments]</a></span>",
        "id": 3607023,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nka0g1/internet_archive_vs_music_labels_693m_copyright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Internet Archive vs. Music Labels: $693m Copyright Battle Ends with Confidential Settlement * TorrentFreak",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CEOofLosing89",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T12:44:21.203022+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T12:01:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been a DrivePool user for 10+ years now. It&#39;s been great until recently.</p> <p>I had 2 systems have this issue with DrivePool and one cropped up right after an update.</p> <p>The issue is your server will boot normally but once you load to the desktop the system slows to a crawl. Programs won&#39;t load. Explorer hangs. The system basically becomes completely unusable.</p> <p>Pulling the drives or uninstalling DrivePool resolves the issue. Had this happen on a brand new install with new disks and had this happen on my own box that has had a pool setup for over 8 years now (pool was moved from an old server to this one a few years ago).</p> <p>All 42 drives have no smart errors or even show any signs of hanging when DrivePool is removed from the equation. Even ran CHKDSK on every one and no file system issues were found.</p> <p>This is a complete showstopper and just wanted to post this in case anyone else had this issue. Needless to s",
        "id": 3605824,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk6cln/psa_drivepool_hanging_systems_server_2022_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "PSA: DrivePool Hanging Systems - Server 2022 & 2025",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sir_Quantum_The_III",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T12:44:21.398647+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T11:53:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want more storage for more hoarding so i don&#39;t have to clock the scrap button</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sir_Quantum_The_III\"> /u/Sir_Quantum_The_III </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk661w/whats_the_cheapest_way_i_can_get_more_hdds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk661w/whats_the_cheapest_way_i_can_get_more_hdds/\">[comments]</a></span>",
        "id": 3605825,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk661w/whats_the_cheapest_way_i_can_get_more_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Whats the cheapest way i can get more hdds?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sir_Quantum_The_III",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T12:44:21.516993+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T11:50:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Sure do love me some data</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sir_Quantum_The_III\"> /u/Sir_Quantum_The_III </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk64i4/where_can_i_get_me_a_at_least_a_5_slot_525in_hdd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk64i4/where_can_i_get_me_a_at_least_a_5_slot_525in_hdd/\">[comments]</a></span>",
        "id": 3605826,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk64i4/where_can_i_get_me_a_at_least_a_5_slot_525in_hdd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where can i get me a at LEAST a 5 slot 5.25in HDD external dock, and how expensive are they?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/d4rk_diamond",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T11:41:12.188893+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T11:24:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been trying to share a large archive of raw data (several terabytes) with a colleague in another region. Some of the files are individually massive over 1TB and the total collection is around 3-4TB. Every cloud drive or common transfer service I\u2019ve checked either refuses uploads at that size, forces me to split things into countless parts, or charges subscription fees that don\u2019t make sense for something I\u2019ll probably need just once or twice a year.</p> <p>Shipping physical drives seems like the fallback option, but I\u2019m hesitant. It feels risky to put everything in the mail, especially when there\u2019s a chance of damage, delays, or customs issues. On top of that, I don\u2019t really want to wait days for something that should be possible digitally.</p> <p>Another issue is stability. My home connection isn\u2019t perfect, so if the upload drops, I need the process to resume without starting over. Integrity checking is another thing I worry about after investing",
        "id": 3605373,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk5lrs/how_do_you_move_multi_terabyte_data_sets_without",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you move multi terabyte data sets without dealing with cloud limitations?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abubin",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T11:41:12.485753+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T11:16:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a few HDD that I use for data backups. They are kept offline all the time. However, some of them are not in good conditions like having bad sectors. So I am trying to start doing yearly data refresh on these drives. However, I find that doing a read-write back on the HDDs are doing more harm than good.</p> <p>For example, a drive have bad sectors. I have no problem copying all the files to another drive. Then I proceed for format the drive, repartition and then copy back data into it. Copying completed without errors. However, when I try reading the data gain, this time large amount of files have I/O error. This happen to 2-3 drives that I have tested so far.</p> <p>Let not discuss about the issue of keep using HDD with bad sectors. I have my reasons to keep using them and it is fine if I lost some of the data.</p> <p>So here is my dilemma. When I first put in the HDD and run a parity check on the data, it all can be read fine. However, after I",
        "id": 3605374,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk5gxr/backup_hdd_doing_data_refresh_for_bitrot_hdd_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup HDD doing data refresh for bitrot? HDD with bad sectors.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/weisineesti",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T12:44:20.938043+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T10:25:19+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk4k42/two_months_after_launching_on_rdatahoarder_open/\"> <img src=\"https://external-preview.redd.it/AADwW6YeR4Nm_zf1OqtsopP227Ls5x7r4VH9kYlwV6E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d73d420f72e6e16727119a36fc3005aa6371974f\" alt=\"Two months after launching on r/DataHoarder, Open Archiver is becoming better, thank you all!\" title=\"Two months after launching on r/DataHoarder, Open Archiver is becoming better, thank you all!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/DataHoarder\">r/DataHoarder</a> , 2 months ago, I <a href=\"https://www.reddit.com/r/DataHoarder/comments/1me2pc2/i_was_paranoid_about_losing_all_my_gmail_data_so/\">launched</a> my open-source email archiving tool Open Archiver here upon approval from the mods team. Now I would like to share with you all some updates on the product and the project.</p> <p>Recently we have launched version 0.3 of the product, which adde",
        "id": 3605823,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk4k42/two_months_after_launching_on_rdatahoarder_open",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/AADwW6YeR4Nm_zf1OqtsopP227Ls5x7r4VH9kYlwV6E.png?width=640&crop=smart&auto=webp&s=d73d420f72e6e16727119a36fc3005aa6371974f",
        "title": "Two months after launching on r/DataHoarder, Open Archiver is becoming better, thank you all!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/abbrechen93",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T10:31:55.226643+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T09:37:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>how do you backup your Spotify playlists and automate the process?</p> <p>For my YouTube playlists e.g. I use <a href=\"/r/4kdownloadapps\">r/4kdownloadapps</a> to watch my playlists and it downloads new videos to my server. But I didn&#39;t find a proper way to do the same for Spotify.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abbrechen93\"> /u/abbrechen93 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk3rpj/how_do_you_sync_your_spotify_playlists_with_an/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nk3rpj/how_do_you_sync_your_spotify_playlists_with_an/\">[comments]</a></span>",
        "id": 3604896,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nk3rpj/how_do_you_sync_your_spotify_playlists_with_an",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you sync your Spotify playlists with an offline backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Loof27",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T03:36:20.293293+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T02:53:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought 5x refurb 12TB ST12000NM0127 drives from GoHardDrive in Dec of 2023. In March this year, I had 1 start throwing read errors in TrueNAS. I fiddled around with cables and determined it was the drive itself, so I had it replaced under warranty. 1 month later in April, I had another drive do the exact same thing. I got that one replaced as well.</p> <p>Even though I was pretty sure it was the drives themselves, I was annoyed and didn&#39;t want to keep dealing with this, so I replaced my HBA and strapped a fan to it in case it was a cooling issue (Originally had an LSI 9211-8i, and I got an AOC-S3008L-L8i)</p> <p>Everything was working fine for a couple of months, but recently I&#39;ve started getting read errors on a 3rd drive. Is it possible I just so happened to get 3 bad drives out of 5? Or could something else be causing this? I feel like my zpool has been degraded for longer than it has been healthy</p> </div><!-- SC_ON --> &#32; submitted ",
        "id": 3602476,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1njx1pr/drives_seem_to_be_failing_in_relatively_quick",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Drives seem to be failing in relatively quick succession",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ninja-Trix",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T02:25:42.188264+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T02:00:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1njvynp/from_my_first_cd_now_to_this_my_complete_flac/\"> <img src=\"https://preview.redd.it/kjtpt3sfwtpf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8851a190f7c7a1ad9ed9ad2d3b94cddfed209754\" alt=\"From my first CD, now to this. My complete FLAC drive\" title=\"From my first CD, now to this. My complete FLAC drive\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>A while back I had the unfortunate occurrence of my hard drive failing me. It was devastating and I wasn&#39;t sure if I&#39;d ever be able to recover everything I&#39;d lost. I can&#39;t remember how long ago that was but needless to say, I bounced back. I actually had cloned my archive a while back and was able to recover most of my rare items, though it was technically an outdated backup. That merged with my friend&#39;s off-site library, lots of time, patience, and good old Johnny Depp, and I&#39;ve gotten my library better than ever.</p> <p>The ",
        "id": 3602133,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1njvynp/from_my_first_cd_now_to_this_my_complete_flac",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/kjtpt3sfwtpf1.jpeg?width=640&crop=smart&auto=webp&s=8851a190f7c7a1ad9ed9ad2d3b94cddfed209754",
        "title": "From my first CD, now to this. My complete FLAC drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wassupluke",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T02:25:42.317968+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T01:53:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1njvsum/is_this_a_good_deal_on_cold_storage_trying_to_get/\"> <img src=\"https://preview.redd.it/2xl0q9shxtpf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e66a0101b96e797158b5670103032f1930fcf0ec\" alt=\"Is this a good deal on cold storage? Trying to get the best bang for buck on my 3-2-1\" title=\"Is this a good deal on cold storage? Trying to get the best bang for buck on my 3-2-1\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wassupluke\"> /u/wassupluke </a> <br/> <span><a href=\"https://i.redd.it/2xl0q9shxtpf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1njvsum/is_this_a_good_deal_on_cold_storage_trying_to_get/\">[comments]</a></span> </td></tr></table>",
        "id": 3602134,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1njvsum/is_this_a_good_deal_on_cold_storage_trying_to_get",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/2xl0q9shxtpf1.png?width=640&crop=smart&auto=webp&s=e66a0101b96e797158b5670103032f1930fcf0ec",
        "title": "Is this a good deal on cold storage? Trying to get the best bang for buck on my 3-2-1",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Nervous_Put_2676",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-18T02:25:42.516777+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-18T00:17:50+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nervous_Put_2676\"> /u/Nervous_Put_2676 </a> <br/> <span><a href=\"/r/Proxmox/comments/1njtr5i/best_way_to_expand_storage_for_a_laptop_proxmox/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1njts4n/best_way_to_expand_storage_for_a_laptop_proxmox/\">[comments]</a></span>",
        "id": 3602135,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1njts4n/best_way_to_expand_storage_for_a_laptop_proxmox",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to expand storage for a laptop Proxmox media server (Jellyfin/Plex)",
        "vote": 0
    }
]