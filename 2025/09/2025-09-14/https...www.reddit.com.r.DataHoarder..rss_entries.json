[
    {
        "age": null,
        "album": "",
        "author": "/u/OkFox105",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T22:44:53.107401+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T22:37:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Did anyone by chance hoard wehatethecolds video on YouTube of the Nepali Revolution from a few days ago? It was taken down but I feel this is something that should have been preserved? Idk if it will ever appear again officially so I was rooting that someone saved it somewhere? Is there any way I can watch it again? The video title was &quot;the side of nepal the media won&#39;t show you&quot; </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OkFox105\"> /u/OkFox105 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nh54p6/wehatethecolds_yt_video_nepal/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nh54p6/wehatethecolds_yt_video_nepal/\">[comments]</a></span>",
        "id": 3573889,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nh54p6/wehatethecolds_yt_video_nepal",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "wehatethecolds yt video nepal",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aleafonthewind28",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T22:44:52.560779+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T21:51:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>First, where does this number come from? The earliest mention I see is from a 2013 Seagate white paper: <a href=\"https://www.seagate.com/files/www-content/ti-dm/tech-insights/en-us/docs/how-hdd-workload-impacts-tco-tp648-2-1309us.pdf\">https://www.seagate.com/files/www-content/ti-dm/tech-insights/en-us/docs/how-hdd-workload-impacts-tco-tp648-2-1309us.pdf</a></p> <p>It\u2019s not drive model specific and clearly is just a minimum standard for their desktop consumer drives. Seems like a copy and paste job to me. If a drive model has a Barracuda label on it, it\u2019s gonna get 2400 regardless of its actual capabilities. </p> <p>All 3.5\u201d Barracudas are 2400hr.</p> <p>All Skyhawk, Ironwolf, and Exos are 8760hr(which is the number of hours in a year)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/aleafonthewind28\"> /u/aleafonthewind28 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nh412g/i_dont_think_t",
        "id": 3573888,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nh412g/i_dont_think_the_seagate_2400hr_per_year_rating",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I don\u2019t think the Seagate 2400hr per year rating matters as much as people think it does.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/xavierhollis",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T21:34:20.986830+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T20:38:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried doing it with mp3tagger but it didn&#39;t work, or at least I couldn&#39;t figure out how to make it work. If there is a section in mkvtoolnix to do it I can&#39;t seem to locate it. </p> <p>Ideally, I&#39;d prefer to find a solution that doesn&#39;t require me making a duplicate file just for the Title tag because I have like 46 videos I intend to remux anyway, so I&#39;d rather sort out the metadata Title as part of that process or else just add it in after the fact vs create a duplicate file then create another one just for the title tag. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/xavierhollis\"> /u/xavierhollis </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nh27kf/anyone_know_how_to_changeadd_metadata_titles_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1nh27kf/anyone_know_how_to_changeadd_metadata_titles_to/\">[comments]</a></s",
        "id": 3573580,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nh27kf/anyone_know_how_to_changeadd_metadata_titles_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone know how to change/add Metadata titles to mkv files that are different to the File Name?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TURB0T0XIK",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T21:34:20.795793+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T20:26:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Citing the page behind the link (<a href=\"https://chng.it/yx4ynmGLHp):\">https://chng.it/yx4ynmGLHp):</a></p> <p>The non-profit library is facing a $700 million copyright infringement suit from labels including UMG and Sony.</p> <p><strong>Open Letter to the Record Labels Suing the Internet Archive</strong></p> <p>We, the undersigned, call on the record labels and members of the Recording Industry Association of America (RIAA)\u2014including UMG, Capitol Records, Concord Bicycle Assets, CMGI Recorded Music Assets, Sony Music Entertainment, and Arista Music\u2014to <strong>drop your lawsuit against the Internet Archive</strong>.</p> <p>Your <strong>$700 million lawsuit</strong>, targeting the Internet Archive\u2019s efforts to preserve and provide access to historical 78rpm records, is not just about music\u2014it\u2019s about whether our digital history survives at all.</p> <p>These fragile recordings are part of a vanishing American culture. They capture early jazz, blues, go",
        "id": 3573579,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1nh1wo9/defend_the_internet_archive_petition_protesting",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Defend the Internet Archive - petition protesting label lawsuit",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EddieOtool2nd",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T14:59:37.038120+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T14:06:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngs7dn/nice_try_cdi_but_i_dont_think_so/\"> <img src=\"https://preview.redd.it/4q9sfjmo05pf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=443a31c4b88bb4af5cd565f8571eca9616b745f0\" alt=\"Nice try CDI, but I don't think so.\" title=\"Nice try CDI, but I don't think so.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>200k hours on a 2.5in spinner? I couldn&#39;t trace the manufacturing date of the drive, but I don&#39;t think it dates back to 2003 - and furthermore it&#39;s been sitting in a pile of junk at my local tech shop, probably for a while already.</p> <p>And it doesn&#39;t even throw a warning for the hours count lol,</p> <p>So... nice try CDI, but I don&#39;t believe you this time. XD</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EddieOtool2nd\"> /u/EddieOtool2nd </a> <br/> <span><a href=\"https://i.redd.it/4q9sfjmo05pf1.png\">[link]</a></span> &#32; <span><a href=",
        "id": 3571302,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngs7dn/nice_try_cdi_but_i_dont_think_so",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/4q9sfjmo05pf1.png?width=640&crop=smart&auto=webp&s=443a31c4b88bb4af5cd565f8571eca9616b745f0",
        "title": "Nice try CDI, but I don't think so.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/redditunderground1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T13:58:09.923263+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T13:53:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was looking for online collections of PDFs that offer free downloads. (Other than the Internet Archive) Content wants is flexible as long as it is interesting. </p> <p>Do you have any to offer? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/redditunderground1\"> /u/redditunderground1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngrwbs/what_are_your_favorite_free_pdf_sites/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngrwbs/what_are_your_favorite_free_pdf_sites/\">[comments]</a></span>",
        "id": 3570894,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngrwbs/what_are_your_favorite_free_pdf_sites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are your favorite free PDF sites?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/planarascendance",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T11:41:38.317988+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T11:12:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngokny/you_youngsters_have_no_idea_how_data_hoardings/\"> <img src=\"https://preview.redd.it/g5aknbvx44pf1.gif?width=108&amp;crop=smart&amp;s=8dbca4137a964adfecf4b9bebf78dba8a26d490c\" alt=\"you youngsters have no idea how data hoarding's easier nowadays\" title=\"you youngsters have no idea how data hoarding's easier nowadays\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/planarascendance\"> /u/planarascendance </a> <br/> <span><a href=\"https://i.redd.it/g5aknbvx44pf1.gif\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngokny/you_youngsters_have_no_idea_how_data_hoardings/\">[comments]</a></span> </td></tr></table>",
        "id": 3570173,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngokny/you_youngsters_have_no_idea_how_data_hoardings",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/g5aknbvx44pf1.gif?width=108&crop=smart&s=8dbca4137a964adfecf4b9bebf78dba8a26d490c",
        "title": "you youngsters have no idea how data hoarding's easier nowadays",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Flags101",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T11:41:39.032531+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T11:08:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried to format exFat to ntfs using file explorer. I also used the cmd method. Are there any options? I want to install something in my usb drive</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Flags101\"> /u/Flags101 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngoi4f/cant_format_exfat_to_ntfs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngoi4f/cant_format_exfat_to_ntfs/\">[comments]</a></span>",
        "id": 3570174,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngoi4f/cant_format_exfat_to_ntfs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can't format exFAT to NTFS:(",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/covered1028",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T10:40:56.288986+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T09:48:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So many of the channels I like are behind a paywall, some have 3/4 of their videos behind at least the lowest membership tier.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/covered1028\"> /u/covered1028 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngn526/when_did_youtube_started_allowing_creators_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngn526/when_did_youtube_started_allowing_creators_to/\">[comments]</a></span>",
        "id": 3569916,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngn526/when_did_youtube_started_allowing_creators_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "When did youtube started allowing creators to make members only videos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/One-Fly298",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T09:39:46.404467+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T09:30:24+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/One-Fly298\"> /u/One-Fly298 </a> <br/> <span><a href=\"/r/Scriptable/comments/1ngmqpe/downloading_podcasts_from_an_rss_feed/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngmuq0/downloading_podcasts_from_an_rss_feed/\">[comments]</a></span>",
        "id": 3569673,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngmuq0/downloading_podcasts_from_an_rss_feed",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading Podcasts from an RSS Feed",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/quick_dry",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T09:39:46.508492+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T09:11:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to figure out a good solution for managing porn videos and integrating them with a viewer.</p> <p>Currently I use Plex on Synology NAS, and AppleTV viewer.</p> <p>The videos are a mix of full length, and some that are just scenes.</p> <p>Plex with PhoenixAdult, PornDB scene and PornDB Movies match a few movies.</p> <p>Stashapp with StashDB and PornDB, looks promising with pHashes, but seems to only match on scenes - and won\u2019t pick up full movies (the PornDB matcher in Plex is able to match the same file (\u2018Pirates\u2019 so it\u2019s not an obscure Indy title).</p> <p>(Likely I\u2019ve missed something in config? They\u2019re all pointing at the same video files/clips)</p> <p>Is Stash not suited for full length movies, it seems like it wants everything to be a scene length?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/quick_dry\"> /u/quick_dry </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngmjs9",
        "id": 3569674,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngmjs9/best_practice_for_mixture_of_full_length_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best practice for mixture of full length and scene videos, and viewing.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/phlaries",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T09:39:46.238478+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T08:46:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking to make periodic file and system image backups on Windows to external drives. </p> <p>I\u2019ve heard acronis and macrium were good, but I\u2019m not a fan of recurring subscription fees. </p> <p>I\u2019m currently using veeam - is this a good tool for what I\u2019m after? It seems to be geared towards VMs\u2026 any free or paid alternatives that aren\u2019t SaaS?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/phlaries\"> /u/phlaries </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngm53u/best_free_or_onetimefee_backup_software/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngm53u/best_free_or_onetimefee_backup_software/\">[comments]</a></span>",
        "id": 3569672,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngm53u/best_free_or_onetimefee_backup_software",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best free OR one-time-fee backup software?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wegettosss",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T07:31:47.704452+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T07:25:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I tried to upgrade my Asus VivoBook 15 Pro OLED (i5-11300H, RTX 3050) from the stock Intel 512 GB SSD to a new 1 TB Adata Legend (PCIe 4.0).</p> <p>What I did:</p> <p>Decrypted the old drive (BitLocker off).</p> <p>Put the new drive in a Ugreen NVMe enclosure.</p> <p>Used Macrium Reflect to clone all partitions (including EFI &amp; recovery).</p> <p>Swapped the drives: put Adata inside laptop, original Intel in enclosure.</p> <p>When I try to boot, I immediately get a blue screen / automatic repair loop. BIOS sees the disk.</p> <p>Things I\u2019ve read about but don\u2019t fully understand:</p> <p>UEFI vs Legacy boot, GPT vs MBR</p> <p>Maybe I need to run Startup Repair or rebuild BCD?</p> <p>Maybe something with Intel RST / AHCI mode?</p> <p>I\u2019d really like to avoid a full reinstall if possible. What steps would you try next?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wegettosss\"> /u/weget",
        "id": 3569210,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngkto5/blue_screen_after_cloning_ssd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Blue screen after cloning ssd.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/st01x",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T07:31:47.841782+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T07:21:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>I&#39;m looking for an application which is able to automatically download from all these 3D print file sites like Thingiverse, Printables, Makerworld etc. because its like always: Somebody publishes a solution for something and then suddenly its gone.</p> <p>Does anything like this exist? I don&#39;t care if has a Web GUI, TUI or is service only without any kind of GUI. Just want a easy way to feed it a link and its downloading in the background to my server.</p> <p>Kind Regards</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/st01x\"> /u/st01x </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngkrgc/how_to_hoard_3d_print_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngkrgc/how_to_hoard_3d_print_files/\">[comments]</a></span>",
        "id": 3569211,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngkrgc/how_to_hoard_3d_print_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to hoard 3D print files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ueommm",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T06:28:45.181820+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T06:17:26+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngjoly/hard_drive_in_docking_station_wont_load_after/\"> <img src=\"https://preview.redd.it/20qj0ea6n2pf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f21848db2dd1d6031c9b39b74b98345bc5ba5918\" alt=\"Hard drive in docking station won't load after idling??\" title=\"Hard drive in docking station won't load after idling??\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>So I have an internal hdd in one of these docking station, when I need to use it I power it up. After using, if I leave the dock station power on but not using the drive, after maybe 20 minutes, it will &quot;eject&quot; itself from Windows, just like as if when you unplugged or &quot;remove safely&quot; a USB drive. This is fine and no poblem. However, it is somewhere during this 20 minutes, when it has been idling for a while but hasn&#39;t unplugged itself yet, (or maybe it should have unplug itself but didn&#39;t?) often when I try to open tha",
        "id": 3569010,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngjoly/hard_drive_in_docking_station_wont_load_after",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/20qj0ea6n2pf1.png?width=640&crop=smart&auto=webp&s=f21848db2dd1d6031c9b39b74b98345bc5ba5918",
        "title": "Hard drive in docking station won't load after idling??",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/5nord",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T06:28:45.558853+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T06:17:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What is your experience with caching filesystems? </p> <p>Currently I have two mostly distinct data dumps: One that is more of an archive, old photos for example and the other one is my live data, that is synced between my mobile devices, for example photos taken 10 years ago.<br/> This dichotomy annoys me pretty much, because it doubles my tech stack and it is a source for chaos and destruction. </p> <p>Recently I found out about caching filesystems: The single source of truth is on your file server, reachable through a network filesystem, such as NFS or CIFS and the SSD on your mobile devices doubles as a cache, when your file server is not accessible. </p> <p>This sounds too good to be true! This is the solution for ALL my problems! &lt;Vsauce-voice&gt;Or is it?&lt;/Vsauce-voice&gt;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/5nord\"> /u/5nord </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/c",
        "id": 3569012,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngjoli/caching_filesystems_have_you_tried_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Caching Filesystems: Have you tried it?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/advance512",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T06:28:45.310715+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T05:50:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there,</p> <p>I am going to clean my WhatsApp of some chats that include starred messages. I want to export them as backup, first. The export chat option does not include which messages were starred in the output text file. I was thinking of fixing it by using a script of some sort, to add the star marking to the text file, but I can see no way of exporting the starred messages at all - not in the Starred Messages view nor in the chat itself.</p> <p>Do you know of any way to do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/advance512\"> /u/advance512 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngj82x/exporting_whatsapp_starred_messages/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngj82x/exporting_whatsapp_starred_messages/\">[comments]</a></span>",
        "id": 3569011,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngj82x/exporting_whatsapp_starred_messages",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Exporting WhatsApp starred messages",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RileyKennels",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T05:26:35.007920+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T05:09:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m on Windows and using eight Seagate Exos Hard drives in my Snapraid array. I noticed that drives I have connected to my H310 (9211-8i in IT mode) dont get proper safe shutdown commands and I can hear them whine to a stop (same sound as power loss event) when shutting down the PC. </p> <p>Is this bad for my Exos drives, or can I feel confident that Enterprise Exos drives will be able to handle these power off events? I am considering connecting my parity drives to the HBA and want to make sure its safe to let then whine to a stop when powering off? (I shut down every couple of weeks)</p> <p>Thanks for any advice. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RileyKennels\"> /u/RileyKennels </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngijso/unsafe_shutdown_only_on_hdds_attached_to_lsi_hba/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ng",
        "id": 3568845,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngijso/unsafe_shutdown_only_on_hdds_attached_to_lsi_hba",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Unsafe shutdown only on HDDs attached to LSI HBA?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/United_Ad5067",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T08:37:03.191268+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T04:45:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.seagate.com/content/dam/seagate/assets/support/internal-hard-drive/enterprise-hard-drives/exos-x24/_shared/files/Seagate_EXOS24_CMR_ISE_SED(10-12-16-20-24TB\">https://www.seagate.com/content/dam/seagate/assets/support/internal-hard-drive/enterprise-hard-drives/exos-x24/_shared/files/Seagate_EXOS24_CMR_ISE_SED(10-12-16-20-24TB)_Rev-C.pdf</a>_Rev-C.pdf)</p> <p>24t has 10 disks and 20 heads. 20 t 9 and 18. They are both 685g. </p> <p>16t have 8 disks 15 heads or 7 disks 14 heads versions, both 670g.</p> <p>12t and 10t both have 5 disks and 10 heads, both 655g.</p> <p>For transfer speeds:<br/> 24t and 20t: 259-272</p> <p>16t: 236-247</p> <p>12t and 10t: 226-236</p> <p>There are two possibilities:</p> <p>1, all x24 are exactly the same physically, but lower capacity ones are binned.</p> <p>2, Seagate used something else to balance the weight. Yet, since 24t have 10 disks, each disk is 2.4t, so 9 disks would be 21.6t. It&#39;s highly unl",
        "id": 3569459,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngi59s/seagate_exos_x24_all_has_same_weight_but_24_t_has",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate exos x24 all has same weight, but 24 t has 10 disks, 20t has 9, 16t 8 or 7. This seems to mean 20t etc are binned?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/moisesmcardona",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T03:16:29.288169+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T03:02:31+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/moisesmcardona\"> /u/moisesmcardona </a> <br/> <span><a href=\"/r/btrfs/comments/1nfvydz/had_my_first_wd_head_crash_btrfs_still/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngg9bp/had_my_first_wd_head_crash_btrfs_still/\">[comments]</a></span>",
        "id": 3568500,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngg9bp/had_my_first_wd_head_crash_btrfs_still",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Had my first WD head crash. BTRFS still operational in degraded mode",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Niko5557",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T03:16:29.527571+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T02:59:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I am in need to transfer hundreds of GB&#39;s of data to another hard disk and I was wondering if it&#39;s fine to use the regular windows file explorer with point n click transfer or should I use some GUI application (that is free) to transfer the files.<br/> I am also worried that it will mess up the metadata such as creation date of the file?<br/> Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Niko5557\"> /u/Niko5557 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngg6yz/gui_tool_for_bulk_file_transfers_hdd_to_hdd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngg6yz/gui_tool_for_bulk_file_transfers_hdd_to_hdd/\">[comments]</a></span>",
        "id": 3568501,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngg6yz/gui_tool_for_bulk_file_transfers_hdd_to_hdd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "GUI tool for bulk file transfers (HDD to HDD)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DamnNJIT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T03:16:29.689967+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T02:38:03+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DamnNJIT\"> /u/DamnNJIT </a> <br/> <span><a href=\"/r/printers/comments/1ngfsf0/hp_477dn_firmware_downgrade_2025_nothing_works/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngfsnp/hp_477dn_firmware_downgrade_2025_nothing_works/\">[comments]</a></span>",
        "id": 3568502,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngfsnp/hp_477dn_firmware_downgrade_2025_nothing_works",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HP 477DN firmware downgrade 2025 nothing works",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/United_Ad5067",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T03:16:29.885906+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T01:22:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A badblocks full write test on 20t+ drive will take ~10 days. I am not sure if it&#39;s worth it. Maybe a read only test is enough?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/United_Ad5067\"> /u/United_Ad5067 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngebj4/do_you_do_badblocks_full_write_test_on_20t_newly/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngebj4/do_you_do_badblocks_full_write_test_on_20t_newly/\">[comments]</a></span>",
        "id": 3568503,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngebj4/do_you_do_badblocks_full_write_test_on_20t_newly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Do you do badblocks full write test on 20t+ newly purchased drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Distantstallion",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-14T00:59:28.817490+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-14T00:03:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for offline windows/linux software I can use to store the research I&#39;m doing as posts of text and images that I can assign tags to for filtering the information I want.</p> <p>I haven&#39;t been able to find software that lets me store and sort posts by tags to include and ezclude, and other fields, the best solution I&#39;ve come up with has been to make a forum using wordpress on a local server.</p> <p>Is there anything off the shelf like that around?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Distantstallion\"> /u/Distantstallion </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngcpti/software_that_i_can_use_to_index_information_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ngcpti/software_that_i_can_use_to_index_information_with/\">[comments]</a></span>",
        "id": 3568107,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ngcpti/software_that_i_can_use_to_index_information_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Software that I can use to index information with searchable tags",
        "vote": 0
    }
]