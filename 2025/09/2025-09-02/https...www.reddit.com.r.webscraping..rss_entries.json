[
    {
        "age": null,
        "album": "",
        "author": "/u/ItsYaBoiAlexYT",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T22:19:13.051994+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T21:45:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1n6wmb6/how_to_webscrape_from_a_page_overlay_inaccessible/\"> <img src=\"https://b.thumbs.redditmedia.com/PQWTCRlN3Znqh9ilO_9LHnTa16nGxbRZgc1H4myU8zo.jpg\" alt=\"How to webscrape from a page overlay inaccessible without clicking?\" title=\"How to webscrape from a page overlay inaccessible without clicking?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi all, looking to scrape data from the stats tables of Premiere League Fantasy (Soccer) players; although I&#39;m facing two issues;</p> <p>- Foremost, I have to manually click to access the page with the FULL tables, but there is no unique URL as it&#39;s an overlay. How can this be avoided with an automatic webscraper?</p> <p>- Second (something I may find issues with in the future) - these pages are only accessible if you log in. Will webscraping be able to ignore this block if I&#39;m logged in on my computer?</p> <p><a href=\"https://preview.redd.it/dov9powlntm",
        "id": 3480534,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6wmb6/how_to_webscrape_from_a_page_overlay_inaccessible",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/PQWTCRlN3Znqh9ilO_9LHnTa16nGxbRZgc1H4myU8zo.jpg",
        "title": "How to webscrape from a page overlay inaccessible without clicking?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Unusual_Chemistry932",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T21:18:47.696896+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T20:27:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m currently working on a project where I need to scrape data from a website (XYZ). I\u2019m using Selenium with ChromeDriver. My strategy was to collect all the possible keywords I want to use for scraping, so I\u2019ve built a list of around 30 keywords.</p> <p>The problem is that each time I run my scraper, I rarely get to the later keywords in the list, since there\u2019s a lot of data to scrape for each one. As a result, most of my data mainly comes from the first few keywords.</p> <p>Does anyone have a solution for this so I can get the most out of all my keywords? I\u2019ve tried randomizing a number between 1 and 30 and picking a new keyword each time (without repeating old ones), but I\u2019d like to know if there\u2019s a better approach.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unusual_Chemistry932\"> /u/Unusual_Chemistry932 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6",
        "id": 3480156,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6ulcw/rotating_keywords_to_randomize_data_across_all",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Rotating Keywords , to randomize data across all ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/strokeright",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T17:16:32.592321+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T17:02:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i found a couple scrapers on a scraper site that I&#39;d like to use. How reliable are they? I see the creators update them, but I&#39;m wondering in general how often do they stop working due to api format changes by the websites?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/strokeright\"> /u/strokeright </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6p45c/how_often_do_the_online_zillow_redfin_realtor/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6p45c/how_often_do_the_online_zillow_redfin_realtor/\">[comments]</a></span>",
        "id": 3478121,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6p45c/how_often_do_the_online_zillow_redfin_realtor",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How often do the online Zillow, Redfin, Realtor scrapers break?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/carishmaa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T17:16:32.711961+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T16:55:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We built <strong>Maxun</strong> to be the open-source no-code sibling to Selenium/Playwright : a visual, point-and-click scraper that lets you extract data by example, not by writing scripts.</p> <p>Here\u2019s how it works:</p> <ul> <li><strong>Click Record \u2192 Show what you want \u2192 Let Maxun repeat it.</strong></li> <li>No coding, no prompts, no YAML.</li> <li>Open-source (self-host forever) + optional cloud for managed scaling &amp; anti bot detection.</li> <li>Handles pagination, scheduling, exports to Sheets/Airtable/API/Webhooks etc.</li> </ul> <p><strong>Why not just use LLMs or agents?</strong><br/> You can. Custom AI agents or DIY Playwright + GPT can work\u2026sometimes. They\u2019re great for unstructured reasoning, but often too slow, expensive, and unreliable for production extraction. Maxun is deterministic as it replays what you show it, exactly, every time.</p> <p><strong>For the community:</strong></p> <ul> <li><strong>GitHub:</strong> <a href=\"https:/",
        "id": 3478122,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6oxqz/we_built_an_oss_nocode_playwright_alternative_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "We Built an OSS No-Code Playwright Alternative for Web Scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Commercial-Soil5974",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T16:16:09.563962+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T16:07:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I\u2019m building a research corpus on <strong>feminist discourse (France\u2013Qu\u00e9bec)</strong>.<br/> Sources I need to collect:</p> <ul> <li>Academic APIs (OpenAlex, HAL, Crossref).</li> <li>Activist sites (WordPress JSON: NousToutes, FFQ, Relais-Femmes).</li> <li>Media feeds (Le Monde, Le Devoir, Radio-Canada via RSS).</li> <li>Reddit testimonies (<a href=\"/r/Feminisme\">r/Feminisme</a>, <a href=\"/r/Quebec\">r/Quebec</a>, <a href=\"/r/france\">r/france</a>).</li> <li>Archives (Gallica/BnF, BANQ).</li> </ul> <p>What I\u2019ve done:</p> <ul> <li>Basic RSS + JSON parsing with Python.</li> <li>Google Apps Script prototypes to push into Sheets.</li> </ul> <p>Main challenges:</p> <ol> <li><strong>Historical depth</strong> \u2192 APIs/RSS don\u2019t go 10+ yrs back. Need scraping + Wayback Machine fallback.</li> <li><strong>Format mix</strong> \u2192 JSON, XML, PDFs, HTML, RSS\u2026 looking for stable parsing + cleaning workflows.</li> <li><strong>Automation</strong> \u2192 would love lig",
        "id": 3477538,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6nn5e/scraping_multisource_feminist_content_looking_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping multi-source feminist content \u2013 looking for strategies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/troywebber",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T13:12:54.484432+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T13:12:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone</p> <p>I maintain a medium size crawling operation.</p> <p>And have noticed around 200 spiders have stopped working all of which are using cloudflare.</p> <p>Before rotating proxies + scrapy impersonate have been enough to suffice.</p> <p>But it seems like cloudflare have really ramped up the protection, I do not want to result to using browser emulation for all of these spiders.</p> <p>Has anyone else noticed a change in their crawling processes today.</p> <p>Thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/troywebber\"> /u/troywebber </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6j3ki/cloudflare_update/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6j3ki/cloudflare_update/\">[comments]</a></span>",
        "id": 3475873,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6j3ki/cloudflare_update",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cloud-flare update?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T13:12:54.345178+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T13:01:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Welcome to the weekly discussion thread!</strong></p> <p>This is a space for web scrapers of all skill levels\u2014whether you&#39;re a seasoned expert or just starting out. Here, you can discuss all things scraping, including:</p> <ul> <li>Hiring and job opportunities</li> <li>Industry news, trends, and insights</li> <li>Frequently asked questions, like &quot;How do I scrape LinkedIn?&quot;</li> <li>Marketing and monetization tips</li> </ul> <p>If you&#39;re new to web scraping, make sure to check out the <a href=\"https://webscraping.fyi\">Beginners Guide</a> \ud83c\udf31</p> <p>Commercial products may be mentioned in replies. If you want to promote your own products and services, continue to use the <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">monthly thread</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 3475872,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6itxj/weekly_webscrapers_hiring_faqs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Webscrapers - Hiring, FAQs, etc",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Blaze0297",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T13:12:54.591685+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T12:35:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to scrape these types of events using puppeteer.</p> <p>Here is a site that I am using to test this <a href=\"https://stream.wikimedia.org/v2/stream/recentchange\">https://stream.wikimedia.org/v2/stream/recentchange</a></p> <p>Only way I succeeded is using:</p> <blockquote> <p>new EventSource(&quot;<a href=\"https://stream.wikimedia.org/v2/stream/recentchange%22\">https://stream.wikimedia.org/v2/stream/recentchange&quot;</a>);</p> </blockquote> <p>and then using CDP:</p> <blockquote> <p>client.on(&#39;Network.eventSourceMessageReceived&#39; ....</p> </blockquote> <p>But I want to make a listener on a existing one not to make a new one with new EventSource</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Blaze0297\"> /u/Blaze0297 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6i9yy/scraping_eventstream_server_side_events/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.c",
        "id": 3475874,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6i9yy/scraping_eventstream_server_side_events",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping EventStream / Server Side Events",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Basic-Disaster1535",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T07:10:41.831324+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T06:30:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Will scraping a sportsbook for odds get you in trouble? Thats public information right or am I wrong. can anyone fill me in on the proper way of doing this or just pay for the expensive api?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Basic-Disaster1535\"> /u/Basic-Disaster1535 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6c7db/web_scraping_info/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6c7db/web_scraping_info/\">[comments]</a></span>",
        "id": 3473773,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6c7db/web_scraping_info",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scraping info",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/joey_tribb_911",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-09-02T05:00:28.107602+00:00",
        "date_dead_since": null,
        "date_published": "2025-09-02T04:57:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is it possible as Camoufox is a fork of Firefox?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/joey_tribb_911\"> /u/joey_tribb_911 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6ao5m/native_messaging_on_camoufox/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n6ao5m/native_messaging_on_camoufox/\">[comments]</a></span>",
        "id": 3473270,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n6ao5m/native_messaging_on_camoufox",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Native Messaging on Camoufox?",
        "vote": 0
    }
]