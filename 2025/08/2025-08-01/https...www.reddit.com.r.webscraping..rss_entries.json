[
    {
        "age": null,
        "album": "",
        "author": "/u/Extension_Track_5188",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-01T17:46:15.607366+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-01T07:29:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/webscraping\">r/webscraping</a>,</p> <p>I need to scale my existing web crawling script from sequential to 500 concurrent crawls. How?</p> <p>I don&#39;t necessarily need proxies/IP rotation since I&#39;m only visiting each domain up to 30 times (the crawler scrapes up to 30 pages of my interest within the website). I need help with infrastructure and network capacity.</p> <p>What I need:</p> <ul> <li>Total workload: ~10 million pages across approximately 500k different domains</li> <li>Crawling within a website ~20 pages per website (ranges from 5-30)</li> </ul> <p>Current Performance Metrics on Sequential crawling:</p> <ul> <li>Average: ~3-4 seconds per page</li> <li>CPU usage: &lt;15%</li> <li>Memory: ~120MB</li> </ul> <p>Can you explain what are the steps to scale my current setup to ~500 concurrent crawls?</p> <p>What I Think I Need Help With:</p> <ul> <li>Infrastructure - Should I use: Multiple VPS instances? Or Kubernetes/contain",
        "id": 3227957,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1meq2hv/scaling_sequential_crawler_to_500_concurrent",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scaling sequential crawler to 500 concurrent crawls. Need Help!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoModerator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-01T17:46:15.266864+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-01T03:00:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello and howdy, digital miners of r/webscraping!</p> <p>The moment you&#39;ve all been waiting for has arrived - it&#39;s our once-a-month, no-holds-barred, show-and-tell thread!</p> <ul> <li>Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you&#39;ve just unleashed on the world?</li> <li>Maybe you&#39;ve got a ground-breaking product in need of some intrepid testers?</li> <li>Got a secret discount code burning a hole in your pocket that you&#39;re just itching to share with our talented tribe of data extractors?</li> <li>Looking to make sure your post doesn&#39;t fall foul of the community rules and get ousted by the spam filter?</li> </ul> <p>Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!</p> <p>Just a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let&#39;s get ",
        "id": 3227955,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1meled5/monthly_selfpromotion_august_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Monthly Self-Promotion - August 2025",
        "vote": 0
    }
]