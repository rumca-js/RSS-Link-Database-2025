[
    {
        "age": null,
        "album": "",
        "author": "/u/Philognosis777",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T21:26:07.894688+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T21:08:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Do you think web scraping is a beginner-friendly career for someone who knows how to code? Is it easy to build a portfolio and apply for small freelance gigs? How valuable are web scraping skills when combined with data manipulation tools like Pandas, SQL, and CSV?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Philognosis777\"> /u/Philognosis777 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ms7t2l/web_scraper_for_beginners/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ms7t2l/web_scraper_for_beginners/\">[comments]</a></span>",
        "id": 3346417,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ms7t2l/web_scraper_for_beginners",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scraper for beginners",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gawddangitno",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T19:16:12.869336+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T18:10:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! I am a beginner programmer and new to web scraping. I built a simple social media bot to download Insta Reels, then repost them for me with credit (of course). I kept getting clapped by Insta for automation. I want to continue using the script, but I am worried that it will be traced back to my main account. I was going to run it in a VM and use a dedicated static residential IP. I want to run 1-2 of these automated accounts. Is this enough to avoid a huge ban on all my real accounts?</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gawddangitno\"> /u/gawddangitno </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ms2zgb/how_to_avoid_a_huge_ban_ip_tracking_hiding/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ms2zgb/how_to_avoid_a_huge_ban_ip_tracking_hiding/\">[comments]</a></span>",
        "id": 3345841,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1ms2zgb/how_to_avoid_a_huge_ban_ip_tracking_hiding",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to avoid a huge Ban? IP tracking/ hiding",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PinguinoCulino",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T14:56:10.848283+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T14:09:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I recently built a small open-source tool for scraping metadata from Hugging Face models and datasets pages and thought it might be useful for others working with HF\u2019s ecosystem. The tool collects information such as the model name, author, tags, license, downloads, and likes, and outputs everything in a CSV file.</p> <p>I originally built this for another personal project, but I figured it might be useful to share. It works through the Hugging Face API to fetch model metadata in a structured way.</p> <p>Here is the repo:<br/> <a href=\"https://github.com/DiegoConce/HuggingFaceMetadataScraper\">https://github.com/DiegoConce/HuggingFaceMetadataScraper</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PinguinoCulino\"> /u/PinguinoCulino </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mrwc0i/opensource_tool_to_scrape_hugging_face_models_and/\">[link]</a></span> &#32; <span",
        "id": 3344606,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mrwc0i/opensource_tool_to_scrape_hugging_face_models_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open-source tool to scrape Hugging Face models and datasets metadata",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Enzo034567",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T14:56:11.360353+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T12:41:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What kind of project involving web scraping can I make? For example i have Made a project using pandas and ML to predict results of serie A matches italian league.How can I integrate web scraping in it or what other project ideas can you suggest me. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Enzo034567\"> /u/Enzo034567 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mru5gj/oss_project/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mru5gj/oss_project/\">[comments]</a></span>",
        "id": 3344608,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mru5gj/oss_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "OSS project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Similar-Onion-6728",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T14:56:11.062771+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T10:57:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently finished a project where the client had a list of 5000+ Swedish companies but <strong>no official websites</strong>. The client needs search the official websites and collect all <strong>CEOs &amp; Project Managers&#39;</strong> contact emails</p> <p>Challenge:</p> <ul> <li>Find each company&#39;s correct domain, local yellow pages websites sometimes occupy the search results</li> <li>Identify which emails are CEO &amp; Project Manager emails</li> <li>Avoid spam or nonsenses like [<a href=\"mailto:user@example.com\">user@example.com</a>](mailto:<a href=\"mailto:user@example.com\">user@example.com</a>) or [2@css](mailto:2@css)...</li> </ul> <p>My approach:</p> <ol> <li><strong>Automated Google search</strong> with yellow page website filtering - with fuzzy matching</li> <li><strong>Full site crawl</strong> under that domain \u2192 collect all emails found</li> <li><strong>Context-based classification</strong>: for each email, grab 500 chars around it",
        "id": 3344607,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mrrxld/how_i_scraped_5000_verified_ceo_pm_contacts_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How I scraped 5,000+ verified CEO & PM contacts from Swedish company",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Impossible-Box6600",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T11:41:09.200293+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T10:46:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Has anyone had any success running Camoufox concurrently? No matter what I do, whether it&#39;s in Docker or on the host, when I run multiple concurrent requests, the page refuses to commit and the Camoufox server will not load any further requests until I restart the server. It just spawns an empty browser, which eventually raises a Playwright exception. I&#39;ve experienced this on occasion with Playwright, but Camoufox appears much less stable, despite having the same API. The only workaround I&#39;ve found is running Camoufox in parallel and ensuring that each worker is only serving one synchronous request at a time, which is far more costly in terms of complexity and resource utilization.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Impossible-Box6600\"> /u/Impossible-Box6600 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mrrq6d/camoufox_issues_with_concurrency/\">[link]</a></span>",
        "id": 3343633,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mrrq6d/camoufox_issues_with_concurrency",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Camoufox Issues With Concurrency",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Willing-Pen5729",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-16T05:11:05.737223+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-16T03:50:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Since I&#39;m running a motivational page I&#39;ve realized that theres certain reels only that will pop off time and time again. like if there are 10 pages (fan pages) and 100 videos, only 10 videos will go viral. its the same 10 videos every time. Run them on a brand new page and they&#39;ll go viral. </p> <p>so I was thinking to myself: why not use a spy tool that allows me to find reels from a select group of pages I choose that have gotten the most views, likes, and comments so I only post those over and over again?</p> <p>The program would ask me something like:<br/> 1) Enter the handles of those pages</p> <p>and then give me 10 of their most viewed videos, filtering them by most liked and most comments.</p> <p>Is that possible? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Willing-Pen5729\"> /u/Willing-Pen5729 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mrjwpn/ig_scraper_is_t",
        "id": 3342185,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mrjwpn/ig_scraper_is_this_idea_possible",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "IG scraper. is this idea possible?",
        "vote": 0
    }
]