[
    {
        "age": null,
        "album": "",
        "author": "/u/Afraid-Layer7383",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-08T22:28:03.258048+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-08T15:33:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Howdy! I work as a corporate communications researcher for a small research consulting company (~150 employees) that relatively recently shifted from a &quot;who said what on The Hill&quot; reporting company to a &quot;We analyze key conversations and provide data-driven insights&quot; posture. But we have none of the necessary infrastructure.</p> <p>We are a spreadsheet-focused org, and most members of the team/company have low tech literacy/skills. My role currently is to drive process design/improvement and support data-intensive (read &quot;anything involving quantitative analysis, no matter how small&quot;) projects.</p> <p>I&#39;ve built out a couple of data pipelines for the team so far, mostly focused on collecting and analyzing social media content, but have yet to find a solution for monitoring corporate newsrooms. I&#39;ve written scrapers for individual pressrooms and for aggregators (i.e., 3BL for ESG-related pressers), but we need to imp",
        "id": 3287162,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mkydvc/anyone_have_experience_scraping_corporate",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone Have Experience Scraping Corporate Pressrooms at Scale?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jam0_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-08T15:58:09.928697+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-08T15:27:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks, I\u2019m on an aggressive learning journey for web scraping and need some real use cases. Here\u2019s my pitch\u2026</p> <p>Reply (or DM) a site + the type of content you are looking to scrape and I\u2019ll send back my suggestions on how you should approach it. </p> <p>What you\u2019ll get: - evaluation of site complexity and suggestions on the most effective approach (anti-bot measures, apis found &amp; tested, static vs. Dynamic content etc) - notes on navigation structure, patterns, any site maps found etc - notes on selectors, page patterns, and consistency between data on pages</p> <p>What you won\u2019t get: - actual code for crawlers, extraction, parsing / cleaning - pipeline or infrastructure set up or notes</p> <p>What I ask in return: - feedback on if you used the info effectively, or, if it the suggestions were off where did I make a mistake.</p> <p>Cheers, </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jam0_\"> /u/Jam",
        "id": 3284411,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mky910/what_are_you_trying_to_scrape_ill_attempt_to_tell",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are you trying to scrape? - I\u2019ll (attempt to) tell you how.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheDoomfire",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-08T10:33:12.260806+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-08T10:32:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have always just webscraped and saved all the data in a json file, which I then replace over my old one. And it has worked for a few years. Primarly using python requests_html (but planning on using more scrapy since I never get request limits using it)</p> <p>Now I run across a issue where I cant simply get everything I want from just a page. And I certainly will have a hard time to get older data. The websites are changing and I sometimes need to change website source and just get parts of data and put it together myself. And I most likely just want to add to my existing data instead of just replacing the old one.</p> <p>So how do you guys handle storing the data and adding to it from several sources? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheDoomfire\"> /u/TheDoomfire </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mkrnlt/how_to_handle_the_data/\">[link]</a></span> &#32; <spa",
        "id": 3281842,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mkrnlt/how_to_handle_the_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to handle the data?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DifficultEvening3608",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-08T08:23:23.337925+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-08T07:20:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i know i know vibe coding is not ideal, i should learn it myself. i have experience with coding in python for like 6ish months, but in a COMPLETELY different niche, and APIs plus webscraping have been super daunting at first, despite all the tutorials and posts ive read.</p> <p>i need this project done ASAP, so yes, i know \u2013 i used ai. however, i still ran into a wall, particularly when it came to working with certain third-party tools for x (since the platform\u2019s official developer access is too expensive for me right now). i only need to scrape 1 account that has 1000 posts and put it into a csv with certain conditions met (as you do with data), but AI has been completely incapable of doing this, yes, even claude code.</p> <p>i\u2019ve tried different services, but both times the code just wasn\u2019t giving what i want (and i tried for hours).</p> <p>is it my prompting \u2013 for those who may have experience with this \u2013 or should i just give up with \u2018vibe coding\u2019",
        "id": 3281182,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mkooaz/webscraping_with_ai",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "webscraping with AI",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vvivan89",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-08T01:53:04.244790+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-08T01:13:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So when I&#39;m doing a certain request using an API of a public facing website, I have different results depending on where I&#39;m doing it from. All the request data and headers is the same.</p> <p>- When doing from local, I get status 200 and the needed data</p> <p>- When doing from Google Cloud Function, I&#39;m getting status 400 &#39;Bad request&quot; with no data. There is also this header in the response: &#39;x-amzn-errortype&#39;: &#39;ForbiddenException&#39;. This started to happen only recently.</p> <p>Is this an IP ban? If so, is there any workaround when using Google Cloud Functions to send requests? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vvivan89\"> /u/vvivan89 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mkhswc/amazon_aws_forbiddenexception_does_this_mean_im/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mkhswc/amazon_a",
        "id": 3279683,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mkhswc/amazon_aws_forbiddenexception_does_this_mean_im",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Amazon AWS \"ForbiddenException\" - does this mean I'm banned by IP?",
        "vote": 0
    }
]