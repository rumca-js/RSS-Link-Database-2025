[
    {
        "age": null,
        "album": "",
        "author": "/u/MajorMagazine3716",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T22:26:14.627842+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T22:17:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey yall, Im relatively new to Webscraping, and I&#39;m wondering if there are any qualms my vps provider will have with me if I run a webscraper that takes up a considerable amount of ram usage and CPU usage (within constraints of course)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MajorMagazine3716\"> /u/MajorMagazine3716 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mz99bb/webscraping_on_vps_issues/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mz99bb/webscraping_on_vps_issues/\">[comments]</a></span>",
        "id": 3408711,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mz99bb/webscraping_on_vps_issues",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webscraping on VPS Issues",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mrcool654321",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T21:20:48.000928+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T21:03:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want a web scraper that scrapes messages from threads in a specific channel</p> <p>Is there anything like that?<br/> Just having the content, links, etc</p> <p>It would only scrape recent messages as they will sent to an LLM to sort</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mrcool654321\"> /u/Mrcool654321 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mz7fye/scraping_a_discord_server/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mz7fye/scraping_a_discord_server/\">[comments]</a></span>",
        "id": 3408451,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mz7fye/scraping_a_discord_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping a Discord server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/anjobanjo102",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T21:20:47.793189+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T20:24:16+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mz6fbr/scraping_300k_listings_in_2_hours_every_day_roast/\"> <img src=\"https://b.thumbs.redditmedia.com/Xh6gbO9skr5mH9BHuZDIplhSoqxqowlAS8suduYLPWA.jpg\" alt=\"Scraping 300k listings in 2 hours every day. Roast my setup?\" title=\"Scraping 300k listings in 2 hours every day. Roast my setup?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey, guys. Just recently figured out about this subreddit. I just have a $7/month Hetzner VM where I host all my cron jobs + workers in for my site <a href=\"https://nipponhomes.com/\">https://nipponhomes.com/</a> . Everything seems to work, but I&#39;m always open to feedback.. Some info about my setup:</p> <ol> <li>No Docker.</li> <li>Local Redis instance.</li> <li>Cron job of scrapy feeds my Redis queues with URL&#39;s (runs daily).</li> <li>Three kinds of workers - listing processor ( seen in diagram), translation processor (LLM translations), and images processor (downloads an",
        "id": 3408450,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mz6fbr/scraping_300k_listings_in_2_hours_every_day_roast",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/Xh6gbO9skr5mH9BHuZDIplhSoqxqowlAS8suduYLPWA.jpg",
        "title": "Scraping 300k listings in 2 hours every day. Roast my setup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sjmittal",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T18:58:16.754412+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T18:45:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I have built a web scraper in Javascript using Playwright, where you can provide basically longitude and latitude of any city on the earth and it will scrape all the google reviews of all the places like restaurants, hospitals etc from that city. The final data is a wide table of following columns:</p> <pre><code>category,business_name,cid,maps_url,review_id,published,last_edited,author_name,author_profile_url,author_url,author_id,rating,text,language,lat,long,friendly_name,source,response_text,response_published,response_last_edited,images_count,images_json </code></pre> <p>I am planning to import this into some columnar database or text search/vector database.</p> <p>Also I was thinking of running the actual reviews via NLP or LLM to provide a context to a search engine and then users can find what they are looking for by simply asking questions to the search engine.</p> <p>Also with user data also embedded, we can also make this a social",
        "id": 3407566,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mz3ty9/web_scrapper_which_can_scrape_all_google_reviews",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web scrapper which can scrape all google reviews for any place",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HackerArgento",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T17:53:17.299391+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T16:52:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, recently i&#39;ve been working on a solver and writeup about arkorse, but i&#39;ve stumbled upon a wall, even though i&#39;m using fully legit BDA&#39;s i&#39;m still getting sent more and more waves of challenges, so i&#39;m guessing they flag stuff other than the BDA? It&#39;d be great if someone with some knowledge on it could shine some light on it</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HackerArgento\"> /u/HackerArgento </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mz0tl5/fully_reversed_arkorse_bda_but_still_not_getting/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mz0tl5/fully_reversed_arkorse_bda_but_still_not_getting/\">[comments]</a></span>",
        "id": 3407292,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mz0tl5/fully_reversed_arkorse_bda_but_still_not_getting",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fully reversed arkorse BDA but still not getting suppressed tokens",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Top-Journalist9785",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T16:48:09.088876+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T15:47:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi Everyone, </p> <p>I&#39;m new to web scraping and recently learned the basics through FreeCodeCamp&#39;s tutorials on Scrapy and Playwright. I&#39;m planning a project to scrape Amazon product listings and would appreciate your feedback on my approach. </p> <p>My Plan: </p> <ul> <li>Forward Proxy: ScrapingAnt&#39;s proxy service to avoid IP blocks.</li> <li>Browser Automation: Playwright to handle JavaScript-rendered content.</li> <li>Data Processing: Scrapy for building robust data pipelines and cleaning.</li> <li>Storage: MySQL database for structured data storage.</li> </ul> <p>Could you advise me on the type of thing I should look out for, like rate limiting strategies, Playwright&#39;s stealth modes against Amazon detection or perhaps a better proxy solutions I should consider. </p> <p>Many Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Top-Journalist9785\"> /u/Top-Journalist9785 </a> <br/> <span><",
        "id": 3406880,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1myz37t/1st_time_scrapping_amazon_any_helpful_tips",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "1st Time scrapping Amazon, any helpful tips",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Nervous_Star_8721",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T13:31:26.987869+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T13:13:01+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1myv8sq/14k_users_in_boring_link_grabber_scraping_tool/\"> <img src=\"https://preview.redd.it/mmgotdbwvykf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=16f8b231ee8c644090c64357d467ff28e0daa13a\" alt=\"14k+ users in boring Link Grabber scraping tool!\" title=\"14k+ users in boring Link Grabber scraping tool!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi here,</p> <p>This is 1 Year after the initial app release and now I have:</p> <ul> <li>14k+ users in Chrome Web Store</li> <li>Top 3 rankings in Google SEO</li> <li>100+ install every day</li> <li>80+ subscribers!</li> </ul> <p><strong>What is all about</strong></p> <p>Link Grabber is ultimate solution for manual scrapping links from open webpages, scrape one-by-one, from entire page, from multiple pages, save, filter, sort, export... All in one,</p> <p><strong>Why posting here?</strong></p> <ul> <li>Looking for options to grow the app and fair feedback fro",
        "id": 3405826,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1myv8sq/14k_users_in_boring_link_grabber_scraping_tool",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/mmgotdbwvykf1.png?width=640&crop=smart&auto=webp&s=16f8b231ee8c644090c64357d467ff28e0daa13a",
        "title": "14k+ users in boring Link Grabber scraping tool!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TownRough790",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T12:27:41.459650+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T11:34:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,<br/> I\u2019m a complete beginner at this. District is a ticket booking website here in India, and I\u2019d like to experiment with extracting information such as how many tickets are sold for each show of a particular movie by analyzing the seat map available on the site.</p> <p>Could you give me some guidance on where to start? By background, I\u2019m a database engineer, but I\u2019m doing this purely out of personal interest. I have some basic knowledge of Python and solid experience with SQL/databases (though I realize that may not help much here).</p> <p>Thanks in advance for any pointers!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TownRough790\"> /u/TownRough790 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myt86s/scraping_a_movie_booking_site/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myt86s/scraping_a_movie_booking_site/\">[comments]<",
        "id": 3405486,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1myt86s/scraping_a_movie_booking_site",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping a movie booking site",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ag789",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T10:15:44.077877+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T10:02:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>learning the ropes as well but that selenium webdriver<br/> <a href=\"https://www.selenium.dev/documentation/webdriver/\">https://www.selenium.dev/documentation/webdriver/</a></p> <p>Is quite a thing, I&#39;m not sure how far it can go where scraping goes.<br/> is playwright better in any sense?<br/> <a href=\"https://playwright.dev/\">https://playwright.dev/</a><br/> I&#39;ve not (yet) tried playwright </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ag789\"> /u/ag789 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myrnjn/selenium_webdriver/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myrnjn/selenium_webdriver/\">[comments]</a></span>",
        "id": 3404905,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1myrnjn/selenium_webdriver",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "selenium webdriver",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ornery_Minute4132",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T14:36:31.664250+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T07:26:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, work for purposes I would need to find 1000+ domains for companies, based on an excel file where I only have the names of the companies. I\u2019ve tried the python code from an AI tool but it hasn\u2019t worked out perfectly\u2026 I don\u2019t have much python experience either, just some very basic stuff\u2026 can someone maybe help here? :) Many thanks!</p> <p>Aleks </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ornery_Minute4132\"> /u/Ornery_Minute4132 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myp7jq/extract_1000_domains_with_python/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myp7jq/extract_1000_domains_with_python/\">[comments]</a></span>",
        "id": 3406205,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1myp7jq/extract_1000_domains_with_python",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Extract 1000+ domains with python",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/laataisu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-24T05:55:51.135306+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-24T05:08:36+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/\"> <img src=\"https://b.thumbs.redditmedia.com/FZkgEyKIV8ObPaw9ERYNh5FX7uNzxNsnVUr3qUrdVwU.jpg\" alt=\"Tried AI for real-world scraping\u2026 it\u2019s basically useless\" title=\"Tried AI for real-world scraping\u2026 it\u2019s basically useless\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>AI scraping is kinda a joke<strong>.</strong><br/> Most demos just scrape <em>toy websites</em> with no bot protection. The moment you throw it at a real, dynamic site with proper defenses, it faceplants hard.</p> <p>Case in point: I asked it to grab data from <a href=\"https://elhkpn.kpk.go.id/\">https://elhkpn.kpk.go.id/</a> by searching <strong>\u201cPrabowo Subianto\u201d</strong> and pulling the dataset.</p> <p>What I got back?</p> <ul> <li>Endless scripts that don\u2019t work \ud83e\udd21</li> <li>Wasted tokens &amp; time</li> <li>Zero progress on bypassing captcha</li> </ul> <p>So yeah\u2026 if your site has more tha",
        "id": 3404071,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/FZkgEyKIV8ObPaw9ERYNh5FX7uNzxNsnVUr3qUrdVwU.jpg",
        "title": "Tried AI for real-world scraping\u2026 it\u2019s basically useless",
        "vote": 0
    }
]