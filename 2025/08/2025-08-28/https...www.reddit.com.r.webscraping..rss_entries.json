[
    {
        "age": null,
        "album": "",
        "author": "/u/FeeDisastrous3320",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T22:46:09.806411+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T19:42:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1n2m4b2/what_filters_do_you_need_for_a_long_list_of/\"> <img src=\"https://b.thumbs.redditmedia.com/gZObwC9zQHkngGuIZqS4sVeUf6Dwzdt_z_rMBH-eWgQ.jpg\" alt=\"What filters do you need for a long list of scraped emails?\" title=\"What filters do you need for a long list of scraped emails?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone! I\u2019m Herman.</p> <p>I recently built a side project \u2013 a Chrome extension that helps collect emails. While working on a new interface, I\u2019ve been wondering:</p> <p>Do you think it\u2019s useful to have filters for the collected email list? And if yes, what kind of filters would you use?</p> <p>So far, the only one I\u2019ve thought of is filtering by domain text.</p> <p>If you\u2019ve used similar extensions or ever wished for a feature like this, I\u2019d love to hear your thoughts or any recommendations!</p> <p><strong>PS</strong>: I\u2019ve read the subreddit rules carefully, and it seems fine to s",
        "id": 3444420,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2m4b2/what_filters_do_you_need_for_a_long_list_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/gZObwC9zQHkngGuIZqS4sVeUf6Dwzdt_z_rMBH-eWgQ.jpg",
        "title": "What filters do you need for a long list of scraped emails?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/iSayWait",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T20:18:17.235780+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T15:21:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I suppose you could prorgram a web crawler using selenium or playwright but would take forever to finish the process should the plan be to run this at least once a day. How would you setup your scraping approach for each of the posts (including downloading the PDFs) of this site?<br/> <a href=\"https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml\">https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iSayWait\"> /u/iSayWait </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2f7fg/impossible_to_webscrape/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2f7fg/impossible_to_webscrape/\">[comments]</a></span>",
        "id": 3443471,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2f7fg/impossible_to_webscrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Impossible to webscrape?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Relevant-Show-3078",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T15:43:41.126505+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T15:12:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a question for the community. </p> <p>I am trying to create a scraper that will scrape jobs from Bamboo, GreenHouse, and Lever for an internal project. I have tried Builtwith and can find some companies, but I know that there are way more businesses using these ATS solutions.</p> <p>Asking here to see if anyone can point me in the right direction or has any ideas.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Relevant-Show-3078\"> /u/Relevant-Show-3078 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2eyrn/trying_to_scrap_popular_ats_sites_looking_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2eyrn/trying_to_scrap_popular_ats_sites_looking_for/\">[comments]</a></span>",
        "id": 3441006,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2eyrn/trying_to_scrap_popular_ats_sites_looking_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to scrap popular ATS sites - looking for advise",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jmarbach",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T15:43:41.266951+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T15:06:22+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jmarbach\"> /u/jmarbach </a> <br/> <span><a href=\"https://anchorbrowser.io/blog/page-load-reliability-on-the-top-100-websites-in-the-us\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2es5q/browserbase_has_a_29_failure_rate_on_basic_page/\">[comments]</a></span>",
        "id": 3441007,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2es5q/browserbase_has_a_29_failure_rate_on_basic_page",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Browserbase has a 29% failure rate on basic page loads",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/techwriter500",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T14:37:44.546666+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T13:31:01+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to build an audience on Social Media for web scraping tools/services. </p> <p>Which roles or professionals would be most relevant to connect with? (e.g., data analysts, marketers, researchers, e-commerce folks, etc.)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/techwriter500\"> /u/techwriter500 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2cay3/which_roles_care_most_about_web_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n2cay3/which_roles_care_most_about_web_scraping/\">[comments]</a></span>",
        "id": 3440192,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2cay3/which_roles_care_most_about_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which roles care most about web scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/antvas",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T14:37:44.406060+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T13:06:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1n2bqfo/why_a_classic_cdp_bot_detection_signal_suddenly/\"> <img src=\"https://external-preview.redd.it/05jWJWrxgvCUQh4xxsYyd8A6UbWl7T_svL4XXRlQa3A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fae609e4b6d4deb25738ff813e2c7653e7e0eb2b\" alt=\"Why a classic CDP bot detection signal suddenly stopped working (and nobody noticed)\" title=\"Why a classic CDP bot detection signal suddenly stopped working (and nobody noticed)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Author here, I\u2019ve written a lot over the years about browser automation detection (Puppeteer, Playwright, etc.), usually from the defender\u2019s side. One of the classic CDP detection signals most anti-bot vendors used was hooking into how DevTools serialized errors and triggered side effects on properties like .stack.</p> <p>That signal has been around for years, and was one of the first things patched by frameworks like nodriver or rebrowser to make au",
        "id": 3440191,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2bqfo/why_a_classic_cdp_bot_detection_signal_suddenly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/05jWJWrxgvCUQh4xxsYyd8A6UbWl7T_svL4XXRlQa3A.png?width=640&crop=smart&auto=webp&s=fae609e4b6d4deb25738ff813e2c7653e7e0eb2b",
        "title": "Why a classic CDP bot detection signal suddenly stopped working (and nobody noticed)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jayn35",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T22:46:10.097678+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T12:05:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello. For a long time i have been trying to find an intelligence LLM navigation based webscraper where i can give it a url and say, go get me all the tech docs for this api relevant to my goals starting from this link and it llm validates pages and content and deep links and navigates based on the markdown links from each pages scrape and only get me the docs i need smartly and turns it into a single markdown file at the end that i can feed to AI </p> <p>I dont get why nothing like this seems to exist yet because its obviously easy to make at this point. Tried a lot of things, crawl4ai, firecrawl, scrapegraph etc and they all dont quite do this to the full degree and make mistakes and there are too man complex settings you need to setup to ensure you get what you want where using intelligent llm analysis and navigating would avoid this tedious deterministic setup.</p> <p>Anybody know of any tool please, im getting sick of manually copying downloading",
        "id": 3444421,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n2ae7k/ai_intelligent_navigating_validating_prompt_based",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI Intelligent Navigating Validating Prompt Based Scraper? Any exist?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Turbulent-Ad9903",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-28T09:17:39.172836+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-28T08:09:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to automate a Dropbox feature which is not currently present within the API. I tried using webdrivers and they work perfectly fine on my local machine. However, I need to have this feature on a server. But when I try to login it detects server and throws captcha at me. That almost never happens locally. I tried camoufox in virtual mode but this didn&#39;t help either. </p> <p>Here&#39;s a simplified example of the script for logging in:</p> <pre><code>from camoufox import Camoufox email = &quot;&quot; password = &quot;&quot; with Camoufox(headless=&quot;virtual&quot;) as p: try: page = p.new_page() page.goto(&quot;https://www.dropbox.com/login&quot;) print(&quot;Page is loaded!&quot;) page.locator(&quot;//input[@type=&#39;email&#39;]&quot;).fill(email) page.locator(&quot;//button[@type=&#39;submit&#39;]&quot;).click() print(&quot;Submitting email&quot;) page.locator(&quot;//input[@type=&#39;password&#39;]&quot;).fill(password) page.locator(&quo",
        "id": 3438079,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n26e5e/how_do_i_hide_remote_server_finger_prints",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I hide remote server finger prints?",
        "vote": 0
    }
]