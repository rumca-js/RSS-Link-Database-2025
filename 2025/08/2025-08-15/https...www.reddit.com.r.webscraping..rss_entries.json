[
    {
        "age": null,
        "album": "",
        "author": "/u/Azerotth",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-15T23:46:07.329184+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-15T23:31:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have tried many different ways to avoid captchas on the websites I\u2019ve been scraping. My only solution so far has been using a extension with Playwright. It works wonderfully, but unfortunately, when I try to use it with proxies to avoid IP blocks, the captcha simply doesn\u2019t load to be solved. I\u2019ve tried many different proxy services, but it\u2019s been in vain \u2014 with none of them the captcha loads or appears, making it impossible to solve and continue with each script\u2019s process. Could anyone help me with this? Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Azerotth\"> /u/Azerotth </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mre4cq/captcha_doesnt_load_with_proxies/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mre4cq/captcha_doesnt_load_with_proxies/\">[comments]</a></span>",
        "id": 3341082,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mre4cq/captcha_doesnt_load_with_proxies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "CAPTCHA doesn't load with proxies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Azerotth",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-15T22:41:02.979135+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-15T22:14:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have tried many different ways to avoid captchas on the websites I\u2019ve been scraping. My only solution so far has been using the Anti-Captcha extension with Playwright and Firefox. It works wonderfully, but unfortunately, when I try to use it with proxies to avoid IP blocks, the captcha simply doesn\u2019t load to be solved. I\u2019ve tried many different proxy services, but it\u2019s been in vain \u2014 with none of them the captcha loads or appears, making it impossible to solve and continue with each script\u2019s process. Could anyone help me with this? Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Azerotth\"> /u/Azerotth </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mrc69u/recaptcha_doesnt_load_with_proxies/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mrc69u/recaptcha_doesnt_load_with_proxies/\">[comments]</a></span>",
        "id": 3340764,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mrc69u/recaptcha_doesnt_load_with_proxies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "reCAPTCHA doesn't load with proxies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jloking",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-15T18:21:15.163455+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-15T17:39:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m toying with the idea of a decentralized scraping <a href=\"https://scrapions.xyz\">platform</a>. Scrapers share workloads, verify each other\u2019s results, and maybe earn tokens for contributing. Think libp2p + blockchain incentives.</p> <p>Could this be a real alternative to traditional scraping setups?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jloking\"> /u/jloking </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mr4r6q/could_a_decentralized_web_scraping_network/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mr4r6q/could_a_decentralized_web_scraping_network/\">[comments]</a></span>",
        "id": 3339298,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mr4r6q/could_a_decentralized_web_scraping_network",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Could a decentralized web scraping network actually work?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Excellent-Yam7782",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-15T11:51:12.786414+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-15T11:07:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m learning Electron by creating a multi-browser with auth proxies. I\u2019ve noticed that a lot of the time my browsers are flagged by bot detection or fingerprinting systems. Even when using a preloader and a few tweaks or testing on sites that check browser fingerprints, the results often indicate I\u2019m being detected as automated.</p> <p>I\u2019m looking for resources, guides, or advice on how to better understand browser fingerprinting and ways to make my Electron instances behave more like \u201creal\u201d browsers. Any tips or tutorials would be super helpful!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Excellent-Yam7782\"> /u/Excellent-Yam7782 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mquncl/electron_browserwindow_bot_detection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mquncl/electron_browserwindow_bot_detection/\">[comments]</a></span>",
        "id": 3336354,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mquncl/electron_browserwindow_bot_detection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Electron browserWindow bot detection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tangawusi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-15T08:37:17.797429+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-15T08:16:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I find Rust to be more robust compared to Python.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tangawusi\"> /u/tangawusi </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mqrja2/anyone_here_use_rust/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mqrja2/anyone_here_use_rust/\">[comments]</a></span>",
        "id": 3335264,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mqrja2/anyone_here_use_rust",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone here use Rust?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/r_Madlad",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-15T03:10:25.389411+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-15T02:58:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For context, I am helping a company write a tool to sync their internal pages from Google Sites to a markdown based system daily. I&#39;ve used google takeouts to download all the raw HTML files from Sites, but it looks like google embeds a lot of scripts and styles deep into their HTML files, that&#39;s very difficult to remove without damaging the actual content. </p> <p>I&#39;ve managed to just delete some of the scripts and styles using BeautifulSoup, but I couldn&#39;t remove the rest without deleting content. This is where I passed the partially-cleaned HTML file to a Large Language Model to handle the rest, but this burned through API credits like crazy every time it was run. Because this would run on a daily basis, this approach would not be the best in the long run.</p> <p>I was hoping if anyone here has any suggestions on a better way to parse the GSites HTML, anything helps :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https",
        "id": 3334138,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mqlmcw/how_do_i_scrape_clean_html_from_google_sites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I scrape clean HTML from Google Sites?",
        "vote": 0
    }
]