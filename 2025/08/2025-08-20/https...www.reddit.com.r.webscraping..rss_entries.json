[
    {
        "age": null,
        "album": "",
        "author": "/u/IcyBackground5204",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T20:13:26.151689+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T20:11:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi so I have tried multiple projects now. You can check me at alexrosulek.com. Now I was trying to get listings for my new project nearestdoor.com. I needed data from multiple sites and formatted well. I used Crawl4ai, it has powerful features but nothing was that easy to use. This was troublesome and about half way through the project I decided to create my own scraping platform with it. Meet Crawl4.com, url discovery and querying. Markdown filtering and extraction with a lot of options; all based on crawl4ai with a redis task management system.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IcyBackground5204\"> /u/IcyBackground5204 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvpv0f/make_a_scrape_platform_would_love_testers_beta/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvpv0f/make_a_scrape_platform_would_love_testers_beta/\">[comments]</a",
        "id": 3377511,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvpv0f/make_a_scrape_platform_would_love_testers_beta",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Make a scrape platform. Would love testers. *Beta*",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/QTop1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T20:13:26.320225+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T19:49:16+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mvp9ni/monopoly_game_web_scrape_project_with_ai_i_have_0/\"> <img src=\"https://preview.redd.it/gj8kbl42b8kf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=54e4210dce3d829a49d77bc4b873fb0eaa1b0a72\" alt=\"Monopoly Game Web Scrape Project With AI (I have 0 coding skill)\" title=\"Monopoly Game Web Scrape Project With AI (I have 0 coding skill)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Step 1: Forensic Analysis I started by moving from assumptions to evidence. Instead of trying to find the game state in static code, I used the AI to build a &quot;forensic&quot; crawler. My initial broad crawl gave me a huge list of all the IDs and classes on the page. The AI helped me filter this list to find the reliable, stable selectors (like data-participant-id) and flag the unstable ones to avoid.</p> <p>Step 2: Building the Scraper With a reliable map of the page, I built a robust scraper. I asked the AI to help me bu",
        "id": 3377512,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvp9ni/monopoly_game_web_scrape_project_with_ai_i_have_0",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/gj8kbl42b8kf1.png?width=108&crop=smart&auto=webp&s=54e4210dce3d829a49d77bc4b873fb0eaa1b0a72",
        "title": "Monopoly Game Web Scrape Project With AI (I have 0 coding skill)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Existing-Crow5098",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T20:13:26.505810+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T19:34:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello. Just wondering if anyone knows how to scrape YouTube comments and its replies? I need it for research but don&#39;t know how to code in Python. Is there an easier way or tool to do it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Existing-Crow5098\"> /u/Existing-Crow5098 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvov92/scraping_youtube_comments_and_its_replies/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvov92/scraping_youtube_comments_and_its_replies/\">[comments]</a></span>",
        "id": 3377513,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvov92/scraping_youtube_comments_and_its_replies",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping YouTube comments and its replies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thalesviniciusf",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T19:08:06.203841+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T18:32:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Share the project that you are working on! I&#39;m excited to know about different use cases :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thalesviniciusf\"> /u/thalesviniciusf </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvn57i/what_are_you_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvn57i/what_are_you_scraping/\">[comments]</a></span>",
        "id": 3376986,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvn57i/what_are_you_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are you scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Complete-Increase936",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T19:08:06.411734+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T18:31:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I&#39;m currently trying to find a book to help me learn web scraping and all things data harvesting related. From what I&#39;ve learn&#39;t so far all the Cloudfare and other bots etc are updated so regularly so I&#39;m not even sure a book would work. If you guys know of anything that would help me please let me know.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Complete-Increase936\"> /u/Complete-Increase936 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvn4i2/best_book_for_web_scrapingdata_mining_pipelines/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvn4i2/best_book_for_web_scrapingdata_mining_pipelines/\">[comments]</a></span>",
        "id": 3376987,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvn4i2/best_book_for_web_scrapingdata_mining_pipelines",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best book for web scraping/data mining/ pipelines etc?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Alarmed_Chest_5146",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T20:13:26.686110+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T17:16:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m a grad student doing non-commercial research on common ophthalmology conditions. I plan to run small-scale text &amp; data mining (TDM) on public, non-login pages from WebMD/Medscape.</p> <p>Scope (narrow and specific) </p> <ul> <li>~a dozen ophthalmic conditions (e.g., cataract, glaucoma, AMD, DR, etc.).</li> <li>For each condition, a few dozen articles (think dozens per condition, not site-wide).</li> <li>Text only (exclude images/videos/ads/comments).</li> <li>Data stays private on secured university servers; access limited to our team; no public redistribution of full text.</li> <li>Publications will show aggregate stats + short quotations with attribution; no full-text republication.</li> <li>Low request rate, respect robots.txt, immediate back-off on errors.</li> </ul> <p>What I think the policies mean (please correct me if wrong)</p> <ul> <li>WebMD/Medscape ToU generally allow personal, non-commercial, single-copy viewing; automated bulk co",
        "id": 3377514,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvkzss/scraperapi_webmdmedscape_is_small_private_tdm_ok",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ScraperAPI + WebMD/Medscape: is small, private TDM OK?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gerardelberg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T15:53:13.835512+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T15:30:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I know nothing about web scraping and very little about coding. For a hobby, I want to collect the name of links from multiple pages. They all link to the same place but are called different things every time. I tried using the Web scraper extension, but I can only use it to grab elements, not search for a specific link. I don&#39;t mind learning how to do this myself if that&#39;s what it takes, but is this possible, or am I on the wrong track? Is there an easier way to do it, or an existing tool?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gerardelberg\"> /u/gerardelberg </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvi140/can_i_scrape_for_a_specific_url/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvi140/can_i_scrape_for_a_specific_url/\">[comments]</a></span>",
        "id": 3375022,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvi140/can_i_scrape_for_a_specific_url",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I scrape for a specific URL?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ikram_Shah512",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T10:56:39.681860+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T08:54:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been working with web scraping and data collection for some time, and I usually build custom datasets from publicly available sources (like e-commerce sites, local businesses, job listings, and real estate platforms).</p> <p>Are there any marketplaces where people actually buy datasets (instead of just free sharing)?</p> <p>Would love to hear if anyone here has first-hand experience selling datasets, or knows which marketplaces are worth trying.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ikram_Shah512\"> /u/Ikram_Shah512 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mv9gyx/is_there_any_platform_where_we_can_sell_our/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mv9gyx/is_there_any_platform_where_we_can_sell_our/\">[comments]</a></span>",
        "id": 3372353,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mv9gyx/is_there_any_platform_where_we_can_sell_our",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there any platform where we can sell our datasets online?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/unstopablex5",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T06:08:52.860661+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T06:02:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t know if this works or if its worth it but it is a thought child of mine. </p> <p>For this setup there are 2 services - service A and service B. Service A sets up an automated browser and moves around a website normally. Not scraping but harvesting browser cookies and other useful data. It stores these in the DB.</p> <p>Service B, selects one of these minted cookies and uses it to send requests to internal endpoints saving the results to a data sink.</p> <p>Assuming they are in sync in terms of endpoints proxy and fingerprints, this setup should ensure constant low latency updates from even hard to scrape sites.</p> <p>Im curious what others think about this approach? I havent attempted it because this is a lot of engineering for a side project but I imagine if I had to do a hard to scrape side for work I&#39;d give it a try. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/unstopablex5\"> /u/unstopable",
        "id": 3371346,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mv6pps/has_anyone_tried_this_architecture_session",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone tried this architecture? Session Harvester + API Replayer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gregpr07",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-20T00:42:14.097013+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-20T00:10:11+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1muznhi/browser_use_no_longer_uses_playwright_but_pure_cdp/\"> <img src=\"https://external-preview.redd.it/PAkB__Sc2858_7NDxUFfj6ZkN3Ye-xE5o5NpaArXYBY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47dfe779dc83da89090b7cb95de618379850ccad\" alt=\"Browser Use no longer uses Playwright, but pure CDP\" title=\"Browser Use no longer uses Playwright, but pure CDP\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gregpr07\"> /u/gregpr07 </a> <br/> <span><a href=\"https://github.com/browser-use/browser-use/releases\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1muznhi/browser_use_no_longer_uses_playwright_but_pure_cdp/\">[comments]</a></span> </td></tr></table>",
        "id": 3370065,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1muznhi/browser_use_no_longer_uses_playwright_but_pure_cdp",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/PAkB__Sc2858_7NDxUFfj6ZkN3Ye-xE5o5NpaArXYBY.png?width=640&crop=smart&auto=webp&s=47dfe779dc83da89090b7cb95de618379850ccad",
        "title": "Browser Use no longer uses Playwright, but pure CDP",
        "vote": 0
    }
]