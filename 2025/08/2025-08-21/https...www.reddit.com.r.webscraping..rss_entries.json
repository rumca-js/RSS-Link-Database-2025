[
    {
        "age": null,
        "album": "",
        "author": "/u/WalkerSyed",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T22:58:40.841044+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T21:23:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mwnpqr/alibaba_cloud_slider/\"> <img src=\"https://preview.redd.it/xyowkfcqwfkf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e910c9be495cf82dfbf219c66151b826cdaae16a\" alt=\"AliBaba Cloud Slider\" title=\"AliBaba Cloud Slider\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Any method to solve the above captcha. I looked into 2captcha but they don&#39;t provide any solution for this.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WalkerSyed\"> /u/WalkerSyed </a> <br/> <span><a href=\"https://i.redd.it/xyowkfcqwfkf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwnpqr/alibaba_cloud_slider/\">[comments]</a></span> </td></tr></table>",
        "id": 3387624,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwnpqr/alibaba_cloud_slider",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/xyowkfcqwfkf1.png?width=640&crop=smart&auto=webp&s=e910c9be495cf82dfbf219c66151b826cdaae16a",
        "title": "AliBaba Cloud Slider",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Maximillion_P",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T21:53:41.688278+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T20:45:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m attempting to find a person&#39;s profile on Meta using the Instant Data Scraper (Chrome extension). I am experiencing an issue where I receive a different total number of search results each time I use the extension, as Meta&#39;s search page stops loading - presumably because of the large number of profiles that meet my search conditions. Is there any free scraping tool available to retrieve all the profiles that meet certain search conditions that doesn&#39;t depend upon continuously scrolling webpages down?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Maximillion_P\"> /u/Maximillion_P </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwmpt0/meta_search_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwmpt0/meta_search_scraper/\">[comments]</a></span>",
        "id": 3387225,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwmpt0/meta_search_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Meta Search Scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/themasterofbation",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T19:29:16.413923+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T19:22:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m getting lots of data using the public API, however, I&#39;ve tried almost every open source tool out there with residential proxies applied and can&#39;t get the &quot;following&quot; of non-private accounts.</p> <p>Anyone can help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/themasterofbation\"> /u/themasterofbation </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwkj9m/reliable_way_of_scraping_insta_following/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwkj9m/reliable_way_of_scraping_insta_following/\">[comments]</a></span>",
        "id": 3386235,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwkj9m/reliable_way_of_scraping_insta_following",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Reliable way of scraping insta Following?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ConferencePure6652",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T19:29:16.583401+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T19:18:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Title, i&#39;m currently reversing arkorse funcaptcha and it seems i&#39;ll need canvas fingerprints, but i don&#39;t want to set up a website that gets at most a few thousands, since i&#39;ll probably need hundred of thousands of fingerprints</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ConferencePure6652\"> /u/ConferencePure6652 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwkfes/is_there_any_way_to_getgenerate_canvas_fps/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwkfes/is_there_any_way_to_getgenerate_canvas_fps/\">[comments]</a></span>",
        "id": 3386236,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwkfes/is_there_any_way_to_getgenerate_canvas_fps",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there any way to get/generate canvas fps",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kamilkowal21",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T19:29:16.750826+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T19:06:23+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mwk3x4/website_change_alerts_might_be_the_most/\"> <img src=\"https://preview.redd.it/t2zsoob48fkf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5aff390c7ea2a98b8a45d9069ce98869f0079d3\" alt=\"Website change alerts might be the most underrated automation\" title=\"Website change alerts might be the most underrated automation\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kamilkowal21\"> /u/kamilkowal21 </a> <br/> <span><a href=\"https://i.redd.it/t2zsoob48fkf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mwk3x4/website_change_alerts_might_be_the_most/\">[comments]</a></span> </td></tr></table>",
        "id": 3386237,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwk3x4/website_change_alerts_might_be_the_most",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/t2zsoob48fkf1.png?width=640&crop=smart&auto=webp&s=d5aff390c7ea2a98b8a45d9069ce98869f0079d3",
        "title": "Website change alerts might be the most underrated automation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nuxxorcoin",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T19:29:16.918859+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T18:48:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m tracking a public announcements page on a large site (web client only). For brand-new IDs, the page looks \u201cplaceholder-ish\u201d for the first 3\u20135 seconds. After that window, it serves the real content instantly. For older IDs, TTFB is consistently ~100\u2013150 ms (Tokyo region).</p> <p>What I\u2019ve observed / tried (sanitized):</p> <ul> <li>Headers on first reveal often show cf-cache-status: DYNAMIC (so not a simple static cache miss).</li> <li>Different PoPs/regions didn\u2019t materially change that initial hold-back.</li> <li>Normal browser-y headers (desktop UA, ko-first Accept-Language), realistic Referer, and small range requests (grabbing only the head) still hit the same delay when the ID is truly fresh.</li> <li>I\u2019m rotating ~600 proxies with per-proxy cookie jars and keeping sessions sticky; request cadence ~100ms overall, but each proxy rests \u22658s between uses.</li> <li>Mirrors (e.g., social/telegram relays) lag minutes, so they\u2019re not helpful.</li> </u",
        "id": 3386238,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwjmz2/how_do_sites_enforce_a_35s_public_delay",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do sites enforce a 3\u20135s public delay?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kakarot_J",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T18:23:41.299011+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T17:58:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I am very new to web scraping and am currently working with a volunteer organization to collect the contact details of various organizations that provide housing for individuals with mental illness or Section 8\u2013related housing across the country, for downstream tasks. I decided to collect the data using web scraping and approach it county by county.</p> <p>So far, I\u2019ve managed to successfully scrape only about 50\u201360% of the websites. Many of the websites are structured differently, and the location of the contact page varies. I expected this, but with each new county I keep encountering different issues when trying to find the contact details.</p> <p>The flow I\u2019m following to locate the contact page is: checking the footer, the navigation bar, and then the header. </p> <p>Any suggestions for a better way to find the contact page?</p> <p>I\u2019m currently using the Google Search API for website links and Playwright for scraping.</p> </div><!-",
        "id": 3385809,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwiane/ideas_for_better_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Ideas for better scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/OutlandishnessLast71",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T08:19:28.819617+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T07:14:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mw42l4/all_startups_info_scraper_scrapes_startups_infor/\"> <img src=\"https://external-preview.redd.it/FP-b6yOOFjYGhzS0Flv-p_jS0_tbbbODBmNxb7vGa88.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4486be89094a5e3382d3700f1ad4cb97eeca3e4\" alt=\"All Startups Info Scraper - Scrapes startups infor into CSV\" title=\"All Startups Info Scraper - Scrapes startups infor into CSV\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><h1><a href=\"http://AllStartups.info\">AllStartups.info</a> Scraper</h1> <p>A python script to scrape all entries from <a href=\"http://allstartups.info\">allstartups.info</a> into CSV/XLSX file.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OutlandishnessLast71\"> /u/OutlandishnessLast71 </a> <br/> <span><a href=\"https://github.com/evilgenius786/allstartups_info_scraper\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mw42l4/a",
        "id": 3380978,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mw42l4/all_startups_info_scraper_scrapes_startups_infor",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/FP-b6yOOFjYGhzS0Flv-p_jS0_tbbbODBmNxb7vGa88.png?width=640&crop=smart&auto=webp&s=f4486be89094a5e3382d3700f1ad4cb97eeca3e4",
        "title": "All Startups Info Scraper - Scrapes startups infor into CSV",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/OutlandishnessLast71",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T08:19:28.989005+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T07:11:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1mw4134/gelbe_seiten_german_yellowpages_scraper/\"> <img src=\"https://external-preview.redd.it/nHM1v41a8bmRiENYH976jVh2v2dVOcSU1TWH4AU5k5Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34201fc8e071f3c3366b1e8d3c4d30b05e2e7577\" alt=\"Gelbe Seiten - German yellowpages scraper\" title=\"Gelbe Seiten - German yellowpages scraper\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><h1>gelbeseiten_scraper</h1> <p>Scrapes data from gelbeseiten on basis of ZIP codes into CSV file.</p> <p>Dependencies: Pandas, BeautifulSoup4, Requests</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OutlandishnessLast71\"> /u/OutlandishnessLast71 </a> <br/> <span><a href=\"https://github.com/evilgenius786/gelbeseiten_scraper\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mw4134/gelbe_seiten_german_yellowpages_scraper/\">[comments]</a></span> </td></tr></table>",
        "id": 3380979,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mw4134/gelbe_seiten_german_yellowpages_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/nHM1v41a8bmRiENYH976jVh2v2dVOcSU1TWH4AU5k5Q.png?width=640&crop=smart&auto=webp&s=34201fc8e071f3c3366b1e8d3c4d30b05e2e7577",
        "title": "Gelbe Seiten - German yellowpages scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Harshith_Reddy_Dev",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T07:13:18.165675+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T07:02:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;ve spent the last couple of days on a deep dive trying to scrape a single, incredibly well-protected website, and I&#39;ve finally hit a wall. I&#39;m hoping to get a sanity check from the experts here to see if my conclusion is correct, or if there&#39;s a technique I&#39;ve completely missed.</p> <p><strong>TL;DR:</strong> Trying to scrape <a href=\"http://health.usnews.com\">health.usnews.com</a> with Python/Playwright. I get blocked with a TimeoutError on the first page load and net::ERR_HTTP2_PROTOCOL_ERROR on all subsequent requests. I&#39;ve thrown every modern evasion library at it (rebrowser-playwright, undetected-playwright, etc.) and even tried hijacking my real browser profile, all with no success. My guess is TLS fingerprinting.</p> <p> </p> <p><strong>I want to basically scrape this website</strong></p> <p>The target is the doctor listing page on U.S. News Health: <a href=\"https://health.usnews.com/best-hospitals",
        "id": 3380704,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mw3vsp/defeated_by_a_antibot_tls_fingerprinting_need",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Defeated by a Anti-Bot TLS Fingerprinting? Need Suggestions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hikizuto1203",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T07:13:18.337377+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T05:10:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I used to scrape data from many Google platforms such as AdMob, Google Ads, Firebase, GAM, YouTube, Google Calendar, etc. And I noticed that the internal APIs used only in the Web UI (the ones you can see in the Network tab of DevTools after logging in) have extremely digitized parameters. They are almost all numbers instead of text, and besides being sometimes encoded, they\u2019re also quite hard to read.</p> <p>I wonder if Google must have some kind of internal mapping table that defines these fields. For example, here\u2019s a parameter you need to send when creating a Google ad unit \u2014 and you can try to see how much of it you can actually understand:</p> <pre><code>{ &quot;1&quot;: { &quot;2&quot;: &quot;xxxx&quot;, &quot;3&quot;: &quot;xxxxx&quot;, &quot;14&quot;: 0, &quot;16&quot;: [0, 1, 2], &quot;21&quot;: true, &quot;23&quot;: { &quot;1&quot;: 2, &quot;2&quot;: 3 }, &quot;27&quot;: { &quot;1&quot;: 1 } } } </code></pre> <p>When I first approached this",
        "id": 3380705,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mw1ztn/what_do_you_think_about_internal_google_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What do you think about internal Google API?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/itwasnteasywasit",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T01:38:42.609040+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T01:00:59+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/itwasnteasywasit\"> /u/itwasnteasywasit </a> <br/> <span><a href=\"https://yacinesellami.com/posts/stealth-clicks/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvwytb/stealth_clicking_in_chromium_vs_cloudflares/\">[comments]</a></span>",
        "id": 3379369,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvwytb/stealth_clicking_in_chromium_vs_cloudflares",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Stealth Clicking in Chromium vs. Cloudflare\u2019s CAPTCHA",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ivelgate",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T01:38:42.777410+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T00:40:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone. Someone can help me make a CSV file of the historic lottery results from 2016 to 2025, from this website: <a href=\"https://lotocrack.com/Resultados-historicos/triplex/\">https://lotocrack.com/Resultados-historicos/triplex/</a> It is asked by chatgpt to apply the Markov chain and calculate probabilities. I am on Android. Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ivelgate\"> /u/ivelgate </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvwivo/chatgpt/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvwivo/chatgpt/\">[comments]</a></span>",
        "id": 3379370,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvwivo/chatgpt",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Chatgpt.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ivelgate",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-21T01:38:42.946632+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-21T00:39:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone. Someone can help me make a CSV file of the historic lottery results from 2016 to 2025, from this website: <a href=\"https://lotocrack.com/Resultados-historicos/triplex/\">https://lotocrack.com/Resultados-historicos/triplex/</a> It is asked by chatgpt to apply the Markov chain and calculate probabilities. I am on Android. Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ivelgate\"> /u/ivelgate </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvwi1y/chatgpt/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mvwi1y/chatgpt/\">[comments]</a></span>",
        "id": 3379371,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mvwi1y/chatgpt",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Chatgpt.",
        "vote": 0
    }
]