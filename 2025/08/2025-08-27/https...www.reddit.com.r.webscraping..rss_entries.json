[
    {
        "age": null,
        "album": "",
        "author": "/u/smrochest",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T23:07:39.394544+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T22:15:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Here is my code, which worked before 8/15 but now it would give me timeout error. Any suggestion on how to make it work again?</p> <p><code>Private Function getYahooFinanceData(stockTicker As String, startDate, endDate) As Worksheet</code></p> <p><code>Dim tickerURL As String</code></p> <p><code>startDate = (startDate - DateValue(&quot;January 1, 1970&quot;)) * 86400</code></p> <p><code>endDate = (endDate - DateValue(&quot;dec 31, 1969&quot;)) * 86400</code></p> <p><code>tickerURL = &quot;https://finance.yahoo.com/quote/&quot; &amp; stockTicker &amp; _</code></p> <p><code>&quot;/history/?period1=&quot; &amp; startDate &amp; &quot;&amp;period2=&quot; &amp; endDate</code></p> <p><code>wd.PageLoadTimeout = 5000</code></p> <p><code>wd.NavigateTo tickerURL</code></p> <p><code>DoEvents</code></p> <p><code>Dim result, elements, element, i As Integer, j As Integer</code></p> <p><code>Set elements = wd.FindElements(By.ClassName, &quot;table-container&quot;)</c",
        "id": 3435500,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n1uwjg/my_web_scraper_stopped_working_with_yahoo_finance",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My web scraper stopped working with Yahoo Finance after 8/15",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok-Method9112",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T20:57:38.216384+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T20:13:55+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1n1rt8f/help_on_bypass_text_captcha/\"> <img src=\"https://preview.redd.it/2sevutbrdmlf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af652fd8ed2c0ba632db39c7e55d8d91dceb028f\" alt=\"help on bypass text captcha\" title=\"help on bypass text captcha\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>somehow when i do screenshot them and put them on ai it always get 3 or two correct and others mistaken i gues its due to low quality or resultion any help please </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok-Method9112\"> /u/Ok-Method9112 </a> <br/> <span><a href=\"https://i.redd.it/2sevutbrdmlf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n1rt8f/help_on_bypass_text_captcha/\">[comments]</a></span> </td></tr></table>",
        "id": 3434659,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n1rt8f/help_on_bypass_text_captcha",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/2sevutbrdmlf1.png?width=640&crop=smart&auto=webp&s=af652fd8ed2c0ba632db39c7e55d8d91dceb028f",
        "title": "help on bypass text captcha",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ohwowlookausername",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T20:57:38.342185+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T19:47:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I have a script that needs to automatically run daily from the cloud. It&#39;s a pretty simple python script using Playwright in headed mode (I&#39;ve tried using headless, but the site I&#39;m scraping won&#39;t let me do it).</p> <p>So I tried throwing it in a Linux instance in Amazon Lightsail, but it wouldn&#39;t seem to let me do it in headed mode and xvfb didn&#39;t work as a workaround.</p> <p>I am kind of new to doing web scraping off my machine, so I need some advice. My intuition is that there&#39;s some kind of cheap service out there that will let me set this to run daily in headed mode and forget about it. But I&#39;ve already sunk 10+ probably wasted hours into Lightsail, so I want to get some advice before diving into something else. </p> <p>I&#39;d be super grateful for your suggestions!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ohwowlookausername\"> /u/ohwowlookausername </a> <br/> <",
        "id": 3434660,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n1r3me/where_to_host_a_headed_browser_scraper_playwright",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where to host a headed browser scraper (playwright)?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/k2rfps",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T17:41:08.924805+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T17:20:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there any way I can create a web scraper that scrapes general company career pages that are powered by workday using python without selenium. Right now I am using selenium but it&#39;s much slower than using requests. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/k2rfps\"> /u/k2rfps </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n1n787/workday_web_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1n1n787/workday_web_scraper/\">[comments]</a></span>",
        "id": 3433139,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n1n787/workday_web_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Workday web scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/doudawak",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T14:26:23.553541+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T13:16:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>As part of a personal project, I am working on testing a local site for cars valuations using machine learning. I was looking to get some real world data for recent ads from LeBonCoin website for the french maket, with just a couple of filters :<br/> - 2000 \u20acminimum (to filter garbage)</p> <p>- ordered by latest available</p> <p>URL : <a href=\"https://www.leboncoin.fr/recherche?category=1&amp;price=2000-max&amp;sort=time&amp;order=desc\">https://www.leboncoin.fr/recherche?category=1&amp;price=2000-max&amp;sort=time&amp;order=desc</a></p> <p>I&#39;ve been trying unsuccessfully to scrape it myself for a while, but end up being f***ed up by datadome almost all the time. so I&#39;m looking for assistance I can pay for the following :</p> <ol> <li><p>First a sample of those data (a few thousands) with details for each ads including all key information (description / all fields / links of imgs / postcode) basically the whole ads</p></li> <li><",
        "id": 3430379,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n1gs1d/assistance_needed_reliable_le_bon_coin_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Assistance needed - reliable le bon coin scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/study_english_br",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T14:26:23.428393+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T12:51:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If anyone can assist me with the arrangements, please note that I had to use AI to write this because I don\u2019t speak English.</p> <p>Context: Scraping system processing ~2,000 requests/day using 500 data-center proxies, facing high 403 error rates on Casas Bahia (Brazilian e-commerce).Stealth Strategies Implemented:Camoufox (Anti-Detection Firefox):</p> <ul> <li><p>geoip=True for automatic proxy-based geolocation</p></li> <li><p>humanize=True with natural cursor movements (max 1.5s)</p></li> <li><p>persistent_context=True for sticky sessions, False for rotating</p></li> <li><p>Isolated user data directories per proxy to prevent fingerprint leakage</p></li> <li><p>pt-BR locale with proxy-based timezone randomization</p></li> </ul> <p>Browser Fingerprinting:</p> <ul> <li><p>Realistic Firefox user agents (versions 128-140, including ESR)</p></li> <li><p>Varied viewports (1366x768 to 3440x1440, including windowed)</p></li> <li><p>Hardware fingerprinting: C",
        "id": 3430378,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1n1g5z7/casas_bahia_web_scraper_with_403_issues_akamai",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Casas Bahia Web Scraper with 403 Issues (AKAMAI)",
        "vote": 0
    }
]