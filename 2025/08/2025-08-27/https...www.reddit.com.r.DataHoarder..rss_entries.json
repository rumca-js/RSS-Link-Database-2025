[
    {
        "age": null,
        "album": "",
        "author": "/u/lmbrito",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T23:42:49.272256+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T23:12:37+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lmbrito\"> /u/lmbrito </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1n1w9qo\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1wa59/free_serverapparently/\">[comments]</a></span>",
        "id": 3435654,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1wa59/free_serverapparently",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Free server(apparently)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BetOver",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T22:37:37.635485+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T22:17:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been using nordvpn and it&#39;s file transfer feature via meshnet to send files to a friend out of my home country securely. It looks like this feature is being removed and I&#39;m trying to figure out a way to give him access to some of my data. It must be an encrypted connection and ideally I want him to be able to browse what he grabs from me vs nordvpn making me send him a specific file or folder ad that&#39;s annoying. Currently he says hey can you send me xyz file and I send it. If I have it. I am running a truenas scale system as a Nad and my Linux knowledge is negligible. I was just able to get it working and still can&#39;t figure out how to setup other apps with access to existing &quot;folders and files&quot; or data sets as Linux calls them. This difference in file systems gives me trouble for sure when trying to setup a new app. Side note I could use some explanation of that too. Anyway though something where I could essential",
        "id": 3435245,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1uywu/sorry_if_this_is_the_wrong_sub_but_need_advice",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sorry if this is the wrong sub but need advice for sharing files with a friend easily and securely",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dizeee23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T22:37:37.065907+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T21:55:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im currently at 500TB and im looking to expand. My current setup is fractal define 7 XL with 19 drives at close to 500TB. looking for inspiration from my seniors in this vice. What is your setup?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dizeee23\"> /u/dizeee23 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1uero/unraid_users_with_1pb_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1uero/unraid_users_with_1pb_storage/\">[comments]</a></span>",
        "id": 3435244,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1uero/unraid_users_with_1pb_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Unraid users with 1PB+ storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Superplay64",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T21:32:32.955593+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T21:26:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, I recently acquired a VCR-DVD combo unit from a family member, and it allows for native HDMI output on the VCR side (tested). Because of that, I was wanting to get an HDMI capture card.</p> <p>However, because I know direct analog capture is complicated, I&#39;m curious if there are any &quot;quirks&quot; to keep in mind when buying/utilizing a capture card for VHS-over-HDMI recording, or if it&#39;ll work the same as any other HDMI device (such as game consoles)?</p> <p>(I&#39;d also like to use the capture card <em>for</em> game consoles and signal-converted retro PC &amp; console footage, in addition to the VHS capture.)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Superplay64\"> /u/Superplay64 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1tojy/capturing_vhs_footage_from_native_hdmi_out/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1",
        "id": 3434878,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1tojy/capturing_vhs_footage_from_native_hdmi_out",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Capturing VHS Footage From Native HDMI Out?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/L_at_nnes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T21:32:33.145348+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T21:08:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a group on Instagram with friends where we&#39;ve been sending each other Reels that we like for several months now. Since there are quite a few of us and the group is quite productive, I think we must have almost 1,000 Reels.</p> <p>As a good data hoarder, I&#39;d like to archive these Reels, so I was thinking of using Instaloader, but I&#39;m facing a problem: how can I automatically extract the links from hundreds of Reels and posts? I don&#39;t want to copy them manually...</p> <p>Does anyone have a tool or solution to suggest?</p> <p>Thank you in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/L_at_nnes\"> /u/L_at_nnes </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1t7i1/extract_links_from_a_private_group_on_instagram/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1t7i1/extract_links_from_a_private_group_on_instagram/\">[comme",
        "id": 3434879,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1t7i1/extract_links_from_a_private_group_on_instagram",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Extract links from a private group on Instagram",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mental-Jacket-35",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T21:32:33.281167+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T21:03:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was thinking of <strong>backing my media in discs</strong> for <strong>longterm cold storage</strong> (Meaning I wont use them often -or at all- unless my digital backups for some reason get damaged), <strong>but</strong> I&#39;ve read in this sub and others that <strong>optical media is bad</strong> for storage, <strong>I wanted to know why.</strong></p> <ul> <li>Is it <strong>too expensive per gb</strong> of data when compared to just buying a new HDD / SSD? </li> <li>Does it <strong>not last longer</strong> than an HDD or SSD? </li> <li>Is it <strong>too impractical</strong> and thus its just more worth it to just invest some extra money into extra digital storage? </li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mental-Jacket-35\"> /u/Mental-Jacket-35 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1t324/optical_media_bad/\">[link]</a></span> &#32; <span><a href=\"https://www.r",
        "id": 3434880,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1t324/optical_media_bad",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Optical media bad?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TILLYTILLYTILLYTILLY",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T21:32:33.411769+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T20:47:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello friends, I unfortunately had to purchase an e-textbook this semester for coursepoint: lippincott. I was unable to find the PDFs anywhere anyway though. Is there any way I can quickly grab all the pages and upload them for other people who only need the texts?</p> <p>Sorry, I&#39;m not even sure what terms to use to begin googling this myself. I am also not very tech savy </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TILLYTILLYTILLYTILLY\"> /u/TILLYTILLYTILLYTILLY </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1so2g/script_or_program_to_screenshot_etextbook_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1so2g/script_or_program_to_screenshot_etextbook_and/\">[comments]</a></span>",
        "id": 3434881,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1so2g/script_or_program_to_screenshot_etextbook_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Script or program to screenshot e-textbook and make a PDF?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FloridianfromAlabama",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T20:27:31.243347+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T20:04:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everybody, I&#39;m new to this whole thing and I&#39;d like to download Benn Jordan&#39;s video on YT about AI police cameras. I downloaded yt-dlp and upon trying to use it, I got a http 403 forbidden error. Is there anyway around this? Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FloridianfromAlabama\"> /u/FloridianfromAlabama </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1rk4f/ytdlp_errors_workarounds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1rk4f/ytdlp_errors_workarounds/\">[comments]</a></span>",
        "id": 3434466,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1rk4f/ytdlp_errors_workarounds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "yt-dlp errors / workarounds",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dwhite21787",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T19:22:31.408972+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T19:01:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Links to old PC software, iOS and Android apps. See <a href=\"https://s3.us-east-1.amazonaws.com/rds.nsrl.nist.gov/software/NSRL_free_bags_README.htm\">https://s3.us-east-1.amazonaws.com/rds.nsrl.nist.gov/software/NSRL_free_bags_README.htm</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dwhite21787\"> /u/dwhite21787 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1pwgo/nist_national_software_reference_library_nsrl_is/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1pwgo/nist_national_software_reference_library_nsrl_is/\">[comments]</a></span>",
        "id": 3433926,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1pwgo/nist_national_software_reference_library_nsrl_is",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NIST National Software Reference Library (NSRL) is posting download links for all freely acquired software in their collection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ScaryestSpider",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T19:22:31.863932+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T18:50:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been trying to archive some bits of imgur because of the high likelihood of it shutting down. I&#39;m having issues due to some sort of rate limiting, VPNs are of limited effectiveness. Does anyone have concrete numbers on how many requests/sec I can do before being blocked? Seems inconsistent but maybe I&#39;m not seeing everything. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ScaryestSpider\"> /u/ScaryestSpider </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1pm40/imgur_backup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1pm40/imgur_backup/\">[comments]</a></span>",
        "id": 3433927,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1pm40/imgur_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Imgur backup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/davislm",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T19:22:32.237403+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T18:37:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have about 20 TB of files and back ups from the last 2 decades on about a dozen random external hard drives. Hoping to finally get it all into one place (with back ups) and I\u2019d love to get some organization to it all. The most cumbersome parts would be video projects with linked files. I may deal with this and I may leave all of the video projects and linked files in organizational tact and just try to deal with and organize other files like images, docs, etc. There will likely be a lot of duplicates, some real, some just by name. Any good tools out there for organizing/tackling all of this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/davislm\"> /u/davislm </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1p9ph/organizing_20tb/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1p9ph/organizing_20tb/\">[comments]</a></span>",
        "id": 3433928,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1p9ph/organizing_20tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Organizing 20TB",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Flaxen_Bobcat",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T19:22:32.464024+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T18:21:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a collection of media around 30-40Tb from Just the last 3 years. I&#39;ve looked online at backup options, I&#39;d ideally like to keep the running costs low, and have the ability to easily expand in the future.</p> <p>I looked at cloud options but it becomes expensive if you want to then restore your data (which I had to recover half of my data earlier this month), but also I don&#39;t wanna have to shell out for another Nas that I&#39;ll have to then eventually upgrade / expand when I run out of space on their along with my main server. </p> <p>I had a look at LTO 7 backup tapes which is cheap long term storage but the actual machine to write the data is expensive.</p> <p>Is anyone aware of any decent options, or could tell when what their solutions where.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Flaxen_Bobcat\"> /u/Flaxen_Bobcat </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/commen",
        "id": 3433929,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1ou2k/backup_option",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backup option",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ypeelS",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T18:18:46.895125+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T17:28:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://imgur.com/a/9sm0qRD\">https://imgur.com/a/9sm0qRD</a></p> <p>Purchased 5 open box Iron Wolf Pros, 3/5 are making this buzzing sound and not spinning. from the 2 that work, 1 of them is from the same batch but seems to work fine, how worried should I be that it wont fail next month?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ypeelS\"> /u/ypeelS </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1ne7i/35_doa_ironwolf_pro/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1ne7i/35_doa_ironwolf_pro/\">[comments]</a></span>",
        "id": 3433428,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1ne7i/35_doa_ironwolf_pro",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "3/5 DOA Ironwolf Pro",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/visiny",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T16:07:59.849712+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T15:43:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>4k re-releases are taking up more storage than I&#39;ve got, I really need to figure out a way to manage besides buying a bunch of external hard drives or stuff my pc with like a bunch of 8tb internal hard drives</p> <p>Before, an entire release of a series would be like 200gb, but with 4k that number shoots up to the thousands </p> <p>That being said, I&#39;m getting a new PC built, and am wondering if I can fill it with very large internal hard drives. I was checking amazon and apparently seagate has as much as 20TB internal hard drives? If not higher? That would be great I think. Currently my old PC has 1 SSD and 4 HDDs that are 4TB a piece. If my next PC fits 4 HDDs and an SSD I&#39;m thinking each HDD at 20+TB, that&#39;ll definite last me forever (I&#39;m looking for as much future proofing as possible)</p> <p>Just looking to get some input out of people here.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user",
        "id": 3432047,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1kkjg/4k_files_are_eating_up_my_harddrive_i_really_need",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "4k files are eating up my harddrive, I really need a long term solution...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Nattends_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T17:12:59.303881+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T15:38:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1kg1p/looking_to_buy_a_nas_for_my_media_library/\"> <img src=\"https://preview.redd.it/5kxtghwrwklf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f359649b776d5f316a6719825c9613f32193dd16\" alt=\"Looking to buy a NAS for my media library\" title=\"Looking to buy a NAS for my media library\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>Right now my setup is only composed of a VPS with a pretty basic installation for the *arr suite :</p> <ul> <li>Radarr/Sonarr handle movies and shows.</li> <li>Prowlarr for indexers.</li> <li>qBittorrent behind a VPN (gluetun).</li> <li>Downloads are synced to a cloud storage provider (~90\u20ac/year for 2TB).</li> <li>Jellyfin streams from the cloud storage (files land there via rsync).</li> </ul> <p>So basically, the VPS does all the downloading, pushes everything to the cloud, and then Jellyfin reads directly from that cloud storage.</p> <p>The problem: I\u2019m ge",
        "id": 3432843,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1kg1p/looking_to_buy_a_nas_for_my_media_library",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/5kxtghwrwklf1.png?width=640&crop=smart&auto=webp&s=f359649b776d5f316a6719825c9613f32193dd16",
        "title": "Looking to buy a NAS for my media library",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lesbaguette1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T16:08:00.165934+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T15:02:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello Data hoarders,</p> <p>I have been tasked with finding a DAS for my family, collectively we don\u2019t have much data, but much of it is very important: like photos. I have been looking for a 4 bay commercial one as-well as making one myself.</p> <p>What would the best raid for redundancy be? What commercial 4 bay DASes support it?</p> <p>If I were to make this on my own could anyone recommend, cases, parts, raid controllers, etc?</p> <p>I would much prefer to make one myself because then I know its reparable.</p> <p>Thank you all in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lesbaguette1\"> /u/lesbaguette1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1jhef/looking_for_a_family_das/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1jhef/looking_for_a_family_das/\">[comments]</a></span>",
        "id": 3432048,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1jhef/looking_for_a_family_das",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a family DAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Nervous_Type_9175",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:42.443996+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T14:43:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you have 10 drives in raid config. But your main drive containing OS needs replacement, how would you go about replacing that drive, installing OS etc, so that your docker and databases etc are recognized by the new OS?</p> <p>Edit : 2 Mini PCs. Both in raid config. 2nd mini pc as a weekly backup.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nervous_Type_9175\"> /u/Nervous_Type_9175 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1iz11/what_if_your_os_drive_gets_damaged_and_needs_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1iz11/what_if_your_os_drive_gets_damaged_and_needs_a/\">[comments]</a></span>",
        "id": 3431166,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1iz11/what_if_your_os_drive_gets_damaged_and_needs_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What if your OS drive gets damaged and needs a replacement?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CactusBoyScout",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:42.719280+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T14:41:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1iwrc/since_we_are_sharing_examples_of_companies_losing/\"> <img src=\"https://external-preview.redd.it/_f5s1Rr1foN1eRXthBAgpD0QwkKkMW7_eUirQswJWGo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2229507f7baed527ca74b4927620e86602d0d73a\" alt=\"Since we are sharing examples of companies losing (or almost losing) their data: The time Pixar almost deleted Toy Story 2 before it was completed.\" title=\"Since we are sharing examples of companies losing (or almost losing) their data: The time Pixar almost deleted Toy Story 2 before it was completed.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CactusBoyScout\"> /u/CactusBoyScout </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=8dhp_20j0Ys\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1iwrc/since_we_are_sharing_examples_of_companies_losing/\">[comments]</a></span> </td></tr></table>",
        "id": 3431167,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1iwrc/since_we_are_sharing_examples_of_companies_losing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/_f5s1Rr1foN1eRXthBAgpD0QwkKkMW7_eUirQswJWGo.jpeg?width=320&crop=smart&auto=webp&s=2229507f7baed527ca74b4927620e86602d0d73a",
        "title": "Since we are sharing examples of companies losing (or almost losing) their data: The time Pixar almost deleted Toy Story 2 before it was completed.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/InternalMode8159",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:42.845054+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T14:22:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m a real noob in the field, I have a 8tb hard disk and some smaller one that I use for jellyfin and I needed a USB for installing Ubuntu, my dad gave me one with 60gb of important file, all saved in the worst possible medium so I wanted to have a simple to use software to backup all his data for now I&#39;m just copying them to an hard disk but what could be a software similar to jellyfin to have a backup solution, maybe with a simple interface and that, if he continues to use the USB, can update the file without needing to overwrite everything by connecting the USB to my PC.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/InternalMode8159\"> /u/InternalMode8159 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1ifge/help_with_choosing_the_best_software_for_doing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1ifge/help_with_choosing_the_b",
        "id": 3431168,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1ifge/help_with_choosing_the_best_software_for_doing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help with choosing the best software for doing backups",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Flikkert",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:42.970556+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T14:19:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m about to purchase a couple of LTO-9 tapes but I have the choice between Quantum and IBM tapes. The Quantum tapes are about 10% cheaper. Is there a known reliability difference or any other reason to go for the IBM tapes over Quantum?</p> <p>I&#39;ve read that the drives are all made by IBM and rebadged for other brands, but I can&#39;t find any info on the tapes. Sorry if this is not the right place to ask :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Flikkert\"> /u/Flikkert </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1icaw/lto_tape_brands_any_differences/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1icaw/lto_tape_brands_any_differences/\">[comments]</a></span>",
        "id": 3431169,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1icaw/lto_tape_brands_any_differences",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LTO tape brands, any differences?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/psych0ticmonk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:43.097728+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T13:49:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Can can anyone recommend the best enclosure that would withstand a drop of certain feet for the regular spinning disk hard drives?</p> <p>Before anyone recommends SSDs, unfortunately this isn&#39;t feasible for me due to the amount of data. This Hard Drive is going to be 20TB.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/psych0ticmonk\"> /u/psych0ticmonk </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1hko0/best_drop_tested_enclosure_for_hard_drive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1hko0/best_drop_tested_enclosure_for_hard_drive/\">[comments]</a></span>",
        "id": 3431170,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1hko0/best_drop_tested_enclosure_for_hard_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "best drop tested enclosure for hard drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Bookkeeper_3481",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:42.277283+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T12:56:56+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1gapn/all_your_flybase_are_belong_to_us_flybase_nih/\"> <img src=\"https://a.thumbs.redditmedia.com/JqrLTV2jL5HZFk3c7-NxE_mx9UdtR3c8VlQeY_Bnor4.jpg\" alt=\"\ud83e\udeb0 ALL YOUR FLYBASE ARE BELONG TO US **[FLYBASE NIH Shutdown Emergency Plan]**\" title=\"\ud83e\udeb0 ALL YOUR FLYBASE ARE BELONG TO US **[FLYBASE NIH Shutdown Emergency Plan]**\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Bookkeeper_3481\"> /u/Ok_Bookkeeper_3481 </a> <br/> <span><a href=\"/r/labrats/comments/1n0w2rg/all_your_flybase_are_belong_to_us_flybase_nih/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1gapn/all_your_flybase_are_belong_to_us_flybase_nih/\">[comments]</a></span> </td></tr></table>",
        "id": 3431165,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1gapn/all_your_flybase_are_belong_to_us_flybase_nih",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/JqrLTV2jL5HZFk3c7-NxE_mx9UdtR3c8VlQeY_Bnor4.jpg",
        "title": "\ud83e\udeb0 ALL YOUR FLYBASE ARE BELONG TO US **[FLYBASE NIH Shutdown Emergency Plan]**",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/chocolatebanana136",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:43.899342+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T12:53:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey!</p> <p>I was looking for a solution to bulk download Street View panoramas, but they all seemed too expensive, so I made my own free alternative.</p> <p>BulkSVD allows you to download panoramas using a single URL, or even multiple URLs at once.</p> <p>The &quot;Area Scan&quot; feature is capable of scanning a specified area for panorama links to automate Street View downloading, so you don&#39;t have to grab all the links by hand.</p> <p>The program is available here, and I would be happy for any feedback!<br/> <a href=\"https://github.com/pbz134/BulkSVD\">https://github.com/pbz134/BulkSVD</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/chocolatebanana136\"> /u/chocolatebanana136 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1g81y/i_vibecoded_a_bulk_street_view_downloader_capable/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1g81y/i_vibe",
        "id": 3431174,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1g81y/i_vibecoded_a_bulk_street_view_downloader_capable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I vibe-coded a bulk Street View Downloader, capable of automated panorama fetching and more",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dominantlegs",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:43.224573+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T12:47:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I wanted to double check if two hard drives in a 2-bay enclosure that are configured as a JBOD, and had backup software like Carbon Copy Cloner scheduled for weekly backups, actually counts as a backup. I\u2019d like the convenience of having two drives in one physical package, with less cables. Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dominantlegs\"> /u/dominantlegs </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1g2vj/does_a_2bay_hard_drive_enclosure_set_up_as_jbod/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1g2vj/does_a_2bay_hard_drive_enclosure_set_up_as_jbod/\">[comments]</a></span>",
        "id": 3431171,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1g2vj/does_a_2bay_hard_drive_enclosure_set_up_as_jbod",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does a 2-bay hard drive enclosure set up as JBOD count as a backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/furchtsaft",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:43.350533+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T10:26:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello. I am acquainted with gallery-dl and what a great tool it is and I managed to download around 2k bookmarked posts with it but now the Instagram API seems to return bad responses, although the account is still not banned. Making alt accounts is not feasible in 2025, what tool would you suggest for downloading images/reels, or what would you suggest to circumvent the apparent rate limit (aside from VPN)?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/furchtsaft\"> /u/furchtsaft </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1daq5/mass_download_instagram_bookmarks_likes_etc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1daq5/mass_download_instagram_bookmarks_likes_etc/\">[comments]</a></span>",
        "id": 3431172,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1daq5/mass_download_instagram_bookmarks_likes_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mass download Instagram bookmarks, likes etc.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hayden_den",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T08:36:34.753849+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T08:33:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a very large collection of animated shows and comics. I would like a browser, to just sort through my collection easier. </p> <p>The program does not need to run them, if i can set my own 3rd party programs that is fine. Many of the franchises I follow, have a lot of comics and other books, and I&#39;d like to have an easier time going through them.</p> <p>I already tried Kodi, but I couldn&#39;t find a decent add on for any pdf files, or cbz ones. I have no interest in running a server, as of now due to limitations.</p> <p>Yes I am aware I could use 2 different programs, but I have that right now. They are quite tedious to switch to when I am working on my projects.</p> <p>Any help is greatly appreciated. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hayden_den\"> /u/Hayden_den </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1bhyc/in_short_i_am_a_big_haorder_on_anything_animate",
        "id": 3428701,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1bhyc/in_short_i_am_a_big_haorder_on_anything_animated",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "In short I am a big haorder on anything animated media, comics, etc. Is there any kind media player/browser on which I can view both?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Obvious-Viking",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T15:02:43.550515+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T08:30:46+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1n1bgos/i_finally_backed_up_wallhaven/\"> <img src=\"https://b.thumbs.redditmedia.com/CoVEk1ik51NhmedP2J361YRSQ47S_fOBZ-Fre7vXI0Y.jpg\" alt=\"I finally backed up WallHaven\" title=\"I finally backed up WallHaven\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi all, just a small success story for hoarding.</p> <p>I love the site and often get myself some new wallpapers from there. I have been toying with grabbing everything for a while now but i finally got around to it. </p> <p>I have been working within the limits of their api so it was slow going for a while but i finally finished backing up the whole set and im now getting every new upload. Obviously using gallery-dl so i also grab the json metadata and have the files organised by purity and then by uploader. </p> <p>Interestingly, after i got them all imported into immich i have more images than they list on their stats by about 5,000.</p> <p><a href=\"https:/",
        "id": 3431173,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n1bgos/i_finally_backed_up_wallhaven",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/CoVEk1ik51NhmedP2J361YRSQ47S_fOBZ-Fre7vXI0Y.jpg",
        "title": "I finally backed up WallHaven",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Immediate_Arm_490",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T06:24:51.482053+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T06:07:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to download this file from <a href=\"https://online.fliphtml5.com/cvobr/ccsd/\">https://online.fliphtml5.com/cvobr/ccsd/</a>. No online downloader or python script works. Would anyone have suggestions on how to download it and convert it to PDF?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Immediate_Arm_490\"> /u/Immediate_Arm_490 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n198x6/download_files_to_pdf_from_fliphtml5/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n198x6/download_files_to_pdf_from_fliphtml5/\">[comments]</a></span>",
        "id": 3428093,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n198x6/download_files_to_pdf_from_fliphtml5",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Download files to PDF from fliphtml5",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PumpkinTime3608",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T06:24:51.606911+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T05:25:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys , so I came across google cloud\u2019s archive storage the other day and I was wondering if this a good solution for older photos/ videos to be stored there ? I\u2019m not going to be needing them very often if ever . But I also don\u2019t want to store it on onedrive or google drive since they\u2019re becoming quite expensive over time . And I\u2019ve nearly filed my local storage and setting up a NAS is gonna take at least 2 years before I can afford the drives for the amount of data I have</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PumpkinTime3608\"> /u/PumpkinTime3608 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n18kxl/is_google_cloud_amazon_glacier_good_for_personal/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n18kxl/is_google_cloud_amazon_glacier_good_for_personal/\">[comments]</a></span>",
        "id": 3428094,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n18kxl/is_google_cloud_amazon_glacier_good_for_personal",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is google cloud/ Amazon glacier good for personal backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/techknowfile",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T05:18:30.583709+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T04:55:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Going to build my first dedicated NAS. Was wondering if this is a good deal or if I should look elsewhere/wait? </p> <p><a href=\"https://www.staples.com/seagate-exos-x24-16tb-3-5-sata-internal-hard-drive-st16000nm002h/product_IM1JR8800\">https://www.staples.com/seagate-exos-x24-16tb-3-5-sata-internal-hard-drive-st16000nm002h/product_IM1JR8800</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/techknowfile\"> /u/techknowfile </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n182j4/is_this_a_good_deal_200_16tb_x24_exos_new_with_5/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n182j4/is_this_a_good_deal_200_16tb_x24_exos_new_with_5/\">[comments]</a></span>",
        "id": 3427854,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n182j4/is_this_a_good_deal_200_16tb_x24_exos_new_with_5",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is this a good deal? $200 16TB X24 Exos New with 5 year warranty on Staples.com",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Experience_NoSelf",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T03:08:32.125116+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T02:38:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you could go back in time and give your \u2018just started on data hoarding journey\u2019 self advice, eg do more of this, less of that, important things to consider vis-a-vis data collection, what you impart? Let\u2019s give some of the newer folks here some nuggets of wisdom!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Experience_NoSelf\"> /u/Experience_NoSelf </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n15g8k/what_do_you_wish_you_would_have_done_differently/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n15g8k/what_do_you_wish_you_would_have_done_differently/\">[comments]</a></span>",
        "id": 3427324,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n15g8k/what_do_you_wish_you_would_have_done_differently",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What do you wish you would have done differently?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DiCaroli-HugonianEPR",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T03:08:31.723749+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T02:03:49+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DiCaroli-HugonianEPR\"> /u/DiCaroli-HugonianEPR </a> <br/> <span><a href=\"/r/Archiveteam/comments/1mrk6vh/roblox_announced_that_in_the_future_games_that/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n14q0o/in_short_we_have_until_september_30_to_archive/\">[comments]</a></span>",
        "id": 3427323,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n14q0o/in_short_we_have_until_september_30_to_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "in short, we have until september 30 to archive old roblox games",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/creamyatealamma",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T00:58:01.246357+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T00:57:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1n13bdy/you_guys_will_like_this_one_server_broke_they/\"> <img src=\"https://external-preview.redd.it/OUZou_1PTFm8KcAgc3bnF9-LNh9CzL4rxyvmrYb60JU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7f1c9e6a4d395085b60d4deb0ae4d7350dda4fb\" alt=\"You guys will like this: One Server Broke. They Lost Everything.\" title=\"You guys will like this: One Server Broke. They Lost Everything.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I wonder if anything came out of this with hps involvement. Paying for support that messes up this bad is wild. Wonder how frequent this happens. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/creamyatealamma\"> /u/creamyatealamma </a> <br/> <span><a href=\"https://youtu.be/pKh5fNuj2-w?si=MLxBOExwh6g4ggCw\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n13bdy/you_guys_will_like_this_one_server_broke_they/\">[commen",
        "id": 3426809,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n13bdy/you_guys_will_like_this_one_server_broke_they",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/OUZou_1PTFm8KcAgc3bnF9-LNh9CzL4rxyvmrYb60JU.jpeg?width=320&crop=smart&auto=webp&s=a7f1c9e6a4d395085b60d4deb0ae4d7350dda4fb",
        "title": "You guys will like this: One Server Broke. They Lost Everything.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Undercovercat2212",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T00:58:01.630066+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T00:35:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So long story short me and another person was talking on Snapchat and then they deleted me but has now added me back. All the messages have disappeared as the chat was on \u201cdelete after read\u201d. </p> <p>I was just wondering if anyone knows how to get back those messages. </p> <p>I have already done a snapchat data download but the only messages I can get are from the most recent conversations. </p> <p>We was talking from about April 2024 until around June 2024. </p> <p>Any help would be appreciated. </p> <p>If I can\u2019t then it\u2019s no big deal. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Undercovercat2212\"> /u/Undercovercat2212 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n12tw2/does_anyone_know_how_to_get_deleted_snapchat/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1n12tw2/does_anyone_know_how_to_get_deleted_snapchat/\">[comments]</a></span>",
        "id": 3426810,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n12tw2/does_anyone_know_how_to_get_deleted_snapchat",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Does anyone know how to get deleted Snapchat messages back?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ViolantEnds",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-27T06:24:51.881066+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-27T00:12:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi reddit! </p> <p>Burner account to ask a stupid question I fear.</p> <p>I own a IIAB (internet in a box) based of a Kiwix archive and I absolutely adore it. I would like to create a similar archive for AO3 using something like <a href=\"https://github.com/IMayBeABitShy/zimfiction\">Zimfiction</a> with a data dump like <a href=\"https://archive.org/details/ao3_mirror_super_final\">this helpful mirror</a> on the internet archive.</p> <p>Unfortunately, my wifi/internet usage has a regular data cap that I hit frequently (gee, thanks Xfinity) and will most likely meet the cap from this file alone (due to the sheer size of it)</p> <p><strong>I&#39;d love to know if there&#39;s any serices or work arounds for downloading a file this size</strong> that don&#39;t involve me using my own wifi because I can&#39;t get sacked with overcharge fees right now. </p> <p>Thanks to anybody who ansers!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.r",
        "id": 3428095,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1n12c7d/is_there_something_to_download_a_massive_file_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there something to download a massive file for me onto an external drive and ship it?",
        "vote": 0
    }
]