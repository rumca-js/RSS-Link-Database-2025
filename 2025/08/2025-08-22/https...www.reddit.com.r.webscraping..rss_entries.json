[
    {
        "age": null,
        "album": "",
        "author": "/u/sleepWOW",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-22T13:27:24.997220+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-22T11:15:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey fellow scrapers,</p> <p>I\u2019m a newbie in the web scraping space and have run into a challenge here. </p> <p>I have built a python script which scrapes car listings and saves the data in my database. I\u2019m doing this locally on my machine. </p> <p>Now, I am trying to set up the scraper on a VM on the cloud so it can run and scrape 24/7. I have reached to the point that I have set up my Ubuntu machine and it is working properly. Though, when I\u2019m trying to keep it running even after I close the terminal session, it shuts down. I\u2019m using headless chrome and undetected driver and I have also set up a GUI for my VM. I have also tried nohup but still gets shut down after a while.</p> <p>It might be due to the fact in terminating the Remote Desktop connection to the GUI but I\u2019m not sure. Thanks !</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sleepWOW\"> /u/sleepWOW </a> <br/> <span><a href=\"https://www.reddit.com/r/web",
        "id": 3392136,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mx3nei/how_can_i_run_a_scraper_on_vm_247",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can I run a scraper on VM 24/7?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Old_Emotion_3646",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-22T10:54:07.834360+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-22T10:35:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have around 18k domain names and like to fetch the name of the companies. Just the name i am interested. No need for location , industry etc.<br/> Challenge i am facing is some of the domains use interal redirection, some of the domains use cloudflare challenge etc So rely on html scraping not working for me,<br/> What is the best approach and even thinking of pay a small fee to some services if anything affordable comes. Its just one time job for me </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Old_Emotion_3646\"> /u/Old_Emotion_3646 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mx2xo6/is_any_services_do_the_scraping_of_company_name/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mx2xo6/is_any_services_do_the_scraping_of_company_name/\">[comments]</a></span>",
        "id": 3391072,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mx2xo6/is_any_services_do_the_scraping_of_company_name",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is any services do the scraping of company name from domain",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/8ta4",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-22T06:33:43.946422+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-22T06:15:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m exploring a scraping idea that sacrifices scalability to leverage my day-to-day browser&#39;s fingerprint.</p> <p>My hypothesis is to skip automation frameworks. The architecture connects two parts:</p> <ul> <li><p>A CLI tool on my local machine.</p></li> <li><p>A companion Chrome extension running in my day-to-day browser.</p></li> </ul> <p>They communicate using Chrome&#39;s native messaging.</p> <p>Now, I can already hear the objections:</p> <ul> <li><p>&quot;Why not use Playwright?&quot;</p></li> <li><p>&quot;Why not CDP?&quot;</p></li> <li><p>&quot;This will never scale!&quot;</p></li> <li><p>&quot;This is a huge security risk!&quot;</p></li> <li><p>&quot;The behavioral fingerprint will be your giveaway!&quot;</p></li> </ul> <p>And for most use cases, you&#39;d be right.</p> <p>But here&#39;s the context. The goal is to feed webpage context into the LLM pipeline I described in <a href=\"https://old.reddit.com/r/SaaS/comments/1msku4e/seekin",
        "id": 3389741,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwyt3z/looking_for_a_scraper_that_controls_an_extension",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a scraper that controls an extension via native messaging",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/storman121",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-22T05:28:34.026854+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-22T04:26:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! I made <strong>PageSift</strong>, a small Chrome extension (open source, just needs your GPT API KEY) that lets you <strong>click the elements</strong> on an e-commerce listing page (title, price, image, specs) and it returns <strong>clean JSON/CSV</strong>. When specs aren\u2019t on the card, it uses a lightweight <strong>LLM step</strong> to infer them from the product name/description.</p> <p><strong>Repo:</strong> <a href=\"https://github.com/alec-kr/pagesift/tree/master\">https://github.com/alec-kr/pagesift</a></p> <p><strong>Why I built it</strong><br/> Copying product info by hand is slow, and scrapers often miss specs because sites are inconsistent. I wanted a quick <strong>point-and-click</strong> workflow + a normalization pass that guesses common fields (e.g., RAM, storage, GPU).</p> <p><strong>What it does</strong></p> <ul> <li>Hover to highlight \u2192 click to select elements you care about</li> <li>Normalizes messy fields (name/descri",
        "id": 3389480,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mwwx73/pagesift_pointandclick_product_data_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "PageSift - point-and-click product data scraper (Chrome Extension)",
        "vote": 0
    }
]