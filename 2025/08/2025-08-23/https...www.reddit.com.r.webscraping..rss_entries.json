[
    {
        "age": null,
        "album": "",
        "author": "/u/cryptoteams",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T23:18:28.881171+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T22:17:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The trick is...clean everything from the page before sending it to the LLM. I am processing pages between 0.001 and 0.003 for bigger pages. No automation yet, but definitely possible...</p> <p>Because you keep the DOM structure, the hierarchy will help to extract data very accurately. Just write a good prompt...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryptoteams\"> /u/cryptoteams </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myer8u/i_am_using_gemini_flash_25_flash_lite_for_web/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1myer8u/i_am_using_gemini_flash_25_flash_lite_for_web/\">[comments]</a></span>",
        "id": 3402981,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1myer8u/i_am_using_gemini_flash_25_flash_lite_for_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I am using Gemini Flash 2.5 Flash Lite for web scraping at scale.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Answer_2544",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T22:12:57.172462+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T19:04:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi y&#39;all,</p> <p>I&#39;m trying to gather dealer listings from cars.com across the entire USA. I need detailed info like make/model, price, dealer location, VIN, etc. I want to do this at scale, not just a few search pages.</p> <p>I&#39;ve looked at their site and tried inspecting network requests, but I&#39;m not seeing a straightforward JSON API returning the listings. Everything seems dynamically loaded, and I\u2019m hitting roadblocks like 403s or dynamic content.</p> <p>I know scraping sites like this can be tricky, so I wanted to ask, has anyone here successfully scraped cars.com at scale?</p> <p>I\u2019m mostly looking for technical guidance on how to structure the scraping process efficiently.</p> <p>Thanks in advance for any advice!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Answer_2544\"> /u/Ok_Answer_2544 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mya0y7/has_anyone_succes",
        "id": 3402696,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mya0y7/has_anyone_successfully_scraped_carscom_at_scale",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone successfully scraped cars.com at scale?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Purple_Glass7412",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T17:47:26.560894+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T17:45:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Title. Anyone got any good go to providers or things I should be doing? Ideally i&#39;d like to get up to 3k accounts which are 1 month old and more!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Purple_Glass7412\"> /u/Purple_Glass7412 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1my7yfv/looking_to_buy_x_accounts_for_scraping_any/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1my7yfv/looking_to_buy_x_accounts_for_scraping_any/\">[comments]</a></span>",
        "id": 3401529,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1my7yfv/looking_to_buy_x_accounts_for_scraping_any",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to buy X accounts for scraping, any recommendations?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/webscraping-net",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T22:12:57.380616+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T13:53:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1my22h8/built_a_scrapy_project_10k30k_news_articlesday/\"> <img src=\"https://b.thumbs.redditmedia.com/wywNHDSDuids2rAJi5wfDvhy4GsG0NKKMezmnrAEJVI.jpg\" alt=\"Built a Scrapy project: 10k-30k news articles/day, 3.8M so far\" title=\"Built a Scrapy project: 10k-30k news articles/day, 3.8M so far\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>The goal was to keep a RAG dataset current with local news at scale, without relying on expensive APIs. Estimated cost of using paid APIs was $3k-4.5k/month; actual infra cost of this setup is around $150/month.</p> <p><strong>Requirements:</strong></p> <ul> <li>Yesterday\u2019s news available by the next morning</li> <li>Consistent schema for ingestion</li> <li>Low-maintenance and fault-tolerant</li> <li>Coverage across 4.5k local/regional news sources</li> <li>Respect for <code>robots.txt</code></li> </ul> <p><strong>Stack / Approach:</strong></p> <ul> <li><strong>Article URL disco",
        "id": 3402697,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1my22h8/built_a_scrapy_project_10k30k_news_articlesday",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/wywNHDSDuids2rAJi5wfDvhy4GsG0NKKMezmnrAEJVI.jpg",
        "title": "Built a Scrapy project: 10k-30k news articles/day, 3.8M so far",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Better-Secretary8498",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T13:29:18.203236+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T13:07:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How can i download onedrive private(view only) pdf files.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Better-Secretary8498\"> /u/Better-Secretary8498 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1my0zqm/microsoft_onedrive_download_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1my0zqm/microsoft_onedrive_download_help/\">[comments]</a></span>",
        "id": 3400054,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1my0zqm/microsoft_onedrive_download_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Microsoft onedrive download help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Actual-Card239",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T05:49:01.957158+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T03:58:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p> <p>I&#39;m a game developer, and I&#39;d like to collect posts and comments from Reddit that mention our game. The goal is to analyze player feedback, find bug reports, and better understand user sentiment to help us improve our service.</p> <p>I am experienced with Python and web development, and I&#39;m comfortable working with APIs.</p> <p>What would be the best way to approach this? I&#39;m looking for recommendations on where to start, such as which libraries or methods would be most effective for this task.</p> <p>Thank you for your guidance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Actual-Card239\"> /u/Actual-Card239 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mxrfa8/how_to_collect_reddit_posts_and_comments_using/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1mxrfa8/how_to_collect_reddit_posts_and_comments_using",
        "id": 3398145,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mxrfa8/how_to_collect_reddit_posts_and_comments_using",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to collect reddit posts and comments using python",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Free-Worry-2459",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-08-23T05:49:01.788094+00:00",
        "date_dead_since": null,
        "date_published": "2025-08-23T02:17:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am building an app and API for it to collect data dedicated to a certain topic from different websites in a structured way. </p> <p>I want my endpoint to accept a link to a webpage as a parameter, extract publicly available data (can be copyright-protected) from the page, process it, and return it to a client in a structured way (JSON) if possible. The client app can display list of items taken from multiple websites, and display each item details with native UI. All the data is stored on user devices, my API is only responsible for the transformation from HTML to JSON. The JSON can contain texts (original or processed), original image links and some extracted numbers.</p> <p>So basically, it is like bookmarking pages, with modifying/extracting some data and displaying it in own UI. No personal data involved. </p> <p>Later I am going to monetize the app (by adding ads or premium features). But I won&#39;t sell any scraped data, it will not even be s",
        "id": 3398144,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1mxph2k/legality_of_the_web_scraping_scraped_data_is_kept",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Legality of the web scraping - scraped data is kept on user devices.",
        "vote": 0
    }
]