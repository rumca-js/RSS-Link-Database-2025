[
    {
        "age": null,
        "album": "",
        "author": "/u/Hephaestus2036",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T23:07:36.649458+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T22:18:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I searched prior posts here going back five years and didn&#39;t find much so thought I&#39;d ask. There are a few Slack groups that I belong to that I&#39;d like to scrape - not for leads or contacts, but more for information and resource recommendations or weekly summaries I can port to an email or use to train AI. </p> <p>I&#39;m not an Admin on these groups and as such probably not able to install native plugins. Has anyone successfully done this before and could share what you did or learned? Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hephaestus2036\"> /u/Hephaestus2036 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kms90z/strategies_resources_tactics_for_scraping_slack/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kms90z/strategies_resources_tactics_for_scraping_slack/\">[comments]</a></span>",
        "id": 2681835,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kms90z/strategies_resources_tactics_for_scraping_slack",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Strategies, Resources, Tactics for scraping Slack?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hot-Character861",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T23:07:36.838910+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T22:01:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Could anybody tell me or at least lead me into the right direction of how to reverse engineer the cookie and header generation for Target? I have made a bot that has a 10-15 second checkout time but with the right generator I could easily drop that to about 2-3 seconds and it could help me get much for product. Any help would be greatly appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hot-Character861\"> /u/Hot-Character861 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmrvid/shape_cookie_and_header_generation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmrvid/shape_cookie_and_header_generation/\">[comments]</a></span>",
        "id": 2681836,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmrvid/shape_cookie_and_header_generation",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Shape cookie and header generation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Icy_Cap9256",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T20:55:28.102673+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T19:32:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I am trying to scrape Walmart and I am not running the scaper in headless mode as of now. When I run the script, there are two pop ups, selecting location and the cookie preferences. </p> <p>The script is not able to scrape unless the two pop-ups go away. I made changes to the script so that it can interact with the pop-ups but it&#39;s 50/50. Sometimes it clicks on the pop up and sometimes it doesn&#39;t. On a successful run, it can scrape many pages but Walmart detects that it&#39;s a bot. Although that&#39;s for later, perhaps I can rate limit the scraping. The main issue are the pop-ups, I did add a browser refresh to get past it still it doesn&#39;t work. </p> <p>Any advice would be appreciated. Thank you. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Icy_Cap9256\"> /u/Icy_Cap9256 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmo8a0/how_to_get_around_walmart_pop_ups",
        "id": 2681017,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmo8a0/how_to_get_around_walmart_pop_ups_for_selenium",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to get around Walmart pop ups for Selenium scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mounirab96",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T18:30:52.974093+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T18:12:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Basically the title, I&#39;d like to learn webscraping, can someone show me the best tutorial or course? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mounirab96\"> /u/Mounirab96 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmm8bg/searching_for_videos_to_understand_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmm8bg/searching_for_videos_to_understand_webscraping/\">[comments]</a></span>",
        "id": 2679799,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmm8bg/searching_for_videos_to_understand_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Searching for videos to understand webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RainElegant1405",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T18:30:53.172814+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T17:57:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Need help finding website and scraping </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RainElegant1405\"> /u/RainElegant1405 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmlu7o/best_website_to_scrape_latin_american_phone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmlu7o/best_website_to_scrape_latin_american_phone/\">[comments]</a></span>",
        "id": 2679800,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmlu7o/best_website_to_scrape_latin_american_phone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best website to scrape Latin American phone sellers and buyers?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RainElegant1405",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T18:30:53.382634+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T17:54:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Need to scrape numbers of sellers on Latin American platforms </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RainElegant1405\"> /u/RainElegant1405 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmls4o/who_here_can_bypass_olxcom_cpf_verification/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmls4o/who_here_can_bypass_olxcom_cpf_verification/\">[comments]</a></span>",
        "id": 2679801,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmls4o/who_here_can_bypass_olxcom_cpf_verification",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Who here can bypass OLX.com CPF verification",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/albert_in_vine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T17:25:52.823082+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T17:15:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am looking for a primary source of the VIN that comes from the website like <a href=\"http://vincheck.info\">vincheck.info</a> and others, they get their data from <a href=\"https://vehiclehistory.bja.ojp.gov/nmvtis_vehiclehistory\">https://vehiclehistory.bja.ojp.gov/nmvtis_vehiclehistory</a><br/> I want to add something like this to our website so people can check their VIN and look up the vehicle history for free en masse without registering. I need to find the primary source of the VIN check data- its available somewhere. Maybe in source code or something that I get directly from vehiclehistory <a href=\"https://vehiclehistory.bja.ojp.gov/nmvtis_vehiclehistory\">https://vehiclehistory.bja.ojp.gov/nmvtis_vehiclehistory</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/albert_in_vine\"> /u/albert_in_vine </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmksbt/looking_for_a_vehicle_history_in",
        "id": 2679287,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmksbt/looking_for_a_vehicle_history_information_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a vehicle history information from somewhere publicly.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ConfidentExcuse9857",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T17:25:53.058328+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T16:36:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone. First time posting on here and I was hoping if someone could help me out. There is a website for an old Collectible Card Game that has every card that was ever printed. I would like to export every single card detail and export it into an Excel Spreadsheet. Is this even possible?</p> <p>Website - <a href=\"https://theaccordlands.com/\">https://theaccordlands.com/</a></p> <p>Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ConfidentExcuse9857\"> /u/ConfidentExcuse9857 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmjs31/need_help_exporting_data_from_a_card_website_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmjs31/need_help_exporting_data_from_a_card_website_to/\">[comments]</a></span>",
        "id": 2679288,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmjs31/need_help_exporting_data_from_a_card_website_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help exporting data from a card website - to Excel",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Gloomy_Chicken5811",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T20:55:28.378873+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T15:35:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m currently working on a scraping script to extract data from this page:<br/> <a href=\"https://textileexchange.org/find-certified-company/\">https://textileexchange.org/find-certified-company/</a></p> <p>The issue is that the data is loaded dynamically inside a Power BI iframe. </p> <p>At the moment, I use a Python + Selenium script that automates thousands of clicks and scrolls to load and scrap all the data. It works, but:</p> <ul> <li>it&#39;s not really scalable </li> <li>it&#39;s fragile,</li> <li>it&#39;s will be hard to maintain in the long run,</li> </ul> <p>I&#39;m looking for a more reliable and scalable solution. Ideally, by reverse-engineering the backend/API calls made by the embedded Power BI report, and using them to fetch the data directly in JSON or another structured format.</p> <p>Has anyone worked on something similar?</p> <ul> <li>Any tips for capturing Power BI network traffic?</li> <li>Is there a known way to reverse Power ",
        "id": 2681018,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmi96p/looking_for_a_robust_way_to_scrape_data_from_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for a robust way to scrape data from a Power BI iframe",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Visual-Librarian6601",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T15:15:57.996997+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T15:07:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>While working with LLMs for structured web data extraction, we saw issues with invalid JSON and broken links in the output. This led us to build a library focused on <strong>robust</strong> extraction and enrichment:</p> <ul> <li><strong>Clean HTML conversion</strong>: transforms HTML into LLM-friendly markdown with an option to extract just the main content</li> <li><strong>LLM structured output</strong>: Uses Gemini 2.5 flash or GPT-4o mini to balance accuracy and cost. Can also also use custom prompt</li> <li><strong>JSON sanitization</strong>: If the LLM structured output fails or doesn&#39;t fully match your schema, a sanitization process attempts to recover and fix the data, especially useful for deeply nested objects and arrays</li> <li><strong>URL validation</strong>: all extracted URLs are validated - handling relative URLs, removing invalid ones, and repairing markdown-escaped links</li> </ul> <p>&#8203;</p> <pre><code>import { extract, Cont",
        "id": 2677910,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmhjqz/open_source_robust_llm_extractor_for_htmlmarkdown",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open source robust LLM extractor for HTML/Markdown in Typescript",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Koninhooz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T13:05:54.099008+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T12:33:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone is using AI to create webscraping? Tools like Cursor, etc.<br/> Which ones are you using?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Koninhooz\"> /u/Koninhooz </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kme0di/ai_for_create_your_webcraping_bots/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kme0di/ai_for_create_your_webcraping_bots/\">[comments]</a></span>",
        "id": 2676699,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kme0di/ai_for_create_your_webcraping_bots",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI for create your webcraping bots?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ajahajahs",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T10:55:48.458261+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T10:23:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am new to scraping and beginner to coding. I managed to use JavaScript to extract webpages content listing and it works on simple websites. However, when I try to use my code to access xiaohongshu, it will pop up registration requirements before I can proceed. I realise the mobile version do not require registration. How can I get pass this? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ajahajahs\"> /u/ajahajahs </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmbow1/get_past_registration_or_access_the_mobile_web/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kmbow1/get_past_registration_or_access_the_mobile_web/\">[comments]</a></span>",
        "id": 2675621,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kmbow1/get_past_registration_or_access_the_mobile_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "get past registration or access the mobile web version for scrap",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FamiliarEnthusiasm87",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T01:11:39.996838+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T01:02:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am currently working on my second project related to scraping. I have liked the reverse engineering process, comes more easily than AI or other shizz. I need to earn in dollars to pay off some little debts from my international student life in US. So, how do I monetize this skill? I have an undergrad CS and economics degree from a very well reputed LAC college in US, and two years of work experience at another well reputed university as a researcher. Any tips are appreciated including whether there is potential to make money. The other field of CS i have liked and been debating dabling in again is systems but I don&#39;t know if there is a big market for it for remote workers. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FamiliarEnthusiasm87\"> /u/FamiliarEnthusiasm87 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1km2nhw/how_to_find_gigs_in_websrapping/\">[link]</a></span> &#32; <spa",
        "id": 2672982,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1km2nhw/how_to_find_gigs_in_websrapping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to find gigs in websrapping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/the_king_of_goats",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-14T01:11:39.723008+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-14T00:51:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you&#39;re able to push it to the absolute max, do you just go for it? OR is there some sort of &quot;rule of thumb&quot; where generally you don&#39;t want to scrape more than X pages per hour, either to maximize odds of success, minimize odds of encountering issues, being respectful to the site owners, etc?</p> <p>For context the highest I pushed it on my current run is running 50 concurrent threads to scrape one specific site. IDK if those are rookie numbers in this space, OR if that&#39;s obscenely excessive compared against best practices. Just trying to find that &quot;sweet spot&quot; where I can do it a solid pace WITHOUT slowing myself down by the issues created by trying to push it too fast and hard.</p> <p>Everything was smooth until about 60,000 pages in over a 24-hour window -- then I started encountering issues. Seemed like a combination of the site potentially throwing some roadblocks, but more likely than that it actually seemed lik",
        "id": 2672981,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1km2fjp/how_fast_is_too_fast_for_webscraping_a_specific",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How fast is TOO fast for webscraping a specific site?",
        "vote": 0
    }
]