[
    {
        "age": null,
        "album": "",
        "author": "/u/Silent-Artichoke7865",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T19:52:10.124138+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T19:40:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>App works great locally but the server IPs must be banned because i can&#39;t fetch transcripts once deployed...</p> <p>New to web scraping, was able to get a proxy working locally for a second but it stopped working today, do proxies get banned after a while too? So do i need to rotate them? And where do i get them from to avoid getting banned</p> <p>EDIT looking for a long-term solution and not just a quick fix</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Silent-Artichoke7865\"> /u/Silent-Artichoke7865 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kk9f07/how_can_i_scrape_youtube_transcripts_if_ive_been/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kk9f07/how_can_i_scrape_youtube_transcripts_if_ive_been/\">[comments]</a></span>",
        "id": 2653593,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kk9f07/how_can_i_scrape_youtube_transcripts_if_ive_been",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How can i scrape YouTube transcripts if i've been banned",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/OrdinaryDry3358",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T12:59:19.099659+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T12:05:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,<br/> I\u2019ve been learning web scraping for about a month now, and I just completed my first real project! It\u2019s a tool that lets you extract all the video links from any YouTube channel. I made it to practice and apply what I\u2019ve learned so far.</p> <p>It was a fun challenge to deal with dynamic content. I\u2019m happy with how it turned out, and I\u2019d love to hear your thoughts</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OrdinaryDry3358\"> /u/OrdinaryDry3358 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kjzhiq/made_my_first_web_scraping_project/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kjzhiq/made_my_first_web_scraping_project/\">[comments]</a></span>",
        "id": 2651463,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kjzhiq/made_my_first_web_scraping_project",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Made my first web scraping project",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kiwialec",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T10:49:45.729401+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T09:47:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been scraping/crawling in various projects/jobs for 15 years, but never connected to the community at all. I&#39;m trying to connect with others now, so would love to know about any conferences that are good.</p> <p>I&#39;m based in the UK, but would travel pretty much anywhere for a good event.</p> <ul> <li>looks like I missed Prague Crawl - definitely on the list for next year</li> <li>Extract Summit in Austin and Dublin looks interesting, but I&#39;m skeptical that it will just be a product/customer conference for Apify. Anyone been?</li> </ul> <p>Anyone know of any others?</p> <p>If there&#39;s no other meetups in the UK, any interest in a regular drinks &amp; shit talking session for london scrapers?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kiwialec\"> /u/kiwialec </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kjxchf/scraping_conferences/\">[link]</a></span> &#32; <spa",
        "id": 2650821,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kjxchf/scraping_conferences",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping conferences?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mohamed__saleh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T10:49:45.508889+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T09:21:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks!</p> <p>I built a Reddit scraper that goes beyond just pulling posts. It uses GPT-4 to: * Filter and score posts based on pain points, emotions, and lead signals * Tag and categorize posts for product validation or marketing * Store everything locally with tagging weights and daily sorting</p> <p>I use it to uncover niche problems people are discussing on Reddit \u2014 super useful for indie hacking, building tools, or marketing.</p> <p>\ud83d\udd17 GitHub: <a href=\"https://github.com/Mohamedsaleh14/Reddit_Scrapper\">https://github.com/Mohamedsaleh14/Reddit_Scrapper</a> \ud83c\udfa5 Video tutorial (step-by-step): <a href=\"https://youtu.be/UeMfjuDnE_0\">https://youtu.be/UeMfjuDnE_0</a></p> <p>Feedback and questions welcome! I\u2019m planning to evolve it into something much bigger in the future \ud83d\ude80</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mohamed__saleh\"> /u/mohamed__saleh </a> <br/> <span><a href=\"https://www.reddit.com/r/webscrapi",
        "id": 2650820,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kjwz0c/opensource_reddit_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open-source Reddit scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aaronn2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T08:39:41.873680+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T08:02:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After reading this sub for a while, it looks like there&#39;s plenty of people who are scraping millions of pages every month with minimal costs - meaning dozens of $ per month (excluding servers, database, etc).</p> <p>I am still new to this, but I get confused by that figure. If I want to reliably (meaning with relatively high success rate) scrape websites, I probably should residential proxies. These are not cheap - the prices are going from roughly $0.50/1GB of bandwidth to almost $10 in some cases.</p> <p>There are web scraping API services on the web that handle headless browsers, proxies, CAPTCHAs etc, which costs starts from around ~$150/month for 1M requests (no bandwidth limits). At glance, it looks like the residential proxies are way cheaper than the API solutions, but because of bandwidth, the price starts to quickly add up and it can actually get more expensive than the API solutions.</p> <p>Back to my first paragraph, to the people who ",
        "id": 2650370,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The real costs of web scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aaronn2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T08:39:42.064782+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T07:47:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I tried to scrape some information from idealista[.][com] - unsuccessfully. After a while, I found out that they use a system called datadome.</p> <p>In order to bypass this protection, I tried:</p> <ul> <li>premium residential proxies</li> <li>Javascript rendering (playwright)</li> <li>Javascript rendering with stealth mode (playwright again)</li> <li>web scraping API services on the web that handle headless browsers, proxies, CAPTCHAs etc.</li> </ul> <p>In all cases, I have either:</p> <ul> <li>received immediately 403 =&gt; was not able to scrape anything</li> <li>received a few successful instances (like 3-5) and then again 403</li> <li>when scraping those 3-5 pages, the information were incomplete - eg. there were missing JSON data in the HTML structure (visible in the classic browser, but not by the scraper)</li> </ul> <p>That leads me thinking about how to actually deal with such a situation? I went through some articles how datadome creates us",
        "id": 2650371,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kjvn2p/how_to_bypass_datadome_in_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to bypass datadome in 2025?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/adroitbot",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-11T07:33:33.183727+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-11T07:28:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Using this wonderful library called <a href=\"https://github.com/mcp-use/mcp-use\">mcp-use</a>, I tried to create a research agent (running on python as a client not on VSC or Claude Desktop) which goes through the web and collects all links and at the end summarizes everything .</p> <p>Video with Experiment is here :: <a href=\"https://youtu.be/khObn4yZJYE\">https://youtu.be/khObn4yZJYE</a></p> <p>These all are <strong><em>EARLY</em></strong> experiments </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/adroitbot\"> /u/adroitbot </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kjvde4/building_own_deep_research_agent_with_mcpuse/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kjvde4/building_own_deep_research_agent_with_mcpuse/\">[comments]</a></span>",
        "id": 2650131,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kjvde4/building_own_deep_research_agent_with_mcpuse",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Building Own Deep Research Agent with mcp-use",
        "vote": 0
    }
]