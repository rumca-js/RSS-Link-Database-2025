[
    {
        "age": null,
        "album": "",
        "author": "/u/madredditscientist",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T20:47:38.126825+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T19:36:58+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ktsi5y/i_built_a_live_dashboard_tracking_the_global/\"> <img src=\"https://external-preview.redd.it/q-wFd5yZOXHHeZiz5GbxsIF2up4o2EmBflX7FVYMdCk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f7132bf0a441b9b3d3a69564e27f2c02fc1dd4d\" alt=\"I built a live dashboard tracking the global waste caused by CAPTCHAs\" title=\"I built a live dashboard tracking the global waste caused by CAPTCHAs\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/madredditscientist\"> /u/madredditscientist </a> <br/> <span><a href=\"https://www.kadoa.com/captcha-impact\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktsi5y/i_built_a_live_dashboard_tracking_the_global/\">[comments]</a></span> </td></tr></table>",
        "id": 2757610,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktsi5y/i_built_a_live_dashboard_tracking_the_global",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/q-wFd5yZOXHHeZiz5GbxsIF2up4o2EmBflX7FVYMdCk.jpg?width=640&crop=smart&auto=webp&s=2f7132bf0a441b9b3d3a69564e27f2c02fc1dd4d",
        "title": "I built a live dashboard tracking the global waste caused by CAPTCHAs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Impressive_City3660",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T14:54:47.161421+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T14:44:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I really want to know a lot of ways to scrape data, because I will have a presentation about ways to scrape data to prepare for it for machine learning, and because this topic is kinda foreign to me, I only know 2 ways:</p> <ol> <li><p>Scraping website&#39;s html and use a programming language (like python and use beautiful soup) to get the content in the elements.</p></li> <li><p>Scraping website&#39;s api endpoint and because the endpoint will return a json, it&#39;s pretty easy to scrape it.</p></li> </ol> <p>Is there any more ways ? I need to pressent more than 2 :( thanks so much for helping.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Impressive_City3660\"> /u/Impressive_City3660 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktlcqi/how_many_way_of_scaping_data_for_machine_learning/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktlcqi/how",
        "id": 2754776,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktlcqi/how_many_way_of_scaping_data_for_machine_learning",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How many way of scaping data for Machine learning?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/G_Wriath",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T12:44:51.790304+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T12:29:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I work at a fintech company and we mostly work for Venture Capital Firms</p> <p>A lot of our clients request to monitor certain websites of their competitors, their portfolio companies for changes or specific updates</p> <p>Till now we were using Sitemaps + some Change Tracking services with a combination of LLM based worlflows to perform this.</p> <p>But this is not scalable, some of these websites have 1000s of subpages and mostly LLMs get confused with which to put the change tracking on.</p> <p>I did try depth based filtering but it does not seem to work on all websites and the services I am using does not natively support it.</p> <p>Looking for suggestions on possible solutions on this ?</p> <p>I am not the most experienced engineer, so suggestions for improvements on the architecture are also very welcomed.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/G_Wriath\"> /u/G_Wriath </a> <br/> <span><a href=\"http",
        "id": 2753613,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktic1n/issues_with_change_tracking_for_large_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Issues with change tracking for large websites",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/musaspacecadet",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T07:19:00.618254+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T07:02:27+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/\"> <img src=\"https://preview.redd.it/gzlu7xb4dh2f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=affd4f93986c42fc79c50fa5e74abda79de2b798\" alt=\"It's not even my repo, it's a fork!\" title=\"It's not even my repo, it's a fork!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>This should confirm all the fears I had, if you write a new bypass for any bot detection or captcha wall, don&#39;t make it public they scan the internet to find and patch them, let&#39;s make it harder </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/musaspacecadet\"> /u/musaspacecadet </a> <br/> <span><a href=\"https://i.redd.it/gzlu7xb4dh2f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/\">[comments]</a></span> </td></tr></table>",
        "id": 2751539,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/gzlu7xb4dh2f1.png?width=640&crop=smart&auto=webp&s=affd4f93986c42fc79c50fa5e74abda79de2b798",
        "title": "It's not even my repo, it's a fork!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/shady_wyliams",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T07:19:00.826933+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T06:36:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is anyone facing the same issue? I am using python, it always gives 200 but empty response.text. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shady_wyliams\"> /u/shady_wyliams </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktcu8q/i_can_no_longer_scrap_nitter_anymore_today/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktcu8q/i_can_no_longer_scrap_nitter_anymore_today/\">[comments]</a></span>",
        "id": 2751540,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktcu8q/i_can_no_longer_scrap_nitter_anymore_today",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I can no longer scrap Nitter anymore today",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hlsp0522",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T07:19:01.036296+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T06:19:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I wanted to create a directory website and was initially thinking of scraping Google Maps to feed data into this site. Is that even okay?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hlsp0522\"> /u/hlsp0522 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktcl6r/is_scraping_google_maps_okay/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktcl6r/is_scraping_google_maps_okay/\">[comments]</a></span>",
        "id": 2751541,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktcl6r/is_scraping_google_maps_okay",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is scraping Google Maps okay?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sherinmaggie",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-23T06:14:01.977561+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-23T05:14:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>$20 for your help with a KYC. US ONLY. Just 20 minutes of your time.</p> <p>Get $20 instantly? Hmu (US)</p> <p>It&#39;s a quick KYC that&#39;s takes not more than 20 minutes. payment is through crypto Lmk if you interested.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sherinmaggie\"> /u/Sherinmaggie </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktblxe/20_for_a_quick_kyc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ktblxe/20_for_a_quick_kyc/\">[comments]</a></span>",
        "id": 2751248,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ktblxe/20_for_a_quick_kyc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "$20 for a quick KYC.",
        "vote": 0
    }
]