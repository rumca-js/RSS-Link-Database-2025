[
    {
        "age": null,
        "album": "",
        "author": "/u/Haorelian",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T23:35:22.045280+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T23:34:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks,<br/> I\u2019ve been working on a personal project: a doomsday-ready PC/phone setup packed with everything you&#39;d need for survival <em>and</em> entertainment.</p> <p>Right now, I\u2019ve got a solid base going. Around 10GB of resources\u2014over 200 books and PDFs\u2014covering blacksmithing, water purification, wildlife ID, medical stuff (treatments + pharma), basic maintenance (car, electrical, general repairs), psychology, and more.</p> <p>I\u2019ve also set up a local LLM (Llama 3.1 8B), downloaded the entire Wikipedia, offline maps of my country (via OSM), and built a bootable USB with a portable Linux OS that has everything preloaded\u2014plug in and go.</p> <p>For entertainment, I\u2019ve loaded enough content to last 10+ years: manga, light novels, classic literature, etc. I\u2019ve also added ~30 practical video tutorials.</p> <p>I\u2019ve mirrored the whole setup across two laptops\u2014one of them stored in a Faraday cage in case of EMP\u2014and also cloned it onto my phone.</p> <",
        "id": 2818335,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0atuw/building_a_doomsdayproof_digital_library",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Building a Doomsday-Proof Digital Library",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/peyton_montana",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T23:35:22.196533+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T23:29:41+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l0apx0/hoarded_on_this_2012_drive_lost_cables_and_dont/\"> <img src=\"https://preview.redd.it/of8tbbsjc74f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f6eb38ddd240b5b2158902dcf79929574971d4\" alt=\"Hoarded on this 2012 drive, lost cables and don\u2019t know what kind to get\" title=\"Hoarded on this 2012 drive, lost cables and don\u2019t know what kind to get\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Went to data dump everything hoarded on this old HD to a new one - and these are the only 2 ports it has.</p> <p>Anyone know which port and cable I need to use to connect to a usb/c Mac or USB Windows?</p> <p>I\u2019ve don\u2019t have any HDs this old and no idea how to connect it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/peyton_montana\"> /u/peyton_montana </a> <br/> <span><a href=\"https://i.redd.it/of8tbbsjc74f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.c",
        "id": 2818336,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l0apx0/hoarded_on_this_2012_drive_lost_cables_and_dont",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/of8tbbsjc74f1.jpeg?width=640&crop=smart&auto=webp&s=78f6eb38ddd240b5b2158902dcf79929574971d4",
        "title": "Hoarded on this 2012 drive, lost cables and don\u2019t know what kind to get",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kyxstrez",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T22:30:26.930831+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T21:48:48+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l08jem/time_to_replace_hard_drives/\"> <img src=\"https://preview.redd.it/uwowp753u64f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69545c1d8649805a14ef38c2fbf49ea997d9125c\" alt=\"Time to replace hard drives?\" title=\"Time to replace hard drives?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I was thinking to sell my 12TB+8TB drives I unshucked many years ago and replace them with an unshucked WD Elements 20TB (350\u20ac). Is that a solid plan? These drives are used in my main PC, and I have another 18TB WD Elements I bought in 2023 that I use as an external backup.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kyxstrez\"> /u/Kyxstrez </a> <br/> <span><a href=\"https://i.redd.it/uwowp753u64f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l08jem/time_to_replace_hard_drives/\">[comments]</a></span> </td></tr></table>",
        "id": 2818062,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l08jem/time_to_replace_hard_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/uwowp753u64f1.png?width=640&crop=smart&auto=webp&s=69545c1d8649805a14ef38c2fbf49ea997d9125c",
        "title": "Time to replace hard drives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Massive_Resort2535",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T22:30:27.117543+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T21:05:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My mom passed away recently and I found a small carry-on suitcase and large plastic tote full of childhood photos and memorabilia. I have a cheap HP printer and scanner combo I\u2019d been using, but after several hours of scanning them one by one and having to crop them, rotate and save them, I became fed up and new there was a faster solution.</p> <p>There are companies that specialize in this - one being LegacyBox. But it can get pricey and worst of all, I worry about them losing or misplacing photos.</p> <p>So, I\u2019d rather do them on my own. According to Reddit, the best scanner for this is the Epson FastFoto FF-680W Photo Scanner. It\u2019s perfect for scenarios like this.</p> <p>However, the only issues I can think of is: 1. There are several larger school photo sizes than I\u2019m not sure if it can fit to scan. Does any know the largest size it will scan?</p> <ol> <li><p>I\u2019d also like to find a printer/scanner hybrid. I have to print a lot for various legal m",
        "id": 2818063,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l07l0r/best_printerscanner_combo_capable_of_scanning",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Printer/Scanner Combo capable of scanning legal size documents and bulk photos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jackzzae",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T22:30:27.420047+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T20:36:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! This is a project i&#39;ve been thinking about doing for a while now, inspired mainly by SearchCord.</p> <p>I only scrape servers that are publically available. Maybe later I could add a feature where you guys can suggest servers to scrape?</p> <p>I made a version for people in the European Union aswell, to comply with GDPR rules.</p> <p>You can opt-out using a form aswell.</p> <p>I&#39;d love to hear feedback on it &lt;3 </p> <p><a href=\"https://skrycord.web1337.net\">https://skrycord.web1337.net</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jackzzae\"> /u/jackzzae </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l06xab/skrycord_a_free_archive_of_discord/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l06xab/skrycord_a_free_archive_of_discord/\">[comments]</a></span>",
        "id": 2818064,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l06xab/skrycord_a_free_archive_of_discord",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "SkryCord: A free archive of Discord",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Silent-OCN",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T22:30:26.749897+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T20:36:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Bit of background I have a 16TB WD or seagate hard drive. Used for backup of my whole pc. I stupidly put encryption on the drive a while back but got sick of the slow time to unlock the drive. I\u2019m not sure why but the decryption got stuck and I ended up turning the pc off. The drive was removed from the system up until this week when I found the drive and decided to plug it back in. </p> <p>Initially the drive works ok I can load the files from it and windows sees it. The problem is the decryption has resumed but it\u2019s taking forever and a day. It\u2019s literally taking a day for 1% decryption at best and now it is stuck at 38.9% decryption. </p> <p>Another issue is if I restart the pc the computer doesn\u2019t load and it\u2019s sheer luck I can get the pc to post with the decrypting drive installed. </p> <p>Anyone know what the problem is here? I would really like to use it for backup but it seems the decryption is causing real issues. </p> <p>Thanks for any advic",
        "id": 2818061,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l06wv4/im_having_an_issue_with_a_16tb_backup_drive_can",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I\u2019m having an issue with a 16TB backup drive, can anyone help please?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/philcolinsfan",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T19:13:00.470841+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T18:56:01+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1l04nhu/storage_spaces_showing_error_even_though_both/\"> <img src=\"https://preview.redd.it/d2mj8yh7z54f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3a9ca9db1d0f9f68cc32b85cd9900b032b2d6c7\" alt=\"Storage Spaces showing error even though both drives online\" title=\"Storage Spaces showing error even though both drives online\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I just bought two ironwolf 4tb drives, and installed them in the OWC Mercury Pro Elite Quad. I set them up in raid 1 configuration. My data seems to be mirrored on both drives, and they&#39;re both online. Why is the storage pool saying there is no resiliency? I know storage spaces isn&#39;t that great, but I only have a windows machine that can handle what I want to do with the data. Is there other windows software I should be using? Do I just ignore the error? Thank you in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a hre",
        "id": 2817269,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l04nhu/storage_spaces_showing_error_even_though_both",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/d2mj8yh7z54f1.png?width=640&crop=smart&auto=webp&s=f3a9ca9db1d0f9f68cc32b85cd9900b032b2d6c7",
        "title": "Storage Spaces showing error even though both drives online",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Spektre99",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T19:13:00.737375+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T18:51:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>&quot;Degraded&quot;</p> <p>I currently have a 20TB RAID 1 array in Windows Server 2019. Is it possible to create a degarded RAID 5 array with only 2 disks? If so, I could do so, copy my 20G data over from a new backup disk, Add that disk to the array and let it rebuild, effectively converting my 2 disk RAID 1 to a 3 disk RAID 5 with only the purchase of 1 disk.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Spektre99\"> /u/Spektre99 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l04jy9/creating_a_degarded_raid5_array_in_windows_server/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l04jy9/creating_a_degarded_raid5_array_in_windows_server/\">[comments]</a></span>",
        "id": 2817270,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l04jy9/creating_a_degarded_raid5_array_in_windows_server",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Creating a degarded RAID5 array in Windows Server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/OneChrononOfPlancks",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T19:13:00.885922+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T18:23:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Proud owner of &quot;Scene It! Star Trek DVD Board Game,&quot; but without a hooked-up DVD player, and desiring to preserve the DVD in case it gets lost or damaged, I opened DVDShrink and created a &quot;No Compression&quot; decrypted ISO rip. (edit: My wife and I played the game this way last week).</p> <p>This is the method I use for all conventional DVDs.</p> <p>Scene It! runs via Kodi like any DVD ISO, including the menus, and it&#39;s mostly playable, but there are bugs:</p> <ol> <li><p>The &quot;How to Play&quot; tutorials seem to run in some combination of a) unreliably, b) at the wrong speed, c) without sound, and d) difficult to control playback.</p></li> <li><p>The &quot;Final Frontier&quot; challenges at the end of the game don&#39;t run properly and aren&#39;t responsive.</p></li> <li><p>Some forms of question don&#39;t pause on the question, they only flash the question and then play right to the answer immediately.</p></li> </ol> <p>Thes",
        "id": 2817271,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l03x4p/difficulty_preserving_scene_it_dvd_iso_for_use",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Difficulty preserving \"Scene It!\" DVD ISO for use via Kodi",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JJPath005",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T18:09:17.084492+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T17:37:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Every Year I get like 1 TB of content. It adds pretty quick and its been a bit over 3 years and my 4TB hard drive is kinda iffy right now. What is the best tool to archive important content, and keep it safe without damage for a long time.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JJPath005\"> /u/JJPath005 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l02sve/long_term_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l02sve/long_term_archive/\">[comments]</a></span>",
        "id": 2816928,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l02sve/long_term_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Long Term Archive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JJPath005",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T18:09:17.237844+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T17:35:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Every week I take like 15 GB of footage and it adds pretty quick. What is the most efficient way to upload and store this content. Im saying 1 TB as it allows me space to leverage and avoids bigger crashing issues. Is an SSD Disk the best option.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JJPath005\"> /u/JJPath005 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l02rh7/fastest_and_most_reliable_1tb_storage_tool/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l02rh7/fastest_and_most_reliable_1tb_storage_tool/\">[comments]</a></span>",
        "id": 2816929,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l02rh7/fastest_and_most_reliable_1tb_storage_tool",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fastest and most reliable 1TB Storage Tool",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/spinnerspin1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T18:09:17.627148+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T17:07:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought a 24tb and turned out to be a Barracuda.. My intention is to use it and run a 24/7 minecraft twitch stream covering my previous twitch vods from the past 3-4 years... Am i screwed? it will basically be reading data 24/7 non stop... I&#39;d be happy if this thing last for even just 3 years but ideally i&#39;d like for it to last at least 5.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/spinnerspin1\"> /u/spinnerspin1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l023op/bought_a_seagate_24tb_expansions_is_a_barracuda/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l023op/bought_a_seagate_24tb_expansions_is_a_barracuda/\">[comments]</a></span>",
        "id": 2816930,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l023op/bought_a_seagate_24tb_expansions_is_a_barracuda",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bought a Seagate 24TB Expansions is a Barracuda. Am i fucked?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/shellshock321",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T17:04:23.858239+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T16:05:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The Reason I ask this has to do with the Ports. the 5tb Enclosures premade has to do with some weird ass Ports. Like its a variation of micro usb apparently. Just Awful.</p> <p>Usb C seems to be more modern. However is there a real difference? Or am I just screwing myself?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shellshock321\"> /u/shellshock321 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l00nml/is_it_better_to_buy_a_premade_5tb_external_hard/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l00nml/is_it_better_to_buy_a_premade_5tb_external_hard/\">[comments]</a></span>",
        "id": 2816589,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l00nml/is_it_better_to_buy_a_premade_5tb_external_hard",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is it better to buy a premade 5tb external hard drive or buy a usb-c 5tb 15mm enclosure?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wifi_cable_rental",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T15:58:03.945837+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T15:44:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, </p> <p>Are the Seagate Exos X18 - for 230 euri a good deal?<br/> Its refurb with 2 year of warr. </p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wifi_cable_rental\"> /u/wifi_cable_rental </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l005ld/seagate_exos_x18_sata_standard_model/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1l005ld/seagate_exos_x18_sata_standard_model/\">[comments]</a></span>",
        "id": 2816252,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1l005ld/seagate_exos_x18_sata_standard_model",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate Exos X18 (SATA, Standard model)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/babybuttoneyes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T13:49:21.278666+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T12:47:33+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzwadd/look_at_this_big_boy/\"> <img src=\"https://b.thumbs.redditmedia.com/JhO9VY1m5vdy9ZMwOtINvjKnWNecPSsT5keEjaEop_w.jpg\" alt=\"Look at this big boy!\" title=\"Look at this big boy!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Not sure what era this is from, but it\u2019s certainly very heavy and bulky!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/babybuttoneyes\"> /u/babybuttoneyes </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1kzwadd\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzwadd/look_at_this_big_boy/\">[comments]</a></span> </td></tr></table>",
        "id": 2815510,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzwadd/look_at_this_big_boy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/JhO9VY1m5vdy9ZMwOtINvjKnWNecPSsT5keEjaEop_w.jpg",
        "title": "Look at this big boy!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Affectionate_Day8849",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T12:43:12.733795+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T12:36:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>hello everyone, I\u2019m new here, but I\u2019ve been on a mission for the last 4 1/2 years to collect all of a old Internet radio station called WFUCKOFF RADIO, I was pointed here as a way of possibly finding more. I have a list of all dates that I\u2019m missing.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Affectionate_Day8849\"> /u/Affectionate_Day8849 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzw2ti/wfuckoff_radio/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzw2ti/wfuckoff_radio/\">[comments]</a></span>",
        "id": 2815166,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzw2ti/wfuckoff_radio",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "WFUCKOFF RADIO",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/babybuttoneyes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T12:43:12.881698+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T12:34:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Not sure what era this is from, but it\u2019s certainly very heavy and bulky!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/babybuttoneyes\"> /u/babybuttoneyes </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzw1uy/look_at_this_big_boy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzw1uy/look_at_this_big_boy/\">[comments]</a></span>",
        "id": 2815167,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzw1uy/look_at_this_big_boy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Look at this big boy",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/_Captain_John_Price_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T11:37:36.735975+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T11:14:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Has anyone used both MEGA and pCloud for a long time? What\u2019s your take on them, especially in terms of privacy?I used pCloud back in 2019, but then I switched to MEGA and have been using it since. However, MEGA has increased its prices recently, so I&#39;m thinking about switching. I\u2019ve seen a lot of reviews saying pCloud is better than MEGA.Should I go for pCloud&#39;s yearly plan, or would it be better to stay in Mega? Also, for those who use pCloud,should I get the additional encryption add-on? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_Captain_John_Price_\"> /u/_Captain_John_Price_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzumgs/mega_vs_pcloud/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzumgs/mega_vs_pcloud/\">[comments]</a></span>",
        "id": 2814878,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzumgs/mega_vs_pcloud",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mega vs Pcloud",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/babybuttoneyes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T10:33:09.368964+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T10:29:19+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kztxew/box_one_of_many/\"> <img src=\"https://preview.redd.it/aujaf40ah34f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a58b4c8e3ef832c9bb123d2f27327584086c419\" alt=\"Box one of many\u2026.\" title=\"Box one of many\u2026.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Just sorting through my dad\u2019s stuff, getting ready to give them a good smash\u2026.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/babybuttoneyes\"> /u/babybuttoneyes </a> <br/> <span><a href=\"https://i.redd.it/aujaf40ah34f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kztxew/box_one_of_many/\">[comments]</a></span> </td></tr></table>",
        "id": 2814548,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kztxew/box_one_of_many",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/aujaf40ah34f1.jpeg?width=640&crop=smart&auto=webp&s=4a58b4c8e3ef832c9bb123d2f27327584086c419",
        "title": "Box one of many\u2026.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/srosenow_98",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T09:28:08.421448+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T09:20:40+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I saw a thread here from a year ago where someone was asking on how to download from AmericanArchive.org.</p> <p>There are some Mount St. Helens-related raw video files I&#39;m looking to acquire, and I&#39;ve tried the Stream Detector method as well as the &quot;Convert/Save&quot; method in VLC, however when trying to do the Convert/Dave method, it only saves the first video in the playlist, which is a ten-second clip of a gameshow promo. The entire video is almost 50 minutes in length.</p> <p>I&#39;ve also tried developer tools to no avail, in efforts to locate the media that way.</p> <p><a href=\"https://americanarchive.org/catalog/cpb-aacip-153-25x69s4f\">https://americanarchive.org/catalog/cpb-aacip-153-25x69s4f</a></p> <p>Is there a way to download the entire video? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/srosenow_98\"> /u/srosenow_98 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataH",
        "id": 2814279,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzsx5t/downloading_from_americanarchiveorg_how_do_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading from AmericanArchive.org - How do I convert/save an entire video playlist?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/General_Category_736",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T09:28:08.572243+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T08:59:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all, I\u2019m planning to upgrade my TerraMaster F4-210 NAS, which currently has 3x 20TB HDDs in RAID 5. It\u2019s been solid for storage, but I\u2019m running into performance issues and want a more powerful NAS (better CPU, software, or features).My biggest worry is keeping all my data safe during the upgrade. Can I transfer my RAID 5 array (drives and all) to a new NAS without losing anything? Are there NAS brands or models that support this kind of migration? I\u2019d really like to avoid rebuilding the array or restoring from backups if possible .Any recommendations for NAS models that can handle 20TB drives in RAID 5, steps for a smooth migration, or pitfalls to avoid? Thanks for any tips!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/General_Category_736\"> /u/General_Category_736 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzsman/upgrading_from_terramaster_f4210_with_3x_20tb/\">[link]</a></sp",
        "id": 2814280,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzsman/upgrading_from_terramaster_f4210_with_3x_20tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Upgrading from TerraMaster F4-210 with 3x 20TB HDDs (RAID 5) to a Better NAS Without Data Loss \u2013 Advice Needed!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Broad_Sheepherder593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T06:13:08.586818+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T05:25:23+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzpfcl/vibration_shock_concern/\"> <img src=\"https://b.thumbs.redditmedia.com/WY50KTMUl17iadXuYLxqzs5xJdAj32m36t_8K7z-XRM.jpg\" alt=\"Vibration/ shock concern?\" title=\"Vibration/ shock concern?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>Was able to set up my nas + mini pc on top of a cabinet to keep away from the kids. Using ironwolf and wd red drives.</p> <p>Just thinking if the normal open closing of cabinets would hurt the drives? I did add some padding to reduce the wood to wood impact but still there&#39;s contact.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Broad_Sheepherder593\"> /u/Broad_Sheepherder593 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1kzpfcl\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzpfcl/vibration_shock_concern/\">[comments]</a></span> </td></tr></table>",
        "id": 2813564,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzpfcl/vibration_shock_concern",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/WY50KTMUl17iadXuYLxqzs5xJdAj32m36t_8K7z-XRM.jpg",
        "title": "Vibration/ shock concern?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/diamondsw",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T06:13:08.382343+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T05:24:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a collection of songs that I&#39;d like to match up to music videos and build metadata. Ideally I&#39;d feed it a bunch of source songs, and then fingerprint audio tracks against that. Scripting isn&#39;t an issue - I can pull out audio tracks from the files, feed them in, and save metadata - I just need the core &quot;does this audio match one of the known songs&quot; piece. I figure this has to exist already - we had ContentID and such well before AI.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/diamondsw\"> /u/diamondsw </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzpeul/audio_fingerprinting_software/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzpeul/audio_fingerprinting_software/\">[comments]</a></span>",
        "id": 2813563,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzpeul/audio_fingerprinting_software",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Audio fingerprinting software?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/XStylus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T06:13:08.232133+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T05:21:43+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just spent a few hours hunting down an alarming issue when copying a folder via MacOS Finder to a Samba share.</p> <p>TL;DR, if you&#39;re using the <strong>veto files = &quot;/.DS_Store/&quot;</strong> global parameter in Samba you&#39;re playing with fire. A bug in either Samba or macOS Finder (or both) will falsely indicate a successful folder copy when, in fact, files within the folder had <em>not</em> been copied.</p> <p>Here&#39;s the conditions on how to replicate the issue:</p> <ol> <li>Set the following global parameter in smb.conf on the Samba file server: <strong>veto files = &quot;/.DS_Store/&quot;</strong></li> <li>Mount the Samba file server on a macOS client.</li> <li>Create three folders and put whatever files you want into each folder.</li> <li>Open up a Terminal window, navigate to the first folder, and run &quot;<strong>ls -hal</strong>&quot; to see if there&#39;s a .DS_Store file in it. If so, delete it.</li> <li>Navigate to the ",
        "id": 2813562,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzpd8z/hidden_data_loss_risk_when_using_samba_veto_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hidden data loss risk when using Samba \"veto files\" parameter to block \".DS_Store\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FlowerKnightForever",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T05:08:49.118402+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T04:43:52+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzoqeh/fakenew_verbatim_mdisc_bdr_stress_test/\"> <img src=\"https://b.thumbs.redditmedia.com/3LZcTMcFkC5uLwtIlsSOn6wUubiI21kmX0rX1CsoqZc.jpg\" alt=\"Fake/new Verbatim MDISC BD-R stress test\" title=\"Fake/new Verbatim MDISC BD-R stress test\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Just like many here I&#39;ve read about M-DISCs and what a great medium they are for long term data storage, with some brands claiming that their M-DISCs can hold data for up to 1000 years! Isn&#39;t that crazy? Doesn&#39;t that sound too good to be true?</p> <p>So of course, as someone who&#39;s interested in preserving his favorite media and even possibly creating time capsules for posterity, and testing wild claims for himself, I had to order a whole bundle of Verbatim M-DISCs directly from German Amazon (and I mean directly, not from some random shady seller).</p> <p>On the very same item page I ordered these discs from, ther",
        "id": 2813408,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzoqeh/fakenew_verbatim_mdisc_bdr_stress_test",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/3LZcTMcFkC5uLwtIlsSOn6wUubiI21kmX0rX1CsoqZc.jpg",
        "title": "Fake/new Verbatim MDISC BD-R stress test",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/djtron99",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T02:56:48.241194+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T02:46:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m looking for an affordable setup and old gen laptop. This will be used when travelling out of town and also acts as a cheap 4th backup of my home NAS.</p> <ol> <li>Is it sensible to do this if I still don&#39;t have a gaming laptop but already have a gaming desktop, main and backup home NAS and 8tb offsite external drive. Outside onnection to my home NAS is not reliable and affordable.</li> <li>Which particular laptop and specs that&#39;s capable of hoarding 15-20tb media, files and playing light games (local install of Windows games in low-mid graphics of NBA2K, GTA V, Brawlhalla, emulation). I&#39;m looking at ThinkPad T480 with i5 8th gen but not sure if it&#39;s gaming and hoarding capable.</li> <li>How do you allocate the needed storage space from SSD, m.2, USB for external drive, etc.? Only got portable 2tb ssd and 4tb hdd.</li> <li>What dual/single boot, OS and file system setup? Do I need file/folder/drive encryption and any securit",
        "id": 2813024,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzmn7m/data_hoarding_capable_laptop_w_light_gaming",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data hoarding capable laptop w/ light gaming",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HopWorks",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T04:01:48.684280+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T01:34:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I would have tagged onto a previous post here &quot;New External Hard Drive Dropped By Delivery Man&quot; but it was archived.</p> <p>I just noticed on my Ring camera my UPS delivery of my new mechanical hard drive that it was dropped on my porch. It was not a large distance, but it made an impact sound and I can see that it dropped 8-12 inches. I would not even care about this, but since there are warranty concerns, longevity, and how fragile mechanical drives can be, I worry about the slightest shock of my new drive. After all, they are still expensive.</p> <p>To be honest, who even knows how my package has been played Hacky Sack with all through the UPS distribution. What comes to mind is the opening scene in Ace Ventura: Pet Detective. LOL</p> <p>So I ask, since this HAS to be a nagging issue with these devices being delivered, and how they are handled, when dealing with warranty issues... have they found a way to place a shock sensor in these dri",
        "id": 2813189,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzlaw7/new_external_hard_drive_dropped_by_ups_smart",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New External Hard Drive Dropped By UPS - Smart Drive Shock Parameter?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/alienccccombobreaker",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T01:51:48.780430+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T01:21:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to find the best value NAS with the most bays possible. </p> <p>Honestly I plan to stick as much storage as I can so the most bays the better. </p> <p>I guess we need some restrictions here so maximum 20 bays.</p> <p>Minimum 8 bays. </p> <p>I plan to start small and put 4-8 hard drives first I already have first then add some later down the line. </p> <p>So what NAS options do I have with these criteria. </p> <p>Optionally are there any NAS I can also add m.2 sticks nvme drives and sata 6 drives such as the Samsung evo 870 4TB it&#39;s not priority just I have a few laying around. </p> <p>If none of the above criteria make sense just give me recommendations for the best lowest price NAS.. But I definitely want minimum 6-8 bays. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/alienccccombobreaker\"> /u/alienccccombobreaker </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzl2",
        "id": 2812849,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzl2is/looking_to_finally_get_a_nas_for_home_office",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to finally get a NAS for home office",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Blazianazn",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T00:47:34.532479+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T00:29:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for a efficient way to organize 20Tb of photos and videos I have on my hard drive.</p> <p>I would like the UI that&#39;s like the android photo gallery on the phones.</p> <p>The Photos app on windows 11 is...idk...because I&#39;ve tried adding my external and it&#39;s been about 5 days of continuous running/scanning and it&#39;s not done yet...</p> <p>Just want something thats: Easy to sort by metadata. Possible map of GPS locations Can display thumbnails of Raw and Mov files (Win11 doesnt show thumbnails of my MOV files).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Blazianazn\"> /u/Blazianazn </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzk0xt/what_is_a_good_photo_gallery_program_for_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzk0xt/what_is_a_good_photo_gallery_program_for_the/\">[comments]</a></span>",
        "id": 2812601,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzk0xt/what_is_a_good_photo_gallery_program_for_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is a good photo gallery program for the windows 11 that has similar attributes to the android photo gallery storage system?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zeroedit",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-31T00:47:34.687781+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-31T00:21:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve got a 10 TB drive in an external dock that I use for images. Just connected it to my newer PC, and many of the folders can&#39;t be accessed due to old permissions. I know the drill...you just have to go into the security settings and update the permissions...but the problem here is that I have HUNDREDS AND HUNDREDS OF MILLIONS of files that Windows has to set security information on.</p> <p>Do I have any alternative other than just waiting for this to complete? At this rate, I&#39;m pretty sure it&#39;s going to take over a week to finish.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zeroedit\"> /u/zeroedit </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzjv5n/windows_file_permissions_nightmare/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kzjv5n/windows_file_permissions_nightmare/\">[comments]</a></span>",
        "id": 2812602,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kzjv5n/windows_file_permissions_nightmare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Windows file permissions nightmare",
        "vote": 0
    }
]