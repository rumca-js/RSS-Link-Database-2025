[
    {
        "age": null,
        "album": "",
        "author": "/u/dick-the-prick",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-22T21:29:44.147966+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-22T21:15:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I see various approaches to testing - test on local machine/CI first and only if that passes build the image etc. That requires orchestration outside docker.</p> <p>I think the best way is to have multistage builds and fail the build of the image if the tests fail, otherwise the image that&#39;ll be built will not be sound/correct.</p> <p>```</p> <h1>pseudo code</h1> <p>FROM python as base COPY requirements.txt . RUN pip install -r requirements.txt COPY src-code .</p> <p>FROM base as tests COPY requirements-test.txt . RUN pip install -r requirements-test.txt COPY test-code . ARG LINT=1 ARG TESTS=1 RUN if [ ${LINT} != &#39;0&#39; ]; then pylint .; fi RUN if [ ${TESTS} != &#39;0&#39; ]; then pytest .; fi RUN touch /tmp/success</p> <p>FROM base as production-image</p> <h1>To make it depend on tests stage completing first</h1> <p>COPY --from=tests /tmp/success /tmp/success ENTRYPOINT ./app.py ```</p> <p>Now whether you use vanilla docker or docker-compose",
        "id": 2748922,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1kt27u3/failing_to_build_an_image_if_the_tests_fail_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Failing to build an image if the tests fail and all done in docker is the only sane way - am I being unreasonable?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/klaasvanschelven",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-22T08:15:53.674645+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-22T08:10:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>While the &quot;one process per container&quot; principle is widely advocated, it&#39;s not always the most practical solution. In this article, I explore scenarios where running multiple tightly-coupled processes within a single Docker container can simplify deployment and maintenance.</p> <p>To address the challenges of managing multiple processes, I introduce <code>monofy</code>, a lightweight Python-based process supervisor. <code>monofy</code> ensures:</p> <ul> <li>Proper signal handling and forwarding (e.g., <code>SIGINT</code>, <code>SIGTERM</code>) to child processes.</li> <li>Unified logging by forwarding <code>stdout</code> and <code>stderr</code> to the main process.</li> <li>Graceful shutdown by terminating all child processes if one exits.</li> <li>Waiting for all child processes to exit before shutting down the parent process.(<a href=\"https://github.com/bugsink/monofy\">GitHub</a>)</li> </ul> <p>This approach is particularly beneficial w",
        "id": 2742177,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1ksldj4/running_multiple_processes_in_a_single_docker",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Running Multiple Processes in a Single Docker Container \u2014 A Pragmatic Approach",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/r0075h3ll",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-22T06:03:08.092446+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-22T05:31:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there!</p> <p>Have a docker image running a binary that pulls docker images from remote repository to perform some sort of scan - which requires credentials. I was looking for ways in which credentials can be passed to the docker image for the binary to be able to pull images.</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/r0075h3ll\"> /u/r0075h3ll </a> <br/> <span><a href=\"https://www.reddit.com/r/docker/comments/1ksj368/registry_credentials_in_docker_image/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/docker/comments/1ksj368/registry_credentials_in_docker_image/\">[comments]</a></span>",
        "id": 2741525,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1ksj368/registry_credentials_in_docker_image",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Registry Credentials in Docker Image",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/srcLegend",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-22T04:58:11.826498+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-22T03:54:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve tried pretty much every option that came to mind or that I could search around (except setting up a reverse proxy natively, outside of Docker), but I&#39;m unable to get a client&#39;s real IP address, whether I have host networking enabled or not (though this is Docker on Windows 10, so might be the actual cause).</p> <p>I tried using nginx-proxy-manager, traefik and caddy, but to no avail. Cannot get the actual IP address I am connecting from no matter what.</p> <p>Here&#39;s my final configuration for nginx-proxy-manager:</p> <ul> <li><a href=\"https://pastebin.com/zRmRdvKN\">docker-compose file</a></li> <li><a href=\"https://pastebin.com/srWMwaY8\">whoami nginx file</a></li> <li><a href=\"https://pastebin.com/XUrXHXSN\">included nginx proxy configuration</a></li> <li><a href=\"https://pastebin.com/HmSFzt6W\">whoami response, connected from my phone through its 5G network</a> <ul> <li>The <code>172.xx.xx.xx</code> IPs are the internal Docker netwo",
        "id": 2741277,
        "language": null,
        "link": "https://www.reddit.com/r/docker/comments/1kshhwi/giving_up_on_retrieving_client_ip_addresses_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 580,
        "source_url": "https://www.reddit.com/r/docker/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Giving up on retrieving client IP addresses from behind a dockerized reverse proxy...",
        "vote": 0
    }
]