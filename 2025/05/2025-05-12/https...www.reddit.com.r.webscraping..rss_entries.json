[
    {
        "age": null,
        "album": "",
        "author": "/u/qwsfrb",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-12T07:54:59.248451+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-12T07:40:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a list of company websites, and I want to write a Python script to help me get the physical addresses of these companies. What are the best ways to approach this? I have already tried JSON-LD, but most of the websites don&#39;t have their information there. Its my first task at work help me \ud83d\ude04</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/qwsfrb\"> /u/qwsfrb </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kkmrfy/company_addresses_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kkmrfy/company_addresses_help/\">[comments]</a></span>",
        "id": 2656224,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kkmrfy/company_addresses_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Company addresses help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DatakeeperFun7770",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-12T07:54:59.439157+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-12T07:03:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I\u2019m building a Scrapy spider (using the scrapy-playwright integration) to scrape product pages from <a href=\"https://www.forestessentialsindia.com\">forestessentialsindia.com</a>. The pages are littered with two different modal overlays that break my scraper by covering the content or intercepting clicks:</p> <ol> <li>AMP Subscription Prompt <ul> <li>Loaded by an external script matching **/*amp-web-push*.js</li> <li>Injects an &lt;iframe&gt; containing a \u201cSubscribe\u201d box with ID #webmessagemodalbody and nested containers</li> </ul></li> <li>Mageplaza \u201cWelcome\u201d Popup <ul> <li>Appears as &lt;div class=&quot;smt-block&quot; id=&quot;DIV\u2026&quot;&gt; inside an &lt;aside class=&quot;modal-popup \u2026&quot;&gt;</li> <li>No distinct script URL in Network tab (it seems inline or bundled)</li> </ul></li> </ol> <h1>What I\u2019ve Tried</h1> <ol> <li>Route-abort external scriptsThis successfully prevents the AMP subscription code, but the Mageplaza popup stil",
        "id": 2656225,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kkm8n2/preventing_javascript_modals_in_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Preventing JavaScript Modals in a Scrapy-Playwright Spider",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pulokjk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-12T03:28:22.668490+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-12T02:33:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone<strong>,</strong> I&#39;m currently building a scraping tool for a client to extract <strong>contact data from Apollo website</strong>.</p> <h1> The Goal:</h1> <ul> <li>Extract <strong>up to 3000 contacts</strong> (Apollo limit: 25 per page \u00d7 120 pages)</li> <li>Complete the scraping <strong>within 2\u20133 minutes max</strong></li> <li>Collect the following fields: <ul> <li>Email Address (revealed after clicking)</li> <li>Company Website URL (requires going into profile)</li> </ul></li> </ul> <h1> Current Challenges:</h1> <ul> <li><strong>Slow Performance with Selenium:</strong> Even with headless mode, scrolling optimizations, and profile caching, scraping 100 pages takes too long.</li> <li><strong>Email Hidden Behind a Button:</strong> The email is not shown by default \u2014 it requires clicking \u201cAccess email,\u201d and sometimes loading additional UI, which slows down automation.</li> <li><strong>Company Website Not on List Page:</strong> I have to",
        "id": 2655301,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kkhugr/need_help_optimizing_apollo_website_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need Help Optimizing Apollo website Scraping",
        "vote": 0
    }
]