[
    {
        "age": null,
        "album": "",
        "author": "/u/marcikque",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T23:03:27.364001+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T22:34:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to create an app which scrapes and aggregates the google maps links for all store locations of a given chain (e.g. input could be &quot;McDonalds&quot;, &quot;Burger King in Sweden&quot;, &quot;Starbucks in Warsaw, Poland&quot;).</p> <p>My approaches:</p> <ul> <li><p>google places api: results limited to 60</p></li> <li><p>Foursquare places api: results limited to 50</p></li> <li><p>Overpass Turbo (OSM api): misses some locations, especially for smaller brands, and is quite sensitive on input spelling</p></li> <li><p>google places api + sub-gridding: tedious and explodes the request count, especially for large areas/worldwide</p></li> </ul> <p>Does anyone know a proper, exhaustive, reliable, complete API? Or some other robust approach?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/marcikque\"> /u/marcikque </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxup2u/getting_all_loc",
        "id": 2793898,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxup2u/getting_all_locations_per_chain",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting all locations per chain",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mrefactor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T20:53:25.262172+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T20:14:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I&#39;ve been seriously thinking about creating a scripting language designed specifically for web scraping. The idea is to have something interpreted (like Python or Lua), with a lightweight VM that runs native functions optimized for HTTP scraping and browser emulation.</p> <p>Each script would be a .scraper file \u2014 a self-contained scraper that can be run individually and easily scaled. I\u2019d like to define a simple input/output structure so it works well in both standalone and distributed setups.</p> <p>I\u2019m building the core in Rust. So far, it supports variables, common data types, conditionals, loops, and a basic print() and fetch().</p> <p>I think this could grow into something powerful, and with community input, we could shape the syntax and standards together. Would love to hear your thoughts!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mrefactor\"> /u/mrefactor </a> <br/> <span><a href=\"ht",
        "id": 2793086,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxr942/i_am_building_a_scripting_language_for_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I am building a scripting language for web scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/schnicel",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T19:48:25.899951+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T19:13:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kxpppv/tool_for_check_european_accessibility_act_via_cli/\"> <img src=\"https://external-preview.redd.it/JHHUqUBtBQOXZWnSsEtg4-Z5k5Jm3XiawKBqJa36phQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8754cc9c66f96db5934d0c30b120cd006b0af866\" alt=\"Tool for check [European Accessibility Act] via CLI\" title=\"Tool for check [European Accessibility Act] via CLI\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/schnicel\"> /u/schnicel </a> <br/> <span><a href=\"https://github.com/be-lenka/be-a11y\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxpppv/tool_for_check_european_accessibility_act_via_cli/\">[comments]</a></span> </td></tr></table>",
        "id": 2792555,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxpppv/tool_for_check_european_accessibility_act_via_cli",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/JHHUqUBtBQOXZWnSsEtg4-Z5k5Jm3XiawKBqJa36phQ.jpg?width=640&crop=smart&auto=webp&s=8754cc9c66f96db5934d0c30b120cd006b0af866",
        "title": "Tool for check [European Accessibility Act] via CLI",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MasterFricker",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T19:48:26.064925+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T18:54:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to automate scrapping some websites, been tried to use browserstack but I got detected as a bot easily, wondering what possible docker based solutions are out there, I tried</p> <p><a href=\"https://github.com/Hudrolax/uc-docker-alpine\">https://github.com/Hudrolax/uc-docker-alpine</a></p> <p>Wondering if there is any docker image that is up to date and consistently maintained.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MasterFricker\"> /u/MasterFricker </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxp8uw/looking_for_docker_based_webscrapping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxp8uw/looking_for_docker_based_webscrapping/\">[comments]</a></span>",
        "id": 2792556,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxp8uw/looking_for_docker_based_webscrapping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking for docker based webscrapping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Separate-Breath2267",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T17:38:25.103463+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T17:02:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I realized many roles are only posted on internal career pages and never appear on classic job boards. So I built an AI script that scrapes listings from 70k+ corporate websites.</p> <p>Then I wrote an ML matching script that filters only the jobs most aligned with your CV, and yes, it actually works.</p> <p>You can try it <a href=\"https://laboro.co\">here</a> (for free). </p> <p>Question for the experts: How can I identify \u201cghost jobs\u201d? I\u2019d love to remove as many of them as possible to improve quality.</p> <p>(If you\u2019re still skeptical but curious to test it, you can just upload a CV with fake personal information, those fields aren\u2019t used in the matching anyway.)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Separate-Breath2267\"> /u/Separate-Breath2267 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxme4b/i_scraped_1m_jobs_directly_from_corporate_websites/\">[link]</a></span> &#32; <sp",
        "id": 2791427,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxme4b/i_scraped_1m_jobs_directly_from_corporate_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I scraped 1M jobs directly from corporate websites.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/adi_kurian",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T17:38:25.363309+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T16:40:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kxlu2g/pdf_to_published_editable_docs_site_drag_and_drop/\"> <img src=\"https://external-preview.redd.it/GCu6VnWvgLgOB5oJJT-GhTWBi-sg5kqWqrEak_6aK7I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=01372a8b19b6d176a2d33132f1ef560c71cf50e4\" alt=\"PDF to published, editable docs site (drag and drop)\" title=\"PDF to published, editable docs site (drag and drop)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/adi_kurian\"> /u/adi_kurian </a> <br/> <span><a href=\"http://www.docshound.com/pdf-to-website\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxlu2g/pdf_to_published_editable_docs_site_drag_and_drop/\">[comments]</a></span> </td></tr></table>",
        "id": 2791428,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxlu2g/pdf_to_published_editable_docs_site_drag_and_drop",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/GCu6VnWvgLgOB5oJJT-GhTWBi-sg5kqWqrEak_6aK7I.jpg?width=640&crop=smart&auto=webp&s=01372a8b19b6d176a2d33132f1ef560c71cf50e4",
        "title": "PDF to published, editable docs site (drag and drop)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tuduun",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T16:33:23.854225+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T15:54:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I am relatively new to scraping, but I do have a coding background, and where can I use and **find** the best open source tools for scraping? Also, I have noticed that the anti-bot is a thing on the internet, and how do people bypass it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tuduun\"> /u/tuduun </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxkojx/best_anti_bot_detection_tools/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxkojx/best_anti_bot_detection_tools/\">[comments]</a></span>",
        "id": 2790662,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxkojx/best_anti_bot_detection_tools",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Anti Bot Detection tools",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Organic_Way_3597",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T13:18:26.986790+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T11:04:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;ve been monitoring a website&#39;s API for price changes, but there&#39;s someone else who found an endpoint that gets updates literally hours before mine does. I&#39;m trying to figure out how to find these earlier data sources.</p> <p>From what I understand, different APIs probably get updated in some kind of hierarchy - like maybe cart/checkout APIs get fresh data first since money is involved, then product pages, then search results, etc. But I&#39;m not sure about the actual order or how to discover these endpoints.</p> <p>Right now I&#39;m just using browser dev tools and monitoring network traffic, but I&#39;m obviously missing something. Should I be looking for admin/staff endpoints, mobile app APIs, or some kind of background sync processes? Are there specific patterns or tools that help find these hidden endpoints?</p> <p>I&#39;m curious about both the technical side (why certain APIs would get priority updates) and the practical si",
        "id": 2787986,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxe8su/another_api_returning_data_hours_earlier",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Another API returning data hours earlier.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/suzal001",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T08:40:18.726985+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T07:50:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,<br/> I am looking for a senior member who is great at web scraping and automation. I, myself am a data scientist so I have less exp with web automation field. Could you guys point out how is this particular JD? Additionally if you know someone who is a good fit, please ask them to dm me. I&#39;ll share the mail of the HR in my firm.<br/> Job Description:<br/> We are seeking a skilled and detail-oriented software developer expert in automation and web scraping to join our team. You will be responsible for designing, building, and maintaining scalable web scraping tools and data pipelines. The ideal candidate will have deep experience with web crawling frameworks, anti-bot bypass techniques, and large-scale data extraction across dynamic and static websites.</p> <p>Key Responsibilities:</p> <p>Develop and maintain scalable and reliable web scraping scripts and frameworks.</p> <p>Extract structured and unstructured data from websites with varying",
        "id": 2787120,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxbcpf/guys_help_me_out_with_creating_this_jd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Guys, help me out with creating this JD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aaronn2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T07:35:20.417432+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T06:41:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There are firewall/bot protections websites use when they detect crawling activities on their websites. I started recently dealing with situations when websites instead of blocking you access to the website, they keep you crawling, but they quietly replace the information on the website for fake ones - an example are e-commerce websites. When they detect a bot activity, they change the price of product, so instead of $1,000, it costs $1,300.</p> <p>I don&#39;t know how to deal with these situations. One thing is to be completely blocked, another one when you are &quot;allowed&quot; to crawl, but you are given false information. Any advice? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/aaronn2\"> /u/aaronn2 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/co",
        "id": 2786797,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Websites provide fake information when detected crawlers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Frequent_Swordfish60",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T07:35:20.589316+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T06:19:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I\u2019m trying to scrape the <strong>EU Funding &amp; Tenders Portal</strong> to extract <strong>grant URLs</strong> that match specific filters, and export them into a spreadsheet.</p> <p>I\u2019ve applied all the necessary filters so that only the grants I want are shown on the site.</p> <p>Here\u2019s the URL I\u2019m trying to scrape:<br/> \ud83d\udd17 <a href=\"https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/calls-for-proposals?order=DESC&amp;pageNumber=1&amp;pageSize=50&amp;sortBy=startDate&amp;isExactMatch=true&amp;status=31094501,31094502&amp;frameworkProgramme=43108390\">https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/calls-for-proposals?order=DESC&amp;pageNumber=1&amp;pageSize=50&amp;sortBy=startDate&amp;isExactMatch=true&amp;status=31094501,31094502&amp;frameworkProgramme=43108390</a></p> <p>I\u2019ve tried:</p> <ul> <li>Making a <strong>GET</strong> request</li> <li>using online scrapers</l",
        "id": 2786798,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kx9zm9/having_trouble_scraping_grant_urls_from_eu",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Having Trouble Scraping Grant URLs from EU Funding & Tenders Portal",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AutoEars",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-28T03:15:20.411496+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-28T02:52:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need someone to scrape subreddit member counts of approximately 400 subreddits - I already have the list. Will pay.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoEars\"> /u/AutoEars </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kx6cxn/i_need_someone_to_scrape_subreddit_member_counts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kx6cxn/i_need_someone_to_scrape_subreddit_member_counts/\">[comments]</a></span>",
        "id": 2785679,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kx6cxn/i_need_someone_to_scrape_subreddit_member_counts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need someone to scrape subreddit member counts. Will pay.",
        "vote": 0
    }
]