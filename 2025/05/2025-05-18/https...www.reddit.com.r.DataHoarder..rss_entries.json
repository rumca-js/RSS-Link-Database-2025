[
    {
        "age": null,
        "album": "",
        "author": "/u/l00ky_here",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T23:40:12.991913+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T22:47:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am a major datahoarder and go into the guts of my Amazon Kindle 4 PC app to get the latest on my Kindle books and docs. I use Calibre. I keep coming across this folder within the cache, and I don&#39;t know why it&#39;s there, it seems like it contains information on hacking passwords.</p> <p>The folder is located</p> <p>&quot;C:\\Users\\l00ky_here\\AppData\\Local\\Amazon\\Kindle\\Cache\\EBWebView\\ZxcvbnData&quot;</p> <p>ZxcvbnData\\3.1.0.0\\english_wikipedia.txt</p> <p>ZxcvbnData\\3.1.0.0\\female_names.txt</p> <p>ZxcvbnData\\3.1.0.0\\male_names.txt</p> <p>ZxcvbnData\\3.1.0.0\\manifest.fingerprint</p> <p>ZxcvbnData\\3.1.0.0\\manifest.json</p> <p>ZxcvbnData\\3.1.0.0\\passwords.txt</p> <p>ZxcvbnData\\3.1.0.0\\ranked_dicts</p> <p>ZxcvbnData\\3.1.0.0\\surnames.txt</p> <p>ZxcvbnData\\3.1.0.0\\test.txt</p> <p>ZxcvbnData\\3.1.0.0\\us_tv_and_film.txt</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/l00ky_here\"> /u/l00ky_here </a> <br/> <span><a hr",
        "id": 2712116,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpwpq4/what_is_this_folder_in_my_kindle_4_pc_app_on_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is this folder in my Kindle 4 PC app on my desktop?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/monopodman",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T22:35:12.753006+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T21:55:51+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/monopodman\"> /u/monopodman </a> <br/> <span><a href=\"/r/Seagate/comments/1kpimgx/exos_x20_20tb_vs_exos_x24_20tb_noise/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpvl2p/exos_x20_20tb_vs_exos_x24_20tb_noise/\">[comments]</a></span>",
        "id": 2711772,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpvl2p/exos_x20_20tb_vs_exos_x24_20tb_noise",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Exos X20 20TB vs Exos X24 20TB noise",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wallacebrf",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T21:30:36.755874+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T20:41:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kptwfp/thoughts_on_diy_15x_disk_jbod/\"> <img src=\"https://preview.redd.it/wa1fleejpl1f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b90c4ab8b3a42d10ab31d65c478712b097b99b06\" alt=\"Thoughts on DIY 15x Disk JBOD\" title=\"Thoughts on DIY 15x Disk JBOD\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>This is what i am thinking to allow me to add 15x drives to a PC i already have available with an open 8x PCIe slot. </p> <ul> <li>9305-16e <a href=\"https://www.amazon.com/gp/product/B01BDZWNG4\">https://www.amazon.com/gp/product/B01BDZWNG4</a></li> <li>10Gtek SFF-8644 to Mini SAS SFF-8088 HD Cable <a href=\"https://www.amazon.com/gp/product/B01KH9OG9E\">https://www.amazon.com/gp/product/B01KH9OG9E</a></li> <li>Internal SFF-8087 to 2 Port External SFF-8088 Mini SAS Adapter Card <a href=\"https://www.amazon.com/gp/product/B0DL9PBYDJ\">https://www.amazon.com/gp/product/B0DL9PBYDJ</a></li> <li>Cable Matters Internal Mini ",
        "id": 2711423,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kptwfp/thoughts_on_diy_15x_disk_jbod",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/wa1fleejpl1f1.png?width=640&crop=smart&auto=webp&s=b90c4ab8b3a42d10ab31d65c478712b097b99b06",
        "title": "Thoughts on DIY 15x Disk JBOD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Adam2013",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T21:30:36.976901+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T20:38:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to get a workflow for backing up old bills/documents into Paperless-ngx. Part of that is scanning them in. </p> <p>I&#39;ve tried everything in the web interface of the Brother scanner that I could think of (even some that shouldn&#39;t work) for the &quot;Network Folder Path&quot;. My NAS (Storage.lolno.com/10.0.33.66) is sharing a folder and all the packet dumps that I grab from the router when it&#39;s trying to connect are just the ADS-4300N trying to do a DNS lookup for whatever I have in the &#39;Network Folder Path&#39;. </p> <p>Some options I&#39;ve tried:</p> <ul> <li>\\\\10.0.33.66\\path\\to\\share\\</li> <li>//10.0.33.66/path/to/share/</li> <li>10.0.33.66</li> <li>10.0.33.66/</li> <li>Storage.lolno.com/path/to/share</li> <li>Storage/path/to/share/</li> <li>&quot;10.0.33.66/&quot;</li> <li>and many more</li> </ul> <p>This is the generic error I get in the Brother web interface:</p> <pre><code>Test Error Server Timeout, This message will app",
        "id": 2711424,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kptuby/brother_ads4300n_wont_connect_to_network_share",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Brother ADS-4300N Won't connect to network share",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/smilingreddit",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T23:40:13.271633+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T19:47:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>For many days now I&#39;ve been struggling with moving from Gmail to Proton mail, and would love to read your advice. I&#39;m basically looking for a service to backup all my emails (with a search function), and ideally also take over all of Gmail&#39;s functions. </p> <p>With Proton, I&#39;ve run into many issues, small and big: default sender address not respected (support says it won&#39;t change if I don&#39;t change my MX records), basically no search on mobile (on the roadmap for this summer apparently), quite a few bugs (needing to uninstall reinstall), huge space needs for the bridge (also buggy), etc.</p> <p>Now, I could just use the built in email clients (in my case iOS and macOS) for my daily email writing, and use Proton just for backup. But that&#39;s an expensive way to backup emails, and not very practical since I&#39;d have to switch client to search (and for now I couldn&#39;t search on mobile). </p> <p>I also tho",
        "id": 2712117,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpso41/affordable_email_backup_service_with_privacy_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Affordable email backup service with privacy and search",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/eleluggi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T19:20:13.220345+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T18:30:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been trying to dig up fragments from an old personal site I ran in the early 2000s (roughly 2003\u20132007). It had hand-drawn artwork, a simple blog, and some community content tied to early PHP-based CMS systems.</p> <p>Domain\u2019s long dead, host\u2019s gone, backups lost.</p> <p>I\u2019ve pulled what I could from archive.org (surface HTML only, no media), tried CommonCrawl, and checked a few public data dumps.</p> <p>gfndc.org helped me about a year ago and recovered some structure-level data I thought was gone forever \u2014 but since mid-2024 they\u2019ve gone fully internal. Legal lockdown or something.</p> <p>I\u2019m still missing a big chunk of visual media and post content.</p> <p>Anyone here had luck with alternative archives, academic crawlers, or gray mirrors that go deeper than IA?</p> <p>I\u2019m open to any obscure lead \u2014 even partial snapshots or weird metadata is better than nothing.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/",
        "id": 2710710,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpqust/still_trying_to_recover_content_from_a_dead_net",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Still trying to recover content from a dead .net site (2003\u20132007) \u2014 where else to look beyond IA and gfndc.org?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Otherwise_Sound_6643",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T18:15:13.429561+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T17:53:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a 50tb Terramaster D5-310 DAS I want to use as just a data dump. As part of the 3-2-1 backup rules, this box is off-site. It has RAID 5 implemented on it. What kind of issues could I have if the box is just sitting around at the off-site location, powered down, maybe months at a time? Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Otherwise_Sound_6643\"> /u/Otherwise_Sound_6643 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kppyyr/data_preservation_question/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kppyyr/data_preservation_question/\">[comments]</a></span>",
        "id": 2710320,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kppyyr/data_preservation_question",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Data Preservation Question",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mobile-Cranberry8920",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T23:40:13.519339+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T17:52:16+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kppy9l/scrap_or_extract_all_followers_of_an_instagram/\"> <img src=\"https://b.thumbs.redditmedia.com/-qj8TYE1yNmGcWFlxwAckH2Mlhx3rX0j_1v_Mzi1C8c.jpg\" alt=\"Scrap or extract all followers of an instagram public page\" title=\"Scrap or extract all followers of an instagram public page\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi Guys,</p> <p>I&#39;m trying to Scrap or extract all followers of an instagram public page !</p> <p>chatGpt recommended instaloader and helped me with the script but I couldn&#39;t set it up:</p> <p>below script example (real words replaced for privacy) with the error when running it with python 3:</p> <p>Thanks</p> <p><a href=\"https://preview.redd.it/2f1aq9ehwk1f1.png?width=1016&amp;format=png&amp;auto=webp&amp;s=05d30a1341442bf10644beb6ddf95ef7a2c63d84\">https://preview.redd.it/2f1aq9ehwk1f1.png?width=1016&amp;format=png&amp;auto=webp&amp;s=05d30a1341442bf10644beb6ddf95ef7a2c63d84</a",
        "id": 2712118,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kppy9l/scrap_or_extract_all_followers_of_an_instagram",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/-qj8TYE1yNmGcWFlxwAckH2Mlhx3rX0j_1v_Mzi1C8c.jpg",
        "title": "Scrap or extract all followers of an instagram public page",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VirginMonk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T18:15:13.618514+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T17:47:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>First of all I would like to thank this community learned a lot from here.</p> <p>I am a mobile app developer and I believe that there are pretty good web portals/ web tools available to self host but very limited good mobile phone applications.</p> <p>I am looking for some good ideas which actually people want because it gives you a lot of motivation when someone is actually using the application and it should not be something very complex which I can&#39;t build in my free time.</p> <p>Some ideas came to my mind are:</p> <p>* Self hosted split wise.</p> <p>* Self hosted workout tracker.</p> <p>* Self hosted &quot;Daily photo memories&quot; after which you can print collages etc.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VirginMonk\"> /u/VirginMonk </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kppu4x/app_developer_looking_out_for_some_cool_ideas_for/\">[link]</a></span> ",
        "id": 2710321,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kppu4x/app_developer_looking_out_for_some_cool_ideas_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "App developer looking out for some cool ideas for self hosting",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Adventurous_Goat1436",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T18:15:13.808059+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T17:18:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I cant figure out how to use wfdownloader. I basically want to download all and sort all my saved posts.. i used 4k stogram but its not sorting any of the posts only by date. Please help :(</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Adventurous_Goat1436\"> /u/Adventurous_Goat1436 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpp5kh/how_to_download_saved_posts_on_ig_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpp5kh/how_to_download_saved_posts_on_ig_with/\">[comments]</a></span>",
        "id": 2710322,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpp5kh/how_to_download_saved_posts_on_ig_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download saved posts on ig with wfdownloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/--ae",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T17:10:35.330773+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T16:33:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is putting critical data on used hard drives really that bad? I feel like if I have a decent raid setup with parity I should be fine but a lot of people on here still say not to put anything critical on them.</p> <p>Is there something with used drives that causes them to all fail at once or something?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/--ae\"> /u/--ae </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpo3pa/buying_used_hdds/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpo3pa/buying_used_hdds/\">[comments]</a></span>",
        "id": 2709964,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpo3pa/buying_used_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Buying Used HDD(s)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BeginningEmotional49",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T17:10:35.565757+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T16:22:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpnu95/how_cooked_am_i/\"> <img src=\"https://a.thumbs.redditmedia.com/ikDkXvAbshUeSD2Ep7wiT8mRQhiriQT5dxejGHlvY94.jpg\" alt=\"How cooked am I?\" title=\"How cooked am I?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Was ripping open a 14tb external HDD that I had laying around and without paying attention and realizing what I was doing. I ripped off the pcb board. Nothing seems broken or anything. I just unscrewed it and took it off. Am I just cooked and taking an L on this? I put it back together and I\u2019m just worried if it\u2019s even worth trying to use. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BeginningEmotional49\"> /u/BeginningEmotional49 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1kpnu95\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpnu95/how_cooked_am_i/\">[comments]</a></span> </td></tr></table>",
        "id": 2709965,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpnu95/how_cooked_am_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/ikDkXvAbshUeSD2Ep7wiT8mRQhiriQT5dxejGHlvY94.jpg",
        "title": "How cooked am I?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheBayAreaGuy1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T16:05:12.928555+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T15:41:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Full archive of every Survivor interview completed on Letterman&#39;s show.<br/> Does anyone how to extract these?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheBayAreaGuy1\"> /u/TheBayAreaGuy1 </a> <br/> <span><a href=\"https://web.archive.org/web/20060707135204/http://www.cbs.com/latenight/lateshow/exclusives/surviving/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpmwpv/lettermans_interviews_with_survivor_players_how/\">[comments]</a></span>",
        "id": 2709622,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpmwpv/lettermans_interviews_with_survivor_players_how",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Letterman's interviews with Survivor players - how to extract them?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Aureste_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T16:05:12.681079+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T15:30:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I plan to use 3 16TB drives to make a zfs pool, with 2 drives for storage and 1 for parity. </p> <p>How much RAM should I allocate to the TrueNAS VM to make it work great ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Aureste_\"> /u/Aureste_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpmo01/ram_usage_with_zfs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpmo01/ram_usage_with_zfs/\">[comments]</a></span>",
        "id": 2709621,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpmo01/ram_usage_with_zfs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "RAM usage with ZFS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/carterjgoff",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T16:05:12.461919+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T15:21:47+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpmgir/buying_used_hdds/\"> <img src=\"https://preview.redd.it/zjjx41hn5k1f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3955fad9470014b9331d31b568fe7696d0131f24\" alt=\"Buying used HDD\u2019s\" title=\"Buying used HDD\u2019s\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I came across some pretty cheap ironwolfs on marketplace near me. Is there a good way to verify if they\u2019re in good condition and worth my while?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/carterjgoff\"> /u/carterjgoff </a> <br/> <span><a href=\"https://i.redd.it/zjjx41hn5k1f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpmgir/buying_used_hdds/\">[comments]</a></span> </td></tr></table>",
        "id": 2709620,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpmgir/buying_used_hdds",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/zjjx41hn5k1f1.jpeg?width=640&crop=smart&auto=webp&s=3955fad9470014b9331d31b568fe7696d0131f24",
        "title": "Buying used HDD\u2019s",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JasonY95",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T15:00:11.536975+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T14:33:03+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kplcv0/im_quite_sure_i_need_an_intervention_at_this_point/\"> <img src=\"https://preview.redd.it/5r5fv3dvwj1f1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=091ba2836bd32bf084b1b86dae8937b184a32d7f\" alt=\"I'm quite sure I need an intervention at this point\" title=\"I'm quite sure I need an intervention at this point\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JasonY95\"> /u/JasonY95 </a> <br/> <span><a href=\"https://i.redd.it/5r5fv3dvwj1f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kplcv0/im_quite_sure_i_need_an_intervention_at_this_point/\">[comments]</a></span> </td></tr></table>",
        "id": 2709259,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kplcv0/im_quite_sure_i_need_an_intervention_at_this_point",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/5r5fv3dvwj1f1.jpeg?width=320&crop=smart&auto=webp&s=091ba2836bd32bf084b1b86dae8937b184a32d7f",
        "title": "I'm quite sure I need an intervention at this point",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NoticeItchy47",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T12:50:13.975895+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T12:14:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone please keep in mind im not tech savvy </p> <p>so ive been using Transcend TS1TSJ25M3G for over 6 years and im very happy with it but i want to buy new one since i heard u should replace it after a few years.</p> <p>so i really wanted to buy same brand (mostly because i dont know anything about those stuff and chat gpt isnt very helpful) but maybe 2tb just in case (i have 1tb now and i still have 1/3 space left) im only using this stuff for my pictures and videos and maybe some movies. </p> <p>i almost purchase Transcend StoreJet 25M3S 2TB and then i found this: If you have a 25M\u00b3 Transcend HDD that is atleast 4+ years old, there is a chance for it to falil, not due to HDD error, but due to corrosion in the inside Metal bracket. 1 hope this post might help some from losing their backup. <a href=\"https://www.reddit.com/r/buildapc/s/Ovmc0qZsZJ\">https://www.reddit.com/r/buildapc/s/Ovmc0qZsZJ</a></p> <p>so now im back on point 0. If anyone hav",
        "id": 2708606,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpiklx/asking_for_recommendation_for_external_drive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Asking for recommendation for external drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DevelopedLogic",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T11:45:12.326084+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T11:13:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Over the years I have avoided ZFS Native Encryption because I have read spoken to various people about it (including in the OpenZFS IRC channels) who say that is is very buggy, has data corruption bugs and is not suitable for production workloads where data integrity is required (the whole damn point of ZFS).</p> <p>By extension, I would assume that any encrypted data backed up via ZFS Send (instead of a general file transfer) would inherit corruption or risk of corruption due to bugs.</p> <p>Is this concern founded or is there more to it than that?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DevelopedLogic\"> /u/DevelopedLogic </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kphjbq/can_we_trust_zfs_native_encryption/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kphjbq/can_we_trust_zfs_native_encryption/\">[comments]</a></span>",
        "id": 2708219,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kphjbq/can_we_trust_zfs_native_encryption",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can we trust ZFS Native Encryption?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Broad_Sheepherder593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T10:40:18.723182+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T09:36:57+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpg4ke/disable_drive_in_dsm/\"> <img src=\"https://b.thumbs.redditmedia.com/uDrPPY3Ymc1M4P5Mt-S_LuurA8bQc24YsT--LbljLSM.jpg\" alt=\"Disable drive in DSM\" title=\"Disable drive in DSM\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi, </p> <p>I have 2 storage pools where the 2nd pool is just 1 drive that is set to JBOD. I don&#39;t like it running all the time so thinking of just disabling it until i need it. When i tried however, DSM does not allow me and seems the error is due to a faulty drive? Weird tho as the drive is reported as healthy.</p> <p>Thinking of just turning off the nas and pull out this drive but maybe I&#39;m missing a step?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Broad_Sheepherder593\"> /u/Broad_Sheepherder593 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1kpg4ke\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarde",
        "id": 2707944,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpg4ke/disable_drive_in_dsm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/uDrPPY3Ymc1M4P5Mt-S_LuurA8bQc24YsT--LbljLSM.jpg",
        "title": "Disable drive in DSM",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DarkOverlord24",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T09:35:13.655971+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T09:00:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;ve recently started growing my Jellyfin collection and am soon going to run out of space. Currently I have 2 8tb SSDs with redundancy, but those are also used for my general storage. I&#39;m looking for decent high capacity drives to expand my Jellyfin data to. The issue is, that (due to limited space) my server is in my bedroom, so I can&#39;t really have loud drives (hence the SSDs). What drives do you recommend? Ideally high capacity low noise. If that isn&#39;t possible the highest capacity possible with bedroom acceptable noise. They&#39;ll only be used for my Jellyfin media, nothing else.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DarkOverlord24\"> /u/DarkOverlord24 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpfmpp/quite_drive_for_bedroom/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpfmpp/quite_drive_for_bedroom/\">[comme",
        "id": 2707698,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpfmpp/quite_drive_for_bedroom",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Quite drive for bedroom",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Arcueid-no-Mikoto",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T18:15:14.321835+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T05:55:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>While they still worked, I&#39;d use chrome addons to download full users media, now they just seem to work for individual tweets, so I started using gallery-dl.</p> <p>The addon I was using gave this format which I find perfect for organizing:</p> <p>[name]-[tweet_id]-[date_hour]-img[num]</p> <p>The file would look like:</p> <p>_azuse-1234495797682528256-20200302_160828-img1</p> <p>I tried using chatgpt to help me and tried stuff like<br/> -o &quot;output={user[username]}-{tweet[id]}-{tweet[date]:%Y%m%d_%H%M%S}-img{num}.{extension}&quot;</p> <p>But I guess this doesn&#39;t make any sense and is just give me what I want even if gallery-dl doesn&#39;t support this format.</p> <p>Is there any way though to download files following that format? Using gallery-dl, a web extension (as long as it downloads in bulk) or any other downloader?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Arcueid-no-Mikoto\"",
        "id": 2710323,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpd0d8/gallerydl_using_custom_filename_for_twitter",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Gallery-dl, using custom filename for Twitter downloads",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/murkomarko",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T18:15:14.572224+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T04:00:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I like Evernote for this because you can clip pages and then the chrome extension will inject results that match to google results pages, it&#39;s quite useful, but I&#39;d like to explore other tools, since the future of Evernote is kind of uncertain and it&#39;s getting more and more expensive</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/murkomarko\"> /u/murkomarko </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpb7ni/tool_to_keep_webpages_and_make_it_searchable/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpb7ni/tool_to_keep_webpages_and_make_it_searchable/\">[comments]</a></span>",
        "id": 2710324,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpb7ni/tool_to_keep_webpages_and_make_it_searchable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Tool to keep webpages and make it searchable (better than Evernote)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ItsNumi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T03:05:16.072941+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T02:59:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I am not new to building PCs but this is a new side of it for me. I&#39;m not sure if I can do this in one device or need several or what exactly.</p> <p>Objective: </p> <ol> <li><p>Have a NAS with redundancy and expandability. Mainly used for media and documents.</p></li> <li><p>Have a PC that I can load windows on, hook up to my 4k tv and stream media from the NAS.</p></li> </ol> <p>Budget: Very flexible, I know it&#39;s HDD size depending. Let&#39;s just say a few thousand without the HDDs.</p> <p>Any ideas what I should be building for these needs? One or two machines? Id love some advice.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ItsNumi\"> /u/ItsNumi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpa5kk/need_advice_on_what_to_build_nastv/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kpa5kk/need_advice_on_what_to_build_nastv/\">[comme",
        "id": 2706589,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kpa5kk/need_advice_on_what_to_build_nastv",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need advice on what to build. (NAS/TV)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/iObserve2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T03:05:15.706921+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T02:08:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve got 8 drives in a RAID configuration with 1 SSD dedicated to cache and 1 hot spare, three drive bays are unused. I want to upgrade all my non SSD drives. I know the safest way is to back up, install new drives and restore, but as I can have a drive fail and replace it with the hot spare without functionality loss, I was wondering if I could do that by pulling one drive at a time, having the RAID adjust then repeating until all have been replaced.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iObserve2\"> /u/iObserve2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp99vy/gradual_replacement_of_raid_drives_in_a_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp99vy/gradual_replacement_of_raid_drives_in_a_nas/\">[comments]</a></span>",
        "id": 2706588,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kp99vy/gradual_replacement_of_raid_drives_in_a_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Gradual Replacement of RAID drives in a NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/little-value-1188",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T16:05:13.466534+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T01:36:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I work in a hospital setting, Radiation Oncology. The center I work at used to use Pinnacle for treatment planning, but we\u2019ve since transitioned to Monaco with MIM. We still have Pinnacle TPS records archived on tape, but unfortunately, we do not have a tape reader.</p> <p>I\u2019d like to pull the dose data into a MIM software system for dose accumulation purposes. Has anyone here worked with Pinnacle TPS archiving and used a specific tape drive to access archived records? If so, could you share details about the model or type of tape reader you used? I\u2019ve had trouble finding compatible options online and would appreciate any guidance.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/little-value-1188\"> /u/little-value-1188 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp8p21/seeking_tape_reader_for_archived_medical/\">[link]</a></span> &#32; <spa",
        "id": 2709623,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kp8p21/seeking_tape_reader_for_archived_medical",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seeking Tape Reader for Archived Medical Radiation Oncology Treatment Planning System (TPS - Pinnacle) Data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/manzurfahim",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T00:55:14.852734+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T00:49:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I got so scared today when I tried to look for a YT channel and couldn&#39;t find it. The videos were about remote living. After an hour long search trying different keywords and what not, I finally saw a thumbnail and recognized it.</p> <p>Anyway, the channel has 239 videos and I am using Stacher (yt-dlp with gui), and I am not using my cookies. Can I download them all or should I do little by little so YT doesn&#39;t ban the IP or anything? My YT is premium if that helps.</p> <p>Thank you very much in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/manzurfahim\"> /u/manzurfahim </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp7tr9/is_there_a_limit_of_how_many_videos_can_i/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp7tr9/is_there_a_limit_of_how_many_videos_can_i/\">[comments]</a></span>",
        "id": 2706233,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kp7tr9/is_there_a_limit_of_how_many_videos_can_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a limit of how many videos can I download from YT?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/rcchurchill",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-18T00:55:15.043843+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-18T00:32:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was just given an old Dell MD-1000 to play with. I&#39;ve got a R740 with a 12Gbps HBA card in it, so a SFF-8644 connector. The MD-1000 has the very old SFF-8470 connectors. I&#39;m having fits trying to find a Mini-SAS SFF-8644 to SFF-8470 cable.</p> <p>Would any of you pack-rats out there happen to have one in your cable stash that you&#39;d be willing to sell to me? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rcchurchill\"> /u/rcchurchill </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp7ipx/minisas_sff8644_to_sff8470_cable/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kp7ipx/minisas_sff8644_to_sff8470_cable/\">[comments]</a></span>",
        "id": 2706234,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kp7ipx/minisas_sff8644_to_sff8470_cable",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mini-SAS SFF-8644 to SFF-8470 cable?",
        "vote": 0
    }
]