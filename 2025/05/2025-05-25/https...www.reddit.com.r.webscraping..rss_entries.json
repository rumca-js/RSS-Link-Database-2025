[
    {
        "age": null,
        "album": "",
        "author": "/u/nggaaaaajajjaj",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T18:13:25.138641+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T18:12:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi guys im pretty new to scraping and english is not my first language since im from europe but does anyone have tips for making a really good strong scallable scraper. Like what should it have to make it bullet proof etc. thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nggaaaaajajjaj\"> /u/nggaaaaajajjaj </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kv93wm/sophisticated_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kv93wm/sophisticated_scraper/\">[comments]</a></span>",
        "id": 2769183,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kv93wm/sophisticated_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sophisticated scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/d_berbatov",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T18:13:25.287069+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T17:37:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is usage of residential proxies enough to prevent WEB RTC leak test, do I need to do anything else when it comes to web rtc?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/d_berbatov\"> /u/d_berbatov </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kv892r/getting_detected/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kv892r/getting_detected/\">[comments]</a></span>",
        "id": 2769184,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kv892r/getting_detected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting detected",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/aky71231",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T16:59:40.466632+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T16:06:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Curious to see what the most challenging scrapper you ever built/worked with and how long it took you to do it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/aky71231\"> /u/aky71231 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kv64h6/whats_the_most_painful_scrapping_youve_ever_done/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kv64h6/whats_the_most_painful_scrapping_youve_ever_done/\">[comments]</a></span>",
        "id": 2768676,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kv64h6/whats_the_most_painful_scrapping_youve_ever_done",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Whats the most painful scrapping you've ever done",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/raaahi",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T13:44:39.393330+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T13:16:43+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kv2b01/can_i_scrape_this_website/\"> <img src=\"https://external-preview.redd.it/Z3RqeGFhOXBoeDJmMbxSPRKl3T1077sC7hQjJgYPAnyrrUHW0vG5_0hRe-JD.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad7374753434ffd9807bba9e4df6f37850f25a67\" alt=\"Can I scrape this website?\" title=\"Can I scrape this website?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have no knowledge about coding, though willing to learn if this website is possible to scrape.</p> <p>This website provides details of property transaction in different parts of city/state. I want to create a data sheet for different kind of transactions (lease, buy/sell) from this website. Is there any way to do it?</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/raaahi\"> /u/raaahi </a> <br/> <span><a href=\"https://v.redd.it/qd4pmj8phx2f1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping",
        "id": 2767481,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kv2b01/can_i_scrape_this_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Z3RqeGFhOXBoeDJmMbxSPRKl3T1077sC7hQjJgYPAnyrrUHW0vG5_0hRe-JD.png?width=640&crop=smart&auto=webp&s=ad7374753434ffd9807bba9e4df6f37850f25a67",
        "title": "Can I scrape this website?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mickspillane",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T11:28:20.708362+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T10:46:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am scraping a site using a single, static residential IP which only I use. </p> <p>Since my target pages are behind a login wall, I&#39;m passing cookies to spoof that I&#39;m logged in. I&#39;m also rate limiting myself so my requests are more human-like. </p> <p>To conserve resources, I&#39;m not using headless browsers, just pycurl.</p> <p>This works well for about a week before I start getting errors from the site saying my requests are coming from a bot.</p> <p>I tried refreshing the cookies, to no avail. So it appears my requests at blocked at the user level, not the session level. As if my user ID is blacklisted.</p> <p>I&#39;ve confirmed the static, residential IP is in good standing because I can make a new user account, new cookies, and use the same IP to resume my scrapes. But a week later, I get blocked.</p> <p>I haven&#39;t invested in TLS fingerprinting at all. I&#39;m wondering if it is worth going down that route. I assume my TLS fin",
        "id": 2766859,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kuznz0/detected_after_a_few_days_could_tls_fingerprint",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Detected after a few days, could TLS fingerprint be the reason?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/REDI02",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T10:22:50.099875+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T09:36:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am using Playwright to download a page by giving any URL. While it avoids bot detection (i assume) but still the content is different from original browser.</p> <p>I ran test by removing headless mode and found this: 1. My web browser loads 60 items from page. 2. Scraping browser loads only 50 objects(checked manually by counting) 3. There is difference in objects too while some objects are common in both.</p> <p>BY objects i mean products on NOON.AE website. Kindly let me know if you have any solution. I can provide URL and script too.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/REDI02\"> /u/REDI02 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kuynhz/different_content_laoding_in_original_browser_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kuynhz/different_content_laoding_in_original_browser_and/\">[comments]</a></span>",
        "id": 2766534,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kuynhz/different_content_laoding_in_original_browser_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Different content laoding in original browser and scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/arjentic",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T10:22:50.253248+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T09:19:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How to extract playlist, list of songs that have been played on the one specific radio station in defined time period, for example from 9PM to 12PM on radioscraper com? And if there is possible to make that extracted list playable \ud83d\ude06\ud83e\udd74</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/arjentic\"> /u/arjentic </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kuyeby/extract_playlist_from_radioscraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kuyeby/extract_playlist_from_radioscraper/\">[comments]</a></span>",
        "id": 2766535,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kuyeby/extract_playlist_from_radioscraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "extract playlist from radioscraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HackerArgento",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T09:17:39.244840+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T08:29:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I&#39;m working on a project where I&#39;m using puppeteer and I&#39;m trying to optimize things by enabling caching via proxies basically, I want the proxies to cache static resources (like images, scripts, etc.) so they don\u2019t fetch the same content on every request/profile, i&#39;ve tried using squidproxy and mitmproxy to do this on windows but the setup was messy and i couldn&#39;t quite get it to work My questions: Is it possible to configure the proxies from the guys i&#39;m buying from (or wrap it somehow) so that it acts as a caching proxy? any pitfalls to avoid? Any advice, diagrams, or tools you recommend would be greatly appreciated, thank you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HackerArgento\"> /u/HackerArgento </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kuxp85/caching_proxy_on_windows_puppeteer/\">[link]</a></span> &#32; <span><a href=\"https://www.r",
        "id": 2766268,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kuxp85/caching_proxy_on_windows_puppeteer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Caching proxy on windows puppeteer?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Flewizzle",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T08:12:48.884003+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T07:48:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys not exactly scraping but i feel someone here might know, im trying to interact with websites across multiple VPS, but the site has high security and can probably detect virtualised environments and the fact they run windows server, im wondering if anyone knows of a company where I can rent PCs and RDC into them but which arent virtual?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Flewizzle\"> /u/Flewizzle </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kux3q9/remotely_using_non_virtual_pc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kux3q9/remotely_using_non_virtual_pc/\">[comments]</a></span>",
        "id": 2766051,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kux3q9/remotely_using_non_virtual_pc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Remotely using non virtual PC",
        "vote": 0
    }
]