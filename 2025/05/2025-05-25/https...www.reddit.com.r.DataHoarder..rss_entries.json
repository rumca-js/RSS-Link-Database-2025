[
    {
        "age": null,
        "album": "",
        "author": "/u/Ok_Caregiver_1355",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T23:25:12.022903+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T23:09:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im kinda new to this game so not sure if this question even makes sense but,would be possible to use gpu to speed up the preview generation trough the &quot;FFmpeg Transcode Output Args&quot; option?Sometimes i download a pack of videos and the preview takes light-years so that could be helpful,so if anyone had any experience with it i would be grateful,tried some parameters myself and itsnt working</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Caregiver_1355\"> /u/Ok_Caregiver_1355 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvfppj/stashapp_hardware_acceleration_for_preview/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvfppj/stashapp_hardware_acceleration_for_preview/\">[comments]</a></span>",
        "id": 2770486,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvfppj/stashapp_hardware_acceleration_for_preview",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Stashapp hardware acceleration for preview generation?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheComebackKid717",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T23:25:11.813683+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T23:09:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I just about pulled the trigger on a NAS, but started seeing things like &quot;RAID is not a backup&quot; and other comments that are making me feel like my research has led me down the wrong path.</p> <p>I&#39;ve begun filming YouTube content, with a plan to film content for 3-5 years before publishing anything.</p> <p>This means that I have a pretty low tolerance for risk. If I get 3 years in and lose all my data, the whole thing dies. 3-2-1 feels a bit excessive since this in still non-prefessional and I&#39;m not looking to spend 1k+ in a solution.</p> <p>My plan was to buy a NAS with a few bays, then fill it with a couple 20th drives, then run raid to mirror between them. I don&#39;t understand raid yet, so I figured this would give me 2 copies and therefore I&#39;d have a backup. I don&#39;t need extremely fast upload/download speeds, my workflow will likely be: Film &gt; Upload files to long-term storage &gt; download to my nvme to edit &gt; up",
        "id": 2770485,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvfpk8/almost_went_nas_now_im_confused",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Almost went NAS, now I'm confused",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/user888ffr",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T23:25:11.544728+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T23:07:01+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvfnrc/whos_gonna_tell_him/\"> <img src=\"https://preview.redd.it/9kpa6at8jz2f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa68884d4e2d42cd9bea7c438bf5495a3a016a9b\" alt=\"Who's gonna tell him\" title=\"Who's gonna tell him\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/user888ffr\"> /u/user888ffr </a> <br/> <span><a href=\"https://i.redd.it/9kpa6at8jz2f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvfnrc/whos_gonna_tell_him/\">[comments]</a></span> </td></tr></table>",
        "id": 2770484,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvfnrc/whos_gonna_tell_him",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/9kpa6at8jz2f1.jpeg?width=640&crop=smart&auto=webp&s=fa68884d4e2d42cd9bea7c438bf5495a3a016a9b",
        "title": "Who's gonna tell him",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Omg_Capacitator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T23:25:12.232039+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T22:52:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want a setup that is affordable but still potentially scalable if I were to get more storage in the future. I love the idea of a jbod bay that stores internal drives like <a href=\"https://www.amazon.ca/Enclosures-Push-Pull-Mounting-Supported-9858RU3/dp/B0DDX8PVH7\">this</a>, and so I can slowly fill it with more enterprise hdd&#39;s as needed (although it runs into the problem of sata/sas compatibility). however, im thinking it might be better to either stick with an array of external drives instead, or forgo RAID entirely and get something like a <a href=\"https://www.amazon.ca/ORICO-Docking-Station-Tool-Free-Support/dp/B0C2HZVG3W\">toaster</a>. I&#39;m honestly a bit lost and the upfront cost of something like a synology nas is kinda scaring me</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Omg_Capacitator\"> /u/Omg_Capacitator </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvfczd/newbi",
        "id": 2770487,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvfczd/newbie_here_whats_the_best_setup_for_laptops",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Newbie here, what's the best setup for laptops without extensive drive bays built into their case?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Exact-Flounder1274",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T22:20:10.195668+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T21:24:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I need a drive for backing up a bunch of old games. I am thinking of buying one of these. Are they fine for cold storage? Which one would you recommend?</p> <p><a href=\"https://www.amazon.de/gp/product/B08GNB466W/ref=ox_sc_act_title_4?smid=A21Q7ORFON2A7Q\">https://www.amazon.de/gp/product/B08GNB466W/ref=ox_sc_act_title_4?smid=A21Q7ORFON2A7Q</a></p> <p><a href=\"https://www.amazon.de/gp/product/B093BVQYYM/ref=ox_sc_act_title_1?smid=A1RXW3TLJ3ZGW0\">https://www.amazon.de/gp/product/B093BVQYYM/ref=ox_sc_act_title_1?smid=A1RXW3TLJ3ZGW0</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Exact-Flounder1274\"> /u/Exact-Flounder1274 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvdi8b/external_hdd_for_cold_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvdi8b/external_hdd_for_cold_storage/\">[comments]</a></span>",
        "id": 2770231,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvdi8b/external_hdd_for_cold_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "External HDD for Cold Storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/genie_2023",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T21:16:47.192529+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T20:28:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all,</p> <p>I am not sure if this is the right subreddit for it but going to try mu luck anyway.</p> <p>First a bit of background: I lived in the UK for about 7 years and then had to moved back to India about 8 years ago, quite suddenly. I left a lot of stuff in my friend&#39;s garage. This is my first trip back to UK so I am emptying her garage that she so graciously provided to store my stuff. Most of the stuff is going to charity or bin. I am making arrangements to ship some sentimental, emotional value stuff to India. </p> <p>Now coming to my issue: over the years I have accumulated quite a few hard drives and CDs as data backups. I am fairly sure I have almost all the data in my cloud account but I can&#39;t be sure. So here are my questions: </p> <ul> <li><p>Is there a way for me to check this data (USB-A and dvd drives) here and transfer to my cloud?</p></li> <li><p>If not, is there a limit for traveling with multiple hard drives? Remembe",
        "id": 2769936,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvc8rw/multiple_hard_drives_for_international_travel",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Multiple hard drives for international travel",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fawzib",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T21:16:47.342363+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T20:23:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>New one just came out and I was wondering how many 3.5&quot; hard drives I can put in it and what do you guys think of a homelab unraid server. it might not be hot swappable but that is fine for my needs.</p> <p><a href=\"https://www.fractal-design.com/products/cases/meshify/meshify-3-xl/ambience-pro-rgb-black-tg-light-tint/\">https://www.fractal-design.com/products/cases/meshify/meshify-3-xl/ambience-pro-rgb-black-tg-light-tint/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fawzib\"> /u/fawzib </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvc41j/fractal_design_meshify_3_xl_is_out/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kvc41j/fractal_design_meshify_3_xl_is_out/\">[comments]</a></span>",
        "id": 2769937,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvc41j/fractal_design_meshify_3_xl_is_out",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Fractal Design Meshify 3 XL is out",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AlAcrab",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T20:10:12.978000+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T19:55:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I had a IronWolf Pro which started making constant mechanical noise (as if seeking back and forth all the time? yet reboots and stopping/restarting the disk did not help), however no errors from the operating system, and the disk seemed to function just fine. but the noise drove me nuts... </p> <p>It was still under a few months of warranty. I called Seagate, they recommended that I exchange it. which I did. the replacement disk was a refurbished one, not brand new. </p> <p>Yesterday, less than a year later (but now out of warranty) it developed the same symptoms of constantly making the mechanical noise, yet seem to function fine. I took it out of the enclosure because I could not stand the noise</p> <p>I have a Mediasonic HF7-SU31C external enclosure that had the IronWolf Pro, another Time Machine Barracuda and also hold a Barracuda (with all my wife&#39;s photos). and now has two empty bays (since I took out the noisy IronWolf Pro)</p> <p>My origin",
        "id": 2769691,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kvbgv6/which_812tb_disk_would_you_recommend_for_a_time",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which 8-12TB disk would you recommend for a Time Machine backup?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chalikta",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T19:06:13.685430+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T18:51:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>What are the limitations after the trial ends? Does it stop working completely, or are some features limited/locked?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Chalikta\"> /u/Chalikta </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kva03w/syncoverycom_what_are_the_limitations_after_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kva03w/syncoverycom_what_are_the_limitations_after_the/\">[comments]</a></span>",
        "id": 2769363,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kva03w/syncoverycom_what_are_the_limitations_after_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Syncovery.com - What are the limitations after the trial ends?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/s4lt3d_h4sh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T19:06:13.833370+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T18:25:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey folks, I\u2019ve got a Lenovo M80q Gen 4 Tiny that I\u2019m using in my homelab. It has a built-in Wi-Fi card on an M.2 slot (probably E-keyed), but I\u2019m not using Wi-Fi at all.</p> <p>Unfortunately, this model doesn\u2019t have a full PCIe slot soldered onto the motherboard, so standard PCIe HBAs are out of the question. That\u2019s why I\u2019m wondering:</p> <p>Is it possible to replace the Wi-Fi card with some sort of SAS HBA or similar interface to connect a JBOD enclosure full of HDDs? Any M.2-to-SAS (or M.2-to-PCIe then to HBA) options that actually work in this kind of setup?</p> <p>I\u2019m running Proxmox and planning to use TrueNAS or similar to manage the disks. Open to creative solutions, including USB 3.2-to-SATA workarounds or other tricks you\u2019ve seen work with Tiny PCs like this.</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/s4lt3d_h4sh\"> /u/s4lt3d_h4sh </a> <br/> <span><a href=\"https://www.reddi",
        "id": 2769364,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv9eqa/can_i_replace_the_wifi_card_in_my_lenovo_m80q_gen",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I replace the Wi-Fi card in my Lenovo M80q Gen 4 with a SAS HBA to connect a JBOD?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Caballep",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T19:06:13.218905+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T18:13:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv94lq/why_shouldnt_i_just_use_mdisc_bluray_for_all_my/\"> <img src=\"https://preview.redd.it/y7bhtopsxy2f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e270c82437631f816e22c48cb5aa896d57b2519\" alt=\"Why Shouldn't I Just Use M-DISC Blu-ray for ALL My Long-Term Photo/Video ANNUAL Backups? I can get discs for ~$1 and make 5 copies of it every year.\" title=\"Why Shouldn't I Just Use M-DISC Blu-ray for ALL My Long-Term Photo/Video ANNUAL Backups? I can get discs for ~$1 and make 5 copies of it every year.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Caballep\"> /u/Caballep </a> <br/> <span><a href=\"https://i.redd.it/y7bhtopsxy2f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv94lq/why_shouldnt_i_just_use_mdisc_bluray_for_all_my/\">[comments]</a></span> </td></tr></table>",
        "id": 2769361,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv94lq/why_shouldnt_i_just_use_mdisc_bluray_for_all_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/y7bhtopsxy2f1.png?width=640&crop=smart&auto=webp&s=8e270c82437631f816e22c48cb5aa896d57b2519",
        "title": "Why Shouldn't I Just Use M-DISC Blu-ray for ALL My Long-Term Photo/Video ANNUAL Backups? I can get discs for ~$1 and make 5 copies of it every year.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Far-Glove-888",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T19:06:13.420353+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T17:59:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A few years ago I bought a motherboard with 6x m.2 slots hoping that I&#39;ll be able to populate it with 16TB m.2 sticks in the near future. Here we are today, still stuck with 8TB m.2 drives.</p> <p>Is there any movement in the industry to offer 16TB m.2 to consumers? The datacenter alternatives use a different form factor and are way too expensive.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Far-Glove-888\"> /u/Far-Glove-888 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv8scv/with_8tb_m2_drives_finally_dropping_in_price_are/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv8scv/with_8tb_m2_drives_finally_dropping_in_price_are/\">[comments]</a></span>",
        "id": 2769362,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv8scv/with_8tb_m2_drives_finally_dropping_in_price_are",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "With 8TB m.2 drives finally dropping in price, are there any hopes of 16TB m.2 ever becoming a reality?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Flaky_Country_3951",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T20:10:13.214765+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T17:59:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m asking for a strange beast. I want to add a new USB-C 3.2 Gen 2 or better port to my computer and it does not have any PCIe slots, but I do have a free M.2 PCIe slot (normally an NVMe drive would drop in here).</p> <p>This is not the normal adapter that everyone can find that allows an M.2 NVMe card to connect to a USB port, I want to go the other way around.</p> <p>Sounds simple, right? Well I have had no luck, except bad luck trying to locate something that does this. I have even looked for an M.2 to PCIe x4 adapter but they all go the wrong way.</p> <p>If someone can offer some help in locating a product which would do this, it would be appreciated.</p> <p>The end goal... Add a USB-C 3.2 Gen 2 or faster to my computer using he available M.2 (nvme) connector.</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Flaky_Country_3951\"> /u/Flaky_Country_3951 </a> <br/> <span><a href=\"https://www.red",
        "id": 2769692,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv8s57/trying_to_locate_an_nvme_to_usb_c_32_gen_2_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying to locate an NVMe to USB C 3.2 Gen 2 or better adapter.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Virtual_Sweet4317",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T17:59:46.952440+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T17:38:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If anyone has a work around pls let me know</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Virtual_Sweet4317\"> /u/Virtual_Sweet4317 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv8aiw/has_anyone_found_a_fix_for_tiktok_full_hd_because/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv8aiw/has_anyone_found_a_fix_for_tiktok_full_hd_because/\">[comments]</a></span>",
        "id": 2768917,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv8aiw/has_anyone_found_a_fix_for_tiktok_full_hd_because",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has anyone found a fix for TikTok full hd because it\u2019s been 2 weeks since full hd videos stopped working and now only download in 576p when I was able to download 4k TikToks and in hdr and Instagram also used to be 1080p now it\u2019s 720p",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/8tomat8",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T17:59:46.773884+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T17:24:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building a home storage for different kinds of not important media. I plan to go with 5x20+TB drivers, but have hard time finding any used drives with more than a 1TB.</p> <p>There are many datacenters around, so there should be second hand market...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/8tomat8\"> /u/8tomat8 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv7yfr/where_do_i_get_refabtished_hdds_in_netherlands/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv7yfr/where_do_i_get_refabtished_hdds_in_netherlands/\">[comments]</a></span>",
        "id": 2768916,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv7yfr/where_do_i_get_refabtished_hdds_in_netherlands",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where do I get refabtished HDDs in Netherlands?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Robin_Landon_3574",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T20:10:13.451158+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T16:33:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><ul> <li><a href=\"https://www.amazon.co.jp/s?k=%22BD-RE%22+XL+100GB&amp;ref=nb_sb_noss_2\">BD-RE XL 100GB on Amazon.jp</a></li> <li><a href=\"https://www.amazon.co.jp/s?k=%22BD-R%22+XL+100GB&amp;ref=nb_sb_noss_2\">BD-R XL 100GB on Amazon.jp</a></li> </ul> <p>Ideal stuff for backups of essential stuff that has to endure on the shelf for decades.</p> <p>\u20ac6-7 per rewritable disk ain&#39;t that bad. And \u20ac2+ per generic BD-R. Interestingly, Verbatim&#39;s M-DISC version isn&#39;t in hwole enother order of magnitude - \u20ac3+...</p> <p>Maybe because it might be not all that different from classic version ?\ud83d\ude44 * <a href=\"https://goughlui.com/2015/10/16/review-tested-verbatim-lifetime-archival-millenniatam-disc-4x-bd-r-25gb/\">Review, Tested: Verbatim Lifetime Archival (Millenniata/M-Disc) 4x BD-R 25Gb</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Robin_Landon_3574\"> /u/Robin_Landon_3574 </a> <br/> <span><a href=\"https://www.re",
        "id": 2769693,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv6r5d/cheap_bdrbdre_xl_100gb_disks_all_over_japan",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cheap BD-R/BD-RE XL 100GB disks all over Japan...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mrcrashoverride",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T16:40:02.734520+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T16:30:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This should be easy yet I\u2019ve forgotten and I\u2019m noob enough that it\u2019s a bit daunting this morning. I have a folder of files that I wish to create a separate folder for each file based on the file name. So a movie file let\u2019s call it \u201cJohn Doe\u201d I wish to have the file moved into a newly created folder labeled \u201cJohn Doe\u201d</p> <p>Can someone please help me with what settings to toggle. It\u2019s so common and simple that I think it should be just a click and it auto configures, (might actually exist) but googling is only coming up with people that were doing it wrong asking for help.</p> <p>Anywho I appreciate any and all help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mrcrashoverride\"> /u/mrcrashoverride </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv6oda/bulk_rename_utility_folderize_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv6oda/bulk_r",
        "id": 2768501,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv6oda/bulk_rename_utility_folderize_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bulk Rename Utility -Folderize help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/tenclowns",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T16:40:02.915438+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T16:20:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking a bit around. Want a solution that is best able to download media files and whole webpages. Cyotek webcopy seems a bit slow to me. I see Offline Explorer recommended on here when i go to search. Is this a reliable software. I see there is an Enterprise edition with more robust features as well. </p> <p>Looking for some feedback from anyone with experience</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tenclowns\"> /u/tenclowns </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv6fyy/webpage_scraper_experience_offline_explorer_vs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv6fyy/webpage_scraper_experience_offline_explorer_vs/\">[comments]</a></span>",
        "id": 2768502,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv6fyy/webpage_scraper_experience_offline_explorer_vs",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webpage scraper experience - Offline Explorer vs. competition",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/kuro68k",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T16:40:03.064096+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T16:04:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to scan a load of LP and Laserdisc covers. The problem is they are 32x32cm, or about 12x12 inches.</p> <p>You can get A3 scanners that are big enough in one dimension, but too short in the other. Some have a flat edge so that you can scan in two halves and stitch the images together, but they are quite expensive. Viisan make one for about \u00a3300, and Plustek make one for about \u00a3550.</p> <p>Are there any good and cheaper alternatives?</p> <p>I looked at taking photos but the results aren&#39;t great unless you have a quite large rig with well controlled lighting and a good camera. LP and LD covers tend to be glossy.</p> <p>There is also the Fujitsu (now Ricoh) Scansnap power scanner which has an actual 1D scanner head in it, not a camera. It would need multiple passes to get the whole cover and the DPI is only about 150 near the edges, which is a bit low for archival copies. I want to capture the detail of the printing method, and then de-screen a",
        "id": 2768503,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv62tm/affordable_scanners_for_lp_and_laserdisc_covers",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Affordable scanners for LP and Laserdisc covers",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/literatebanjo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T16:40:03.213298+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T15:59:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone know where to find crocodile hunter I&#39;m trying to get all of Steve Irwins stuff together it&#39;s really hard to find.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/literatebanjo\"> /u/literatebanjo </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv5y86/crocodile_hunter/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv5y86/crocodile_hunter/\">[comments]</a></span>",
        "id": 2768504,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv5y86/crocodile_hunter",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Crocodile hunter?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FillSensitive248",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T20:10:12.770548+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T15:49:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv5ptc/converted_my_parents_old_vhs_tapes_to_digital/\"> <img src=\"https://b.thumbs.redditmedia.com/1lpo0zPyaVv4eELlWTDQWXInKn6iqrMGEyLjIb5D0jk.jpg\" alt=\"Converted my parents old vhs tapes to digital.\" title=\"Converted my parents old vhs tapes to digital.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>My parents own a panasonic 300x digital camera recorder. As a family watched some tapes. Noticed the microphone sounding horrible on the camrecorder. Wanted to salvage family memories so I decided to restore these tapes digital. I purchased a rca, S video convertor to hdmi and hdmi convertor to usb. Took a little but but got the cam connected to my pc. I&#39;ve recorded almost all tapes from obs at 720x480, to mp4. The audio is perfect on the digital side. Super happy to learn how to do this and see nice memories. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FillSensit",
        "id": 2769690,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv5ptc/converted_my_parents_old_vhs_tapes_to_digital",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/1lpo0zPyaVv4eELlWTDQWXInKn6iqrMGEyLjIb5D0jk.jpg",
        "title": "Converted my parents old vhs tapes to digital.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/2020_2904",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T15:35:13.789519+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T15:19:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi. I want to download all archived pages of a specific domain. How can I do that?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/2020_2904\"> /u/2020_2904 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv511o/webarchiveorg_download_specific_websitedomain/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv511o/webarchiveorg_download_specific_websitedomain/\">[comments]</a></span>",
        "id": 2768095,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv511o/webarchiveorg_download_specific_websitedomain",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "web.archive.org download specific website/domain",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Electronic-Support34",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T20:10:13.718362+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T15:08:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone \ud83d\udc4b</p> <p>I\u2019m a developer and recently built a simple web tool called <a href=\"http://mediahubtools.com/\"><strong>MediaHubTools</strong></a> that lets you:</p> <ul> <li>\ud83d\udd3b Download YouTube videos (including Shorts)</li> <li>\ud83c\udfb5 Convert them to MP3</li> <li>\ud83d\udce5 Download Instagram videos</li> <li>\ud83d\udcbb Use it on browser (no install or extension needed)</li> </ul> <p>Made this mainly for friends who didn\u2019t want to mess with <code>yt-dlp</code> or shady downloader apps. Works well on mobile too.</p> <p>Just looking for honest feedback from this awesome community \u2014 does it load fast? Anything missing?</p> <p>\u27a1\ufe0f <a href=\"http://mediahubtools.com\">https://mediahubtools.com</a></p> <p>Thanks in advance \ud83d\ude4f</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic-Support34\"> /u/Electronic-Support34 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv4sbs/i_made_a_free_tool_to_download_youtube_sho",
        "id": 2769694,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv4sbs/i_made_a_free_tool_to_download_youtube_shorts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I made a free tool to download YouTube Shorts, Instagram videos & convert them to audio \u2014 feedback welcome \ud83d\ude4f",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jared555",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T14:29:10.958486+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T14:26:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Are there parts readily available for making a custom jbod enclosure? </p> <p>Thinking being able to use quieter hardware/cooling than what is meant for datacenter use. Something like building in a fractal design case and when it is out of bays get another one and just connect it to the first server. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jared555\"> /u/jared555 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv3tvi/diy_jbod_enclosures/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv3tvi/diy_jbod_enclosures/\">[comments]</a></span>",
        "id": 2767685,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv3tvi/diy_jbod_enclosures",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "DIY JBOD enclosures?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sockpuppets",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T14:29:10.696294+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T14:04:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Auto generated return labels from the Seagate store have the wrong address. My returns are bouncing around 500 miles from where they&#39;re supposed to be. UPS claims to have corrected the address so hopefully they&#39;ll make it where they need to go. 95014 is not Torrance, CA- hopefully the street address is correct.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sockpuppets\"> /u/sockpuppets </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv3buq/psa_seagate_return_labels_have_wrong_address/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv3buq/psa_seagate_return_labels_have_wrong_address/\">[comments]</a></span>",
        "id": 2767684,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv3buq/psa_seagate_return_labels_have_wrong_address",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "PSA: Seagate return labels have wrong address",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/manzurfahim",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T14:29:11.107136+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T13:51:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been backing up some YT channels, and the Stacher software (yt-dlp based app with a GUI) is downloading AV1 files when best quality video / audio in mp4 format is selected.</p> <p>My question is: Do these AV1 files offer anything else other than space saving? Quality is I think better on the AVC or VP9 file since they are the source, am I right? AV1 re-encodes them, which is probably reducing the quality even if a little bit, right?</p> <p>So, if I want the best quality possible, should I download the AV1 files? Also, do YT even keeps the original format file once they encode them to AV1?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/manzurfahim\"> /u/manzurfahim </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv31sl/hoarding_yt_channels_av1_or_h264_vp9/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv31sl/hoarding_yt_channels_av1_or_h264_v",
        "id": 2767686,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv31sl/hoarding_yt_channels_av1_or_h264_vp9",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hoarding YT channels: AV1 or H.264 / VP9?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Abject-Point-6236",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T13:24:22.319304+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T12:58:35+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv1xfv/opinions_i_thinking_buying_one_as_media_drive_for/\"> <img src=\"https://preview.redd.it/te65i0ghex2f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb179392d616d0efc910558a68bb16478693862f\" alt=\"Opinions? I thinking buying one as media drive for not important media files\" title=\"Opinions? I thinking buying one as media drive for not important media files\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Abject-Point-6236\"> /u/Abject-Point-6236 </a> <br/> <span><a href=\"https://i.redd.it/te65i0ghex2f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv1xfv/opinions_i_thinking_buying_one_as_media_drive_for/\">[comments]</a></span> </td></tr></table>",
        "id": 2767344,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv1xfv/opinions_i_thinking_buying_one_as_media_drive_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/te65i0ghex2f1.jpeg?width=640&crop=smart&auto=webp&s=cb179392d616d0efc910558a68bb16478693862f",
        "title": "Opinions? I thinking buying one as media drive for not important media files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hardest_punch",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T12:17:56.635376+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T11:50:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i dont know if this is the right subreddit but is there any tools or site that archived deleted pornhub videos? I&#39;m looking for a specific videos that got deleted.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hardest_punch\"> /u/hardest_punch </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv0ot1/pornhub_deleted_videos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv0ot1/pornhub_deleted_videos/\">[comments]</a></span>",
        "id": 2767020,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv0ot1/pornhub_deleted_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "pornhub deleted videos",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/EducationalArmy9152",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T12:17:56.166718+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T11:29:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;m a bit of a noob at Python but want to use AI (because I&#39;m also lazy) to code / scrape / automate web activities. Most AI&#39;s can&#39;t read source code without you pasting it in and I can only seem to do that element by element with devtools. I just got Cyotek webcopy which seems to be doing it&#39;s job but it&#39;s scraping like half a gig from one simple website and I selected just HTML output. Can anyone suggest a better workaround or am I already on the right track?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EducationalArmy9152\"> /u/EducationalArmy9152 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv0c70/how_to_scrape_full_html/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kv0c70/how_to_scrape_full_html/\">[comments]</a></span>",
        "id": 2767019,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kv0c70/how_to_scrape_full_html",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "how to scrape full HTML",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/The_One_Who_Crafts",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T09:04:35.991656+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T08:25:54+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/The_One_Who_Crafts\"> /u/The_One_Who_Crafts </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=reHm2F-CBxY\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kuxn7y/anyone_have_this_now_private_video/\">[comments]</a></span>",
        "id": 2766170,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kuxn7y/anyone_have_this_now_private_video",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone have this now private video?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/StarsInTears",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T04:41:26.267665+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T04:11:35+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/StarsInTears\"> /u/StarsInTears </a> <br/> <span><a href=\"/r/selfhosted/comments/1kutrgs/is_there_a_bookmarkmanager_that_integrates_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kutt8r/is_there_a_bookmarkmanager_that_integrates_with/\">[comments]</a></span>",
        "id": 2765404,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kutt8r/is_there_a_bookmarkmanager_that_integrates_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a Bookmark-manager that integrates with SingleFile?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/VariousSheepherder58",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T04:41:26.626208+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T03:38:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve collected many books, sacred scrolls, videos , and overall historical content over the years that&#39;s been lost to time. I want to make free videos online to display what&#39;s inside them in a way that&#39;s easier to digest but it would take years doing it manually.</p> <p>My overall plan is to launch a page using an educational mascot on all major social platforms and load them with impactful videos that summarize each topic/module. I have over 800 different topics/modules.</p> <p>I&#39;m wondering what ai tools would be best to achieve this. My budget is around $50-$100 for now as it&#39;s a passion project I don&#39;t tend to profit from any of it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VariousSheepherder58\"> /u/VariousSheepherder58 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kut9na/need_advice_repurposing_7_terabytes_of_ancient/\">[link]</a></span> &#32; <span",
        "id": 2765405,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kut9na/need_advice_repurposing_7_terabytes_of_ancient",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need advice repurposing 7 Terabytes of ancient forgotten knowledge to display to a newer audience",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hiimawalrus",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T05:47:45.842068+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T03:30:24+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kut4i2/fractal_define_7_xl_almost_maxed_out_temperature/\"> <img src=\"https://a.thumbs.redditmedia.com/9B2KUso_46EgTFHfG7mdu9ogOAoU7ZOeCd9iJt1eAQ4.jpg\" alt=\"Fractal Define 7 XL (almost) maxed out \u2013 temperature problems\" title=\"Fractal Define 7 XL (almost) maxed out \u2013 temperature problems\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I finally filled almost every bay of my Define 7 XL full of drives (16 total, 2 for parity), I am now maxed out on sata connections at least. The last two drives I installed sitting behind the main stack in the lower part of the case are cooking themselves to death. Even after swapping the front intake fans to those new Noctua G2 140 mm, they still cant last through a parity check with the front door closed. The fan swap did help, but not solve the problem. With the front door open they top out around 47 \u00b0C, but with it closed they simply can\u2019t finish a pari",
        "id": 2765606,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kut4i2/fractal_define_7_xl_almost_maxed_out_temperature",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/9B2KUso_46EgTFHfG7mdu9ogOAoU7ZOeCd9iJt1eAQ4.jpg",
        "title": "Fractal Define 7 XL (almost) maxed out \u2013 temperature problems",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Broad_Sheepherder593",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T03:35:50.620332+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T03:28:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, relatively new to nas. Currently have raid 1 with 2 new drives and working well. </p> <p>Plan to build another with a 20tb capacity. Is there such a thing as a primary disk in raid 1? Was thinking to get a new disk for the primary and just a refurb for the 2nd disk. Which one should i setup first where all the data would be replicated from? Or since its gonna be raid 1 anyway, then it should not matter?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Broad_Sheepherder593\"> /u/Broad_Sheepherder593 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kut3it/hdd_mix_raid_1/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kut3it/hdd_mix_raid_1/\">[comments]</a></span>",
        "id": 2765251,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kut3it/hdd_mix_raid_1",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hdd mix raid 1",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/HTWingNut",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T02:30:27.353507+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T01:56:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought parts for a new NAS build. I ordered Crucial 32GB Crucial Pro CP2K16G60C48U5. I have ordered them several times before no problem. </p> <p>This time the package came wrapped in a plastic package, the Crucial package clearly damaged, and the part number on the RAM was CMK32GX4M2B3200C16. Corsair 32GB DDR4. Not bad ram by any right, but it&#39;s clearly used, not the right kind, and possibly dead.</p> <p><a href=\"https://i.imgur.com/yjWGmu6.jpeg\">https://i.imgur.com/yjWGmu6.jpeg</a></p> <p>Why Amazon do this?</p> <p>Thankfully they have a lenient return policy, but when you&#39;re trying to do a build and this stuff happens, it just irritates me.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HTWingNut\"> /u/HTWingNut </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kurhqt/oh_amazon_ordered_new_dddr5_ram_got_used_ddr4_ram/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.c",
        "id": 2765084,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kurhqt/oh_amazon_ordered_new_dddr5_ram_got_used_ddr4_ram",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Oh Amazon... ordered NEW DDDR5 RAM. Got USED DDR4 RAM and different manufacturer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/KingSupernova",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-25T02:30:26.941195+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-25T01:39:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to avoid link rot on my websites and discussions with others, so I like to make sure that anything I link to has a version in the Wayback machine. (Or <a href=\"http://archive.is\">archive.is</a>, or some other archival site.) Doing this manually is a pain, so I&#39;d like to have an extension that automatically archives any page I visit. (Ideally only if no archived version already exists, to avoid wasting their storage space.)</p> <p>I haven&#39;t been able to find any though. Does anybody know of one?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KingSupernova\"> /u/KingSupernova </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kur6zf/is_there_an_extension_that_automatically_archives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kur6zf/is_there_an_extension_that_automatically_archives/\">[comments]</a></span>",
        "id": 2765083,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kur6zf/is_there_an_extension_that_automatically_archives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there an extension that automatically archives every webpage I visit?",
        "vote": 0
    }
]