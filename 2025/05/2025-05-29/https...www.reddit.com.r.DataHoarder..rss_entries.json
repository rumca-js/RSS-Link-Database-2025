[
    {
        "age": null,
        "album": "",
        "author": "/u/astrae_research",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T23:07:30.047989+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T22:17:06+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kynwiz/consistently_higher_temps_for_seagate_hamr_recert/\"> <img src=\"https://a.thumbs.redditmedia.com/ly4bbggnWo2MT6UpEoVY_VcCzgAurkp5Rv-Ix1saxx4.jpg\" alt=\"Consistently higher temps for Seagate HAMR Recert HDD vs non-HAMR - normal?\" title=\"Consistently higher temps for Seagate HAMR Recert HDD vs non-HAMR - normal?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/jlxfi0dgps3f1.png?width=838&amp;format=png&amp;auto=webp&amp;s=5bf18c86b93b08daaa8479613b59d25704d00a28\">https://preview.redd.it/jlxfi0dgps3f1.png?width=838&amp;format=png&amp;auto=webp&amp;s=5bf18c86b93b08daaa8479613b59d25704d00a28</a></p> <p>Hail Datahoarders, </p> <p>Hope this question fits the subreddit. I&#39;m new to HAMR HDD technology and my first HAMR HDD from Seagate is a recert from SPD (+46C) and is consistently hotter than non-HAMR drives (+39C) on indle. Difference is 7C to 9C between the drives with sim",
        "id": 2803127,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kynwiz/consistently_higher_temps_for_seagate_hamr_recert",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/ly4bbggnWo2MT6UpEoVY_VcCzgAurkp5Rv-Ix1saxx4.jpg",
        "title": "Consistently higher temps for Seagate HAMR Recert HDD vs non-HAMR - normal?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AggravatingTear4919",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T22:02:10.160890+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T21:54:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Someone i know recently asked if i could share my entire collection with them. Theyre hesitant because their uncle did this and absolutely refused to share with anyone he kept them under lock in key. So would i share my data? the data ive been actively hoarding and collecting for 5+ years? while he gets it all in a matter of minutes? abso freaking lutely. Im hoarding this stuff TOO potentially share and he can act as a back up. He can spread the information ive collected to others and keep it alive. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AggravatingTear4919\"> /u/AggravatingTear4919 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyndjb/how_open_are_you_to_sharing_your_hoards/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyndjb/how_open_are_you_to_sharing_your_hoards/\">[comments]</a></span>",
        "id": 2802739,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyndjb/how_open_are_you_to_sharing_your_hoards",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How open are you to sharing your hoards?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lixxus_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T22:02:10.400256+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T21:29:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all</p> <p>Ive run out of bays in my xpenology build, I have 2 spare 8tb 3.5&quot;SATA III/6Gb/s i want to utilise them<br/> I cant afford to buy another nas</p> <p>Is there anything i can buy that can utilise usb port which is 3.2 10gbps port </p> <p>some sort of enclosure that will expose them as independent drives and allow me to raid them in synology</p> <ul> <li><strong>USB 3.2 Gen 2 (10 Gbps)</strong> = ~1,250 MB/s theoretical bandwidth</li> <li><strong>SATA III drive max speed</strong> = ~550 MB/s actual max speed</li> </ul> <p>im guess there will be no bottleneck in theory</p> <p>is there any particular sata chipset that is really good for usb enclosure</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lixxus_\"> /u/lixxus_ </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kymsgl/some_advice_please_sata_usb_jbod/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataH",
        "id": 2802740,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kymsgl/some_advice_please_sata_usb_jbod",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "some advice please, sata usb jbod ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/FatDog69",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T20:56:45.094360+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T20:48:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have 2 Win10 PC&#39;s (i5 - 8 gigs memory) that are not compatible with Win 11. I was thinking of putting in some new NVME drives and switching to Mint Linux when Win10 stops being supported. </p> <p>To mimic my Win10 setup - here is my list of software. Please suggest others or should I run everything in docker containers? What setup suggestions do you have and best practices?</p> <p>MY INTENDED SOFTWARE:</p> <ul> <li>OS: Mint Linux (Ubuntu based)</li> <li>Indexer Utility: NZBHydra</li> <li>Downloader: Sabnzbd - for .nzb files</li> <li>Downloader videos: JDownloader2 (I will re-buy for the linux version)</li> <li>Transcoder: Handbrake</li> <li>File Renamer: TinyMediaManager</li> <li>File Viewer: UnixTree</li> <li>Newsgroup Reader: ??? - (I love Forte Agent but it&#39;s obsolete now)</li> <li>Browser: Brave &amp; Chrome.<br/></li> <li>Catalog Software: ??? (I mainly search Sabnzb to see if I have downloaded something previously)</li> <li>Code Editor",
        "id": 2802256,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyls3c/what_software_switching_to_linux_from_win10_do",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What software switching to Linux from Win10 do you suggest?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Afterlast1",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T20:56:44.682897+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T19:57:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I feel like I&#39;m in the right place to ask this question - I have too many god damn hard drives! They got all kinds of stuff on them; old school projects, ADHD hyperfixations, hundreds of gigabytes of raw photos. I&#39;ve got hard drives that are backups of other hard drives and at this point I don&#39;t know what&#39;s what. Does anyone here know of any process that can scan all the attached harddrives and highlight or ignore all the duplicate files so I can start clean and get organized and only have, idk maybe 3 full back ups? instead of half a dozen partial back ups?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Afterlast1\"> /u/Afterlast1 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kykigk/what_are_you_guys_using_to_keep_track_of_where/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kykigk/what_are_you_guys_using_to_keep_track_of_where/\"",
        "id": 2802255,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kykigk/what_are_you_guys_using_to_keep_track_of_where",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are you guys using to keep track of where all your damn files are?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dwhite21787",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T19:50:44.747915+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T19:34:26+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dwhite21787\"> /u/dwhite21787 </a> <br/> <span><a href=\"https://s3.us-east-1.amazonaws.com/rds.nsrl.nist.gov/software/NSRL_free_bags_README.htm\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyjxx1/national_software_reference_library_is_posting/\">[comments]</a></span>",
        "id": 2801794,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyjxx1/national_software_reference_library_is_posting",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "National Software Reference Library is posting download links for all the freely acquired software in their collection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/completeudderidiot",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T19:50:44.959608+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T19:17:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all; I&#39;m working on setting up a personal archive of random data that&#39;s important to me (youtube videos, music, art etc.) and want some advice on how to expand further. What I have now is just the storage inside my PC that I use daily:</p> <p>- 1 TB SSD 1, OS and user stuff (WDC WDS100T2B0C-00PXH0 : 1000.2GB)</p> <p>- 2TB SSD 2, games and downloads (WD_BLACKSN850X 2000GB : 2000.3 GB)</p> <p>The HDDs I have are what I&#39;ve been trying to set up and organize as an archive, it&#39;s mostly a lot of video files; My one hard drive is fairly older but I recently got a brand new 6TB one that&#39;s quickly filling up</p> <p>- 2TB HDD (ST2000DM008-2FR102 : 2000.3 GB)</p> <p>- 6TB HDD (WDC WD6004FZBX-00C9FA0 : 6001.1 GB)</p> <p>Basically I wanted to ask how to start expanding? My PC case is out of slots for HDDs so I think i&#39;ll need an external enclosure/rack or something (I think I&#39;ll start having issues with power consumption at a certain",
        "id": 2801795,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyjiro/expansion_advicesetting_up_personal_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Expansion advice/setting up personal archive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mourningmage",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T18:46:29.122250+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T18:31:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Idk if this is the right sub, but I have a few old computers/loose HDDs that I want to get recovered and put on a cloud location or a flash drive.. is there a consensus service that I can mail these into and get this old data for a reasonable ish cost? I\u2019m talking 4 or 5 drives with maybe 100gb each max. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mourningmage\"> /u/mourningmage </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyichs/hdd_recovery_service/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyichs/hdd_recovery_service/\">[comments]</a></span>",
        "id": 2801342,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyichs/hdd_recovery_service",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HDD recovery service",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JIMMY_RUSTLING_9000",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T18:46:29.330896+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T17:39:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Title</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JIMMY_RUSTLING_9000\"> /u/JIMMY_RUSTLING_9000 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyh0aj/anyone_know_if_there_is_an_official_source_of_old/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyh0aj/anyone_know_if_there_is_an_official_source_of_old/\">[comments]</a></span>",
        "id": 2801343,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyh0aj/anyone_know_if_there_is_an_official_source_of_old",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Anyone know if there is an official source of old Apple keynotes, I'm looking for original quality files of steve jobs announcing stuff.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/pokeyfortnite",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T18:46:28.910538+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T17:38:51+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyh01i/multiversus_preservation_effort/\"> <img src=\"https://preview.redd.it/1xdzs1ntar3f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c17c3165676f64a6e1b5ac370dded89cfeb15eb6\" alt=\"Multiversus Preservation Effort\" title=\"Multiversus Preservation Effort\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello all, new here. The game Multiversus will have its servers turned off, then delisted on May 30th, 2025 at 9am PST. The developers were kind enough to include an offline mode, but only if you log into Season 5 before the game&#39;s shutoff date. The strange thing is, they&#39;re delisting the game off all platforms. This means that new players will never be able to download this game because it&#39;s gone off all platforms. So that&#39;s why I took time out of my day to download the game from Steam, and personally compress the game folder for archival purposes. This is a gray area, but after May 30, this",
        "id": 2801341,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyh01i/multiversus_preservation_effort",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/1xdzs1ntar3f1.png?width=320&crop=smart&auto=webp&s=c17c3165676f64a6e1b5ac370dded89cfeb15eb6",
        "title": "Multiversus Preservation Effort",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kryptic_Anthology",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T18:46:29.482277+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T17:22:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Doesn&#39;t need to be an all in one. A dedicated and reliable scanner is what I am looking for. I appreciate the help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kryptic_Anthology\"> /u/Kryptic_Anthology </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kygl74/need_assistance_looking_for_a_reliable_document/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kygl74/need_assistance_looking_for_a_reliable_document/\">[comments]</a></span>",
        "id": 2801344,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kygl74/need_assistance_looking_for_a_reliable_document",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need assistance looking for a reliable document scanner for payroll under $400",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Spikas",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T18:46:29.631342+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T16:56:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been meaning to start my own home backup as I&#39;ve gone on longer in life, graduation, wedding, holiday pics and vids, films, old research documents and everything else.</p> <p>I looked into a Raid set up but the base bay itself was far more expensive than I had expected so the idea was put on hold.</p> <p>Jump back to last week and I find a owc thunderbay 8 in a shop asking for the equivalent of 50USD, asked for 45 and he said yes. </p> <p>I haven&#39;t had the time to plug it in and check it, or get the software but if all&#39;s OK then I&#39;m curious where to start as a small time data hoarder...</p> <p>I was thinking of starting with two 1TB drives and then adding to it (since you need pairs for a Raid, right?), but my friend said I may as well start either one 4TB drive then adding when I can. I honestly think that I could start with 2TB drive and just add on over time until I fill all eight bays.</p> <p>The question is though, can on",
        "id": 2801345,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyfx8f/found_a_owc_thunderbay_8_second_hand_how_to_go",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Found a owc thunderbay 8 second hand, how to go forward.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Rhoken",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T16:51:49.030592+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T16:48:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It&#39;s been from some years that i have start to hoard and backup most of my data and in particular when i was starting to do photography, to a point that i have now various drives around.</p> <p>Some days ago i have buyed a new HDD cage for my two backup HDDs (Toshiba P300) which is one 2 TB unit and one 1 TB unit, synching everything with Synkron.</p> <p>But i want to ditch the 1 TB unit and add a 4 TB unit but the HDD market has changed drastically from last times (4-5 years ago).</p> <p>I don&#39;t want to spend many money (budget is 100-120 euros) and with this budget i have find these models:</p> <p>- Seagate Ironwolf 4 TB ST4000VN006 (CMR)</p> <p>- Seagate Barracuda 4 TB ST4000DM004 (SMR)</p> <p>- HGST Ultrastar 7 K6000 4 TB HUS726040ALE610 (CMR?)</p> <p>- Toshiba P300 4 TB HDWD240UZSVA (SMR)</p> <p>The Toshiba is the cheapest one (75 euros) while the Ironwolf the most expensive (94 euros) the others one are in between 80 to 100 euros, which ",
        "id": 2800402,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyfqdg/i_need_advice_for_a_new_4_tb_hdd_for_my_backup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I need advice for a new 4 TB HDD for my backup stash",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Fuzzy-Zone-5535",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T16:51:49.178900+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T16:41:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am unable to use yt-dlp even though I tried and failed to use it many times even followung step-by-step tutorials on YouTube. There are a few movies in 4K I found on YT that I would like to download. Are there any alternative way to do it? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fuzzy-Zone-5535\"> /u/Fuzzy-Zone-5535 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyfjiw/how_to_download_4k_youtube_videos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyfjiw/how_to_download_4k_youtube_videos/\">[comments]</a></span>",
        "id": 2800403,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyfjiw/how_to_download_4k_youtube_videos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to download 4K YouTube videos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/phenrys",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T16:51:48.823289+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T16:39:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Super happy to share with you the latest version of my YouTube Downloader Program, v1.2. This version introduces a new feature that allows you to download multiple videos simultaneously (concurrent mode). The concurrent video downloading mode is a significant improvement, as it saves time and prevents task switching. </p> <p>To install and set up the program, follow these simple steps: <a href=\"https://github.com/pH-7/Download-Simply-Videos-From-YouTube\">https://github.com/pH-7/Download-Simply-Videos-From-YouTube</a></p> <p>I\u2019m excited to share this project with you! It holds great significance for me, and it was born from my frustration with online services like SaveFrom, Clipto, Submagic, and T2Mate. These services often restrict video resolutions to 360p, bombard you with intrusive ads, fail frequently, don\u2019t allow multiple concurrent downloads, and don\u2019t support downloading playlists.</p> <p>I hope you&#39;ll find this useful, if you have any feed",
        "id": 2800401,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyfhgb/a_selfhosted_script_that_downloads_multiple",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A self-hosted script that downloads multiple YouTube videos simultaneously in their highest quality.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lucky_Influence901",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T15:46:42.456230+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T15:04:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m losing my mind over here. It\u2019s 2025, and I\u2019m STILL wrestling with file system chaos like it\u2019s 2005. I have a perfectly good M.2 SSD full of family data in NTFS format, and now I want to watch some simple movies on my tablet that only reads FAT32 or exFAT. Sounds easy, right? Nope. And before you little assholes say &quot;then just use exfat!!~!!!!!!!!!&quot; Well shit.... The documentation says it SHOULD support exfat but that fucker told me to go format it like the bitch it is when the documentation literally says IT WORKS ON EXFAT. WHAT THE FRCICCCFKCKCKC</p> <p>I\u2019ve spent <strong>six hours</strong> trying to convert, clone, partition, and split files without destroying a single byte. Windows crashes, file explorers freeze, formatting tools act like they\u2019re from the stone age, and then my tablet STILL can\u2019t read the drive properly.</p> <p>Why do we still have to jump through hoops to <em>just watch a movie</em>? Why can\u2019t there be one single, un",
        "id": 2799676,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyd3mh/why_the_hell_in_2025_do_we_still_have_no",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why the hell in 2025 do we STILL have no universal damn file system?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TheThingCreator",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T14:41:42.466407+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T14:21:26+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kyc1e7/pocket_is_shutting_down_dont_lose_your_folders/\"> <img src=\"https://external-preview.redd.it/aL8kq9lmCnjiCOCc22SDW-WtKZ2K1obAgf4fVwwOH5w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d3285c32ec3a8a23509b7cddd6b69cb4337b82d\" alt=\"Pocket is Shutting down: Don't lose your folders and tags when importing your data somewhere else. Use this free/open-source tool to extract the meta data from the export file into a format that can easily migrate anywhere.\" title=\"Pocket is Shutting down: Don't lose your folders and tags when importing your data somewhere else. Use this free/open-source tool to extract the meta data from the export file into a format that can easily migrate anywhere.\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheThingCreator\"> /u/TheThingCreator </a> <br/> <span><a href=\"https://github.com/webcull/PocketExportConverter\">[link]</a></span> &#32; <span><a hre",
        "id": 2799092,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyc1e7/pocket_is_shutting_down_dont_lose_your_folders",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/aL8kq9lmCnjiCOCc22SDW-WtKZ2K1obAgf4fVwwOH5w.jpg?width=640&crop=smart&auto=webp&s=0d3285c32ec3a8a23509b7cddd6b69cb4337b82d",
        "title": "Pocket is Shutting down: Don't lose your folders and tags when importing your data somewhere else. Use this free/open-source tool to extract the meta data from the export file into a format that can easily migrate anywhere.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ken852",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T14:41:42.175867+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T13:42:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I submitted a data request today. It was processed in less than one hour. Which is kind of nice. It can normally take companies anywhere from 1 to 30 days, sometimes more to process this kind of request if it&#39;s handled manually.</p> <p>But I&#39;m surprised that all I got are 37 CSV files inside a ZIP file. The ZIP is only 6.14 MB. There are no media files, like the many images I uploaded. Also, everything seems to be sorted by ID, which is alphanumeric. Instead of sorting by date, which I think would make more sense. This applies to posts and messages. There is also no clear separation between them. So the whole thing is very hard to read and make sense of, for exmaple to verify its completeness. I requested everything. But I&#39;m not sure how far back this goes until I sort it.</p> <p>So I was wondering if there iis a third party tool, either free or paid, that will let me get a complete copy of my account data, including the images? Preferably",
        "id": 2799091,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kyb45i/is_there_any_tool_that_will_let_me_backup_and",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there any tool that will let me backup and view my Reddit account data?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Pyryara",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T12:31:19.813226+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T11:19:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I currently have a raid10 setup with 6x3TB drives, of which one has recently failed, and an additional raid1 mirror of two 13TB drives. Instead of getting a replacement 3TB drive, I want to get away from this towards a snapraid setup, because the main data I store on my small N100 home server is large unchanging media files, of which I simply want to have a backup without being totally wasteful of space.</p> <p>I have understood that with 5+ drives I should probably go for two parity drives for my data, but since I only have two larger drives, that&#39;s of course not easily possible. So I was thinking if I could maybe divide the 13 TB drives into 10+3 TB, and then I&#39;d pool the 3 TB partitions into a snapraid with the 6 other drives, and then do a single-parity snapraid with the 10 TB partitions on the larger drives. This would also allow me to change the setup quite easily in the future if I replace further 3 TB drives with larger 13 TB drives.</",
        "id": 2797918,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ky889i/snapraid_setup_for_differentlysized_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Snapraid setup for differently-sized drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Not_So_Calm",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T11:13:22.995388+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T10:29:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve just migrated 5TB of personal files to a nextcloud (cloud service) and am looking into additional self hosting at home, using Immich and more stuff. And all that got me thinking:</p> <p>How do you ensure or rather verify the integrity of your files?</p> <p>Even when having multiple backups (3-2-1 strategy), you can&#39;t be sure there is no file corruption / bit rot somewhere. You cannot possible open all your pictures and documents once a year. Do you create checksum files for your data to test against? If yes, what tools are you using to generate those?</p> <p>Edit: I checked <a href=\"https://www.reddit.com/r/DataHoarder/wiki/backups/\">https://www.reddit.com/r/DataHoarder/wiki/backups/</a> , which hardly mentions &quot;checksum&quot; or &quot;verify&quot;.</p> <p>I have not yet a ZFS filessystem at home (which uses checksums), and tools like BORG might do checksums, but they use it for change detection and comparision of source and target, ",
        "id": 2797307,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ky7e6z/how_to_test_file_integrity_longterm",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to test file integrity longterm?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JonVonBasslake",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T11:13:23.201801+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T10:25:07+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m running the latest version of Linux Mint, and I used to be able to get images with a wget script (I&#39;m kinda new to Linux, I mainly switched because I hate what Windows has become and is becoming), but ever since the site went down for several days recently and came back, I get 403&#39;d if i try to run the old wget script and I don&#39;t know how to modify it to get it to work again. I do have a secondary win10 install for games and mods that don&#39;t work well on Linux, so I can use that if needed...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JonVonBasslake\"> /u/JonVonBasslake </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ky7bid/what_is_the_best_easiest_way_to_download_all/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ky7bid/what_is_the_best_easiest_way_to_download_all/\">[comments]</a></span>",
        "id": 2797308,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ky7bid/what_is_the_best_easiest_way_to_download_all",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the best / easiest way to download all images from a 4chan thread?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PixalmasterStudios24",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T07:38:52.687027+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T06:49:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been planning to make some custom blu rays for some streaming exclusive films, but i need a safe and hopefully not too expensive way to get it. 4K would be nice I guess but since I\u2019m gonna get a BD writer that doesn\u2019t support 4K it wont be necessary right now. I\u2019ve heard of Streamfab, Anystream, Keepstreams, etc. so what\u2019s the best software for this and what would be the best way to get it as uncompressed and high quality as possible. Thanks! Again, I\u2019m new to the sub, so sorry if anything i say sounds dumb \ud83d\ude05</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PixalmasterStudios24\"> /u/PixalmasterStudios24 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ky43iz/new_to_this_sub_what_is_the_best_way_to_get/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1ky43iz/new_to_this_sub_what_is_the_best_way_to_get/\">[comments]</a></span>",
        "id": 2796156,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1ky43iz/new_to_this_sub_what_is_the_best_way_to_get",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New to this sub. What is the best way to get streaming exclusive films in high quality?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NotSFW4719",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T02:39:16.129345+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T02:00:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I run a subreddit (Its just me) where I regularly crosspost using custom flair. When I try to browse by flair in my subreddit using the Reddit iOS app, it only loads posts from the last ~2 months under one flair, and only up to ~8 months on another \u2014 even though I know I&#39;ve posted much more before that. (July 2023 it should go back to)</p> <p>I\u2019ve tried:</p> <ul> <li>Switching to the old Reddit in a browser on my laptop (same issue \u2014 cuts off after a certain point) <ul> <li>I downloaded the following chrome extensions <ul> <li>Reddit Enhancement Suite</li> <li>UI Changer for Reddit</li> </ul></li> </ul></li> <li>Using the Reddit iOS app with different sort orders (New, Top, etc.) <ul> <li>Sometimes i can get older posts but the majority are still missing.</li> </ul></li> </ul> <p>Reddit still won&#39;t show posts older than those cutoffs, even though they weren&#39;t deleted or removed.</p> <p>This seems like a search or filtering limitation, not ",
        "id": 2795020,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kxz2lq/ive_lost_a_few_hundred_posts_in_my_own_subreddit",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I've lost a few hundred posts in my own subreddit looking for advice on how to access or how to better save posts in the future.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/stacker103",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-29T00:29:15.833203+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-29T00:08:50+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kxws8l/just_came_in/\"> <img src=\"https://preview.redd.it/4bz935ss4m3f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bb7cd030b6809914d2a048b8f1378cd2b56de99\" alt=\"just came in\" title=\"just came in\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/stacker103\"> /u/stacker103 </a> <br/> <span><a href=\"https://i.redd.it/4bz935ss4m3f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kxws8l/just_came_in/\">[comments]</a></span> </td></tr></table>",
        "id": 2794484,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kxws8l/just_came_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/4bz935ss4m3f1.jpeg?width=640&crop=smart&auto=webp&s=9bb7cd030b6809914d2a048b8f1378cd2b56de99",
        "title": "just came in",
        "vote": 0
    }
]