[
    {
        "age": null,
        "album": "",
        "author": "/u/cryptoteams",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T23:02:52.189526+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T22:28:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I created a bookmarklet that uses <strong>&quot;postMessage&quot;</strong> to send data to another page, which can enrich the data. This is powerful and compliant since the &#39;scraping&#39; happens on the client and doesn&#39;t breach any TOS.</p> <p>Does anyone have any experience with this type of &#39;scraping&#39;? I&#39;m very curious how this can work legally.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryptoteams\"> /u/cryptoteams </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kodpts/bookmarklet_scraping_clientside/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kodpts/bookmarklet_scraping_clientside/\">[comments]</a></span>",
        "id": 2699966,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kodpts/bookmarklet_scraping_clientside",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bookmarklet Scraping (client-side)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/albert_in_vine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T20:53:44.131922+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T20:52:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Has anyone tried using OfferUp outside of the US? I attempted to access the website using a VPN, but I couldn&#39;t get in no matter what I did. I&#39;m also using datacenter proxies to try to gain access, but I&#39;m still encountering a 403 error. I don&#39;t want to invest in ISP or residential proxies until I can confirm that it will work. Can someone share their thoughts on this? I would really appreciate it!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/albert_in_vine\"> /u/albert_in_vine </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kobjp7/trying_offerup/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kobjp7/trying_offerup/\">[comments]</a></span>",
        "id": 2699175,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kobjp7/trying_offerup",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Trying offerup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cursed-scholar",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T14:17:26.740453+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T13:10:06+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im scraping KYC data for my company but the problem is to get all the data i need to scrape the data of 20k customers now the problem is my normal scraper cant do that much and maxes out around 1.5k how do i scrape 20k sites and while keeping it all intact and not frying my computer . Im currently writing a script where it does this for me on this scale using selenium but running into quirks and errors especially with login details</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cursed-scholar\"> /u/Cursed-scholar </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ko0ghy/scraping_over_20k_links/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ko0ghy/scraping_over_20k_links/\">[comments]</a></span>",
        "id": 2695971,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ko0ghy/scraping_over_20k_links",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping over 20k links",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LinuxTux01",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T14:17:26.966304+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T13:08:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I&#39;m trying to listen to pokemon center&#39;s http requests using burp suite pro browser + awesome tls extension to spoof real chrome tls fingerprint. This combo works on cloudfare websites as I don&#39;t get challenges anymore but on pokemon center during drops I get blocked after solving hcaptcha, how could they detect me? Burp suite extension? Thanks in advance </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LinuxTux01\"> /u/LinuxTux01 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ko0f8i/burp_suite_pro_browser_detected_by_imperva/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ko0f8i/burp_suite_pro_browser_detected_by_imperva/\">[comments]</a></span>",
        "id": 2695972,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ko0f8i/burp_suite_pro_browser_detected_by_imperva",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Burp suite pro browser detected by imperva",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Specialist-Carpet465",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T16:33:46.703520+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T13:00:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am building a project for which I will need some of the basic statistics of users when they give basic user name.</p> <p>leetcode has a API endpoint for this :<a href=\"https://leetcode-stats-api.herokuapp.com/\">https://leetcode-stats-api.herokuapp.com/</a></p> <p>Need Something like this for Hackerrank and Geeksfor geeks </p> <pre><code>{&quot;status&quot;:&quot;error&quot;,&quot;message&quot;:&quot;please enter your username (ex: leetcode-stats-api.herokuapp.com/LeetCodeUsername)&quot;,&quot;totalSolved&quot;:0,&quot;totalQuestions&quot;:0,&quot;easySolved&quot;:0,&quot;totalEasy&quot;:0,&quot;mediumSolved&quot;:0,&quot;totalMedium&quot;:0,&quot;hardSolved&quot;:0,&quot;totalHard&quot;:0,&quot;acceptanceRate&quot;:0.0,&quot;ranking&quot;:0,&quot;contributionPoints&quot;:0,&quot;reputation&quot;:0,&quot;submissionCalendar </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Specialist-Carpet465\"> /u/Specia",
        "id": 2697311,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ko08uz/need_help_in_getting_user_details_from_hackerrank",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help in getting user details from hackerRank",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MagicPogostickMP",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T12:31:18.006995+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T11:27:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My commercial real estate company often identifies buildings scheduled for demolition or refurbishment. We then have the specific address but face challenges in compiling a complete list of tenant companies.</p> <p>Is there a tool capable of extracting all registered businesses from Google Maps using a specific address or GPS coordinates? We&#39;ve found Google Maps data to be generally more accurate and promptly updated by companies, especially compared to other sources - Companies want to be seen, so they update their Google address as soon as they move.</p> <p>Currently, we utilize ZoomInfo and CoStar, but their data can be limited or inaccurate. Government directories also present issues, as businesses frequently register using their accountant&#39;s or solicitor&#39;s address.</p> <p>We are looking for more reliable methods to search for companies by address and would appreciate any suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <",
        "id": 2695134,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1knyg7i/scraping_google_maps_by_address",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Google Maps by address",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ambermason315",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T11:26:11.640102+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T11:23:50+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I used a scraping tool called tryinstantdata.com. Worked pretty well to scrape Google business for business name, website, review rating, phone numbers. </p> <p>It doesn\u2019t give me:</p> <p>Address Contact name Email</p> <p>What\u2019s the best tool for bulk upload to get these extra data points? Do I need to use two different tools to accomplish my goal? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ambermason315\"> /u/ambermason315 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1knye54/emails_contact_names_and_addresses/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1knye54/emails_contact_names_and_addresses/\">[comments]</a></span>",
        "id": 2694738,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1knye54/emails_contact_names_and_addresses",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Emails, contact names and addresses",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RevolutionaryGood445",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T12:31:18.225079+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T10:19:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone!</p> <p>I&#39;m here to present my latest little project, which I developed as part of a larger project for my work.</p> <p>What&#39;s more, the lib is written in pure Python and has no dependencies other than the standard lib.</p> <p><strong>What My Project Does</strong></p> <p>It&#39;s called Refinedoc, and it&#39;s a little python lib that lets you remove headers and footers from poorly structured texts in a fairly robust and normally not very RAM-intensive way (appreciate the scientific precision of that last point), based on this paper <a href=\"https://www.researchgate.net/publication/221253782_Header_and_Footer_Extraction_by_Page-Association\">https://www.researchgate.net/publication/221253782_Header_and_Footer_Extraction_by_Page-Association</a></p> <p>I developed it initially to manage content extracted from PDFs I process as part of a professional project.</p> <p><strong>When Should You Use My Project?</strong></p> <p>The idea be",
        "id": 2695135,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1knxc89/refinedoc_little_text_processing_lib",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Refinedoc - Little text processing lib",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DatakeeperFun7770",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T09:14:28.926332+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T08:49:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to scrape a ecom website, but all the different product pages have different type to css selector, putting all manually is time consuming and frustrating and you never know when the tag will change. What is the best practice? I am using scrapy playwrite setup</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DatakeeperFun7770\"> /u/DatakeeperFun7770 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1knw2c0/how_to_scrape_dynamic_websites/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1knw2c0/how_to_scrape_dynamic_websites/\">[comments]</a></span>",
        "id": 2693776,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1knw2c0/how_to_scrape_dynamic_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape dynamic websites",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Baberooo",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T07:47:10.906252+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T06:02:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I&#39;ve been trying to scrape an insurance website that provides premium quotes. </p> <ul> <li>Website URL: <a href=\"https://www.123.ie/insurance/car/#/search-reg\">https://www.123.ie/insurance/car/#/search-reg</a> (but also <a href=\"https://www.axa.ie/car-insurance/quote/your-details\">https://www.axa.ie/car-insurance/quote/your-details</a>)</li> <li>Data points: the website consists of several pages where potential customers are asked to enter some basic information: age, vehicle type, license plate number type, etc...</li> <li>Project goal: I want to build a simple quotes aggregator, not for commercial purposes</li> </ul> <p>I&#39;ve tried several Python libraries (Selenium, Playwright, etc..) but most importantly I&#39;ve tried to pass different user agents combinations as parameters.</p> <p>No matter what I do, that website detects that I&#39;m a bot.</p> <p>What would be your approach in this situation? Is there any specific p",
        "id": 2693366,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kntpzq/blocked_blocked_and_blocked_again_by_some_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Blocked, blocked, and blocked again by some website",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Afraid_Ad4270",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-16T06:40:54.321227+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-16T05:51:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>Hey everyone, I\u2019m trying to scrape all reviews from my restaurant\u2019s Google Maps listing but running into issues. Here\u2019s what I\u2019ve done so far:</strong></p> <ul> <li><strong>Objective:</strong> Extract 827 reviews into an Excel sheet with these fields: <ol> <li>Reviewer name</li> <li>Star rating</li> <li>Review text</li> <li>Photo(s) indicator</li> <li>\u201cShare\u201d link URL (the three-dots menu)</li> </ol></li> <li><strong>My background:</strong> <ul> <li>Not a professional developer</li> <li>Used Claude to generate a step-by-step Python guide</li> </ul></li> <li><strong>Setup:</strong> <ul> <li>MacBook Pro on macOS Big Sur</li> <li>Chrome browser</li> <li>Python 3 via Terminal</li> </ul></li> <li><strong>Problems encountered:</strong> <ol> <li>Some reviews have no text (empty strings)</li> <li>Long reviews require clicking \u201cMore\u201d to reveal full text</li> <li>Reviews with photos need special handling to detect and download images</li> <li>Scripts ke",
        "id": 2693133,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kntkid/scraping_all_reviews_in_maps_failed_how_to_scrape",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping all Reviews in Maps failed - How to scrape all reviews",
        "vote": 0
    }
]