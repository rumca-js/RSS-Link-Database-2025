[
    {
        "age": null,
        "album": "",
        "author": "/u/jptyt",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-30T23:18:12.568553+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-30T22:42:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi community,</p> <p>I&#39;ve started scraping not for so long, bear with my lack of knowledge if so..</p> <p>So I&#39;m trying to <strong>mimic clicks on certain buttons on Walmart in order to change the store location</strong>. I previously used a free package running on local, it worked for a while until getting blocked by the captcha. </p> <p>Then I resort to paid services, I tried several, either they don&#39;t support interaction during scraping or return message like &quot;Element cannot be found&quot; or &quot;Request blocked by Walmart Captcha&quot; when the very first click happens. (I assume that &quot;Element cannot be found&quot; is caused by Captcha correct?). The services usually give a simple log without any visibility to the browser which make more difficult to troubleshoot. </p> <p>So I wonder, what mechanism causes the click to be detected? Has anyone succeeded to do clicks on shopping websites (I would like to talk to you further)?",
        "id": 2812143,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kzhqmy/mimicking_clicks_on_walmart_website_seems_to_be",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Mimicking clicks on Walmart website seems to be detected",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GoldTea7698",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-30T22:13:09.378315+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-30T21:59:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone \ud83d\udc4b</p> <p>I&#39;m an Automation Testing Engineer based in Egypt, and I help people save time and reduce errors by automating boring, repetitive tasks \ud83d\udcbb\u2699\ufe0f</p> <p>\ud83d\udd27 What I can do:</p> <p>Build custom automation scripts for any web task (data entry, scraping, testing forms, etc.)</p> <p>Create and maintain automation testing frameworks using Selenium + Java</p> <p>Automate e-commerce flows, dashboards, login systems, and more</p> <p>Run tests and generate detailed reports with screenshots and logs</p> <p>\u2705 Real examples:</p> <p>Automated test cases for full web apps (search, add/edit/delete items, form validation)</p> <p>If you:</p> <p>Run a small business and need to test your website automatically</p> <p>Hate doing the same web task every day</p> <p>Are a dev who needs help with testing your frontend/backend</p> <p>Or just want to save time...</p> <p>Let\u2019s talk! I can tailor a solution for your needs \ud83c\udfaf</p> <p>Feel free to DM me or drop a co",
        "id": 2811777,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kzgr7w/i_can_automate_ur_boring_tasks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "i can automate ur boring tasks",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/RespectNarrow450",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-30T09:42:37.117907+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-30T09:12:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Came across this blog on <a href=\"http://blog.scalefusion.com/web-content-filtering/?utm_campaign=Scalefusion%20Promotion&amp;utm_source=Reddit&amp;utm_medium=social&amp;utm_term=SP\">web content filtering</a>\u2014what it is, why it matters for businesses, and how to implement it effectively across remote and office teams. Read the full blog and take the first step toward safer browsing for your org!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RespectNarrow450\"> /u/RespectNarrow450 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kyzn8u/web_filtering_keeps_your_team_secure_online/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kyzn8u/web_filtering_keeps_your_team_secure_online/\">[comments]</a></span>",
        "id": 2805891,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kyzn8u/web_filtering_keeps_your_team_secure_online",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Web Filtering keeps your team secure online.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/biolds",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-30T07:32:43.745197+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-30T07:22:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi!</p> <p>I\u2019m the main dev behind <a href=\"https://sosse.io\">Sosse</a>, an open-source search engine that does web data extraction and indexing.</p> <p>We\u2019re preparing for an upcoming release, and I\u2019ve put together some <strong>Ethical Use Guidelines</strong> to help set a respectful, responsible tone for how the project is used.</p> <p>Would love your feedback before we publish:<br/> \ud83d\udc49 <a href=\"https://sosse.readthedocs.io/en/latest/crawl_guidelines.html\">https://sosse.readthedocs.io/en/latest/crawl_guidelines.html</a></p> <p>All thoughts welcome \ud83d\ude4f, many thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/biolds\"> /u/biolds </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kyy30s/feedback_wanted_ethical_use_guidelines_for_sosse/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kyy30s/feedback_wanted_ethical_use_guidelines_for_sosse/\">[comments]</a",
        "id": 2805227,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kyy30s/feedback_wanted_ethical_use_guidelines_for_sosse",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Feedback wanted \u2013 Ethical Use Guidelines for Sosse",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/No-Willow176",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-30T07:32:43.910883+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-30T07:02:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im scraping moneycontrol for financials of indian stocks and I have found an endpoint for the income sheet. <a href=\"https://www.moneycontrol.com/mc/widget/mcfinancials/getFinancialData?classic=true&amp;device_type=desktop&amp;referenceId=income&amp;requestType=S&amp;scId=YHT&amp;frequency=3\">https://www.moneycontrol.com/mc/widget/mcfinancials/getFinancialData?classic=true&amp;device_type=desktop&amp;referenceId=income&amp;requestType=S&amp;scId=YHT&amp;frequency=3</a></p> <p>This gives quarterly income sheet for YATHARTH.</p> <p>i wanted to automate this for all stocks, is there a way to find all the &quot;scId&quot; for every stock. this isnt the trading symbol which is why its a little hard. moneycontrol decided to make their own ids for their endpoints.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Willow176\"> /u/No-Willow176 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kyxspv",
        "id": 2805228,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kyxspv/moneycontrol_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Moneycontrol scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/New_Needleworker7830",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-30T06:23:36.328457+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-30T06:05:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Ciao a tutti,</p> <p>I\u2019m working on a Python module for scraping/crawling/spidering. I needed something fast when you have 100-10000 of websites to scrape and it happened to me already 3-4 times - whether for email gathering or e-commerce or any kind of information - so I packed it till with just 2 simple lines of code you fetch all of them at high speed.</p> <p>It features a separated queue system to avoid congestion, spreads requests across the same domain, and supports retries with different backends (currently <strong>httpx</strong> and <strong>curl</strong> via subprocess for HTTP/2; Seleniumbase support coming soon, but at last chance because would reduce the speed 1000 times). It also gets robots and sitemaps, provides full JSON logging for each request, and can run multiprocess and multithreaded workflows in parallel while collecting stats, and more. It works also just for one website, but it\u2019s more efficient when more websites are scraped. </",
        "id": 2804952,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Project for fast scraping of thousands of websites",
        "vote": 0
    }
]