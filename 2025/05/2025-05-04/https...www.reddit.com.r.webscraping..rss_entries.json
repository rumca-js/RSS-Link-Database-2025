[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T21:32:53.917687+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T20:49:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been scraping some undocumented public APIs (found via browser dev tools) and want to write some code capturing the endpoints and arguments I\u2019ve teased out so it\u2019s reusable across projects.</p> <p>I\u2019m looking for advice on how to structure things so that:</p> <ul> <li><p>I can use the API in both sync and async contexts (scripts, bots, apps, notebooks).</p></li> <li><p>I\u2019m not tied to one HTTP library or request model.</p></li> <li><p>If the API changes, I only have to fix it in one place.</p></li> </ul> <p>How would you approach this, particularly in python? Any patterns, or examples would be helpful.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Disorderedsystem\"> /u/Disorderedsystem </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1keu1o7/how_do_you_design_reusable_interfaces_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1keu1o7/how_do_",
        "id": 2598276,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1keu1o7/how_do_you_design_reusable_interfaces_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do you design reusable interfaces for undocumented public APIs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T20:27:35.023517+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T19:40:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, for a side project I need to scrape multiple job boards. As you can image, each of them has a different page structure and some of them have parameters that can be inserted in the url (eg: location or keywords filter).</p> <p>I already built some ad-hoc scrapers but I don&#39;t want to maintain multiple and different scrapers.</p> <p>What do you recommend me to do? Is there any AI Scrapers that will easily allow me to scrape the information in the joab boards and that is able to understand if there are filters accepted in the url, apply them and scrape again and so on?</p> <p>Thanks in advance </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BlackLands123\"> /u/BlackLands123 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kesfko/how_to_scrape_multiple_and_different_job_boards/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kesfko/how_to_scrape_mu",
        "id": 2597931,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kesfko/how_to_scrape_multiple_and_different_job_boards",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape multiple and different job boards with AI?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T13:56:44.999996+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T13:20:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Google became extremely aggressive against any sort of scraping in the past months.<br/> It started by forcing javascript to remove simple scraping and AI tools using python to get results and by now I found even my normal home IP to be regularly blocked with a reCaptcha and any proxies I used are blocked from the start.</p> <p>Aside of building a recaptcha solver using AI and selenium, what is the goto solution which is not immediately blocked for accessing some search result pages of keywords ?</p> <p>Using mobile proxies or &quot;residential&quot; proxies is likely a way forward but the origin of those proxies is extremely shady and the pricing is high.<br/> And I dislike using an API of some provider, I want to access it myself.</p> <p>I read people seem to be using IPV6 for the purpose, however my attempts on V6 IPs were without success (always captcha page).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/L",
        "id": 2595865,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kejnby/what_affordable_way_of_accessing_google_search",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What affordable way of accessing Google search results is left ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T13:56:45.130629+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T12:49:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, I am new to webscraping. I want to scrape customers&#39; reviews and property&#39;s response to the reviews on <a href=\"http://Booking.com\">Booking.com</a> for my academic project using Python. I am looking into the APIs of Booking to see whether I can do it. </p> <p>Is anyone already familiar with Booking APIs to tell me this? Looking on the API website makes me quite confused. Thanks a lot!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MortgageWeary3344\"> /u/MortgageWeary3344 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kej1gb/webscraping_with_bookingcom_apis/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kej1gb/webscraping_with_bookingcom_apis/\">[comments]</a></span>",
        "id": 2595866,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kej1gb/webscraping_with_bookingcom_apis",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webscraping with Booking.com APIs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T13:56:45.262398+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T08:37:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>The MCP servers are all the rage nowadays, where one can use MCP servers to do a lot of automations. </p> <p>I also tried using the Playwright MCP server to try a few things on VS Code. </p> <p>Here is one such experiment <a href=\"https://youtu.be/IDEZA-yu34o\">https://youtu.be/IDEZA-yu34o</a></p> <p>Please review and give feedback.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/adroitbot\"> /u/adroitbot </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kef4g4/using_playwright_mcp_servers_for_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kef4g4/using_playwright_mcp_servers_for_scraping/\">[comments]</a></span>",
        "id": 2595867,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kef4g4/using_playwright_mcp_servers_for_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Using Playwright MCP Servers for Scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T05:17:38.958893+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T03:21:10+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>If you are new to web scraping or looking to build a professional-grade scraping infrastructure, this project is your launchpad.<br/> Over the past few days, I have assembled a complete template for web scraping + browser automation that includes:</p> <ul> <li>Playwright (headless browser)</li> <li>asyncio + httpx (parallel HTTP scraping)</li> <li>Fingerprint spoofing (WebGL, Canvas, AudioContext)</li> <li>Proxy rotation with retry logic</li> <li>Session + cookie reuse</li> <li>Pagination &amp; login support</li> </ul> <p>It is not fully working, but can be use as a foundation project. Feel free to use it for whatever project you have.<br/> <a href=\"https://github.com/JRBusiness/scraper-make-ez\">https://github.com/JRBusiness/scraper-make-ez</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OkParticular2289\"> /u/OkParticular2289 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kead4v/an_e",
        "id": 2593883,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kead4v/an_exampletemplate_for_an_advanced_web_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "An example/template for an advanced web scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T02:03:11.453512+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T01:29:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been playing around with web scraping recently with Python. </p> <p>I had a few questions:</p> <ol> <li>Is there a go to method people use to scrape website first before moving on to other methods if that doesn&#39;t work? </li> </ol> <p>Ex. Do you try a headless browser first for anything (Playwright + requests) or some other way? Trying to find a reliable method.</p> <ol> <li>Other than robots.txt, what else do you have to check to be on the right side of the law? Assuming you want the safest and most legal method (ready to be commercialized) </li> </ol> <p>Any other tips are welcome as well. What would you say are must knows before web scraping?</p> <p>Thank you! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Affectionate_Pear977\"> /u/Affectionate_Pear977 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ke8equ/need_practical_and_legal_advice_on_web_scraping/\">[link]</a></span",
        "id": 2593395,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ke8equ/need_practical_and_legal_advice_on_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need practical and legal advice on web scraping!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-04T00:57:44.754033+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-04T00:36:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys I\u2019m trying to figure out the most efficient and cheapest way to scrape all of users tweets since they started their profile. Is this possible? \ud83d\udc40</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LiveATheHudson\"> /u/LiveATheHudson </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ke7g71/best_way_to_scrape_x_profile_tweet_history/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ke7g71/best_way_to_scrape_x_profile_tweet_history/\">[comments]</a></span>",
        "id": 2593193,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ke7g71/best_way_to_scrape_x_profile_tweet_history",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best way to scrape X profile tweet history?",
        "vote": 0
    }
]