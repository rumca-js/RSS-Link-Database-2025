[
    {
        "age": null,
        "album": "",
        "author": "/u/DFW_BjornFree",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T23:32:00.991118+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T23:12:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I work in PE and wear a vast number of hats, one of which is developing/ maintaining our scraper infrastructure and also running it. </p> <p>I have litterally built everything from the ground up in python and currently scrape 100k pages a day on average. </p> <p>I need to improve my operations so that I can scrape 1M pages a day lol.</p> <p>Current setup is a tower desktop with a thread ripper and I can run up to 12 scrapers in parallel (though in rare occasions it causes the pc to crash). </p> <p>All of my scrapers use proxies, have captcha solvers, etc. </p> <p>I want to avoid the scenario where I buy more desktops but that might be my only option. </p> <p>Looking to see alternatives, I&#39;m not very interested in racking up a 6 figure bill every time we refresh our data. One of our datasets has 20M rows with unique urls and we scrape up to 20 pages per site, I want to refresh it annually and that&#39;s just one dataset. I scrape other datasets on ",
        "id": 2634124,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ki431b/scraping_at_scale_with_a_high_success_rate_think",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping at scale with a high success rate (think 1M pages per day)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Striking-Science8945",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T22:27:38.019893+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T21:13:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ki1fap/problem_scraping_cinepolischile/\"> <img src=\"https://external-preview.redd.it/RTaZ-U9Pm0BHeJj7IgL3qhHPABRtV1Aj9t1rTQz9WAQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=40822000e8eae547993efcfef42f3630db110e49\" alt=\"Problem scraping cinepolischile\" title=\"Problem scraping cinepolischile\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi there,</p> <p>I have a problem when scraping or rather consuming an endpoint from a paperspace C4 VPS of <a href=\"https://cinepolischile.cl/\">https://cinepolischile.cl/</a><br/> este es el fetch: </p> <pre><code>fetch(&quot;https://sls-api-compra.cinepolis.com/api/tickets&quot;, { &quot;headers&quot;: { &quot;accept&quot;: &quot;application/json, text/plain, */*&quot;, &quot;accept-language&quot;: &quot;es-419,es;q=0.9&quot;, &quot;content-type&quot;: &quot;application/json&quot;, &quot;priority&quot;: &quot;u=1, i&quot;, &quot;sec-ch-ua&quot;: &quot;\\&quot;Google Ch",
        "id": 2633729,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ki1fap/problem_scraping_cinepolischile",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/RTaZ-U9Pm0BHeJj7IgL3qhHPABRtV1Aj9t1rTQz9WAQ.png?width=108&crop=smart&auto=webp&s=40822000e8eae547993efcfef42f3630db110e49",
        "title": "Problem scraping cinepolischile",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lupical712",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T21:21:56.582373+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T21:01:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a programmer who prefers python or C++, and I&#39;m trying to make my first web scraper. </p> <p>However, every single tutorial I find online is somewhat outdated and I&#39;m being overwhelmed with obstacles like anti-bot and stuff. </p> <p>Does anyone know the best way to get started, or recommend me a tutorial that isn&#39;t outdated and useless? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lupical712\"> /u/Lupical712 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ki15e5/how_to_get_started_with_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ki15e5/how_to_get_started_with_webscraping/\">[comments]</a></span>",
        "id": 2633341,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ki15e5/how_to_get_started_with_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to get started with webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/eddiedoidao",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T20:17:53.482782+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T19:58:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im building a leaderboard of brands based on few metrics from my scraped data.</p> <p>Source includes social media platforms, common crawl, google ads.</p> <p>Currently throwing everything into r2 and processing to supabase.</p> <p>Since I want to have daily historical reports of for example active ads, ranking, I\u2019m noticing by having 150k URLs and track their stats daily will make it really big.</p> <p>What\u2019s the most common approach by handling this type of setup?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/eddiedoidao\"> /u/eddiedoidao </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1khzlrn/issues_with_storage/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1khzlrn/issues_with_storage/\">[comments]</a></span>",
        "id": 2632879,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1khzlrn/issues_with_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Issues with storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Ecstatic-Drop-1239",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T22:27:38.213449+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T17:19:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m sure this is the most generic and commonly asked question on this subreddit, but im just interested to hear what people recommend.</p> <p>Of course using resi/mobile proxies and humanizing actions, but just any other general tips when it comes to scraping would be great! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ecstatic-Drop-1239\"> /u/Ecstatic-Drop-1239 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1khvocv/new_to_webscraping_any_advice_for_avoiding_bot/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1khvocv/new_to_webscraping_any_advice_for_avoiding_bot/\">[comments]</a></span>",
        "id": 2633730,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1khvocv/new_to_webscraping_any_advice_for_avoiding_bot",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New to webscraping - any advice for avoiding bot detection?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ConfidentIncident802",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T14:51:45.350006+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T13:09:08+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1khpp76/reverse_of_cloudflare/\"> <img src=\"https://preview.redd.it/6ktggfyl4kze1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4485af7c1fd4d4e24e08ffdde92772baf0a75cb9\" alt=\"Reverse of cloudflare\" title=\"Reverse of cloudflare\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>i am trying to reverse cf i need this token but every time when i start debugging i put breakpoint i m getting issue on reload its not stopping the script for debugging its skipping the breakpoint anyone can help me for this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ConfidentIncident802\"> /u/ConfidentIncident802 </a> <br/> <span><a href=\"https://i.redd.it/6ktggfyl4kze1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1khpp76/reverse_of_cloudflare/\">[comments]</a></span> </td></tr></table>",
        "id": 2629310,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1khpp76/reverse_of_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/6ktggfyl4kze1.jpeg?width=640&crop=smart&auto=webp&s=4485af7c1fd4d4e24e08ffdde92772baf0a75cb9",
        "title": "Reverse of cloudflare",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CombinationDense4917",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-08T14:51:45.509185+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-08T11:23:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I&#39;m trying to access the DVSA practical driving test site using Puppeteer with stealth mode enabled, but I keep getting <strong>Error 15: Access Denied</strong>. I\u2019m not doing anything aggressive \u2014 just trying to load the page \u2014 and I believe I\u2019m being blocked by bot detection.</p> <p>Here\u2019s my code:</p> <pre><code>javascriptCopyEditconst puppeteer = require(&#39;puppeteer-extra&#39;); const StealthPlugin = require(&#39;puppeteer-extra-plugin-stealth&#39;); // Enable stealth plugin to evade bot detection puppeteer.use(StealthPlugin()); (async () =&gt; { const browser = await puppeteer.launch({ headless: false, // Run with GUI (less suspicious) args: [&#39;--start-maximized&#39;], defaultViewport: null, executablePath: &#39;Path to Chrome&#39; // e.g., C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe }); const page = await browser.newPage(); // Set a modern and realistic user agent await page.setUserAgent( &quot;Mozilla/5.0",
        "id": 2629311,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1khnnns/getting_error_15_on_dvsa_website_using_puppeteer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Getting Error 15 on DVSA Website Using Puppeteer \u2014 Need Help",
        "vote": 0
    }
]