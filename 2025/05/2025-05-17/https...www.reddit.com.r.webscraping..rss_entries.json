[
    {
        "age": null,
        "album": "",
        "author": "/u/Kilnarix",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T21:54:36.363169+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T20:53:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to extract data from a cloudfare protected site. I am trying a new approach. First I navigate to the site in a regular Firefox browser. I solve the captcha manually. Once the homepage is loaded I export all of the network traffic as a HAR file. I have a Python script which loads up the HAR file and extracts all the cookies, the headers and the payload of the relevant request. This data is used to create a request in Python.</p> <p>I am getting a 403 error. I have checked that the request made the browser is identical to the request made in Python.</p> <p>Has anyone else had this approach work for them? Am I missing something obvious?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kilnarix\"> /u/Kilnarix </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kp31ml/extracting_cookies_from_har_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/",
        "id": 2705686,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kp31ml/extracting_cookies_from_har_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Extracting cookies from HAR files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/NoPin618",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T20:48:44.793710+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T20:24:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I\u2019ve been curious about how services like <strong>SSYouTube</strong> or other websites that allow users to download YouTube videos manage to avoid getting blocked by YouTube.</p> <p>I\u2019m not talking about their public-facing frontend IPs (where users visit the site), but specifically their backend infrastructure, where the actual downloading/scraping logic runs. These systems must make repeated requests to YouTube to fetch video data.</p> <p>My questions:</p> <p><strong>1. How do these services avoid getting their backend IPs banned by YouTube, considering that they&#39;re making thousands of automated requests?</strong></p> <p><strong>2. Does YouTube detect and block repeated access from a single IP?</strong></p> <p><strong>3. How do proxy rotation systems work, and are they used in this context?</strong></p> <p>I&#39;m considering building something similar (educational purposes only), and I want to understand the technical strat",
        "id": 2705464,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kp2f2h/how_do_youtube_video_downloader_sites_avoid",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do YouTube video downloader sites avoid getting blocked?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/d_berbatov",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T17:34:27.392385+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T17:02:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://antcpt.com/eng/information/demo-form/recaptcha-3-test-score.html\">https://antcpt.com/eng/information/demo-form/recaptcha-3-test-score.html</a></p> <p>Anyone able to get more than 0.7 constantly here with puppeteer?</p> <p>I use proxies, rotate agents, etc., am able to pass cloudflare captcha (sometimes automatically sometimes by clicking) but on this test score very rarely get more than 0.7</p> <p>Also, sometimes I get 0.1 and then during same session get 0.7 or more which is very weird</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/d_berbatov\"> /u/d_berbatov </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1koxsuk/antcpt_score_with_puppeteer/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1koxsuk/antcpt_score_with_puppeteer/\">[comments]</a></span>",
        "id": 2704539,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1koxsuk/antcpt_score_with_puppeteer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "ANTCPT score with puppeteer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mobile-Perspective17",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T18:39:34.479390+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T14:51:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This is a list of the tallest proposed buildings in the world:</p> <p><a href=\"https://www.skyscrapercenter.com/buildings?status=proposed&amp;material=all&amp;function=all&amp;location=world&amp;year=2025\">https://www.skyscrapercenter.com/buildings?status=proposed&amp;material=all&amp;function=all&amp;location=world&amp;year=2025</a></p> <p>This is a list of the tallest in-construction buildings in the world:</p> <p><a href=\"https://www.skyscrapercenter.com/buildings?status=construction&amp;material=all&amp;function=all&amp;location=world&amp;year=2025\">https://www.skyscrapercenter.com/buildings?status=construction&amp;material=all&amp;function=all&amp;location=world&amp;year=2025</a></p> <p>Is it possible to fetch the list of corresponding architects for the first 100 entries in both lists ?</p> <p>I&#39;m a complete computer newbie. It would be nice if someone could help me. It&#39;s for an urban planning project.</p> </div><!-- SC_ON --> &#32; subm",
        "id": 2704821,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kousxk/can_someone_please_help_me_find_a_list_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can someone please help me find a list of architects ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Dependent_Cap5918",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T18:39:34.756347+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T13:24:00+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kosx62/footcrawl_asynchronous_webscraper_to_crawl_data/\"> <img src=\"https://external-preview.redd.it/McV7uHwtRQaNPemYWd9QiyKGabY1jZxpS6TUaBkXrZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bedc9312d5d7caec7d7a4a9882c724953c01757\" alt=\"Footcrawl - Asynchronous webscraper to crawl data from Transfermarkt\" title=\"Footcrawl - Asynchronous webscraper to crawl data from Transfermarkt\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><strong>What?</strong> </p> <p>I built an asynchronous webscraper to extract season by season data from Transfermarkt on players, clubs, fixtures, and match day stats.</p> <p><strong>Why?</strong> </p> <p>I wanted to built a <code>Python</code> package that can be easily used and extended by others, and is well tested - something many projects leave out.</p> <p>I also wanted to develop my asynchronous programming too, utilising <code>asyncio</code>, <code>aiohttp</code>, and <code>",
        "id": 2704822,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kosx62/footcrawl_asynchronous_webscraper_to_crawl_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/McV7uHwtRQaNPemYWd9QiyKGabY1jZxpS6TUaBkXrZU.jpg?width=640&crop=smart&auto=webp&s=8bedc9312d5d7caec7d7a4a9882c724953c01757",
        "title": "Footcrawl - Asynchronous webscraper to crawl data from Transfermarkt",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/West-Arm-625",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T13:10:31.821483+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T12:44:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>For context: I have basic python knowledge (Can do 5 kata problems on CodeWars) from my first year engineering degree, love python and found i have a passion for it. I want to get into webscraping/botting. Where do i start? I want to try (eventually) build a checkout bot for nike, scraping bot for ebay, stuff like that but i found out really quickly its much harder than it looks. </p> <ol> <li><p>I want to know if its even possible to do this stuff for bigger websites like eBay/Nike etc. </p></li> <li><p>What do i research? I started off with Selenium, learnt a bit but then heard playwright is better. When i asked chatGPT what i should research to get into this it gave a fairly big list of stuff. But would love to hear the communities opinion on this. </p></li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/West-Arm-625\"> /u/West-Arm-625 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1",
        "id": 2703116,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kos47q/beginner_getting_into_this_tips_and_trick_please",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Beginner getting into this - tips and trick please !!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vulcantrixter97",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-17T03:25:45.487394+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-17T02:31:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m using Google\u2019s Custom Search JSON API. My script sends a product name to Google\u2019s API, gets back official search results (titles, snippets, links), then checks if any are cheaper than my set price so I can possibly buy an resell. It then saves matches to Excel. That\u2019s it.</p> <p>From Google\u2019s own docs:</p> <p>Google literally says the API is made for developers to get and use search results in their apps.</p> <p>\u201cThe Custom Search JSON API lets you develop websites and applications to retrieve and display search results from Google Custom Search programmatically.\u201d</p> <p>Is it allowed, or is it still risky?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vulcantrixter97\"> /u/vulcantrixter97 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1koih5d/can_i_get_banned_for_scraping_google_with_their/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1koih5d",
        "id": 2700919,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1koih5d/can_i_get_banned_for_scraping_google_with_their",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I get banned for scraping Google with their API?",
        "vote": 0
    }
]