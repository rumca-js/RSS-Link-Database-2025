[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T17:42:56.657937+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T17:40:37+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kcer8s/i_built_a_google_reviews_scraper_with_advanced/\"> <img src=\"https://external-preview.redd.it/NKXGAz_iCjuFR0VWmkL8gxAu-m5DDdevYbW3R4csD94.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cbfa9c8271c88979d4af5e5f02886152c3237eb5\" alt=\"I built a Google Reviews scraper with advanced features in Python.\" title=\"I built a Google Reviews scraper with advanced features in Python.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I recently developed a tool to scrape Google Reviews, aiming to overcome the usual challenges like detection and data formatting.</p> <p>Key Features: - Supports multiple languages - Downloads associated images - Integrates with MongoDB for data storage - Implements detection bypass mechanisms - Allows incremental scraping to avoid duplicates - Includes URL replacement functionality - Exports data to JSON files for easy analysis \ufffc \ufffc \ufffc</p> <p>It\u2019s been a valuable ass",
        "id": 2576010,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kcer8s/i_built_a_google_reviews_scraper_with_advanced",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/NKXGAz_iCjuFR0VWmkL8gxAu-m5DDdevYbW3R4csD94.jpg?width=640&crop=smart&auto=webp&s=cbfa9c8271c88979d4af5e5f02886152c3237eb5",
        "title": "I built a Google Reviews scraper with advanced features in Python.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T22:03:01.234754+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T17:26:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After spending the last 5 years working with web scraping projects, I wanted to share some insights that might help others who are just getting started or facing common challenges.</p> <p><strong>The biggest challenges I&#39;ve faced:</strong></p> <p><strong>1. Website Anti-Bot Measures</strong></p> <p>These have gotten incredibly sophisticated. Simple requests with Python&#39;s requests library rarely work on modern sites anymore. I&#39;ve had to adapt by using headless browsers, rotating proxies, and mimicking human behavior patterns.</p> <p><strong>2. Maintenance Nightmare</strong></p> <p>About 10-15% of my scrapers break EVERY WEEK due to website changes. This is the hidden cost nobody talks about - the ongoing maintenance. I&#39;ve started implementing monitoring systems that alert me when data patterns change significantly.</p> <p><strong>3. Resource Consumption</strong></p> <p>Browser-based scraping (which is often necessary to handle JavaScrip",
        "id": 2578051,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What I've Learned After 5 Years in the Web Scraping Trenches",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T16:37:51.791382+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T16:14:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been doing some scraping of names and biographies from some sites. I\u2019ve been using python and beautiful soup. Basic info like name, hometown, bio, and a few other fields. It takes me between 30 minutes and an hour on most sites. </p> <p>But, I have 50 sites to do. Any advice on where to outsource? I would want the python code back.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DancingNancies1234\"> /u/DancingNancies1234 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kcco1y/outsource_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kcco1y/outsource_scraping/\">[comments]</a></span>",
        "id": 2575351,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kcco1y/outsource_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Outsource scraping?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T16:37:51.659546+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T15:51:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I&#39;m Ritik, a web scraping and automation specialist with 5+ years of experience helping businesses and individuals automate repetitive tasks, extract valuable data, and scale faster\u2014without burning money on third-party tools or APIs.</p> <p>What I Offer:</p> <p>Custom Web Scrapers: Extract leads, product info, listings, reviews, or anything else you need.</p> <p>Workflow Automation: Turn manual, time-consuming tasks into one-click operations.</p> <p>AI-Enhanced Agents: Automatically send personalized cold emails, fill forms, or process data.</p> <p>No-Code-Free: I build clean, efficient tools\u2014no Zapier or APIs that add recurring costs.</p> <p>Why Work With Me:</p> <p>Pay Only When Satisfied: No upfront payment required.</p> <p>Fast Delivery: Most tools delivered within 24\u201372 hours.</p> <p>Clear Communication: Regular updates and collaboration.</p> <p>One-Click Simplicity: Designed for non-tech users too.</p> <p>If you\u2019re spending hou",
        "id": 2575350,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kcc3uf/save_10_hours_a_week_with_one_tool_ill_build_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Save 10 hours a week with one tool. I\u2019ll build it for you",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T15:32:54.787010+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T15:07:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am a complete novice when it comes to web scraping but I am looking for a easy way to scrape the data from a jotform form and get that into excel if possible. What tools, software, or wizards would you suggest to achieve this goal?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RustySchakleford88\"> /u/RustySchakleford88 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kcb15n/scrape_data_from_a_jotform/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kcb15n/scrape_data_from_a_jotform/\">[comments]</a></span>",
        "id": 2574818,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kcb15n/scrape_data_from_a_jotform",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scrape data from a jotform",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T14:27:51.437473+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T13:34:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to retrieve full html for msn articles e.g. <a href=\"https://www.msn.com/en-us/sports/other/warren-gatland-denies-italy-clash-is-biggest-wales-game-for-20-years/ar-AA1ywRQD\">https://www.msn.com/en-us/sports/other/warren-gatland-denies-italy-clash-is-biggest-wales-game-for-20-years/ar-AA1ywRQD</a></p> <p>But I only ever seem to get partial html. I&#39;m using PuppeteerSharp with the Stealth plugin. I&#39;ve tried scrolling to activate lazy loading, javascript evaluation and played with headless mode and user agent. What am I missing? </p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tom_p_legend\"> /u/tom_p_legend </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kc8vak/msn/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kc8vak/msn/\">[comments]</a></span>",
        "id": 2574204,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kc8vak/msn",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Msn",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T12:17:43.361153+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T11:21:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>How do I scrape the same 10 data points from websites that are all completely different and unstructured?</p> <p>I\u2019m building a directory site and trying to automate populating it. I want to scrape about 10 data points from each site to add to my directory. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/edskellington\"> /u/edskellington </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kc6a6i/scraping_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kc6a6i/scraping_help/\">[comments]</a></span>",
        "id": 2573203,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kc6a6i/scraping_help",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T04:41:43.191285+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T04:37:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1kc0f10/ai_agent_for_creating_web_scrapers_247_scraping/\"> <img src=\"https://b.thumbs.redditmedia.com/TdGWCcc42gwni8vI81rCYnsPtg8jHUzAZG6l7BRB08c.jpg\" alt=\"AI Agent for Creating Web Scrapers 24/7 scraping B2B businesses\" title=\"AI Agent for Creating Web Scrapers 24/7 scraping B2B businesses\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Soon I will be adding a demo on how it works.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Alert-Ad-5918\"> /u/Alert-Ad-5918 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1kc0f10\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kc0f10/ai_agent_for_creating_web_scrapers_247_scraping/\">[comments]</a></span> </td></tr></table>",
        "id": 2571068,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kc0f10/ai_agent_for_creating_web_scrapers_247_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/TdGWCcc42gwni8vI81rCYnsPtg8jHUzAZG6l7BRB08c.jpg",
        "title": "AI Agent for Creating Web Scrapers 24/7 scraping B2B businesses",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T03:37:51.824148+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T03:00:57+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello and howdy, digital miners of r/webscraping!</p> <p>The moment you&#39;ve all been waiting for has arrived - it&#39;s our once-a-month, no-holds-barred, show-and-tell thread!</p> <ul> <li>Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you&#39;ve just unleashed on the world?</li> <li>Maybe you&#39;ve got a ground-breaking product in need of some intrepid testers?</li> <li>Got a secret discount code burning a hole in your pocket that you&#39;re just itching to share with our talented tribe of data extractors?</li> <li>Looking to make sure your post doesn&#39;t fall foul of the community rules and get ousted by the spam filter?</li> </ul> <p>Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!</p> <p>Just a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let&#39;s get ",
        "id": 2570825,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbyrdk/monthly_selfpromotion_may_2025",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Monthly Self-Promotion - May 2025",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T01:27:50.492836+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T01:25:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When connected to a VPN and in a new incognito session of my browser I go to <a href=\"https://finance.yahoo.com/quote/AAPL/key-statistics/\">https://finance.yahoo.com/quote/AAPL/key-statistics/</a> everything works fine. However, if I try to replicate the request via curl by using the curl command generated by selecting &quot;copy as curl&quot; in my browser&#39;s developer mode, I get error 404. </p> <p>How is Yahoo Finance differentiating between the browser and curl requests? The curl request seems to contain all of the same header information as the browser request would and I&#39;ve never had this problem on other sites. Any suggestions on how to get around this detection?</p> <p>When I turn off my VPN the curl request works fine, but I&#39;d like it to work through my VPN.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jnkmail11\"> /u/jnkmail11 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/c",
        "id": 2570326,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbwyf4/copy_as_curl_request_detected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Copy As Curl Request Detected",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T00:22:48.979422+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T00:20:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve found that it&#39;s possible to access some <a href=\"https://www.sports-reference.com/\">Sports-Reference</a> sites programmatically, without a browser. However, I get an HTTP 403 error when trying to access <a href=\"https://www.baseball-reference.com/\">Baseball-Reference</a> in this way.</p> <p>Here&#39;s what I mean, using Python in the interactive shell:</p> <p>&gt;&gt;&gt; import requests<br/> &gt;&gt;&gt; requests.get(&#39;<a href=\"https://www.basketball-reference.com/&#x27;\">https://www.basketball-reference.com/&#39;</a>) # OK<br/> &lt;Response \\[200\\]&gt;<br/> &gt;&gt;&gt; requests.get(&#39;<a href=\"https://www.hockey-reference.com/&#x27;\">https://www.hockey-reference.com/&#39;</a>) # OK<br/> &lt;Response \\[200\\]&gt;<br/> &gt;&gt;&gt; requests.get(&#39;<a href=\"https://www.baseball-reference.com/&#x27;\">https://www.baseball-reference.com/&#39;</a>) # Error!<br/> &lt;Response \\[403\\]&gt;</p> <p>Any thoughts on what I could/should be doin",
        "id": 2570009,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kbvo9i/sportsreference_sites_differ_in_accessibility_via",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Sports-Reference sites differ in accessibility via Python requests.",
        "vote": 0
    }
]