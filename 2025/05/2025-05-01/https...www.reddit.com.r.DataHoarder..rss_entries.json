[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T23:25:48.145429+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T22:54:42+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SnooBunnies9252\"> /u/SnooBunnies9252 </a> <br/> <span><a href=\"/r/unRAID/comments/1kcm2l7/im_sick_of_making_bad_choices/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcm3ry/im_sick_of_making_bad_choices/\">[comments]</a></span>",
        "id": 2578620,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcm3ry/im_sick_of_making_bad_choices",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I'm sick of making bad choices",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T22:20:47.689442+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T21:36:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Anyone have a recent backup of <a href=\"/r/genp\">r/genp</a> by chance?</p> <p>I was just about to update my copy of Lightroom Classic. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/G8M8N8\"> /u/G8M8N8 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kckd2s/rgenp_adobe_piracy_banned/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kckd2s/rgenp_adobe_piracy_banned/\">[comments]</a></span>",
        "id": 2578250,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kckd2s/rgenp_adobe_piracy_banned",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "r/GenP Adobe Piracy Banned",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T22:20:48.053926+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T21:36:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello all, I&#39;m planning on a purchase, getting straight to the point -</p> <ol> <li><strong>Crucial x9</strong> <em>(5.8k | 1000gbps | 3y Warranty | Reputable)</em> : <a href=\"https://www.amazon.in/gp/product/B0CGW1FQV4/ref=ox_sc_act_title_3?smid=AJ6SIZC8YQDZX&amp;th=1\">https://www.amazon.in/gp/product/B0CGW1FQV4/ref=ox_sc_act_title_3?smid=AJ6SIZC8YQDZX&amp;th=1</a></li> <li><strong>Adata SE880</strong> <em>(6.6k | 2000gbps | 5y Warranty | Terrible Reputation | Sus | Might loose data)</em> : <a href=\"https://www.amazon.in/gp/product/B09VS3FCQ2/ref=ox_sc_act_title_4?smid=AJ6SIZC8YQDZX&amp;th=1\">https://www.amazon.in/gp/product/B09VS3FCQ2/ref=ox_sc_act_title_4?smid=AJ6SIZC8YQDZX&amp;th=1</a></li> <li><strong>Seagate One Touch</strong> <em>(6.2k | 1000gbps | 3y Warranty | Very Reputable | Data recovery program)</em> : <a href=\"https://www.amazon.in/gp/product/B08XKMYCBB/ref=ox_sc_act_title_1?smid=AJ6SIZC8YQDZX&amp;psc=1\">https://www.amazon.in/gp/prod",
        "id": 2578251,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kckd1o/decision_making_external_ssd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Decision making | External SSD",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T22:20:48.184886+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T21:12:04+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;am looking for a SIMPLE booru downloader that i don&#39;t have to paste 5000 lines of code to use each time GOODDD WHY IS IT SO HARD!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Additional_Prompt841\"> /u/Additional_Prompt841 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcjseq/booru_downloader/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcjseq/booru_downloader/\">[comments]</a></span>",
        "id": 2578252,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcjseq/booru_downloader",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Booru Downloader",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T21:15:47.597392+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T21:07:33+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcjolf/is_the_media_post_number_for_twitter_accurate/\"> <img src=\"https://b.thumbs.redditmedia.com/zuJPvviPyV0TkRzp4W-utPZJZ8q-BNFgOtdVgN6IeKU.jpg\" alt=\"Is the media post number for twitter accurate?\" title=\"Is the media post number for twitter accurate?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/42w4vcbgj8ye1.png?width=278&amp;format=png&amp;auto=webp&amp;s=f4a2c814d895f8d32333149a49f1d65ac614bcdd\">https://preview.redd.it/42w4vcbgj8ye1.png?width=278&amp;format=png&amp;auto=webp&amp;s=f4a2c814d895f8d32333149a49f1d65ac614bcdd</a></p> <p>I ran across an account that says it has 12,000 photos and videos. but when i scroll through the media tab, there&#39;s no way there is 11,000 images there. what&#39;s going on here exactly? even if use twitter advanced search it doesn&#39;t show that many. I&#39;m wanting to backup this account entirely </p> </div><!-- SC_ON --> &#32; sub",
        "id": 2577737,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcjolf/is_the_media_post_number_for_twitter_accurate",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/zuJPvviPyV0TkRzp4W-utPZJZ8q-BNFgOtdVgN6IeKU.jpg",
        "title": "Is the media post number for twitter accurate?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T21:15:47.263809+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T20:54:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This may be an obvious question. I have an external hard drive that is a WD. I\u2019ve been using their encryption, but other external drive I have are VeraCrypt. Am wondering if I should reformat the WD drive and redo it as a Veracrypt volume. </p> <p>My goal is to have the best encryption. What are your suggestions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/autoliberty\"> /u/autoliberty </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcjday/is_veracrypt_better_than_wd_encryption/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcjday/is_veracrypt_better_than_wd_encryption/\">[comments]</a></span>",
        "id": 2577736,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcjday/is_veracrypt_better_than_wd_encryption",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is Veracrypt better than WD encryption!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T20:10:46.815230+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T19:47:26+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>its just one short page with a very few pics. any way to make it smaller? the html download on the same page is like 50k</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Defiant-Fuel3627\"> /u/Defiant-Fuel3627 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kchsfr/why_does_one_page_of_one_site_takes_15_gb_using/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kchsfr/why_does_one_page_of_one_site_takes_15_gb_using/\">[comments]</a></span>",
        "id": 2577379,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kchsfr/why_does_one_page_of_one_site_takes_15_gb_using",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Why does one page of one site takes 1.5 gb using httrack?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T20:10:47.222054+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T19:40:11+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kchmep/i_received_my_18tb_seagate_drive_from_seagate/\"> <img src=\"https://preview.redd.it/4hycxw6948ye1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca1dce7698b6f451bb2f5ce27ef70529b18f5854\" alt=\"I received my 18tb Seagate drive from Seagate packaged like this--send back?\" title=\"I received my 18tb Seagate drive from Seagate packaged like this--send back?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Every other time they have come in a hard mailer box with padding on the inside. This time, just the retail box inside a partially ripped mailer. Kind of unreal that a major hard drive brand like Seagate thinks this is acceptable. Return? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LithiumBizkit\"> /u/LithiumBizkit </a> <br/> <span><a href=\"https://i.redd.it/4hycxw6948ye1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1",
        "id": 2577381,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kchmep/i_received_my_18tb_seagate_drive_from_seagate",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/4hycxw6948ye1.jpeg?width=640&crop=smart&auto=webp&s=ca1dce7698b6f451bb2f5ce27ef70529b18f5854",
        "title": "I received my 18tb Seagate drive from Seagate packaged like this--send back?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T20:10:46.946103+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T19:15:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently acquired a new old stock HP LTO5 drive and its 1U rack. I decided to slot an Blu-Ray drive in the second slot next to it. I&#39;m debating how I best want to connect it, but am currently thinking to use a USB to Sata for the drive and pass a connector through the back, and power the drive over the existing PSU in the rack.</p> <p>This would mean that to use the disc drive I would have to power on the tape drive as well. The tape drive would be sitting there idle, but turned on. My question is, does this &quot;on time&quot; degrade the life span of the tape drive in any significant way?</p> <p>If so I can power the unit using an adapter that has a barrel jack and pass that through the rear (I have ideas), but would love to consolidate as much as possible for cleanliness.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IroesStrongarm\"> /u/IroesStrongarm </a> <br/> <span><a href=\"https://www.reddit.com/r/",
        "id": 2577380,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kch1g4/lto_drives_do_power_on_hours_degrade_life",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LTO Drives: Do power on hours degrade life?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T22:20:48.403088+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T19:14:51+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I dont know if this is the right place to ask this but might as well. I think i have an issue with the metadata not being added to whatever i am downloading with scdl (to download soundcloud songs) or yt-dlp. I want to keep the tracks in the right order when downloading playlists. I dowloaded my profile (-a) (every liked songs and playlists) the liked playlist kept the original track order while the liked songs did not. I tried to download other people&#39;s playlist and the track wasn&#39;t kept either. I tried to run the command adding &quot;--force-metadata&quot; but it did not solve or/ do annything :(. I am running into the same issue with yt-dlp (track order). If anyone has suggestions, thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/polkrom\"> /u/polkrom </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kch104/how_do_i_keep_the_metadata_when_downloading_with/",
        "id": 2578253,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kch104/how_do_i_keep_the_metadata_when_downloading_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do i keep the metadata when downloading with scdl or yt-dcl",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T18:00:55.042915+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T17:52:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I don&#39;t have an immediate need for HDDs but I may need drives around the end of the year. I would be looking at 2-4 Exos 20 or 22 HDDs. In late 2023 I got a 4-pack of new Exos 20TB drives for around $1000 and have never seen them that low again. Now, it seems prices may skyrocket even further after going up for quite some time.</p> <p>Any people watching the market more closely? Should I wait to buy when I need, or would it be foolish to not get some drives now if things are destined to get worse and stay worse for quite a while?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/filmguy123\"> /u/filmguy123 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcf25i/hdds_wait_or_buy_now_tariffs_etc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcf25i/hdds_wait_or_buy_now_tariffs_etc/\">[comments]</a></span>",
        "id": 2576289,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcf25i/hdds_wait_or_buy_now_tariffs_etc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HDDs: Wait or Buy now? (Tariffs, etc.)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T18:00:55.261870+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T17:39:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It might be a long shot, but in case anyone out there has Seasons 03 and 04 of the TV Show Family Business, i&#39;d be greatful to get a download link to those episodes</p> <p>In case anyone wondering, i&#39;m attaching the <a href=\"http://imdb.com\">imdb.com</a> link to that show for reference:</p> <p><a href=\"https://www.imdb.com/title/tt0354293/?ref_=fn_all_ttl_5\">https://www.imdb.com/title/tt0354293/?ref_=fn_all_ttl_5</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/H2CO3HCO3\"> /u/H2CO3HCO3 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kceq9u/family_business_s03_and_s04/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kceq9u/family_business_s03_and_s04/\">[comments]</a></span>",
        "id": 2576290,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kceq9u/family_business_s03_and_s04",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Family Business S03 and S04",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T18:00:55.392609+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T17:32:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m worried about the loss of information related to history and culture and would like to do my part to help preserve what I can. What is the best way to back up the <a href=\"/r/AskHistorians\">r/AskHistorians</a> subreddit, or has anyone created a backup of this reddit that is up to date? I am only seeing a downloader focused on photoes and videos. Are there other resources for history or culture that we should be backing up? I admire the stringent requirements of <a href=\"/r/AskHistorians\">r/AskHistorians</a> -- are there other specialist subreddits you all consider to be valuable to back up?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/squirrel-eggs\"> /u/squirrel-eggs </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcek5u/backups_of_raskhistorians_other_specialist/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcek5u/backups_of_raskhistor",
        "id": 2576291,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcek5u/backups_of_raskhistorians_other_specialist",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Backups of r/AskHistorians, other specialist subreddits, or resources related to history and culture?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T18:00:54.910569+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T16:57:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey Everyone,</p> <p>Its been about 8 years with my Synology ds1817+ and I&#39;m running out of space so its time for an upgrade...</p> <p><strong>Does anyone have any first hand experience with loading up a prosumer NAS (6-8+ Bay) with the 28tb exos recerts from serverpartdeals?</strong> I&#39;m a little hesitant because they are HAMR drives and there isn&#39;t a ton of long term testing but I&#39;m a lot more concerned with compatibility, spending $2.8k on drives to find out I can&#39;t use them would be pretty frustrating...</p> <p>I saw reports of success with the Syno 1821+ when enabling PUIS (I figure this makes sense regardless) but apparently the 571 expansions are a no-go...</p> <p>I might just break down and build something but I really like the low power consumption of the appliances...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ThatWeirdHomelessGuy\"> /u/ThatWeirdHomelessGuy </a> <br/> <span><a hr",
        "id": 2576288,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcdp93/28tb_exos_in_consumer_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "28TB Exos in consumer NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T16:55:30.617062+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T16:44:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone. This tool exists as a way to quickly and easily download all of Wikipedia (as a .bz2 archive) from the Wikimedia data dumps, but it also prompts you to automate the process by downloading an updated version and replacing the old download every week. I plan to throw this on a Linux server and thought it may come in useful for others!</p> <p>Inspiration came from the <a href=\"https://www.reddit.com/r/YouShouldKnow/comments/whxmhc/comment/ij8iym5/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">this comment</a> on Reddit, which asked about automating the process.</p> <p>Here is a link to the open-source script: <a href=\"https://github.com/ternera/auto-wikipedia-download\">https://github.com/ternera/auto-wikipedia-download</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ternera\"> /u/ternera </a> <br/> <span><a href=\"https://www.reddit.com/r/Data",
        "id": 2575684,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcdefi/made_a_little_tool_to_download_all_of_wikipedia",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Made a little tool to download all of Wikipedia on a weekly basis",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T15:48:47.559786+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T15:36:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>According to <a href=\"https://old.reddit.com/r/GamingLeaksAndRumours/comments/1kc937u/giant_bomb_as_we_know_it_now_is_proabbly_gone/\">https://old.reddit.com/r/GamingLeaksAndRumours/comments/1kc937u/giant_bomb_as_we_know_it_now_is_proabbly_gone/</a> </p> <p>GiantBomb will have an ... bad transformation soon.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sheeproomer\"> /u/sheeproomer </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbqzw/giantbomb_may_change_drastically/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbqzw/giantbomb_may_change_drastically/\">[comments]</a></span>",
        "id": 2575114,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcbqzw/giantbomb_may_change_drastically",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "GiantBomb may change drastically",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T15:48:47.719573+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T15:34:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>serverpartdeals used to be the place for deals, but now that it\u2019s so popular prices have soared upwards. Does anybody have any other idea or source for deals on Hard Drives?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/yooames\"> /u/yooames </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbp06/what_the_latest_source_for_used_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbp06/what_the_latest_source_for_used_drives/\">[comments]</a></span>",
        "id": 2575115,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcbp06/what_the_latest_source_for_used_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What the latest source for used drives ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T22:20:48.534405+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T15:25:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>It uses the Wayback Machine to analyze URLs from U.S. federal websites and track changes since Trump\u2019s inauguration. It highlights which webpages were removed and generates a word cloud of deleted terms.<br/> I&#39;d love your feedback \u2014 and if you have ideas for other websites to monitor, feel free to share!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Internal-Ad-2771\"> /u/Internal-Ad-2771 </a> <br/> <span><a href=\"https://censortrace.org\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbh7v/i_built_a_website_to_track_content_removal_from/\">[comments]</a></span>",
        "id": 2578254,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcbh7v/i_built_a_website_to_track_content_removal_from",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I built a website to track content removal from U.S. federal websites under the Trump administration",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T15:48:47.853184+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T15:18:09+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently downloaded a small archive of audio files, not music, sort of asmr-esque style. Anyway, I made the filename the same as the audio title, however I would also like to include the description of each file. </p> <p>I&#39;m aware that a lot of people use json for this, but there&#39;s also a &quot;comment&quot; section in the properties of the files (.m4a). </p> <p>Which method would be the best to use? There aren&#39;t many files anyway so it&#39;ll probably be quicker to do it by hand whichever method I use, however using json files seems to be the preferred method for things like this? </p> <p>Thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/_spaghettiv2\"> /u/_spaghettiv2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbafc/adding_data_about_the_files_in_an_archive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kcbafc/adding_dat",
        "id": 2575116,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kcbafc/adding_data_about_the_files_in_an_archive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Adding data about the files in an archive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T14:44:31.585069+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T14:04:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>i have visited this sub many times but never thought to share my hoarding.</p> <p>at one point i hoarded pc software, books and hardware. i mean i love computers but for some reason i wanted a copy of every windows version, be it on floppy, cd, dvd and beta&#39;s too. plus books on microsoft os and networking, web design etc.</p> <p>didn&#39;t sell those, instead i ripped many of them to pieces to release some stress.</p> <p>kept a few. all the ripped up books i just recycled. (none of them were rare or special) i&#39;m not a monster.</p> <p>eventually the collection got too big so i sold it all off. felt good to have space again.</p> <p>i also had a problem collecting video games but stopped that too. but it was like a fixation on one series which happened to be a favourite at the time.</p> <p>i think i had one for far cry, half life, battlefield, battlefront, rainbow six, ghost recon.</p> <p>now i have zero physical games, just digital and backups o",
        "id": 2574473,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc9jht/a_lurker_and_my_hoard",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "a lurker and my hoard",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T14:44:31.363680+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T13:44:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Looking for software to copy an old windows drive to an SSD before installing in a new pc.</p> <p>Happy to pay but don&#39;t want to sign up to a subscription, was recommended Acronis disk image but its now a subscription service.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Responsible-Pay102\"> /u/Responsible-Pay102 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc9320/hard_drive_cloning_software_recommendations/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc9320/hard_drive_cloning_software_recommendations/\">[comments]</a></span>",
        "id": 2574472,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc9320/hard_drive_cloning_software_recommendations",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hard drive Cloning Software recommendations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T11:29:00.800077+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T11:13:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc64tt/we_might_be_about_to_lose_a_powerful_force_in_the/\"> <img src=\"https://external-preview.redd.it/Clto6XZ-M9TFUnSLRX1kNgB4k6mFxKINbgI6s7kzcLo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=020f43be4ddc5bbc9564422b44b497978e95d84f\" alt=\"We Might Be About To Lose A Powerful Force In The World Of Video Game Preservation\" title=\"We Might Be About To Lose A Powerful Force In The World Of Video Game Preservation\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SeriousKano\"> /u/SeriousKano </a> <br/> <span><a href=\"https://www.timeextension.com/features/we-might-be-about-to-lose-a-powerful-force-in-the-world-of-video-game-preservation\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc64tt/we_might_be_about_to_lose_a_powerful_force_in_the/\">[comments]</a></span> </td></tr></table>",
        "id": 2572986,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc64tt/we_might_be_about_to_lose_a_powerful_force_in_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Clto6XZ-M9TFUnSLRX1kNgB4k6mFxKINbgI6s7kzcLo.jpg?width=640&crop=smart&auto=webp&s=020f43be4ddc5bbc9564422b44b497978e95d84f",
        "title": "We Might Be About To Lose A Powerful Force In The World Of Video Game Preservation",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T10:23:46.907750+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T09:39:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I currently have a 32gb usb, 250gb ssd (4,000 power on), 500gb ssd (new) and 8 hdds. I could also buy new 120gb to 1tb ssd if it is needed.</p> <p><em>I have a DIY n100 8gb 4x2.5&quot;+4x3.5&quot; main NAS that I plan to have low power consumption by running day time only and installing more ssd and few hdd. I will put OMV (ext4), dockers, 5gb docs, 3gb software, 1gb music, 1gb pictures and 10gb videos.</em></p> <p><em>I also have another DIY i7 5775c 16gb 6bay backup NAS that I plan to install Proxmox (ext4) and run as needed for OMV &amp; files backup/testing/vm/lxc.</em></p> <ol> <li><strong>(Main NAS)</strong> Is it better to install OMV to 32gb usb, 240gb or 500gb ssd? I&#39;ve heard it easy to backup and replace OMV if it installed to a usb but performance may degrade when updating or in GUI?</li> <li><strong>(Main NAS)</strong> Where do you suggest to install docs, music and dockers? In the 240 or 500gb ssd? Seldom used and big files lik",
        "id": 2572543,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc4pxz/main_nas_omv_and_backuptest_nas_proxmox_storage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Main NAS (OMV) and Backup/Test NAS (Proxmox) storage",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T10:23:46.696612+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T09:25:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just launched a basic website that lets you download videos from <strong>TikTok</strong> and <strong>Instagram</strong> easily. No ads, no sign-up, just paste the link and go.</p> <p>I\u2019m working on adding support for <strong>YouTube</strong>, <strong>X (Twitter)</strong>, and other platforms next.</p> <p>Also planning to add AI-powered <strong>video analytics and insights</strong> features soon for creators who want deeper info.</p> <p>Would love any feedback or feature suggestions!</p> <p><strong>Link:</strong> <a href=\"http://getloady.com\"><strong>getloady.com</strong></a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RhinoInsight\"> /u/RhinoInsight </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc4itz/i_built_a_simple_site_to_download_tiktok/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc4itz/i_built_a_simple_site_to_download_tiktok/\">[comme",
        "id": 2572542,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc4itz/i_built_a_simple_site_to_download_tiktok",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I built a simple site to download TikTok & Instagram videos (more platforms soon)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T08:13:28.486080+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T07:35:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I&#39;ve found the 14TB MB014000GWTFF HPE SATA drive at 160\u20ac that&#39;s a good price in Europe, but I&#39;m wondering if it works with a non HPE hardware (I&#39;ll probably install it in an HP Elitedesk 800G5). Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/andreape_x\"> /u/andreape_x </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc31rg/hpe_mb014000gwtff_on_a_non_hpe_hw/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc31rg/hpe_mb014000gwtff_on_a_non_hpe_hw/\">[comments]</a></span>",
        "id": 2571912,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc31rg/hpe_mb014000gwtff_on_a_non_hpe_hw",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "HPE MB014000GWTFF on a non HPE HW",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T06:05:02.298346+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T05:29:21+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc18kv/wisecrack_is_dead_anyone_know_how_to_download_the/\"> <img src=\"https://external-preview.redd.it/yeDs3cG7PtRIcDaqf5BBdr8G7RFzbZW8Ku2q2ENubPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2001a76f20dea38b3fd63bd3171535babda1feb3\" alt=\"Wisecrack is dead, anyone know how to download the channel?\" title=\"Wisecrack is dead, anyone know how to download the channel?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Yendis4750\"> /u/Yendis4750 </a> <br/> <span><a href=\"https://youtu.be/ByMPfm90r0I?si=dIlGpVzP1s-IntTT\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kc18kv/wisecrack_is_dead_anyone_know_how_to_download_the/\">[comments]</a></span> </td></tr></table>",
        "id": 2571419,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kc18kv/wisecrack_is_dead_anyone_know_how_to_download_the",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/yeDs3cG7PtRIcDaqf5BBdr8G7RFzbZW8Ku2q2ENubPI.jpg?width=320&crop=smart&auto=webp&s=2001a76f20dea38b3fd63bd3171535babda1feb3",
        "title": "Wisecrack is dead, anyone know how to download the channel?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T03:53:24.596112+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T03:03:25+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I downloaded Bulk Image Downloader today and it originally showed me about 1,200 posts from an Instagram account. I don&#39;t need every single image or video, but to bypass the 100 download limit, I uninstalled the trial version then installed a registered version, but it keeps showing me only 60 posts for whatever reason. I have changed the number of max pages from 20 to 2,000 (in case 200 isn&#39;t enough), and even tried 0 (unlimited), but still end up with only 60 posts (images/videos). No clue what&#39;s going on. I have already tried the obvious by deleting files, re-installing and/or resetting BID. Also, I am logged in to Instagram on both BID and Firefox.</p> <p>Any thoughts?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JonnySpears\"> /u/JonnySpears </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbyszp/bulk_image_downloader_not_working_properly/\">[link]</a></span> &#32; <sp",
        "id": 2570917,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kbyszp/bulk_image_downloader_not_working_properly",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Bulk Image Downloader Not Working Properly",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T14:44:31.980016+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T01:52:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, so just want to start off with saying I am a tech novice, and I am learning from my mistake sadly. I had a USB drive that I was using for storage for videos and pics just suddenly die on me (yes I know, I have just learned that it\u2019s a terrible idea since they can easily spontaneously die) and I lost basically everything on there (not as much as I have recently learned that people have, probably like 200 gigabytes worth? But I spent a lot of time and effort finding videos and pics I liked, downloading/screenshotting them, &amp; organizing the folders and tagging them to my preference). Actually quite sad at all my work going down the drain. My question to yall, so I don\u2019t repeat the same mistake, is how are all of you storing your NSFW content? Again, I\u2019m a tech novice, so not really looking for anything too crazy, and not really planning on buying anything super expensive since really, all I wanted was just a storage place so that I can more easil",
        "id": 2574474,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kbxh1a/storing_nsfw_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Storing NSFW Content",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T01:44:28.371699+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T01:33:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a giant DVD collection of complete RV series but to preserve them I want to rip them down into MKV episodes and wondered how to keep the quality EXACTLY like the DVD\u2019s</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Rotisseriejedi\"> /u/Rotisseriejedi </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbx3vx/whats_the_best_free_program_for_ripping_down_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbx3vx/whats_the_best_free_program_for_ripping_down_a/\">[comments]</a></span>",
        "id": 2570464,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kbx3vx/whats_the_best_free_program_for_ripping_down_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s the best free program for ripping down a HUGE TV series DVD collection into MKV but keeping 100% quality?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T01:44:28.148150+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T01:07:02+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My parents have a ton of old photo prints from my siblings and I as kids. I know the Epson FastFoto would be the best option just for speed, but I\u2019m looking to really only digitize the photos of myself and the ones of my parents. </p> <p>While there\u2019s a lot of photos, it\u2019s not so many I\u2019d be able to justify spending over $500 on that scanner. I used to work digitizing in archives so I\u2019d be able to handle the monotony of scanning one by one, so what would be a good price flatbed scanner option to do this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/topnotchbreadstick\"> /u/topnotchbreadstick </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbwl7d/what_is_a_good_flatbed_scanner_for_photos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbwl7d/what_is_a_good_flatbed_scanner_for_photos/\">[comments]</a></span>",
        "id": 2570463,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kbwl7d/what_is_a_good_flatbed_scanner_for_photos",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is a good flatbed scanner for photos?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-01T00:38:52.316820+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-01T00:09:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This group seems to be the experts on drives type of stuff. </p> <p>My question is do I get just larger drives when I expand or more drives. </p> <p>Apart me likes the more drives option because it would take all those drives to fail to be an issue. But if it was just one large drive one would fail and you would have a ton of data you have to recover. Depending where your backup is. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Novapixel1010\"> /u/Novapixel1010 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbvg2i/expanding_storage_more_drives_or_bigger_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1kbvg2i/expanding_storage_more_drives_or_bigger_drives/\">[comments]</a></span>",
        "id": 2570172,
        "language": null,
        "link": "https://www.reddit.com/r/DataHoarder/comments/1kbvg2i/expanding_storage_more_drives_or_bigger_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Expanding storage more drives or bigger drives",
        "vote": 0
    }
]