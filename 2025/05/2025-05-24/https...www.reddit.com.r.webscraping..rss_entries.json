[
    {
        "age": null,
        "album": "",
        "author": "/u/dogweather",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T22:21:10.870705+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T21:20:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I use strict type checking (mypy / pylance / pyright) in my projects. It catches lots of mistakes I make. My BeautifulSoup code though, can&#39;t be understood by the type checkers and lots of warnings are flagged. I didn&#39;t see a project like this, so I made a simple wrapper for it. Simply doing this:</p> <p><code>soup = TypedSoup(BeautifulSoup(...))</code></p> <p>...removes all the red squiggles and allows the IDE to give good method hints.</p> <p><a href=\"https://github.com/public-law/typed-soup\">https://github.com/public-law/typed-soup</a></p> <p>It supports a working subset of BeautifulSoup&#39;s large API. I added methods as I needed them. I extracted it from a larger Scrapy spider collection.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dogweather\"> /u/dogweather </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1kum5im/typedsoup_wrapper_for_beautifulsoup_to_play_well/\">[link]</",
        "id": 2764378,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kum5im/typedsoup_wrapper_for_beautifulsoup_to_play_well",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "TypedSoup: Wrapper for BeautifulSoup to play well with type checking",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/nolinearbanana",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T13:31:08.200154+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T12:07:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using rotating proxies together with a fingerprint impersonator to scrape data off Amazon.</p> <p>Was working fine until this week, with only the odd error, but suddenly I&#39;m getting a much higher proportion of errors. Initially a warning &quot;Please enable cookies so we can see you&#39;re not a bot&quot; etc, then 502 errors which I presume are when the server decides I am a bot and just blocks.</p> <p>Contemplating changing my headers, but not sure how matched these are to my fingerprint impersonator.</p> <p>My headers are currently all set by the impersonator which defaults to Mac<br/> e,g, </p> <pre><code>&quot;Sec-Ch-Ua-Platform&quot;: [ &quot;\\&quot;macOS\\&quot;&quot; ], &quot;User-Agent&quot;: [ &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36&quot; ], </code></pre> <p>Can I change these to &quot;Windows&quot; and &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x6",
        "id": 2761697,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1kua33e/502_response_from_amazon",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "502 response from Amazon",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PM_AEROFOIL_PICS",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T12:02:28.939234+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T11:54:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I was thinking of making a simple webscraper to automatically gather house prices from popular real estate websites, but they all specify no webscraping in their website T&#39;s&amp;C&#39;s. I just want to check, is it illegal in the UK to scrape these websites if I am not publishing/selling the data and not making excessive requests, but do break their terms of use? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PM_AEROFOIL_PICS\"> /u/PM_AEROFOIL_PICS </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku9uw4/website_tscs_and_legality_of_webscraping_uk/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku9uw4/website_tscs_and_legality_of_webscraping_uk/\">[comments]</a></span>",
        "id": 2761253,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku9uw4/website_tscs_and_legality_of_webscraping_uk",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Website T's&C's and legality of webscraping UK",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/worldtest2k",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T10:57:38.157725+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T09:53:52+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been using open-meteo for months for current weather data without any issues, but today I am getting error response 429 - too many requests. The free tier allows 600 requests per minute and I only do 2 every 5 minutes. My app is hosted on pythonanywhere and uses flet - is it possible someone else on this host is abusing open-meteo which has lead to every flet request from from pythonanywhere being blocked?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/worldtest2k\"> /u/worldtest2k </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku7z8l/openmeteo_api_giving_error/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku7z8l/openmeteo_api_giving_error/\">[comments]</a></span>",
        "id": 2760963,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku7z8l/openmeteo_api_giving_error",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "open-meteo API giving error",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JarJarThinks-909",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T13:31:08.380682+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T09:37:05+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1ku7qj7/help_building_an_amazon_sales_estimator_scraper/\"> <img src=\"https://b.thumbs.redditmedia.com/D1fU4ZXnbsNTkk_dRCrDr1-lrdZkbM-MM9rXE9EkkiA.jpg\" alt=\"Help Building an Amazon Sales Estimator Scraper Patchright &amp; Proxies\" title=\"Help Building an Amazon Sales Estimator Scraper Patchright &amp; Proxies\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I&#39;d like to first point out I&#39;m very new to this and I&#39;m doing this for fun and to learn more about web scraping.</p> <p>I\u2019m working on a learning project to scrape sales estimates from Helium 10 free estimator using Python. I\u2019ve run into some challenges and would love your input.</p> <h1>Project Overview</h1> <ul> <li><p>Goal: Automate scraping of sales estimates from Helium10\u2019s Free estimator.</p></li> <li><p>Inputs: Marketplace, Category, and BSR (Best Sellers Rank).</p></li> <li><p>Tech Stack: Python, Patchright (for stealth), and rotating pr",
        "id": 2761698,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku7qj7/help_building_an_amazon_sales_estimator_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/D1fU4ZXnbsNTkk_dRCrDr1-lrdZkbM-MM9rXE9EkkiA.jpg",
        "title": "Help Building an Amazon Sales Estimator Scraper Patchright & Proxies",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/obviously-not-a-bot",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T08:47:30.152737+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T08:16:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to scrape data from a website.</p> <p>The goal is to get some data with-in milli seconds, why you might ask because the said data is getting updated through websockets and javascript. If it takes any longer to return the data its useless.</p> <p>I cannot reverse engineer apis as the incoming data in encrypted and for obvious reasons decryption key is not available on frontend.</p> <p>What I have tried (I am using document object mostly to scrape the data off of website and also for simulating the user interactions):</p> <pre><code>1. I have made a express server with puppeteer-stealth in headless mode 2. Before server starts accepting the requestes it will start a browser instance and login to the website so that the session is shared and I dont have to login for every subsequent request. 3. I have 3 apis, which another application/server will be using that does following 3.1. ```/``` ```GET Method```: fetches the all fully qualified urls ",
        "id": 2760413,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku6ltl/puppeteer_scraper_for_websocket_data_facing",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Puppeteer Scraper for WebSocket Data \u2013 Facing Timeouts & Issues",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/mateoslife",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T05:32:25.185697+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T05:30:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>As the title says, I\u2019m trying to find all the links to a photobucket prefix The prefix is something like for example: <a href=\"http://img.photobucket.com/albums/v000/username/\">http://img.photobucket.com/albums/v000/username/</a> and then a file like example.jpg Please let me know how if you know\u2026 (and if it\u2019s even possible) I tried all those crawl sites but just shows me the description to photobucket.com\ud83d\ude2d\ud83d\ude2d</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mateoslife\"> /u/mateoslife </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku46rm/how_do_i_find_all_the_active_links_to_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku46rm/how_do_i_find_all_the_active_links_to_a/\">[comments]</a></span>",
        "id": 2759749,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku46rm/how_do_i_find_all_the_active_links_to_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How do I find all the active links to a photobucket prefix?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sys_admin",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T04:27:43.236468+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T04:17:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m new to scraping and trying to get details from a website into Google Sheets. In the future this could be Python+db, but for now I&#39;ll be happy with just populating a spreadsheet.</p> <p>I&#39;m using Chrome to inspect the website. In the Sources and Application tabs I can find the data I&#39;m looking for in what looks to me like a dynamic JSON block. See code block below.</p> <p>Is scraping this into Google Sheets feasible? Or should I go straight to Python? Maybe Playwright/Selenium? I&#39;m a mediocre (at best) programmer, but more C/C++ and not web/html or python. Just looking to get pointed in the right direction. Any good recommendations or articles/guides pertinent to what I&#39;m trying to do would be very helpful. Thanks</p> <p><code>&lt;body&gt;</code><br/> <code>&lt;noscript&gt;</code><br/> <code>&lt;!-- Google Tag Manager (noscript) --&gt;</code><br/> <code>&lt;iframe src=&quot;ns &quot; height=&quot;0&quot; width=&quot;0&quot; ",
        "id": 2759545,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku30lb/noob_scraping_can_i_import_this_into_google_sheets",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "noob scraping - Can I import this into Google Sheets?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Slight_Surround2458",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T03:22:29.849653+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T03:19:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am interested in scraping a <a href=\"https://fortnitetracker.com/events/epicgames_S34_FNCSMajor2_Final_NAC?window=S34_FNCSMajor2_Final_Day1_NAC&amp;sm=S34_FNCSMajor2_Final_CumulativeLeaderboardDef\">Fortnite Tracker</a> leaderboard.</p> <p>I have a working Selenium script but it always gets caught by Cloudflare on headless. Running without headless is quite annoying, and I have to ensure the pop-up window is always in fullscreen.</p> <p>I&#39;ve heard there are ways to scrape dynamic sites without using Selenium? Would that be possible here? Just from looking and poking around the linked page, if I am interested in the leaderboard data, does anyone have any recommendations?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Slight_Surround2458\"> /u/Slight_Surround2458 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku20w8/possible_to_scrape_dynamic_site_cloudflare/\">[link]</a></span> &#32; ",
        "id": 2759358,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku20w8/possible_to_scrape_dynamic_site_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Possible to Scrape Dynamic Site (Cloudflare) Without Selenium?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/too_much_lag",
        "bookmarked": false,
        "comments": [],
        "date_created": "2025-05-24T02:17:31.959664+00:00",
        "date_dead_since": null,
        "date_published": "2025-05-24T01:52:11+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Lately, I\u2019ve been experimenting with web scraping and web development in general. One thing that\u2019s caught my interest is web cloning. I\u2019ve successfully cloned some basic static websites, but I ran into trouble when trying to clone a site built with Next.js.</p> <p>Is there a reliable way to clone a Next.js website, at least to replicate the UI and layout? Any tools, techniques, or advice would be appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/too_much_lag\"> /u/too_much_lag </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku0g4y/how_to_clone_any_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ku0g4y/how_to_clone_any_website/\">[comments]</a></span>",
        "id": 2759193,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1ku0g4y/how_to_clone_any_website",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to clone any website?",
        "vote": 0
    }
]